[2024-09-08 00:19:55,419][flwr][INFO] - Starting Flower simulation, config: num_rounds=100, no round_timeout
[2024-09-08 00:19:58,722][flwr][INFO] - Flower VCE: Ray initialized with resources: {'memory': 169111655834.0, 'node:__internal_head__': 1.0, 'CPU': 256.0, 'node:115.145.172.224': 1.0, 'object_store_memory': 76762138214.0, 'accelerator_type:G': 1.0, 'GPU': 5.0}
[2024-09-08 00:19:58,722][flwr][INFO] - Optimize your simulation with Flower VCE: https://flower.ai/docs/framework/how-to-run-simulations.html
[2024-09-08 00:19:58,722][flwr][INFO] - Flower VCE: Resources for each Virtual Client: {'num_cpus': 16, 'num_gpus': 0.25}
[2024-09-08 00:19:58,741][flwr][INFO] - Flower VCE: Creating VirtualClientEngineActorPool with 16 actors
[2024-09-08 00:19:58,741][flwr][INFO] - [INIT]
[2024-09-08 00:19:58,741][flwr][INFO] - Requesting initial parameters from one random client
[2024-09-08 00:20:03,880][flwr][INFO] - Received initial parameters from one random client
[2024-09-08 00:20:03,881][flwr][INFO] - Evaluating initial global parameters
[2024-09-08 00:21:02,369][flwr][INFO] - initial parameters (loss, other metrics): 3201.11914229393, {'accuracy': 0.04733502538071066}
[2024-09-08 00:21:02,370][flwr][INFO] - 
[2024-09-08 00:21:02,370][flwr][INFO] - [ROUND 1]
[2024-09-08 00:21:02,370][flwr][INFO] - configure_fit: strategy sampled 5 clients (out of 50)
[2024-09-08 00:21:06,554][flwr][ERROR] - Traceback (most recent call last):
  File "/home/abdenour/.local/lib/python3.10/site-packages/flwr/simulation/ray_transport/ray_client_proxy.py", line 94, in _submit_job
    out_mssg, updated_context = self.actor_pool.get_client_result(
  File "/home/abdenour/.local/lib/python3.10/site-packages/flwr/simulation/ray_transport/ray_actor.py", line 398, in get_client_result
    return self._fetch_future_result(cid)
  File "/home/abdenour/.local/lib/python3.10/site-packages/flwr/simulation/ray_transport/ray_actor.py", line 279, in _fetch_future_result
    res_cid, out_mssg, updated_context = ray.get(
  File "/home/abdenour/.local/lib/python3.10/site-packages/ray/_private/auto_init_hook.py", line 21, in auto_init_wrapper
    return fn(*args, **kwargs)
  File "/home/abdenour/.local/lib/python3.10/site-packages/ray/_private/client_mode_hook.py", line 103, in wrapper
    return func(*args, **kwargs)
  File "/home/abdenour/.local/lib/python3.10/site-packages/ray/_private/worker.py", line 2667, in get
    values, debugger_breakpoint = worker.get_objects(object_refs, timeout=timeout)
  File "/home/abdenour/.local/lib/python3.10/site-packages/ray/_private/worker.py", line 864, in get_objects
    raise value.as_instanceof_cause()
ray.exceptions.RayTaskError(ClientAppException): [36mray::ClientAppActor.run()[39m (pid=339895, ip=115.145.172.224, actor_id=ac762956181fa358f66f22d001000000, repr=<flwr.simulation.ray_transport.ray_actor.ClientAppActor object at 0x7f7a8020d720>)
  File "/home/abdenour/.local/lib/python3.10/site-packages/flwr/client/client_app.py", line 143, in __call__
    return self._call(message, context)
  File "/home/abdenour/.local/lib/python3.10/site-packages/flwr/client/client_app.py", line 126, in ffn
    out_message = handle_legacy_message_from_msgtype(
  File "/home/abdenour/.local/lib/python3.10/site-packages/flwr/client/message_handler/message_handler.py", line 129, in handle_legacy_message_from_msgtype
    fit_res = maybe_call_fit(
  File "/home/abdenour/.local/lib/python3.10/site-packages/flwr/client/client.py", line 234, in maybe_call_fit
    return client.fit(fit_ins)
  File "/home/abdenour/.local/lib/python3.10/site-packages/flwr/client/numpy_client.py", line 238, in _fit
    results = self.numpy_client.fit(parameters, ins.config)  # type: ignore
  File "/home/abdenour/fl_with_flower/fedavg_mobilnet/fedavg_mobilnet/client.py", line 46, in fit
    train(self.model, trainloader, optimizer, epochs=epochs, device=self.device)
  File "/home/abdenour/fl_with_flower/fedavg_mobilnet/fedavg_mobilnet/utils.py", line 40, in train
    loss = criterion(net(images), labels)
  File "/home/abdenour/anaconda3/envs/fl/lib/python3.10/site-packages/torch/nn/modules/module.py", line 1553, in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
  File "/home/abdenour/anaconda3/envs/fl/lib/python3.10/site-packages/torch/nn/modules/module.py", line 1562, in _call_impl
    return forward_call(*args, **kwargs)
  File "/home/abdenour/anaconda3/envs/fl/lib/python3.10/site-packages/torchvision/models/mobilenetv2.py", line 174, in forward
    return self._forward_impl(x)
  File "/home/abdenour/anaconda3/envs/fl/lib/python3.10/site-packages/torchvision/models/mobilenetv2.py", line 166, in _forward_impl
    x = self.features(x)
  File "/home/abdenour/anaconda3/envs/fl/lib/python3.10/site-packages/torch/nn/modules/module.py", line 1553, in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
  File "/home/abdenour/anaconda3/envs/fl/lib/python3.10/site-packages/torch/nn/modules/module.py", line 1562, in _call_impl
    return forward_call(*args, **kwargs)
  File "/home/abdenour/anaconda3/envs/fl/lib/python3.10/site-packages/torch/nn/modules/container.py", line 219, in forward
    input = module(input)
  File "/home/abdenour/anaconda3/envs/fl/lib/python3.10/site-packages/torch/nn/modules/module.py", line 1553, in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
  File "/home/abdenour/anaconda3/envs/fl/lib/python3.10/site-packages/torch/nn/modules/module.py", line 1562, in _call_impl
    return forward_call(*args, **kwargs)
  File "/home/abdenour/anaconda3/envs/fl/lib/python3.10/site-packages/torchvision/models/mobilenetv2.py", line 64, in forward
    return self.conv(x)
  File "/home/abdenour/anaconda3/envs/fl/lib/python3.10/site-packages/torch/nn/modules/module.py", line 1553, in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
  File "/home/abdenour/anaconda3/envs/fl/lib/python3.10/site-packages/torch/nn/modules/module.py", line 1562, in _call_impl
    return forward_call(*args, **kwargs)
  File "/home/abdenour/anaconda3/envs/fl/lib/python3.10/site-packages/torch/nn/modules/container.py", line 219, in forward
    input = module(input)
  File "/home/abdenour/anaconda3/envs/fl/lib/python3.10/site-packages/torch/nn/modules/module.py", line 1553, in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
  File "/home/abdenour/anaconda3/envs/fl/lib/python3.10/site-packages/torch/nn/modules/module.py", line 1562, in _call_impl
    return forward_call(*args, **kwargs)
  File "/home/abdenour/anaconda3/envs/fl/lib/python3.10/site-packages/torch/nn/modules/container.py", line 219, in forward
    input = module(input)
  File "/home/abdenour/anaconda3/envs/fl/lib/python3.10/site-packages/torch/nn/modules/module.py", line 1553, in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
  File "/home/abdenour/anaconda3/envs/fl/lib/python3.10/site-packages/torch/nn/modules/module.py", line 1562, in _call_impl
    return forward_call(*args, **kwargs)
  File "/home/abdenour/anaconda3/envs/fl/lib/python3.10/site-packages/torch/nn/modules/activation.py", line 243, in forward
    return F.hardtanh(input, self.min_val, self.max_val, self.inplace)
  File "/home/abdenour/anaconda3/envs/fl/lib/python3.10/site-packages/torch/nn/functional.py", line 1551, in hardtanh
    result = torch._C._nn.hardtanh_(input, min_val, max_val)
torch.OutOfMemoryError: CUDA out of memory. Tried to allocate 28.00 MiB. GPU 0 has a total capacity of 10.75 GiB of which 28.75 MiB is free. Process 339896 has 5.56 GiB memory in use. Including non-PyTorch memory, this process has 4.66 GiB memory in use. Process 339894 has 322.00 MiB memory in use. Process 339893 has 182.00 MiB memory in use. Of the allocated memory 4.38 GiB is allocated by PyTorch, and 103.97 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)

The above exception was the direct cause of the following exception:

[36mray::ClientAppActor.run()[39m (pid=339895, ip=115.145.172.224, actor_id=ac762956181fa358f66f22d001000000, repr=<flwr.simulation.ray_transport.ray_actor.ClientAppActor object at 0x7f7a8020d720>)
  File "/home/abdenour/.local/lib/python3.10/site-packages/flwr/simulation/ray_transport/ray_actor.py", line 63, in run
    raise ClientAppException(str(ex)) from ex
flwr.client.client_app.ClientAppException: 
Exception ClientAppException occurred. Message: CUDA out of memory. Tried to allocate 28.00 MiB. GPU 0 has a total capacity of 10.75 GiB of which 28.75 MiB is free. Process 339896 has 5.56 GiB memory in use. Including non-PyTorch memory, this process has 4.66 GiB memory in use. Process 339894 has 322.00 MiB memory in use. Process 339893 has 182.00 MiB memory in use. Of the allocated memory 4.38 GiB is allocated by PyTorch, and 103.97 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)

[2024-09-08 00:21:06,554][flwr][ERROR] - [36mray::ClientAppActor.run()[39m (pid=339895, ip=115.145.172.224, actor_id=ac762956181fa358f66f22d001000000, repr=<flwr.simulation.ray_transport.ray_actor.ClientAppActor object at 0x7f7a8020d720>)
  File "/home/abdenour/.local/lib/python3.10/site-packages/flwr/client/client_app.py", line 143, in __call__
    return self._call(message, context)
  File "/home/abdenour/.local/lib/python3.10/site-packages/flwr/client/client_app.py", line 126, in ffn
    out_message = handle_legacy_message_from_msgtype(
  File "/home/abdenour/.local/lib/python3.10/site-packages/flwr/client/message_handler/message_handler.py", line 129, in handle_legacy_message_from_msgtype
    fit_res = maybe_call_fit(
  File "/home/abdenour/.local/lib/python3.10/site-packages/flwr/client/client.py", line 234, in maybe_call_fit
    return client.fit(fit_ins)
  File "/home/abdenour/.local/lib/python3.10/site-packages/flwr/client/numpy_client.py", line 238, in _fit
    results = self.numpy_client.fit(parameters, ins.config)  # type: ignore
  File "/home/abdenour/fl_with_flower/fedavg_mobilnet/fedavg_mobilnet/client.py", line 46, in fit
    train(self.model, trainloader, optimizer, epochs=epochs, device=self.device)
  File "/home/abdenour/fl_with_flower/fedavg_mobilnet/fedavg_mobilnet/utils.py", line 40, in train
    loss = criterion(net(images), labels)
  File "/home/abdenour/anaconda3/envs/fl/lib/python3.10/site-packages/torch/nn/modules/module.py", line 1553, in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
  File "/home/abdenour/anaconda3/envs/fl/lib/python3.10/site-packages/torch/nn/modules/module.py", line 1562, in _call_impl
    return forward_call(*args, **kwargs)
  File "/home/abdenour/anaconda3/envs/fl/lib/python3.10/site-packages/torchvision/models/mobilenetv2.py", line 174, in forward
    return self._forward_impl(x)
  File "/home/abdenour/anaconda3/envs/fl/lib/python3.10/site-packages/torchvision/models/mobilenetv2.py", line 166, in _forward_impl
    x = self.features(x)
  File "/home/abdenour/anaconda3/envs/fl/lib/python3.10/site-packages/torch/nn/modules/module.py", line 1553, in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
  File "/home/abdenour/anaconda3/envs/fl/lib/python3.10/site-packages/torch/nn/modules/module.py", line 1562, in _call_impl
    return forward_call(*args, **kwargs)
  File "/home/abdenour/anaconda3/envs/fl/lib/python3.10/site-packages/torch/nn/modules/container.py", line 219, in forward
    input = module(input)
  File "/home/abdenour/anaconda3/envs/fl/lib/python3.10/site-packages/torch/nn/modules/module.py", line 1553, in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
  File "/home/abdenour/anaconda3/envs/fl/lib/python3.10/site-packages/torch/nn/modules/module.py", line 1562, in _call_impl
    return forward_call(*args, **kwargs)
  File "/home/abdenour/anaconda3/envs/fl/lib/python3.10/site-packages/torchvision/models/mobilenetv2.py", line 64, in forward
    return self.conv(x)
  File "/home/abdenour/anaconda3/envs/fl/lib/python3.10/site-packages/torch/nn/modules/module.py", line 1553, in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
  File "/home/abdenour/anaconda3/envs/fl/lib/python3.10/site-packages/torch/nn/modules/module.py", line 1562, in _call_impl
    return forward_call(*args, **kwargs)
  File "/home/abdenour/anaconda3/envs/fl/lib/python3.10/site-packages/torch/nn/modules/container.py", line 219, in forward
    input = module(input)
  File "/home/abdenour/anaconda3/envs/fl/lib/python3.10/site-packages/torch/nn/modules/module.py", line 1553, in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
  File "/home/abdenour/anaconda3/envs/fl/lib/python3.10/site-packages/torch/nn/modules/module.py", line 1562, in _call_impl
    return forward_call(*args, **kwargs)
  File "/home/abdenour/anaconda3/envs/fl/lib/python3.10/site-packages/torch/nn/modules/container.py", line 219, in forward
    input = module(input)
  File "/home/abdenour/anaconda3/envs/fl/lib/python3.10/site-packages/torch/nn/modules/module.py", line 1553, in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
  File "/home/abdenour/anaconda3/envs/fl/lib/python3.10/site-packages/torch/nn/modules/module.py", line 1562, in _call_impl
    return forward_call(*args, **kwargs)
  File "/home/abdenour/anaconda3/envs/fl/lib/python3.10/site-packages/torch/nn/modules/activation.py", line 243, in forward
    return F.hardtanh(input, self.min_val, self.max_val, self.inplace)
  File "/home/abdenour/anaconda3/envs/fl/lib/python3.10/site-packages/torch/nn/functional.py", line 1551, in hardtanh
    result = torch._C._nn.hardtanh_(input, min_val, max_val)
torch.OutOfMemoryError: CUDA out of memory. Tried to allocate 28.00 MiB. GPU 0 has a total capacity of 10.75 GiB of which 28.75 MiB is free. Process 339896 has 5.56 GiB memory in use. Including non-PyTorch memory, this process has 4.66 GiB memory in use. Process 339894 has 322.00 MiB memory in use. Process 339893 has 182.00 MiB memory in use. Of the allocated memory 4.38 GiB is allocated by PyTorch, and 103.97 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)

The above exception was the direct cause of the following exception:

[36mray::ClientAppActor.run()[39m (pid=339895, ip=115.145.172.224, actor_id=ac762956181fa358f66f22d001000000, repr=<flwr.simulation.ray_transport.ray_actor.ClientAppActor object at 0x7f7a8020d720>)
  File "/home/abdenour/.local/lib/python3.10/site-packages/flwr/simulation/ray_transport/ray_actor.py", line 63, in run
    raise ClientAppException(str(ex)) from ex
flwr.client.client_app.ClientAppException: 
Exception ClientAppException occurred. Message: CUDA out of memory. Tried to allocate 28.00 MiB. GPU 0 has a total capacity of 10.75 GiB of which 28.75 MiB is free. Process 339896 has 5.56 GiB memory in use. Including non-PyTorch memory, this process has 4.66 GiB memory in use. Process 339894 has 322.00 MiB memory in use. Process 339893 has 182.00 MiB memory in use. Of the allocated memory 4.38 GiB is allocated by PyTorch, and 103.97 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)
[2024-09-08 00:21:06,595][flwr][ERROR] - Traceback (most recent call last):
  File "/home/abdenour/.local/lib/python3.10/site-packages/flwr/simulation/ray_transport/ray_client_proxy.py", line 94, in _submit_job
    out_mssg, updated_context = self.actor_pool.get_client_result(
  File "/home/abdenour/.local/lib/python3.10/site-packages/flwr/simulation/ray_transport/ray_actor.py", line 398, in get_client_result
    return self._fetch_future_result(cid)
  File "/home/abdenour/.local/lib/python3.10/site-packages/flwr/simulation/ray_transport/ray_actor.py", line 279, in _fetch_future_result
    res_cid, out_mssg, updated_context = ray.get(
  File "/home/abdenour/.local/lib/python3.10/site-packages/ray/_private/auto_init_hook.py", line 21, in auto_init_wrapper
    return fn(*args, **kwargs)
  File "/home/abdenour/.local/lib/python3.10/site-packages/ray/_private/client_mode_hook.py", line 103, in wrapper
    return func(*args, **kwargs)
  File "/home/abdenour/.local/lib/python3.10/site-packages/ray/_private/worker.py", line 2667, in get
    values, debugger_breakpoint = worker.get_objects(object_refs, timeout=timeout)
  File "/home/abdenour/.local/lib/python3.10/site-packages/ray/_private/worker.py", line 864, in get_objects
    raise value.as_instanceof_cause()
ray.exceptions.RayTaskError(ClientAppException): [36mray::ClientAppActor.run()[39m (pid=339894, ip=115.145.172.224, actor_id=764e710dd94a075b3ece0e3a01000000, repr=<flwr.simulation.ray_transport.ray_actor.ClientAppActor object at 0x7fcd2c771690>)
  File "/home/abdenour/.local/lib/python3.10/site-packages/flwr/client/client_app.py", line 143, in __call__
    return self._call(message, context)
  File "/home/abdenour/.local/lib/python3.10/site-packages/flwr/client/client_app.py", line 126, in ffn
    out_message = handle_legacy_message_from_msgtype(
  File "/home/abdenour/.local/lib/python3.10/site-packages/flwr/client/message_handler/message_handler.py", line 129, in handle_legacy_message_from_msgtype
    fit_res = maybe_call_fit(
  File "/home/abdenour/.local/lib/python3.10/site-packages/flwr/client/client.py", line 234, in maybe_call_fit
    return client.fit(fit_ins)
  File "/home/abdenour/.local/lib/python3.10/site-packages/flwr/client/numpy_client.py", line 238, in _fit
    results = self.numpy_client.fit(parameters, ins.config)  # type: ignore
  File "/home/abdenour/fl_with_flower/fedavg_mobilnet/fedavg_mobilnet/client.py", line 46, in fit
    train(self.model, trainloader, optimizer, epochs=epochs, device=self.device)
  File "/home/abdenour/fl_with_flower/fedavg_mobilnet/fedavg_mobilnet/utils.py", line 40, in train
    loss = criterion(net(images), labels)
  File "/home/abdenour/anaconda3/envs/fl/lib/python3.10/site-packages/torch/nn/modules/module.py", line 1553, in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
  File "/home/abdenour/anaconda3/envs/fl/lib/python3.10/site-packages/torch/nn/modules/module.py", line 1562, in _call_impl
    return forward_call(*args, **kwargs)
  File "/home/abdenour/anaconda3/envs/fl/lib/python3.10/site-packages/torchvision/models/mobilenetv2.py", line 174, in forward
    return self._forward_impl(x)
  File "/home/abdenour/anaconda3/envs/fl/lib/python3.10/site-packages/torchvision/models/mobilenetv2.py", line 166, in _forward_impl
    x = self.features(x)
  File "/home/abdenour/anaconda3/envs/fl/lib/python3.10/site-packages/torch/nn/modules/module.py", line 1553, in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
  File "/home/abdenour/anaconda3/envs/fl/lib/python3.10/site-packages/torch/nn/modules/module.py", line 1562, in _call_impl
    return forward_call(*args, **kwargs)
  File "/home/abdenour/anaconda3/envs/fl/lib/python3.10/site-packages/torch/nn/modules/container.py", line 219, in forward
    input = module(input)
  File "/home/abdenour/anaconda3/envs/fl/lib/python3.10/site-packages/torch/nn/modules/module.py", line 1553, in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
  File "/home/abdenour/anaconda3/envs/fl/lib/python3.10/site-packages/torch/nn/modules/module.py", line 1562, in _call_impl
    return forward_call(*args, **kwargs)
  File "/home/abdenour/anaconda3/envs/fl/lib/python3.10/site-packages/torch/nn/modules/container.py", line 219, in forward
    input = module(input)
  File "/home/abdenour/anaconda3/envs/fl/lib/python3.10/site-packages/torch/nn/modules/module.py", line 1553, in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
  File "/home/abdenour/anaconda3/envs/fl/lib/python3.10/site-packages/torch/nn/modules/module.py", line 1562, in _call_impl
    return forward_call(*args, **kwargs)
  File "/home/abdenour/anaconda3/envs/fl/lib/python3.10/site-packages/torch/nn/modules/batchnorm.py", line 176, in forward
    return F.batch_norm(
  File "/home/abdenour/anaconda3/envs/fl/lib/python3.10/site-packages/torch/nn/functional.py", line 2512, in batch_norm
    return torch.batch_norm(
torch.OutOfMemoryError: CUDA out of memory. Tried to allocate 98.00 MiB. GPU 0 has a total capacity of 10.75 GiB of which 4.75 MiB is free. Process 339896 has 5.56 GiB memory in use. Process 339895 has 4.66 GiB memory in use. Including non-PyTorch memory, this process has 346.00 MiB memory in use. Process 339893 has 182.00 MiB memory in use. Of the allocated memory 143.48 MiB is allocated by PyTorch, and 20.52 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)

The above exception was the direct cause of the following exception:

[36mray::ClientAppActor.run()[39m (pid=339894, ip=115.145.172.224, actor_id=764e710dd94a075b3ece0e3a01000000, repr=<flwr.simulation.ray_transport.ray_actor.ClientAppActor object at 0x7fcd2c771690>)
  File "/home/abdenour/.local/lib/python3.10/site-packages/flwr/simulation/ray_transport/ray_actor.py", line 63, in run
    raise ClientAppException(str(ex)) from ex
flwr.client.client_app.ClientAppException: 
Exception ClientAppException occurred. Message: CUDA out of memory. Tried to allocate 98.00 MiB. GPU 0 has a total capacity of 10.75 GiB of which 4.75 MiB is free. Process 339896 has 5.56 GiB memory in use. Process 339895 has 4.66 GiB memory in use. Including non-PyTorch memory, this process has 346.00 MiB memory in use. Process 339893 has 182.00 MiB memory in use. Of the allocated memory 143.48 MiB is allocated by PyTorch, and 20.52 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)

[2024-09-08 00:21:06,596][flwr][ERROR] - [36mray::ClientAppActor.run()[39m (pid=339894, ip=115.145.172.224, actor_id=764e710dd94a075b3ece0e3a01000000, repr=<flwr.simulation.ray_transport.ray_actor.ClientAppActor object at 0x7fcd2c771690>)
  File "/home/abdenour/.local/lib/python3.10/site-packages/flwr/client/client_app.py", line 143, in __call__
    return self._call(message, context)
  File "/home/abdenour/.local/lib/python3.10/site-packages/flwr/client/client_app.py", line 126, in ffn
    out_message = handle_legacy_message_from_msgtype(
  File "/home/abdenour/.local/lib/python3.10/site-packages/flwr/client/message_handler/message_handler.py", line 129, in handle_legacy_message_from_msgtype
    fit_res = maybe_call_fit(
  File "/home/abdenour/.local/lib/python3.10/site-packages/flwr/client/client.py", line 234, in maybe_call_fit
    return client.fit(fit_ins)
  File "/home/abdenour/.local/lib/python3.10/site-packages/flwr/client/numpy_client.py", line 238, in _fit
    results = self.numpy_client.fit(parameters, ins.config)  # type: ignore
  File "/home/abdenour/fl_with_flower/fedavg_mobilnet/fedavg_mobilnet/client.py", line 46, in fit
    train(self.model, trainloader, optimizer, epochs=epochs, device=self.device)
  File "/home/abdenour/fl_with_flower/fedavg_mobilnet/fedavg_mobilnet/utils.py", line 40, in train
    loss = criterion(net(images), labels)
  File "/home/abdenour/anaconda3/envs/fl/lib/python3.10/site-packages/torch/nn/modules/module.py", line 1553, in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
  File "/home/abdenour/anaconda3/envs/fl/lib/python3.10/site-packages/torch/nn/modules/module.py", line 1562, in _call_impl
    return forward_call(*args, **kwargs)
  File "/home/abdenour/anaconda3/envs/fl/lib/python3.10/site-packages/torchvision/models/mobilenetv2.py", line 174, in forward
    return self._forward_impl(x)
  File "/home/abdenour/anaconda3/envs/fl/lib/python3.10/site-packages/torchvision/models/mobilenetv2.py", line 166, in _forward_impl
    x = self.features(x)
  File "/home/abdenour/anaconda3/envs/fl/lib/python3.10/site-packages/torch/nn/modules/module.py", line 1553, in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
  File "/home/abdenour/anaconda3/envs/fl/lib/python3.10/site-packages/torch/nn/modules/module.py", line 1562, in _call_impl
    return forward_call(*args, **kwargs)
  File "/home/abdenour/anaconda3/envs/fl/lib/python3.10/site-packages/torch/nn/modules/container.py", line 219, in forward
    input = module(input)
  File "/home/abdenour/anaconda3/envs/fl/lib/python3.10/site-packages/torch/nn/modules/module.py", line 1553, in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
  File "/home/abdenour/anaconda3/envs/fl/lib/python3.10/site-packages/torch/nn/modules/module.py", line 1562, in _call_impl
    return forward_call(*args, **kwargs)
  File "/home/abdenour/anaconda3/envs/fl/lib/python3.10/site-packages/torch/nn/modules/container.py", line 219, in forward
    input = module(input)
  File "/home/abdenour/anaconda3/envs/fl/lib/python3.10/site-packages/torch/nn/modules/module.py", line 1553, in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
  File "/home/abdenour/anaconda3/envs/fl/lib/python3.10/site-packages/torch/nn/modules/module.py", line 1562, in _call_impl
    return forward_call(*args, **kwargs)
  File "/home/abdenour/anaconda3/envs/fl/lib/python3.10/site-packages/torch/nn/modules/batchnorm.py", line 176, in forward
    return F.batch_norm(
  File "/home/abdenour/anaconda3/envs/fl/lib/python3.10/site-packages/torch/nn/functional.py", line 2512, in batch_norm
    return torch.batch_norm(
torch.OutOfMemoryError: CUDA out of memory. Tried to allocate 98.00 MiB. GPU 0 has a total capacity of 10.75 GiB of which 4.75 MiB is free. Process 339896 has 5.56 GiB memory in use. Process 339895 has 4.66 GiB memory in use. Including non-PyTorch memory, this process has 346.00 MiB memory in use. Process 339893 has 182.00 MiB memory in use. Of the allocated memory 143.48 MiB is allocated by PyTorch, and 20.52 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)

The above exception was the direct cause of the following exception:

[36mray::ClientAppActor.run()[39m (pid=339894, ip=115.145.172.224, actor_id=764e710dd94a075b3ece0e3a01000000, repr=<flwr.simulation.ray_transport.ray_actor.ClientAppActor object at 0x7fcd2c771690>)
  File "/home/abdenour/.local/lib/python3.10/site-packages/flwr/simulation/ray_transport/ray_actor.py", line 63, in run
    raise ClientAppException(str(ex)) from ex
flwr.client.client_app.ClientAppException: 
Exception ClientAppException occurred. Message: CUDA out of memory. Tried to allocate 98.00 MiB. GPU 0 has a total capacity of 10.75 GiB of which 4.75 MiB is free. Process 339896 has 5.56 GiB memory in use. Process 339895 has 4.66 GiB memory in use. Including non-PyTorch memory, this process has 346.00 MiB memory in use. Process 339893 has 182.00 MiB memory in use. Of the allocated memory 143.48 MiB is allocated by PyTorch, and 20.52 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)
[2024-09-08 00:21:06,628][flwr][ERROR] - Traceback (most recent call last):
  File "/home/abdenour/.local/lib/python3.10/site-packages/flwr/simulation/ray_transport/ray_client_proxy.py", line 94, in _submit_job
    out_mssg, updated_context = self.actor_pool.get_client_result(
  File "/home/abdenour/.local/lib/python3.10/site-packages/flwr/simulation/ray_transport/ray_actor.py", line 398, in get_client_result
    return self._fetch_future_result(cid)
  File "/home/abdenour/.local/lib/python3.10/site-packages/flwr/simulation/ray_transport/ray_actor.py", line 279, in _fetch_future_result
    res_cid, out_mssg, updated_context = ray.get(
  File "/home/abdenour/.local/lib/python3.10/site-packages/ray/_private/auto_init_hook.py", line 21, in auto_init_wrapper
    return fn(*args, **kwargs)
  File "/home/abdenour/.local/lib/python3.10/site-packages/ray/_private/client_mode_hook.py", line 103, in wrapper
    return func(*args, **kwargs)
  File "/home/abdenour/.local/lib/python3.10/site-packages/ray/_private/worker.py", line 2667, in get
    values, debugger_breakpoint = worker.get_objects(object_refs, timeout=timeout)
  File "/home/abdenour/.local/lib/python3.10/site-packages/ray/_private/worker.py", line 864, in get_objects
    raise value.as_instanceof_cause()
ray.exceptions.RayTaskError(ClientAppException): [36mray::ClientAppActor.run()[39m (pid=339893, ip=115.145.172.224, actor_id=0c84811c91a9d451c803122301000000, repr=<flwr.simulation.ray_transport.ray_actor.ClientAppActor object at 0x7f6548585660>)
  File "/home/abdenour/.local/lib/python3.10/site-packages/flwr/client/client_app.py", line 143, in __call__
    return self._call(message, context)
  File "/home/abdenour/.local/lib/python3.10/site-packages/flwr/client/client_app.py", line 126, in ffn
    out_message = handle_legacy_message_from_msgtype(
  File "/home/abdenour/.local/lib/python3.10/site-packages/flwr/client/message_handler/message_handler.py", line 129, in handle_legacy_message_from_msgtype
    fit_res = maybe_call_fit(
  File "/home/abdenour/.local/lib/python3.10/site-packages/flwr/client/client.py", line 234, in maybe_call_fit
    return client.fit(fit_ins)
  File "/home/abdenour/.local/lib/python3.10/site-packages/flwr/client/numpy_client.py", line 238, in _fit
    results = self.numpy_client.fit(parameters, ins.config)  # type: ignore
  File "/home/abdenour/fl_with_flower/fedavg_mobilnet/fedavg_mobilnet/client.py", line 46, in fit
    train(self.model, trainloader, optimizer, epochs=epochs, device=self.device)
  File "/home/abdenour/fl_with_flower/fedavg_mobilnet/fedavg_mobilnet/utils.py", line 38, in train
    images, labels = batch["image"].to(device), batch["label"].to(device)
torch.OutOfMemoryError: CUDA out of memory. Tried to allocate 38.00 MiB. GPU 0 has a total capacity of 10.75 GiB of which 4.75 MiB is free. Process 339896 has 5.56 GiB memory in use. Process 339895 has 4.66 GiB memory in use. Process 339894 has 346.00 MiB memory in use. Including non-PyTorch memory, this process has 182.00 MiB memory in use. Of the allocated memory 8.73 MiB is allocated by PyTorch, and 19.27 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)

The above exception was the direct cause of the following exception:

[36mray::ClientAppActor.run()[39m (pid=339893, ip=115.145.172.224, actor_id=0c84811c91a9d451c803122301000000, repr=<flwr.simulation.ray_transport.ray_actor.ClientAppActor object at 0x7f6548585660>)
  File "/home/abdenour/.local/lib/python3.10/site-packages/flwr/simulation/ray_transport/ray_actor.py", line 63, in run
    raise ClientAppException(str(ex)) from ex
flwr.client.client_app.ClientAppException: 
Exception ClientAppException occurred. Message: CUDA out of memory. Tried to allocate 38.00 MiB. GPU 0 has a total capacity of 10.75 GiB of which 4.75 MiB is free. Process 339896 has 5.56 GiB memory in use. Process 339895 has 4.66 GiB memory in use. Process 339894 has 346.00 MiB memory in use. Including non-PyTorch memory, this process has 182.00 MiB memory in use. Of the allocated memory 8.73 MiB is allocated by PyTorch, and 19.27 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)

[2024-09-08 00:21:06,631][flwr][ERROR] - [36mray::ClientAppActor.run()[39m (pid=339893, ip=115.145.172.224, actor_id=0c84811c91a9d451c803122301000000, repr=<flwr.simulation.ray_transport.ray_actor.ClientAppActor object at 0x7f6548585660>)
  File "/home/abdenour/.local/lib/python3.10/site-packages/flwr/client/client_app.py", line 143, in __call__
    return self._call(message, context)
  File "/home/abdenour/.local/lib/python3.10/site-packages/flwr/client/client_app.py", line 126, in ffn
    out_message = handle_legacy_message_from_msgtype(
  File "/home/abdenour/.local/lib/python3.10/site-packages/flwr/client/message_handler/message_handler.py", line 129, in handle_legacy_message_from_msgtype
    fit_res = maybe_call_fit(
  File "/home/abdenour/.local/lib/python3.10/site-packages/flwr/client/client.py", line 234, in maybe_call_fit
    return client.fit(fit_ins)
  File "/home/abdenour/.local/lib/python3.10/site-packages/flwr/client/numpy_client.py", line 238, in _fit
    results = self.numpy_client.fit(parameters, ins.config)  # type: ignore
  File "/home/abdenour/fl_with_flower/fedavg_mobilnet/fedavg_mobilnet/client.py", line 46, in fit
    train(self.model, trainloader, optimizer, epochs=epochs, device=self.device)
  File "/home/abdenour/fl_with_flower/fedavg_mobilnet/fedavg_mobilnet/utils.py", line 38, in train
    images, labels = batch["image"].to(device), batch["label"].to(device)
torch.OutOfMemoryError: CUDA out of memory. Tried to allocate 38.00 MiB. GPU 0 has a total capacity of 10.75 GiB of which 4.75 MiB is free. Process 339896 has 5.56 GiB memory in use. Process 339895 has 4.66 GiB memory in use. Process 339894 has 346.00 MiB memory in use. Including non-PyTorch memory, this process has 182.00 MiB memory in use. Of the allocated memory 8.73 MiB is allocated by PyTorch, and 19.27 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)

The above exception was the direct cause of the following exception:

[36mray::ClientAppActor.run()[39m (pid=339893, ip=115.145.172.224, actor_id=0c84811c91a9d451c803122301000000, repr=<flwr.simulation.ray_transport.ray_actor.ClientAppActor object at 0x7f6548585660>)
  File "/home/abdenour/.local/lib/python3.10/site-packages/flwr/simulation/ray_transport/ray_actor.py", line 63, in run
    raise ClientAppException(str(ex)) from ex
flwr.client.client_app.ClientAppException: 
Exception ClientAppException occurred. Message: CUDA out of memory. Tried to allocate 38.00 MiB. GPU 0 has a total capacity of 10.75 GiB of which 4.75 MiB is free. Process 339896 has 5.56 GiB memory in use. Process 339895 has 4.66 GiB memory in use. Process 339894 has 346.00 MiB memory in use. Including non-PyTorch memory, this process has 182.00 MiB memory in use. Of the allocated memory 8.73 MiB is allocated by PyTorch, and 19.27 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)
[2024-09-08 00:21:17,273][flwr][INFO] - aggregate_fit: received 2 results and 3 failures
[2024-09-08 00:21:17,360][flwr][WARNING] - No fit_metrics_aggregation_fn provided

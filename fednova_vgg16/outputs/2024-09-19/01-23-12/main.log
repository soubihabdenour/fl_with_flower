[2024-09-19 01:23:15,471][flwr][INFO] - Starting Flower simulation, config: num_rounds=100, no round_timeout
[2024-09-19 01:23:18,960][flwr][INFO] - Flower VCE: Ray initialized with resources: {'memory': 147889409844.0, 'node:__internal_head__': 1.0, 'node:115.145.172.224': 1.0, 'object_store_memory': 67666889932.0, 'CPU': 256.0, 'accelerator_type:G': 1.0, 'GPU': 1.0}
[2024-09-19 01:23:18,961][flwr][INFO] - Optimize your simulation with Flower VCE: https://flower.ai/docs/framework/how-to-run-simulations.html
[2024-09-19 01:23:18,961][flwr][INFO] - Flower VCE: Resources for each Virtual Client: {'num_cpus': 64, 'num_gpus': 0.1}
[2024-09-19 01:23:18,973][flwr][INFO] - Flower VCE: Creating VirtualClientEngineActorPool with 4 actors
[2024-09-19 01:23:18,973][flwr][INFO] - [INIT]
[2024-09-19 01:23:18,973][flwr][INFO] - Requesting initial parameters from one random client
[2024-09-19 01:23:26,077][flwr][INFO] - Received initial parameters from one random client
[2024-09-19 01:23:26,077][flwr][INFO] - Evaluating initial global parameters
[2024-09-19 01:23:38,590][flwr][INFO] - initial parameters (loss, other metrics): 217.56962072849274, {'accuracy': 0.19906460099386145}
[2024-09-19 01:23:38,591][flwr][INFO] - 
[2024-09-19 01:23:38,591][flwr][INFO] - [ROUND 1]
[2024-09-19 01:23:38,592][flwr][INFO] - configure_fit: strategy sampled 5 clients (out of 50)
[2024-09-19 01:23:46,464][flwr][ERROR] - Traceback (most recent call last):
  File "/home/abdenour/.local/lib/python3.10/site-packages/flwr/simulation/ray_transport/ray_client_proxy.py", line 94, in _submit_job
    out_mssg, updated_context = self.actor_pool.get_client_result(
  File "/home/abdenour/.local/lib/python3.10/site-packages/flwr/simulation/ray_transport/ray_actor.py", line 398, in get_client_result
    return self._fetch_future_result(cid)
  File "/home/abdenour/.local/lib/python3.10/site-packages/flwr/simulation/ray_transport/ray_actor.py", line 279, in _fetch_future_result
    res_cid, out_mssg, updated_context = ray.get(
  File "/home/abdenour/.local/lib/python3.10/site-packages/ray/_private/auto_init_hook.py", line 21, in auto_init_wrapper
    return fn(*args, **kwargs)
  File "/home/abdenour/.local/lib/python3.10/site-packages/ray/_private/client_mode_hook.py", line 103, in wrapper
    return func(*args, **kwargs)
  File "/home/abdenour/.local/lib/python3.10/site-packages/ray/_private/worker.py", line 2667, in get
    values, debugger_breakpoint = worker.get_objects(object_refs, timeout=timeout)
  File "/home/abdenour/.local/lib/python3.10/site-packages/ray/_private/worker.py", line 864, in get_objects
    raise value.as_instanceof_cause()
ray.exceptions.RayTaskError(ClientAppException): [36mray::ClientAppActor.run()[39m (pid=2330871, ip=115.145.172.224, actor_id=cdc8cff1bec6044f7ef6408701000000, repr=<flwr.simulation.ray_transport.ray_actor.ClientAppActor object at 0x7f75b8199750>)
  File "/home/abdenour/.local/lib/python3.10/site-packages/flwr/client/client_app.py", line 143, in __call__
    return self._call(message, context)
  File "/home/abdenour/.local/lib/python3.10/site-packages/flwr/client/client_app.py", line 126, in ffn
    out_message = handle_legacy_message_from_msgtype(
  File "/home/abdenour/.local/lib/python3.10/site-packages/flwr/client/message_handler/message_handler.py", line 129, in handle_legacy_message_from_msgtype
    fit_res = maybe_call_fit(
  File "/home/abdenour/.local/lib/python3.10/site-packages/flwr/client/client.py", line 234, in maybe_call_fit
    return client.fit(fit_ins)
  File "/home/abdenour/.local/lib/python3.10/site-packages/flwr/client/numpy_client.py", line 238, in _fit
    results = self.numpy_client.fit(parameters, ins.config)  # type: ignore
  File "/home/abdenour/fl_with_flower/fednova_vgg16/fednova_vgg16/client.py", line 69, in fit
    train(self.model, trainloader, self.optimizer, epochs=num_epochs, device=self.device)
  File "/home/abdenour/fl_with_flower/fednova_vgg16/fednova_vgg16/utils.py", line 57, in train
    loss = criterion(net(images), labels)
  File "/home/abdenour/anaconda3/envs/fl/lib/python3.10/site-packages/torch/nn/modules/module.py", line 1553, in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
  File "/home/abdenour/anaconda3/envs/fl/lib/python3.10/site-packages/torch/nn/modules/module.py", line 1562, in _call_impl
    return forward_call(*args, **kwargs)
  File "/home/abdenour/anaconda3/envs/fl/lib/python3.10/site-packages/torchvision/models/vgg.py", line 66, in forward
    x = self.features(x)
  File "/home/abdenour/anaconda3/envs/fl/lib/python3.10/site-packages/torch/nn/modules/module.py", line 1553, in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
  File "/home/abdenour/anaconda3/envs/fl/lib/python3.10/site-packages/torch/nn/modules/module.py", line 1562, in _call_impl
    return forward_call(*args, **kwargs)
  File "/home/abdenour/anaconda3/envs/fl/lib/python3.10/site-packages/torch/nn/modules/container.py", line 219, in forward
    input = module(input)
  File "/home/abdenour/anaconda3/envs/fl/lib/python3.10/site-packages/torch/nn/modules/module.py", line 1553, in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
  File "/home/abdenour/anaconda3/envs/fl/lib/python3.10/site-packages/torch/nn/modules/module.py", line 1562, in _call_impl
    return forward_call(*args, **kwargs)
  File "/home/abdenour/anaconda3/envs/fl/lib/python3.10/site-packages/torch/nn/modules/conv.py", line 458, in forward
    return self._conv_forward(input, self.weight, self.bias)
  File "/home/abdenour/anaconda3/envs/fl/lib/python3.10/site-packages/torch/nn/modules/conv.py", line 454, in _conv_forward
    return F.conv2d(input, weight, bias, self.stride,
torch.OutOfMemoryError: CUDA out of memory. Tried to allocate 784.00 MiB. GPU 0 has a total capacity of 47.33 GiB of which 552.12 MiB is free. Process 2279413 has 11.11 GiB memory in use. Process 2265701 has 2.37 GiB memory in use. Process 2279412 has 11.11 GiB memory in use. Process 2330873 has 11.11 GiB memory in use. Process 2317160 has 2.37 GiB memory in use. Process 2330872 has 7.87 GiB memory in use. Including non-PyTorch memory, this process has 816.00 MiB memory in use. Of the allocated memory 549.92 MiB is allocated by PyTorch, and 6.08 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)

The above exception was the direct cause of the following exception:

[36mray::ClientAppActor.run()[39m (pid=2330871, ip=115.145.172.224, actor_id=cdc8cff1bec6044f7ef6408701000000, repr=<flwr.simulation.ray_transport.ray_actor.ClientAppActor object at 0x7f75b8199750>)
  File "/home/abdenour/.local/lib/python3.10/site-packages/flwr/simulation/ray_transport/ray_actor.py", line 63, in run
    raise ClientAppException(str(ex)) from ex
flwr.client.client_app.ClientAppException: 
Exception ClientAppException occurred. Message: CUDA out of memory. Tried to allocate 784.00 MiB. GPU 0 has a total capacity of 47.33 GiB of which 552.12 MiB is free. Process 2279413 has 11.11 GiB memory in use. Process 2265701 has 2.37 GiB memory in use. Process 2279412 has 11.11 GiB memory in use. Process 2330873 has 11.11 GiB memory in use. Process 2317160 has 2.37 GiB memory in use. Process 2330872 has 7.87 GiB memory in use. Including non-PyTorch memory, this process has 816.00 MiB memory in use. Of the allocated memory 549.92 MiB is allocated by PyTorch, and 6.08 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)

[2024-09-19 01:23:46,465][flwr][ERROR] - [36mray::ClientAppActor.run()[39m (pid=2330871, ip=115.145.172.224, actor_id=cdc8cff1bec6044f7ef6408701000000, repr=<flwr.simulation.ray_transport.ray_actor.ClientAppActor object at 0x7f75b8199750>)
  File "/home/abdenour/.local/lib/python3.10/site-packages/flwr/client/client_app.py", line 143, in __call__
    return self._call(message, context)
  File "/home/abdenour/.local/lib/python3.10/site-packages/flwr/client/client_app.py", line 126, in ffn
    out_message = handle_legacy_message_from_msgtype(
  File "/home/abdenour/.local/lib/python3.10/site-packages/flwr/client/message_handler/message_handler.py", line 129, in handle_legacy_message_from_msgtype
    fit_res = maybe_call_fit(
  File "/home/abdenour/.local/lib/python3.10/site-packages/flwr/client/client.py", line 234, in maybe_call_fit
    return client.fit(fit_ins)
  File "/home/abdenour/.local/lib/python3.10/site-packages/flwr/client/numpy_client.py", line 238, in _fit
    results = self.numpy_client.fit(parameters, ins.config)  # type: ignore
  File "/home/abdenour/fl_with_flower/fednova_vgg16/fednova_vgg16/client.py", line 69, in fit
    train(self.model, trainloader, self.optimizer, epochs=num_epochs, device=self.device)
  File "/home/abdenour/fl_with_flower/fednova_vgg16/fednova_vgg16/utils.py", line 57, in train
    loss = criterion(net(images), labels)
  File "/home/abdenour/anaconda3/envs/fl/lib/python3.10/site-packages/torch/nn/modules/module.py", line 1553, in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
  File "/home/abdenour/anaconda3/envs/fl/lib/python3.10/site-packages/torch/nn/modules/module.py", line 1562, in _call_impl
    return forward_call(*args, **kwargs)
  File "/home/abdenour/anaconda3/envs/fl/lib/python3.10/site-packages/torchvision/models/vgg.py", line 66, in forward
    x = self.features(x)
  File "/home/abdenour/anaconda3/envs/fl/lib/python3.10/site-packages/torch/nn/modules/module.py", line 1553, in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
  File "/home/abdenour/anaconda3/envs/fl/lib/python3.10/site-packages/torch/nn/modules/module.py", line 1562, in _call_impl
    return forward_call(*args, **kwargs)
  File "/home/abdenour/anaconda3/envs/fl/lib/python3.10/site-packages/torch/nn/modules/container.py", line 219, in forward
    input = module(input)
  File "/home/abdenour/anaconda3/envs/fl/lib/python3.10/site-packages/torch/nn/modules/module.py", line 1553, in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
  File "/home/abdenour/anaconda3/envs/fl/lib/python3.10/site-packages/torch/nn/modules/module.py", line 1562, in _call_impl
    return forward_call(*args, **kwargs)
  File "/home/abdenour/anaconda3/envs/fl/lib/python3.10/site-packages/torch/nn/modules/conv.py", line 458, in forward
    return self._conv_forward(input, self.weight, self.bias)
  File "/home/abdenour/anaconda3/envs/fl/lib/python3.10/site-packages/torch/nn/modules/conv.py", line 454, in _conv_forward
    return F.conv2d(input, weight, bias, self.stride,
torch.OutOfMemoryError: CUDA out of memory. Tried to allocate 784.00 MiB. GPU 0 has a total capacity of 47.33 GiB of which 552.12 MiB is free. Process 2279413 has 11.11 GiB memory in use. Process 2265701 has 2.37 GiB memory in use. Process 2279412 has 11.11 GiB memory in use. Process 2330873 has 11.11 GiB memory in use. Process 2317160 has 2.37 GiB memory in use. Process 2330872 has 7.87 GiB memory in use. Including non-PyTorch memory, this process has 816.00 MiB memory in use. Of the allocated memory 549.92 MiB is allocated by PyTorch, and 6.08 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)

The above exception was the direct cause of the following exception:

[36mray::ClientAppActor.run()[39m (pid=2330871, ip=115.145.172.224, actor_id=cdc8cff1bec6044f7ef6408701000000, repr=<flwr.simulation.ray_transport.ray_actor.ClientAppActor object at 0x7f75b8199750>)
  File "/home/abdenour/.local/lib/python3.10/site-packages/flwr/simulation/ray_transport/ray_actor.py", line 63, in run
    raise ClientAppException(str(ex)) from ex
flwr.client.client_app.ClientAppException: 
Exception ClientAppException occurred. Message: CUDA out of memory. Tried to allocate 784.00 MiB. GPU 0 has a total capacity of 47.33 GiB of which 552.12 MiB is free. Process 2279413 has 11.11 GiB memory in use. Process 2265701 has 2.37 GiB memory in use. Process 2279412 has 11.11 GiB memory in use. Process 2330873 has 11.11 GiB memory in use. Process 2317160 has 2.37 GiB memory in use. Process 2330872 has 7.87 GiB memory in use. Including non-PyTorch memory, this process has 816.00 MiB memory in use. Of the allocated memory 549.92 MiB is allocated by PyTorch, and 6.08 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)
[2024-09-19 01:23:47,212][flwr][ERROR] - Traceback (most recent call last):
  File "/home/abdenour/.local/lib/python3.10/site-packages/flwr/simulation/ray_transport/ray_client_proxy.py", line 94, in _submit_job
    out_mssg, updated_context = self.actor_pool.get_client_result(
  File "/home/abdenour/.local/lib/python3.10/site-packages/flwr/simulation/ray_transport/ray_actor.py", line 398, in get_client_result
    return self._fetch_future_result(cid)
  File "/home/abdenour/.local/lib/python3.10/site-packages/flwr/simulation/ray_transport/ray_actor.py", line 279, in _fetch_future_result
    res_cid, out_mssg, updated_context = ray.get(
  File "/home/abdenour/.local/lib/python3.10/site-packages/ray/_private/auto_init_hook.py", line 21, in auto_init_wrapper
    return fn(*args, **kwargs)
  File "/home/abdenour/.local/lib/python3.10/site-packages/ray/_private/client_mode_hook.py", line 103, in wrapper
    return func(*args, **kwargs)
  File "/home/abdenour/.local/lib/python3.10/site-packages/ray/_private/worker.py", line 2667, in get
    values, debugger_breakpoint = worker.get_objects(object_refs, timeout=timeout)
  File "/home/abdenour/.local/lib/python3.10/site-packages/ray/_private/worker.py", line 864, in get_objects
    raise value.as_instanceof_cause()
ray.exceptions.RayTaskError(ClientAppException): [36mray::ClientAppActor.run()[39m (pid=2330870, ip=115.145.172.224, actor_id=18884b649bbd3d098a6ec02b01000000, repr=<flwr.simulation.ray_transport.ray_actor.ClientAppActor object at 0x7ffb805757b0>)
  File "/home/abdenour/.local/lib/python3.10/site-packages/flwr/client/client_app.py", line 143, in __call__
    return self._call(message, context)
  File "/home/abdenour/.local/lib/python3.10/site-packages/flwr/client/client_app.py", line 126, in ffn
    out_message = handle_legacy_message_from_msgtype(
  File "/home/abdenour/.local/lib/python3.10/site-packages/flwr/client/message_handler/message_handler.py", line 95, in handle_legacy_message_from_msgtype
    client = client_fn(context)
  File "/home/abdenour/fl_with_flower/fednova_vgg16/fednova_vgg16/client.py", line 129, in client_fn
    return FlowerClient(trainset, valset, num_classes, config_fit, client_dataset_ratio, context.node_config).to_client()
  File "/home/abdenour/fl_with_flower/fednova_vgg16/fednova_vgg16/client.py", line 36, in __init__
    self.model = self.model.to(self.device)
  File "/home/abdenour/anaconda3/envs/fl/lib/python3.10/site-packages/torch/nn/modules/module.py", line 1174, in to
    return self._apply(convert)
  File "/home/abdenour/anaconda3/envs/fl/lib/python3.10/site-packages/torch/nn/modules/module.py", line 780, in _apply
    module._apply(fn)
  File "/home/abdenour/anaconda3/envs/fl/lib/python3.10/site-packages/torch/nn/modules/module.py", line 780, in _apply
    module._apply(fn)
  File "/home/abdenour/anaconda3/envs/fl/lib/python3.10/site-packages/torch/nn/modules/module.py", line 805, in _apply
    param_applied = fn(param)
  File "/home/abdenour/anaconda3/envs/fl/lib/python3.10/site-packages/torch/nn/modules/module.py", line 1160, in convert
    return t.to(
RuntimeError: CUDA error: out of memory
CUDA kernel errors might be asynchronously reported at some other API call, so the stacktrace below might be incorrect.
For debugging consider passing CUDA_LAUNCH_BLOCKING=1
Compile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.


The above exception was the direct cause of the following exception:

[36mray::ClientAppActor.run()[39m (pid=2330870, ip=115.145.172.224, actor_id=18884b649bbd3d098a6ec02b01000000, repr=<flwr.simulation.ray_transport.ray_actor.ClientAppActor object at 0x7ffb805757b0>)
  File "/home/abdenour/.local/lib/python3.10/site-packages/flwr/simulation/ray_transport/ray_actor.py", line 63, in run
    raise ClientAppException(str(ex)) from ex
flwr.client.client_app.ClientAppException: 
Exception ClientAppException occurred. Message: CUDA error: out of memory
CUDA kernel errors might be asynchronously reported at some other API call, so the stacktrace below might be incorrect.
For debugging consider passing CUDA_LAUNCH_BLOCKING=1
Compile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.

[2024-09-19 01:23:47,213][flwr][ERROR] - [36mray::ClientAppActor.run()[39m (pid=2330870, ip=115.145.172.224, actor_id=18884b649bbd3d098a6ec02b01000000, repr=<flwr.simulation.ray_transport.ray_actor.ClientAppActor object at 0x7ffb805757b0>)
  File "/home/abdenour/.local/lib/python3.10/site-packages/flwr/client/client_app.py", line 143, in __call__
    return self._call(message, context)
  File "/home/abdenour/.local/lib/python3.10/site-packages/flwr/client/client_app.py", line 126, in ffn
    out_message = handle_legacy_message_from_msgtype(
  File "/home/abdenour/.local/lib/python3.10/site-packages/flwr/client/message_handler/message_handler.py", line 95, in handle_legacy_message_from_msgtype
    client = client_fn(context)
  File "/home/abdenour/fl_with_flower/fednova_vgg16/fednova_vgg16/client.py", line 129, in client_fn
    return FlowerClient(trainset, valset, num_classes, config_fit, client_dataset_ratio, context.node_config).to_client()
  File "/home/abdenour/fl_with_flower/fednova_vgg16/fednova_vgg16/client.py", line 36, in __init__
    self.model = self.model.to(self.device)
  File "/home/abdenour/anaconda3/envs/fl/lib/python3.10/site-packages/torch/nn/modules/module.py", line 1174, in to
    return self._apply(convert)
  File "/home/abdenour/anaconda3/envs/fl/lib/python3.10/site-packages/torch/nn/modules/module.py", line 780, in _apply
    module._apply(fn)
  File "/home/abdenour/anaconda3/envs/fl/lib/python3.10/site-packages/torch/nn/modules/module.py", line 780, in _apply
    module._apply(fn)
  File "/home/abdenour/anaconda3/envs/fl/lib/python3.10/site-packages/torch/nn/modules/module.py", line 805, in _apply
    param_applied = fn(param)
  File "/home/abdenour/anaconda3/envs/fl/lib/python3.10/site-packages/torch/nn/modules/module.py", line 1160, in convert
    return t.to(
RuntimeError: CUDA error: out of memory
CUDA kernel errors might be asynchronously reported at some other API call, so the stacktrace below might be incorrect.
For debugging consider passing CUDA_LAUNCH_BLOCKING=1
Compile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.


The above exception was the direct cause of the following exception:

[36mray::ClientAppActor.run()[39m (pid=2330870, ip=115.145.172.224, actor_id=18884b649bbd3d098a6ec02b01000000, repr=<flwr.simulation.ray_transport.ray_actor.ClientAppActor object at 0x7ffb805757b0>)
  File "/home/abdenour/.local/lib/python3.10/site-packages/flwr/simulation/ray_transport/ray_actor.py", line 63, in run
    raise ClientAppException(str(ex)) from ex
flwr.client.client_app.ClientAppException: 
Exception ClientAppException occurred. Message: CUDA error: out of memory
CUDA kernel errors might be asynchronously reported at some other API call, so the stacktrace below might be incorrect.
For debugging consider passing CUDA_LAUNCH_BLOCKING=1
Compile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.
[2024-09-19 01:23:52,742][flwr][INFO] - aggregate_fit: received 3 results and 2 failures
[2024-09-19 01:23:53,563][flwr][WARNING] - No fit_metrics_aggregation_fn provided
[2024-09-19 01:24:05,205][flwr][INFO] - fit progress: (1, 246.6170336008072, {'accuracy': 0.19467991815258695}, 26.613380854018033)
[2024-09-19 01:24:05,205][flwr][INFO] - configure_evaluate: strategy sampled 2 clients (out of 50)
[2024-09-19 01:24:09,119][flwr][INFO] - aggregate_evaluate: received 2 results and 0 failures
[2024-09-19 01:24:09,120][flwr][INFO] - 
[2024-09-19 01:24:09,120][flwr][INFO] - [ROUND 2]
[2024-09-19 01:24:09,120][flwr][INFO] - configure_fit: strategy sampled 5 clients (out of 50)
[2024-09-19 01:24:14,325][flwr][ERROR] - Traceback (most recent call last):
  File "/home/abdenour/.local/lib/python3.10/site-packages/flwr/simulation/ray_transport/ray_client_proxy.py", line 94, in _submit_job
    out_mssg, updated_context = self.actor_pool.get_client_result(
  File "/home/abdenour/.local/lib/python3.10/site-packages/flwr/simulation/ray_transport/ray_actor.py", line 398, in get_client_result
    return self._fetch_future_result(cid)
  File "/home/abdenour/.local/lib/python3.10/site-packages/flwr/simulation/ray_transport/ray_actor.py", line 279, in _fetch_future_result
    res_cid, out_mssg, updated_context = ray.get(
  File "/home/abdenour/.local/lib/python3.10/site-packages/ray/_private/auto_init_hook.py", line 21, in auto_init_wrapper
    return fn(*args, **kwargs)
  File "/home/abdenour/.local/lib/python3.10/site-packages/ray/_private/client_mode_hook.py", line 103, in wrapper
    return func(*args, **kwargs)
  File "/home/abdenour/.local/lib/python3.10/site-packages/ray/_private/worker.py", line 2667, in get
    values, debugger_breakpoint = worker.get_objects(object_refs, timeout=timeout)
  File "/home/abdenour/.local/lib/python3.10/site-packages/ray/_private/worker.py", line 864, in get_objects
    raise value.as_instanceof_cause()
ray.exceptions.RayTaskError(ClientAppException): [36mray::ClientAppActor.run()[39m (pid=2330870, ip=115.145.172.224, actor_id=18884b649bbd3d098a6ec02b01000000, repr=<flwr.simulation.ray_transport.ray_actor.ClientAppActor object at 0x7ffb805757b0>)
  File "/home/abdenour/.local/lib/python3.10/site-packages/flwr/client/client_app.py", line 143, in __call__
    return self._call(message, context)
  File "/home/abdenour/.local/lib/python3.10/site-packages/flwr/client/client_app.py", line 126, in ffn
    out_message = handle_legacy_message_from_msgtype(
  File "/home/abdenour/.local/lib/python3.10/site-packages/flwr/client/message_handler/message_handler.py", line 95, in handle_legacy_message_from_msgtype
    client = client_fn(context)
  File "/home/abdenour/fl_with_flower/fednova_vgg16/fednova_vgg16/client.py", line 129, in client_fn
    return FlowerClient(trainset, valset, num_classes, config_fit, client_dataset_ratio, context.node_config).to_client()
  File "/home/abdenour/fl_with_flower/fednova_vgg16/fednova_vgg16/client.py", line 36, in __init__
    self.model = self.model.to(self.device)
  File "/home/abdenour/anaconda3/envs/fl/lib/python3.10/site-packages/torch/nn/modules/module.py", line 1174, in to
    return self._apply(convert)
  File "/home/abdenour/anaconda3/envs/fl/lib/python3.10/site-packages/torch/nn/modules/module.py", line 780, in _apply
    module._apply(fn)
  File "/home/abdenour/anaconda3/envs/fl/lib/python3.10/site-packages/torch/nn/modules/module.py", line 780, in _apply
    module._apply(fn)
  File "/home/abdenour/anaconda3/envs/fl/lib/python3.10/site-packages/torch/nn/modules/module.py", line 805, in _apply
    param_applied = fn(param)
  File "/home/abdenour/anaconda3/envs/fl/lib/python3.10/site-packages/torch/nn/modules/module.py", line 1160, in convert
    return t.to(
torch.OutOfMemoryError: CUDA out of memory. Tried to allocate 392.00 MiB. GPU 0 has a total capacity of 47.33 GiB of which 218.81 MiB is free. Process 2279413 has 11.11 GiB memory in use. Process 2265701 has 2.37 GiB memory in use. Process 2279412 has 11.11 GiB memory in use. Process 2330873 has 11.11 GiB memory in use. Process 2317160 has 2.37 GiB memory in use. Process 2330872 has 7.87 GiB memory in use. Process 2330871 has 816.00 MiB memory in use. Including non-PyTorch memory, this process has 322.00 MiB memory in use. Of the allocated memory 57.01 MiB is allocated by PyTorch, and 4.99 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)

The above exception was the direct cause of the following exception:

[36mray::ClientAppActor.run()[39m (pid=2330870, ip=115.145.172.224, actor_id=18884b649bbd3d098a6ec02b01000000, repr=<flwr.simulation.ray_transport.ray_actor.ClientAppActor object at 0x7ffb805757b0>)
  File "/home/abdenour/.local/lib/python3.10/site-packages/flwr/simulation/ray_transport/ray_actor.py", line 63, in run
    raise ClientAppException(str(ex)) from ex
flwr.client.client_app.ClientAppException: 
Exception ClientAppException occurred. Message: CUDA out of memory. Tried to allocate 392.00 MiB. GPU 0 has a total capacity of 47.33 GiB of which 218.81 MiB is free. Process 2279413 has 11.11 GiB memory in use. Process 2265701 has 2.37 GiB memory in use. Process 2279412 has 11.11 GiB memory in use. Process 2330873 has 11.11 GiB memory in use. Process 2317160 has 2.37 GiB memory in use. Process 2330872 has 7.87 GiB memory in use. Process 2330871 has 816.00 MiB memory in use. Including non-PyTorch memory, this process has 322.00 MiB memory in use. Of the allocated memory 57.01 MiB is allocated by PyTorch, and 4.99 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)

[2024-09-19 01:24:14,325][flwr][ERROR] - Traceback (most recent call last):
  File "/home/abdenour/.local/lib/python3.10/site-packages/flwr/simulation/ray_transport/ray_client_proxy.py", line 94, in _submit_job
    out_mssg, updated_context = self.actor_pool.get_client_result(
  File "/home/abdenour/.local/lib/python3.10/site-packages/flwr/simulation/ray_transport/ray_actor.py", line 398, in get_client_result
    return self._fetch_future_result(cid)
  File "/home/abdenour/.local/lib/python3.10/site-packages/flwr/simulation/ray_transport/ray_actor.py", line 279, in _fetch_future_result
    res_cid, out_mssg, updated_context = ray.get(
  File "/home/abdenour/.local/lib/python3.10/site-packages/ray/_private/auto_init_hook.py", line 21, in auto_init_wrapper
    return fn(*args, **kwargs)
  File "/home/abdenour/.local/lib/python3.10/site-packages/ray/_private/client_mode_hook.py", line 103, in wrapper
    return func(*args, **kwargs)
  File "/home/abdenour/.local/lib/python3.10/site-packages/ray/_private/worker.py", line 2667, in get
    values, debugger_breakpoint = worker.get_objects(object_refs, timeout=timeout)
  File "/home/abdenour/.local/lib/python3.10/site-packages/ray/_private/worker.py", line 864, in get_objects
    raise value.as_instanceof_cause()
ray.exceptions.RayTaskError(ClientAppException): [36mray::ClientAppActor.run()[39m (pid=2330871, ip=115.145.172.224, actor_id=cdc8cff1bec6044f7ef6408701000000, repr=<flwr.simulation.ray_transport.ray_actor.ClientAppActor object at 0x7f75b8199750>)
  File "/home/abdenour/.local/lib/python3.10/site-packages/flwr/client/client_app.py", line 143, in __call__
    return self._call(message, context)
  File "/home/abdenour/.local/lib/python3.10/site-packages/flwr/client/client_app.py", line 126, in ffn
    out_message = handle_legacy_message_from_msgtype(
  File "/home/abdenour/.local/lib/python3.10/site-packages/flwr/client/message_handler/message_handler.py", line 129, in handle_legacy_message_from_msgtype
    fit_res = maybe_call_fit(
  File "/home/abdenour/.local/lib/python3.10/site-packages/flwr/client/client.py", line 234, in maybe_call_fit
    return client.fit(fit_ins)
  File "/home/abdenour/.local/lib/python3.10/site-packages/flwr/client/numpy_client.py", line 238, in _fit
    results = self.numpy_client.fit(parameters, ins.config)  # type: ignore
  File "/home/abdenour/fl_with_flower/fednova_vgg16/fednova_vgg16/client.py", line 69, in fit
    train(self.model, trainloader, self.optimizer, epochs=num_epochs, device=self.device)
  File "/home/abdenour/fl_with_flower/fednova_vgg16/fednova_vgg16/utils.py", line 57, in train
    loss = criterion(net(images), labels)
  File "/home/abdenour/anaconda3/envs/fl/lib/python3.10/site-packages/torch/nn/modules/module.py", line 1553, in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
  File "/home/abdenour/anaconda3/envs/fl/lib/python3.10/site-packages/torch/nn/modules/module.py", line 1562, in _call_impl
    return forward_call(*args, **kwargs)
  File "/home/abdenour/anaconda3/envs/fl/lib/python3.10/site-packages/torchvision/models/vgg.py", line 66, in forward
    x = self.features(x)
  File "/home/abdenour/anaconda3/envs/fl/lib/python3.10/site-packages/torch/nn/modules/module.py", line 1553, in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
  File "/home/abdenour/anaconda3/envs/fl/lib/python3.10/site-packages/torch/nn/modules/module.py", line 1562, in _call_impl
    return forward_call(*args, **kwargs)
  File "/home/abdenour/anaconda3/envs/fl/lib/python3.10/site-packages/torch/nn/modules/container.py", line 219, in forward
    input = module(input)
  File "/home/abdenour/anaconda3/envs/fl/lib/python3.10/site-packages/torch/nn/modules/module.py", line 1553, in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
  File "/home/abdenour/anaconda3/envs/fl/lib/python3.10/site-packages/torch/nn/modules/module.py", line 1562, in _call_impl
    return forward_call(*args, **kwargs)
  File "/home/abdenour/anaconda3/envs/fl/lib/python3.10/site-packages/torch/nn/modules/conv.py", line 458, in forward
    return self._conv_forward(input, self.weight, self.bias)
  File "/home/abdenour/anaconda3/envs/fl/lib/python3.10/site-packages/torch/nn/modules/conv.py", line 454, in _conv_forward
    return F.conv2d(input, weight, bias, self.stride,
torch.OutOfMemoryError: CUDA out of memory. Tried to allocate 784.00 MiB. GPU 0 has a total capacity of 47.33 GiB of which 566.81 MiB is free. Process 2279413 has 11.11 GiB memory in use. Process 2265701 has 2.37 GiB memory in use. Process 2279412 has 11.11 GiB memory in use. Process 2330873 has 11.11 GiB memory in use. Process 2317160 has 2.37 GiB memory in use. Process 2330872 has 7.53 GiB memory in use. Including non-PyTorch memory, this process has 816.00 MiB memory in use. Process 2330870 has 322.00 MiB memory in use. Of the allocated memory 549.92 MiB is allocated by PyTorch, and 6.08 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)

The above exception was the direct cause of the following exception:

[36mray::ClientAppActor.run()[39m (pid=2330871, ip=115.145.172.224, actor_id=cdc8cff1bec6044f7ef6408701000000, repr=<flwr.simulation.ray_transport.ray_actor.ClientAppActor object at 0x7f75b8199750>)
  File "/home/abdenour/.local/lib/python3.10/site-packages/flwr/simulation/ray_transport/ray_actor.py", line 63, in run
    raise ClientAppException(str(ex)) from ex
flwr.client.client_app.ClientAppException: 
Exception ClientAppException occurred. Message: CUDA out of memory. Tried to allocate 784.00 MiB. GPU 0 has a total capacity of 47.33 GiB of which 566.81 MiB is free. Process 2279413 has 11.11 GiB memory in use. Process 2265701 has 2.37 GiB memory in use. Process 2279412 has 11.11 GiB memory in use. Process 2330873 has 11.11 GiB memory in use. Process 2317160 has 2.37 GiB memory in use. Process 2330872 has 7.53 GiB memory in use. Including non-PyTorch memory, this process has 816.00 MiB memory in use. Process 2330870 has 322.00 MiB memory in use. Of the allocated memory 549.92 MiB is allocated by PyTorch, and 6.08 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)

[2024-09-19 01:24:14,325][flwr][ERROR] - [36mray::ClientAppActor.run()[39m (pid=2330870, ip=115.145.172.224, actor_id=18884b649bbd3d098a6ec02b01000000, repr=<flwr.simulation.ray_transport.ray_actor.ClientAppActor object at 0x7ffb805757b0>)
  File "/home/abdenour/.local/lib/python3.10/site-packages/flwr/client/client_app.py", line 143, in __call__
    return self._call(message, context)
  File "/home/abdenour/.local/lib/python3.10/site-packages/flwr/client/client_app.py", line 126, in ffn
    out_message = handle_legacy_message_from_msgtype(
  File "/home/abdenour/.local/lib/python3.10/site-packages/flwr/client/message_handler/message_handler.py", line 95, in handle_legacy_message_from_msgtype
    client = client_fn(context)
  File "/home/abdenour/fl_with_flower/fednova_vgg16/fednova_vgg16/client.py", line 129, in client_fn
    return FlowerClient(trainset, valset, num_classes, config_fit, client_dataset_ratio, context.node_config).to_client()
  File "/home/abdenour/fl_with_flower/fednova_vgg16/fednova_vgg16/client.py", line 36, in __init__
    self.model = self.model.to(self.device)
  File "/home/abdenour/anaconda3/envs/fl/lib/python3.10/site-packages/torch/nn/modules/module.py", line 1174, in to
    return self._apply(convert)
  File "/home/abdenour/anaconda3/envs/fl/lib/python3.10/site-packages/torch/nn/modules/module.py", line 780, in _apply
    module._apply(fn)
  File "/home/abdenour/anaconda3/envs/fl/lib/python3.10/site-packages/torch/nn/modules/module.py", line 780, in _apply
    module._apply(fn)
  File "/home/abdenour/anaconda3/envs/fl/lib/python3.10/site-packages/torch/nn/modules/module.py", line 805, in _apply
    param_applied = fn(param)
  File "/home/abdenour/anaconda3/envs/fl/lib/python3.10/site-packages/torch/nn/modules/module.py", line 1160, in convert
    return t.to(
torch.OutOfMemoryError: CUDA out of memory. Tried to allocate 392.00 MiB. GPU 0 has a total capacity of 47.33 GiB of which 218.81 MiB is free. Process 2279413 has 11.11 GiB memory in use. Process 2265701 has 2.37 GiB memory in use. Process 2279412 has 11.11 GiB memory in use. Process 2330873 has 11.11 GiB memory in use. Process 2317160 has 2.37 GiB memory in use. Process 2330872 has 7.87 GiB memory in use. Process 2330871 has 816.00 MiB memory in use. Including non-PyTorch memory, this process has 322.00 MiB memory in use. Of the allocated memory 57.01 MiB is allocated by PyTorch, and 4.99 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)

The above exception was the direct cause of the following exception:

[36mray::ClientAppActor.run()[39m (pid=2330870, ip=115.145.172.224, actor_id=18884b649bbd3d098a6ec02b01000000, repr=<flwr.simulation.ray_transport.ray_actor.ClientAppActor object at 0x7ffb805757b0>)
  File "/home/abdenour/.local/lib/python3.10/site-packages/flwr/simulation/ray_transport/ray_actor.py", line 63, in run
    raise ClientAppException(str(ex)) from ex
flwr.client.client_app.ClientAppException: 
Exception ClientAppException occurred. Message: CUDA out of memory. Tried to allocate 392.00 MiB. GPU 0 has a total capacity of 47.33 GiB of which 218.81 MiB is free. Process 2279413 has 11.11 GiB memory in use. Process 2265701 has 2.37 GiB memory in use. Process 2279412 has 11.11 GiB memory in use. Process 2330873 has 11.11 GiB memory in use. Process 2317160 has 2.37 GiB memory in use. Process 2330872 has 7.87 GiB memory in use. Process 2330871 has 816.00 MiB memory in use. Including non-PyTorch memory, this process has 322.00 MiB memory in use. Of the allocated memory 57.01 MiB is allocated by PyTorch, and 4.99 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)
[2024-09-19 01:24:14,326][flwr][ERROR] - [36mray::ClientAppActor.run()[39m (pid=2330871, ip=115.145.172.224, actor_id=cdc8cff1bec6044f7ef6408701000000, repr=<flwr.simulation.ray_transport.ray_actor.ClientAppActor object at 0x7f75b8199750>)
  File "/home/abdenour/.local/lib/python3.10/site-packages/flwr/client/client_app.py", line 143, in __call__
    return self._call(message, context)
  File "/home/abdenour/.local/lib/python3.10/site-packages/flwr/client/client_app.py", line 126, in ffn
    out_message = handle_legacy_message_from_msgtype(
  File "/home/abdenour/.local/lib/python3.10/site-packages/flwr/client/message_handler/message_handler.py", line 129, in handle_legacy_message_from_msgtype
    fit_res = maybe_call_fit(
  File "/home/abdenour/.local/lib/python3.10/site-packages/flwr/client/client.py", line 234, in maybe_call_fit
    return client.fit(fit_ins)
  File "/home/abdenour/.local/lib/python3.10/site-packages/flwr/client/numpy_client.py", line 238, in _fit
    results = self.numpy_client.fit(parameters, ins.config)  # type: ignore
  File "/home/abdenour/fl_with_flower/fednova_vgg16/fednova_vgg16/client.py", line 69, in fit
    train(self.model, trainloader, self.optimizer, epochs=num_epochs, device=self.device)
  File "/home/abdenour/fl_with_flower/fednova_vgg16/fednova_vgg16/utils.py", line 57, in train
    loss = criterion(net(images), labels)
  File "/home/abdenour/anaconda3/envs/fl/lib/python3.10/site-packages/torch/nn/modules/module.py", line 1553, in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
  File "/home/abdenour/anaconda3/envs/fl/lib/python3.10/site-packages/torch/nn/modules/module.py", line 1562, in _call_impl
    return forward_call(*args, **kwargs)
  File "/home/abdenour/anaconda3/envs/fl/lib/python3.10/site-packages/torchvision/models/vgg.py", line 66, in forward
    x = self.features(x)
  File "/home/abdenour/anaconda3/envs/fl/lib/python3.10/site-packages/torch/nn/modules/module.py", line 1553, in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
  File "/home/abdenour/anaconda3/envs/fl/lib/python3.10/site-packages/torch/nn/modules/module.py", line 1562, in _call_impl
    return forward_call(*args, **kwargs)
  File "/home/abdenour/anaconda3/envs/fl/lib/python3.10/site-packages/torch/nn/modules/container.py", line 219, in forward
    input = module(input)
  File "/home/abdenour/anaconda3/envs/fl/lib/python3.10/site-packages/torch/nn/modules/module.py", line 1553, in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
  File "/home/abdenour/anaconda3/envs/fl/lib/python3.10/site-packages/torch/nn/modules/module.py", line 1562, in _call_impl
    return forward_call(*args, **kwargs)
  File "/home/abdenour/anaconda3/envs/fl/lib/python3.10/site-packages/torch/nn/modules/conv.py", line 458, in forward
    return self._conv_forward(input, self.weight, self.bias)
  File "/home/abdenour/anaconda3/envs/fl/lib/python3.10/site-packages/torch/nn/modules/conv.py", line 454, in _conv_forward
    return F.conv2d(input, weight, bias, self.stride,
torch.OutOfMemoryError: CUDA out of memory. Tried to allocate 784.00 MiB. GPU 0 has a total capacity of 47.33 GiB of which 566.81 MiB is free. Process 2279413 has 11.11 GiB memory in use. Process 2265701 has 2.37 GiB memory in use. Process 2279412 has 11.11 GiB memory in use. Process 2330873 has 11.11 GiB memory in use. Process 2317160 has 2.37 GiB memory in use. Process 2330872 has 7.53 GiB memory in use. Including non-PyTorch memory, this process has 816.00 MiB memory in use. Process 2330870 has 322.00 MiB memory in use. Of the allocated memory 549.92 MiB is allocated by PyTorch, and 6.08 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)

The above exception was the direct cause of the following exception:

[36mray::ClientAppActor.run()[39m (pid=2330871, ip=115.145.172.224, actor_id=cdc8cff1bec6044f7ef6408701000000, repr=<flwr.simulation.ray_transport.ray_actor.ClientAppActor object at 0x7f75b8199750>)
  File "/home/abdenour/.local/lib/python3.10/site-packages/flwr/simulation/ray_transport/ray_actor.py", line 63, in run
    raise ClientAppException(str(ex)) from ex
flwr.client.client_app.ClientAppException: 
Exception ClientAppException occurred. Message: CUDA out of memory. Tried to allocate 784.00 MiB. GPU 0 has a total capacity of 47.33 GiB of which 566.81 MiB is free. Process 2279413 has 11.11 GiB memory in use. Process 2265701 has 2.37 GiB memory in use. Process 2279412 has 11.11 GiB memory in use. Process 2330873 has 11.11 GiB memory in use. Process 2317160 has 2.37 GiB memory in use. Process 2330872 has 7.53 GiB memory in use. Including non-PyTorch memory, this process has 816.00 MiB memory in use. Process 2330870 has 322.00 MiB memory in use. Of the allocated memory 549.92 MiB is allocated by PyTorch, and 6.08 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)
[2024-09-19 01:24:16,296][flwr][ERROR] - Traceback (most recent call last):
  File "/home/abdenour/.local/lib/python3.10/site-packages/flwr/simulation/ray_transport/ray_client_proxy.py", line 94, in _submit_job
    out_mssg, updated_context = self.actor_pool.get_client_result(
  File "/home/abdenour/.local/lib/python3.10/site-packages/flwr/simulation/ray_transport/ray_actor.py", line 398, in get_client_result
    return self._fetch_future_result(cid)
  File "/home/abdenour/.local/lib/python3.10/site-packages/flwr/simulation/ray_transport/ray_actor.py", line 279, in _fetch_future_result
    res_cid, out_mssg, updated_context = ray.get(
  File "/home/abdenour/.local/lib/python3.10/site-packages/ray/_private/auto_init_hook.py", line 21, in auto_init_wrapper
    return fn(*args, **kwargs)
  File "/home/abdenour/.local/lib/python3.10/site-packages/ray/_private/client_mode_hook.py", line 103, in wrapper
    return func(*args, **kwargs)
  File "/home/abdenour/.local/lib/python3.10/site-packages/ray/_private/worker.py", line 2667, in get
    values, debugger_breakpoint = worker.get_objects(object_refs, timeout=timeout)
  File "/home/abdenour/.local/lib/python3.10/site-packages/ray/_private/worker.py", line 864, in get_objects
    raise value.as_instanceof_cause()
ray.exceptions.RayTaskError(ClientAppException): [36mray::ClientAppActor.run()[39m (pid=2330870, ip=115.145.172.224, actor_id=18884b649bbd3d098a6ec02b01000000, repr=<flwr.simulation.ray_transport.ray_actor.ClientAppActor object at 0x7ffb805757b0>)
  File "/home/abdenour/.local/lib/python3.10/site-packages/flwr/client/client_app.py", line 143, in __call__
    return self._call(message, context)
  File "/home/abdenour/.local/lib/python3.10/site-packages/flwr/client/client_app.py", line 126, in ffn
    out_message = handle_legacy_message_from_msgtype(
  File "/home/abdenour/.local/lib/python3.10/site-packages/flwr/client/message_handler/message_handler.py", line 129, in handle_legacy_message_from_msgtype
    fit_res = maybe_call_fit(
  File "/home/abdenour/.local/lib/python3.10/site-packages/flwr/client/client.py", line 234, in maybe_call_fit
    return client.fit(fit_ins)
  File "/home/abdenour/.local/lib/python3.10/site-packages/flwr/client/numpy_client.py", line 238, in _fit
    results = self.numpy_client.fit(parameters, ins.config)  # type: ignore
  File "/home/abdenour/fl_with_flower/fednova_vgg16/fednova_vgg16/client.py", line 69, in fit
    train(self.model, trainloader, self.optimizer, epochs=num_epochs, device=self.device)
  File "/home/abdenour/fl_with_flower/fednova_vgg16/fednova_vgg16/utils.py", line 57, in train
    loss = criterion(net(images), labels)
  File "/home/abdenour/anaconda3/envs/fl/lib/python3.10/site-packages/torch/nn/modules/module.py", line 1553, in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
  File "/home/abdenour/anaconda3/envs/fl/lib/python3.10/site-packages/torch/nn/modules/module.py", line 1562, in _call_impl
    return forward_call(*args, **kwargs)
  File "/home/abdenour/anaconda3/envs/fl/lib/python3.10/site-packages/torchvision/models/vgg.py", line 66, in forward
    x = self.features(x)
  File "/home/abdenour/anaconda3/envs/fl/lib/python3.10/site-packages/torch/nn/modules/module.py", line 1553, in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
  File "/home/abdenour/anaconda3/envs/fl/lib/python3.10/site-packages/torch/nn/modules/module.py", line 1562, in _call_impl
    return forward_call(*args, **kwargs)
  File "/home/abdenour/anaconda3/envs/fl/lib/python3.10/site-packages/torch/nn/modules/container.py", line 219, in forward
    input = module(input)
  File "/home/abdenour/anaconda3/envs/fl/lib/python3.10/site-packages/torch/nn/modules/module.py", line 1553, in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
  File "/home/abdenour/anaconda3/envs/fl/lib/python3.10/site-packages/torch/nn/modules/module.py", line 1562, in _call_impl
    return forward_call(*args, **kwargs)
  File "/home/abdenour/anaconda3/envs/fl/lib/python3.10/site-packages/torch/nn/modules/conv.py", line 458, in forward
    return self._conv_forward(input, self.weight, self.bias)
  File "/home/abdenour/anaconda3/envs/fl/lib/python3.10/site-packages/torch/nn/modules/conv.py", line 454, in _conv_forward
    return F.conv2d(input, weight, bias, self.stride,
torch.OutOfMemoryError: CUDA out of memory. Tried to allocate 784.00 MiB. GPU 0 has a total capacity of 47.33 GiB of which 34.81 MiB is free. Process 2279413 has 11.11 GiB memory in use. Process 2265701 has 2.37 GiB memory in use. Process 2279412 has 11.11 GiB memory in use. Process 2330873 has 11.11 GiB memory in use. Process 2317160 has 2.37 GiB memory in use. Process 2330872 has 6.75 GiB memory in use. Process 2330871 has 816.00 MiB memory in use. Including non-PyTorch memory, this process has 1.61 GiB memory in use. Of the allocated memory 1.30 GiB is allocated by PyTorch, and 6.08 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)

The above exception was the direct cause of the following exception:

[36mray::ClientAppActor.run()[39m (pid=2330870, ip=115.145.172.224, actor_id=18884b649bbd3d098a6ec02b01000000, repr=<flwr.simulation.ray_transport.ray_actor.ClientAppActor object at 0x7ffb805757b0>)
  File "/home/abdenour/.local/lib/python3.10/site-packages/flwr/simulation/ray_transport/ray_actor.py", line 63, in run
    raise ClientAppException(str(ex)) from ex
flwr.client.client_app.ClientAppException: 
Exception ClientAppException occurred. Message: CUDA out of memory. Tried to allocate 784.00 MiB. GPU 0 has a total capacity of 47.33 GiB of which 34.81 MiB is free. Process 2279413 has 11.11 GiB memory in use. Process 2265701 has 2.37 GiB memory in use. Process 2279412 has 11.11 GiB memory in use. Process 2330873 has 11.11 GiB memory in use. Process 2317160 has 2.37 GiB memory in use. Process 2330872 has 6.75 GiB memory in use. Process 2330871 has 816.00 MiB memory in use. Including non-PyTorch memory, this process has 1.61 GiB memory in use. Of the allocated memory 1.30 GiB is allocated by PyTorch, and 6.08 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)

[2024-09-19 01:24:16,296][flwr][ERROR] - [36mray::ClientAppActor.run()[39m (pid=2330870, ip=115.145.172.224, actor_id=18884b649bbd3d098a6ec02b01000000, repr=<flwr.simulation.ray_transport.ray_actor.ClientAppActor object at 0x7ffb805757b0>)
  File "/home/abdenour/.local/lib/python3.10/site-packages/flwr/client/client_app.py", line 143, in __call__
    return self._call(message, context)
  File "/home/abdenour/.local/lib/python3.10/site-packages/flwr/client/client_app.py", line 126, in ffn
    out_message = handle_legacy_message_from_msgtype(
  File "/home/abdenour/.local/lib/python3.10/site-packages/flwr/client/message_handler/message_handler.py", line 129, in handle_legacy_message_from_msgtype
    fit_res = maybe_call_fit(
  File "/home/abdenour/.local/lib/python3.10/site-packages/flwr/client/client.py", line 234, in maybe_call_fit
    return client.fit(fit_ins)
  File "/home/abdenour/.local/lib/python3.10/site-packages/flwr/client/numpy_client.py", line 238, in _fit
    results = self.numpy_client.fit(parameters, ins.config)  # type: ignore
  File "/home/abdenour/fl_with_flower/fednova_vgg16/fednova_vgg16/client.py", line 69, in fit
    train(self.model, trainloader, self.optimizer, epochs=num_epochs, device=self.device)
  File "/home/abdenour/fl_with_flower/fednova_vgg16/fednova_vgg16/utils.py", line 57, in train
    loss = criterion(net(images), labels)
  File "/home/abdenour/anaconda3/envs/fl/lib/python3.10/site-packages/torch/nn/modules/module.py", line 1553, in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
  File "/home/abdenour/anaconda3/envs/fl/lib/python3.10/site-packages/torch/nn/modules/module.py", line 1562, in _call_impl
    return forward_call(*args, **kwargs)
  File "/home/abdenour/anaconda3/envs/fl/lib/python3.10/site-packages/torchvision/models/vgg.py", line 66, in forward
    x = self.features(x)
  File "/home/abdenour/anaconda3/envs/fl/lib/python3.10/site-packages/torch/nn/modules/module.py", line 1553, in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
  File "/home/abdenour/anaconda3/envs/fl/lib/python3.10/site-packages/torch/nn/modules/module.py", line 1562, in _call_impl
    return forward_call(*args, **kwargs)
  File "/home/abdenour/anaconda3/envs/fl/lib/python3.10/site-packages/torch/nn/modules/container.py", line 219, in forward
    input = module(input)
  File "/home/abdenour/anaconda3/envs/fl/lib/python3.10/site-packages/torch/nn/modules/module.py", line 1553, in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
  File "/home/abdenour/anaconda3/envs/fl/lib/python3.10/site-packages/torch/nn/modules/module.py", line 1562, in _call_impl
    return forward_call(*args, **kwargs)
  File "/home/abdenour/anaconda3/envs/fl/lib/python3.10/site-packages/torch/nn/modules/conv.py", line 458, in forward
    return self._conv_forward(input, self.weight, self.bias)
  File "/home/abdenour/anaconda3/envs/fl/lib/python3.10/site-packages/torch/nn/modules/conv.py", line 454, in _conv_forward
    return F.conv2d(input, weight, bias, self.stride,
torch.OutOfMemoryError: CUDA out of memory. Tried to allocate 784.00 MiB. GPU 0 has a total capacity of 47.33 GiB of which 34.81 MiB is free. Process 2279413 has 11.11 GiB memory in use. Process 2265701 has 2.37 GiB memory in use. Process 2279412 has 11.11 GiB memory in use. Process 2330873 has 11.11 GiB memory in use. Process 2317160 has 2.37 GiB memory in use. Process 2330872 has 6.75 GiB memory in use. Process 2330871 has 816.00 MiB memory in use. Including non-PyTorch memory, this process has 1.61 GiB memory in use. Of the allocated memory 1.30 GiB is allocated by PyTorch, and 6.08 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)

The above exception was the direct cause of the following exception:

[36mray::ClientAppActor.run()[39m (pid=2330870, ip=115.145.172.224, actor_id=18884b649bbd3d098a6ec02b01000000, repr=<flwr.simulation.ray_transport.ray_actor.ClientAppActor object at 0x7ffb805757b0>)
  File "/home/abdenour/.local/lib/python3.10/site-packages/flwr/simulation/ray_transport/ray_actor.py", line 63, in run
    raise ClientAppException(str(ex)) from ex
flwr.client.client_app.ClientAppException: 
Exception ClientAppException occurred. Message: CUDA out of memory. Tried to allocate 784.00 MiB. GPU 0 has a total capacity of 47.33 GiB of which 34.81 MiB is free. Process 2279413 has 11.11 GiB memory in use. Process 2265701 has 2.37 GiB memory in use. Process 2279412 has 11.11 GiB memory in use. Process 2330873 has 11.11 GiB memory in use. Process 2317160 has 2.37 GiB memory in use. Process 2330872 has 6.75 GiB memory in use. Process 2330871 has 816.00 MiB memory in use. Including non-PyTorch memory, this process has 1.61 GiB memory in use. Of the allocated memory 1.30 GiB is allocated by PyTorch, and 6.08 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)
[2024-09-19 01:24:17,648][flwr][INFO] - aggregate_fit: received 2 results and 3 failures
[2024-09-19 01:24:31,995][flwr][INFO] - fit progress: (2, 220.16360247135162, {'accuracy': 0.1353405437006723}, 53.40409162826836)
[2024-09-19 01:24:31,997][flwr][INFO] - configure_evaluate: strategy sampled 2 clients (out of 50)
[2024-09-19 01:24:35,552][flwr][INFO] - aggregate_evaluate: received 2 results and 0 failures
[2024-09-19 01:24:35,552][flwr][INFO] - 
[2024-09-19 01:24:35,552][flwr][INFO] - [ROUND 3]
[2024-09-19 01:24:35,552][flwr][INFO] - configure_fit: strategy sampled 5 clients (out of 50)
[2024-09-19 01:24:39,978][flwr][ERROR] - Traceback (most recent call last):
  File "/home/abdenour/.local/lib/python3.10/site-packages/flwr/simulation/ray_transport/ray_client_proxy.py", line 94, in _submit_job
    out_mssg, updated_context = self.actor_pool.get_client_result(
  File "/home/abdenour/.local/lib/python3.10/site-packages/flwr/simulation/ray_transport/ray_actor.py", line 398, in get_client_result
    return self._fetch_future_result(cid)
  File "/home/abdenour/.local/lib/python3.10/site-packages/flwr/simulation/ray_transport/ray_actor.py", line 279, in _fetch_future_result
    res_cid, out_mssg, updated_context = ray.get(
  File "/home/abdenour/.local/lib/python3.10/site-packages/ray/_private/auto_init_hook.py", line 21, in auto_init_wrapper
    return fn(*args, **kwargs)
  File "/home/abdenour/.local/lib/python3.10/site-packages/ray/_private/client_mode_hook.py", line 103, in wrapper
    return func(*args, **kwargs)
  File "/home/abdenour/.local/lib/python3.10/site-packages/ray/_private/worker.py", line 2667, in get
    values, debugger_breakpoint = worker.get_objects(object_refs, timeout=timeout)
  File "/home/abdenour/.local/lib/python3.10/site-packages/ray/_private/worker.py", line 864, in get_objects
    raise value.as_instanceof_cause()
ray.exceptions.RayTaskError(ClientAppException): [36mray::ClientAppActor.run()[39m (pid=2330872, ip=115.145.172.224, actor_id=d38096ee5352eefa91005c8e01000000, repr=<flwr.simulation.ray_transport.ray_actor.ClientAppActor object at 0x7fcf0478d750>)
  File "/home/abdenour/.local/lib/python3.10/site-packages/flwr/client/client_app.py", line 143, in __call__
    return self._call(message, context)
  File "/home/abdenour/.local/lib/python3.10/site-packages/flwr/client/client_app.py", line 126, in ffn
    out_message = handle_legacy_message_from_msgtype(
  File "/home/abdenour/.local/lib/python3.10/site-packages/flwr/client/message_handler/message_handler.py", line 129, in handle_legacy_message_from_msgtype
    fit_res = maybe_call_fit(
  File "/home/abdenour/.local/lib/python3.10/site-packages/flwr/client/client.py", line 234, in maybe_call_fit
    return client.fit(fit_ins)
  File "/home/abdenour/.local/lib/python3.10/site-packages/flwr/client/numpy_client.py", line 238, in _fit
    results = self.numpy_client.fit(parameters, ins.config)  # type: ignore
  File "/home/abdenour/fl_with_flower/fednova_vgg16/fednova_vgg16/client.py", line 69, in fit
    train(self.model, trainloader, self.optimizer, epochs=num_epochs, device=self.device)
  File "/home/abdenour/fl_with_flower/fednova_vgg16/fednova_vgg16/utils.py", line 58, in train
    loss.backward()
  File "/home/abdenour/anaconda3/envs/fl/lib/python3.10/site-packages/torch/_tensor.py", line 521, in backward
    torch.autograd.backward(
  File "/home/abdenour/anaconda3/envs/fl/lib/python3.10/site-packages/torch/autograd/__init__.py", line 289, in backward
    _engine_run_backward(
  File "/home/abdenour/anaconda3/envs/fl/lib/python3.10/site-packages/torch/autograd/graph.py", line 769, in _engine_run_backward
    return Variable._execution_engine.run_backward(  # Calls into the C++ engine to run the backward pass
torch.OutOfMemoryError: CUDA out of memory. Tried to allocate 26.00 MiB. GPU 0 has a total capacity of 47.33 GiB of which 8.81 MiB is free. Process 2279413 has 11.11 GiB memory in use. Process 2265701 has 2.37 GiB memory in use. Process 2279412 has 11.11 GiB memory in use. Process 2330873 has 11.11 GiB memory in use. Process 2317160 has 2.37 GiB memory in use. Including non-PyTorch memory, this process has 6.78 GiB memory in use. Process 2330871 has 816.00 MiB memory in use. Process 2330870 has 1.61 GiB memory in use. Of the allocated memory 6.33 GiB is allocated by PyTorch, and 128.85 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)

The above exception was the direct cause of the following exception:

[36mray::ClientAppActor.run()[39m (pid=2330872, ip=115.145.172.224, actor_id=d38096ee5352eefa91005c8e01000000, repr=<flwr.simulation.ray_transport.ray_actor.ClientAppActor object at 0x7fcf0478d750>)
  File "/home/abdenour/.local/lib/python3.10/site-packages/flwr/simulation/ray_transport/ray_actor.py", line 63, in run
    raise ClientAppException(str(ex)) from ex
flwr.client.client_app.ClientAppException: 
Exception ClientAppException occurred. Message: CUDA out of memory. Tried to allocate 26.00 MiB. GPU 0 has a total capacity of 47.33 GiB of which 8.81 MiB is free. Process 2279413 has 11.11 GiB memory in use. Process 2265701 has 2.37 GiB memory in use. Process 2279412 has 11.11 GiB memory in use. Process 2330873 has 11.11 GiB memory in use. Process 2317160 has 2.37 GiB memory in use. Including non-PyTorch memory, this process has 6.78 GiB memory in use. Process 2330871 has 816.00 MiB memory in use. Process 2330870 has 1.61 GiB memory in use. Of the allocated memory 6.33 GiB is allocated by PyTorch, and 128.85 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)

[2024-09-19 01:24:39,979][flwr][ERROR] - [36mray::ClientAppActor.run()[39m (pid=2330872, ip=115.145.172.224, actor_id=d38096ee5352eefa91005c8e01000000, repr=<flwr.simulation.ray_transport.ray_actor.ClientAppActor object at 0x7fcf0478d750>)
  File "/home/abdenour/.local/lib/python3.10/site-packages/flwr/client/client_app.py", line 143, in __call__
    return self._call(message, context)
  File "/home/abdenour/.local/lib/python3.10/site-packages/flwr/client/client_app.py", line 126, in ffn
    out_message = handle_legacy_message_from_msgtype(
  File "/home/abdenour/.local/lib/python3.10/site-packages/flwr/client/message_handler/message_handler.py", line 129, in handle_legacy_message_from_msgtype
    fit_res = maybe_call_fit(
  File "/home/abdenour/.local/lib/python3.10/site-packages/flwr/client/client.py", line 234, in maybe_call_fit
    return client.fit(fit_ins)
  File "/home/abdenour/.local/lib/python3.10/site-packages/flwr/client/numpy_client.py", line 238, in _fit
    results = self.numpy_client.fit(parameters, ins.config)  # type: ignore
  File "/home/abdenour/fl_with_flower/fednova_vgg16/fednova_vgg16/client.py", line 69, in fit
    train(self.model, trainloader, self.optimizer, epochs=num_epochs, device=self.device)
  File "/home/abdenour/fl_with_flower/fednova_vgg16/fednova_vgg16/utils.py", line 58, in train
    loss.backward()
  File "/home/abdenour/anaconda3/envs/fl/lib/python3.10/site-packages/torch/_tensor.py", line 521, in backward
    torch.autograd.backward(
  File "/home/abdenour/anaconda3/envs/fl/lib/python3.10/site-packages/torch/autograd/__init__.py", line 289, in backward
    _engine_run_backward(
  File "/home/abdenour/anaconda3/envs/fl/lib/python3.10/site-packages/torch/autograd/graph.py", line 769, in _engine_run_backward
    return Variable._execution_engine.run_backward(  # Calls into the C++ engine to run the backward pass
torch.OutOfMemoryError: CUDA out of memory. Tried to allocate 26.00 MiB. GPU 0 has a total capacity of 47.33 GiB of which 8.81 MiB is free. Process 2279413 has 11.11 GiB memory in use. Process 2265701 has 2.37 GiB memory in use. Process 2279412 has 11.11 GiB memory in use. Process 2330873 has 11.11 GiB memory in use. Process 2317160 has 2.37 GiB memory in use. Including non-PyTorch memory, this process has 6.78 GiB memory in use. Process 2330871 has 816.00 MiB memory in use. Process 2330870 has 1.61 GiB memory in use. Of the allocated memory 6.33 GiB is allocated by PyTorch, and 128.85 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)

The above exception was the direct cause of the following exception:

[36mray::ClientAppActor.run()[39m (pid=2330872, ip=115.145.172.224, actor_id=d38096ee5352eefa91005c8e01000000, repr=<flwr.simulation.ray_transport.ray_actor.ClientAppActor object at 0x7fcf0478d750>)
  File "/home/abdenour/.local/lib/python3.10/site-packages/flwr/simulation/ray_transport/ray_actor.py", line 63, in run
    raise ClientAppException(str(ex)) from ex
flwr.client.client_app.ClientAppException: 
Exception ClientAppException occurred. Message: CUDA out of memory. Tried to allocate 26.00 MiB. GPU 0 has a total capacity of 47.33 GiB of which 8.81 MiB is free. Process 2279413 has 11.11 GiB memory in use. Process 2265701 has 2.37 GiB memory in use. Process 2279412 has 11.11 GiB memory in use. Process 2330873 has 11.11 GiB memory in use. Process 2317160 has 2.37 GiB memory in use. Including non-PyTorch memory, this process has 6.78 GiB memory in use. Process 2330871 has 816.00 MiB memory in use. Process 2330870 has 1.61 GiB memory in use. Of the allocated memory 6.33 GiB is allocated by PyTorch, and 128.85 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)
[2024-09-19 01:24:40,105][flwr][ERROR] - Traceback (most recent call last):
  File "/home/abdenour/.local/lib/python3.10/site-packages/flwr/simulation/ray_transport/ray_client_proxy.py", line 94, in _submit_job
    out_mssg, updated_context = self.actor_pool.get_client_result(
  File "/home/abdenour/.local/lib/python3.10/site-packages/flwr/simulation/ray_transport/ray_actor.py", line 398, in get_client_result
    return self._fetch_future_result(cid)
  File "/home/abdenour/.local/lib/python3.10/site-packages/flwr/simulation/ray_transport/ray_actor.py", line 279, in _fetch_future_result
    res_cid, out_mssg, updated_context = ray.get(
  File "/home/abdenour/.local/lib/python3.10/site-packages/ray/_private/auto_init_hook.py", line 21, in auto_init_wrapper
    return fn(*args, **kwargs)
  File "/home/abdenour/.local/lib/python3.10/site-packages/ray/_private/client_mode_hook.py", line 103, in wrapper
    return func(*args, **kwargs)
  File "/home/abdenour/.local/lib/python3.10/site-packages/ray/_private/worker.py", line 2667, in get
    values, debugger_breakpoint = worker.get_objects(object_refs, timeout=timeout)
  File "/home/abdenour/.local/lib/python3.10/site-packages/ray/_private/worker.py", line 864, in get_objects
    raise value.as_instanceof_cause()
ray.exceptions.RayTaskError(ClientAppException): [36mray::ClientAppActor.run()[39m (pid=2330870, ip=115.145.172.224, actor_id=18884b649bbd3d098a6ec02b01000000, repr=<flwr.simulation.ray_transport.ray_actor.ClientAppActor object at 0x7ffb805757b0>)
  File "/home/abdenour/.local/lib/python3.10/site-packages/flwr/client/client_app.py", line 143, in __call__
    return self._call(message, context)
  File "/home/abdenour/.local/lib/python3.10/site-packages/flwr/client/client_app.py", line 126, in ffn
    out_message = handle_legacy_message_from_msgtype(
  File "/home/abdenour/.local/lib/python3.10/site-packages/flwr/client/message_handler/message_handler.py", line 129, in handle_legacy_message_from_msgtype
    fit_res = maybe_call_fit(
  File "/home/abdenour/.local/lib/python3.10/site-packages/flwr/client/client.py", line 234, in maybe_call_fit
    return client.fit(fit_ins)
  File "/home/abdenour/.local/lib/python3.10/site-packages/flwr/client/numpy_client.py", line 238, in _fit
    results = self.numpy_client.fit(parameters, ins.config)  # type: ignore
  File "/home/abdenour/fl_with_flower/fednova_vgg16/fednova_vgg16/client.py", line 69, in fit
    train(self.model, trainloader, self.optimizer, epochs=num_epochs, device=self.device)
  File "/home/abdenour/fl_with_flower/fednova_vgg16/fednova_vgg16/utils.py", line 57, in train
    loss = criterion(net(images), labels)
  File "/home/abdenour/anaconda3/envs/fl/lib/python3.10/site-packages/torch/nn/modules/module.py", line 1553, in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
  File "/home/abdenour/anaconda3/envs/fl/lib/python3.10/site-packages/torch/nn/modules/module.py", line 1562, in _call_impl
    return forward_call(*args, **kwargs)
  File "/home/abdenour/anaconda3/envs/fl/lib/python3.10/site-packages/torchvision/models/vgg.py", line 66, in forward
    x = self.features(x)
  File "/home/abdenour/anaconda3/envs/fl/lib/python3.10/site-packages/torch/nn/modules/module.py", line 1553, in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
  File "/home/abdenour/anaconda3/envs/fl/lib/python3.10/site-packages/torch/nn/modules/module.py", line 1562, in _call_impl
    return forward_call(*args, **kwargs)
  File "/home/abdenour/anaconda3/envs/fl/lib/python3.10/site-packages/torch/nn/modules/container.py", line 219, in forward
    input = module(input)
  File "/home/abdenour/anaconda3/envs/fl/lib/python3.10/site-packages/torch/nn/modules/module.py", line 1553, in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
  File "/home/abdenour/anaconda3/envs/fl/lib/python3.10/site-packages/torch/nn/modules/module.py", line 1562, in _call_impl
    return forward_call(*args, **kwargs)
  File "/home/abdenour/anaconda3/envs/fl/lib/python3.10/site-packages/torch/nn/modules/conv.py", line 458, in forward
    return self._conv_forward(input, self.weight, self.bias)
  File "/home/abdenour/anaconda3/envs/fl/lib/python3.10/site-packages/torch/nn/modules/conv.py", line 454, in _conv_forward
    return F.conv2d(input, weight, bias, self.stride,
torch.OutOfMemoryError: CUDA out of memory. Tried to allocate 784.00 MiB. GPU 0 has a total capacity of 47.33 GiB of which 8.81 MiB is free. Process 2279413 has 11.11 GiB memory in use. Process 2265701 has 2.37 GiB memory in use. Process 2279412 has 11.11 GiB memory in use. Process 2330873 has 11.11 GiB memory in use. Process 2317160 has 2.37 GiB memory in use. Process 2330872 has 6.78 GiB memory in use. Process 2330871 has 816.00 MiB memory in use. Including non-PyTorch memory, this process has 1.61 GiB memory in use. Of the allocated memory 1.30 GiB is allocated by PyTorch, and 6.08 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)

The above exception was the direct cause of the following exception:

[36mray::ClientAppActor.run()[39m (pid=2330870, ip=115.145.172.224, actor_id=18884b649bbd3d098a6ec02b01000000, repr=<flwr.simulation.ray_transport.ray_actor.ClientAppActor object at 0x7ffb805757b0>)
  File "/home/abdenour/.local/lib/python3.10/site-packages/flwr/simulation/ray_transport/ray_actor.py", line 63, in run
    raise ClientAppException(str(ex)) from ex
flwr.client.client_app.ClientAppException: 
Exception ClientAppException occurred. Message: CUDA out of memory. Tried to allocate 784.00 MiB. GPU 0 has a total capacity of 47.33 GiB of which 8.81 MiB is free. Process 2279413 has 11.11 GiB memory in use. Process 2265701 has 2.37 GiB memory in use. Process 2279412 has 11.11 GiB memory in use. Process 2330873 has 11.11 GiB memory in use. Process 2317160 has 2.37 GiB memory in use. Process 2330872 has 6.78 GiB memory in use. Process 2330871 has 816.00 MiB memory in use. Including non-PyTorch memory, this process has 1.61 GiB memory in use. Of the allocated memory 1.30 GiB is allocated by PyTorch, and 6.08 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)

[2024-09-19 01:24:40,106][flwr][ERROR] - [36mray::ClientAppActor.run()[39m (pid=2330870, ip=115.145.172.224, actor_id=18884b649bbd3d098a6ec02b01000000, repr=<flwr.simulation.ray_transport.ray_actor.ClientAppActor object at 0x7ffb805757b0>)
  File "/home/abdenour/.local/lib/python3.10/site-packages/flwr/client/client_app.py", line 143, in __call__
    return self._call(message, context)
  File "/home/abdenour/.local/lib/python3.10/site-packages/flwr/client/client_app.py", line 126, in ffn
    out_message = handle_legacy_message_from_msgtype(
  File "/home/abdenour/.local/lib/python3.10/site-packages/flwr/client/message_handler/message_handler.py", line 129, in handle_legacy_message_from_msgtype
    fit_res = maybe_call_fit(
  File "/home/abdenour/.local/lib/python3.10/site-packages/flwr/client/client.py", line 234, in maybe_call_fit
    return client.fit(fit_ins)
  File "/home/abdenour/.local/lib/python3.10/site-packages/flwr/client/numpy_client.py", line 238, in _fit
    results = self.numpy_client.fit(parameters, ins.config)  # type: ignore
  File "/home/abdenour/fl_with_flower/fednova_vgg16/fednova_vgg16/client.py", line 69, in fit
    train(self.model, trainloader, self.optimizer, epochs=num_epochs, device=self.device)
  File "/home/abdenour/fl_with_flower/fednova_vgg16/fednova_vgg16/utils.py", line 57, in train
    loss = criterion(net(images), labels)
  File "/home/abdenour/anaconda3/envs/fl/lib/python3.10/site-packages/torch/nn/modules/module.py", line 1553, in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
  File "/home/abdenour/anaconda3/envs/fl/lib/python3.10/site-packages/torch/nn/modules/module.py", line 1562, in _call_impl
    return forward_call(*args, **kwargs)
  File "/home/abdenour/anaconda3/envs/fl/lib/python3.10/site-packages/torchvision/models/vgg.py", line 66, in forward
    x = self.features(x)
  File "/home/abdenour/anaconda3/envs/fl/lib/python3.10/site-packages/torch/nn/modules/module.py", line 1553, in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
  File "/home/abdenour/anaconda3/envs/fl/lib/python3.10/site-packages/torch/nn/modules/module.py", line 1562, in _call_impl
    return forward_call(*args, **kwargs)
  File "/home/abdenour/anaconda3/envs/fl/lib/python3.10/site-packages/torch/nn/modules/container.py", line 219, in forward
    input = module(input)
  File "/home/abdenour/anaconda3/envs/fl/lib/python3.10/site-packages/torch/nn/modules/module.py", line 1553, in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
  File "/home/abdenour/anaconda3/envs/fl/lib/python3.10/site-packages/torch/nn/modules/module.py", line 1562, in _call_impl
    return forward_call(*args, **kwargs)
  File "/home/abdenour/anaconda3/envs/fl/lib/python3.10/site-packages/torch/nn/modules/conv.py", line 458, in forward
    return self._conv_forward(input, self.weight, self.bias)
  File "/home/abdenour/anaconda3/envs/fl/lib/python3.10/site-packages/torch/nn/modules/conv.py", line 454, in _conv_forward
    return F.conv2d(input, weight, bias, self.stride,
torch.OutOfMemoryError: CUDA out of memory. Tried to allocate 784.00 MiB. GPU 0 has a total capacity of 47.33 GiB of which 8.81 MiB is free. Process 2279413 has 11.11 GiB memory in use. Process 2265701 has 2.37 GiB memory in use. Process 2279412 has 11.11 GiB memory in use. Process 2330873 has 11.11 GiB memory in use. Process 2317160 has 2.37 GiB memory in use. Process 2330872 has 6.78 GiB memory in use. Process 2330871 has 816.00 MiB memory in use. Including non-PyTorch memory, this process has 1.61 GiB memory in use. Of the allocated memory 1.30 GiB is allocated by PyTorch, and 6.08 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)

The above exception was the direct cause of the following exception:

[36mray::ClientAppActor.run()[39m (pid=2330870, ip=115.145.172.224, actor_id=18884b649bbd3d098a6ec02b01000000, repr=<flwr.simulation.ray_transport.ray_actor.ClientAppActor object at 0x7ffb805757b0>)
  File "/home/abdenour/.local/lib/python3.10/site-packages/flwr/simulation/ray_transport/ray_actor.py", line 63, in run
    raise ClientAppException(str(ex)) from ex
flwr.client.client_app.ClientAppException: 
Exception ClientAppException occurred. Message: CUDA out of memory. Tried to allocate 784.00 MiB. GPU 0 has a total capacity of 47.33 GiB of which 8.81 MiB is free. Process 2279413 has 11.11 GiB memory in use. Process 2265701 has 2.37 GiB memory in use. Process 2279412 has 11.11 GiB memory in use. Process 2330873 has 11.11 GiB memory in use. Process 2317160 has 2.37 GiB memory in use. Process 2330872 has 6.78 GiB memory in use. Process 2330871 has 816.00 MiB memory in use. Including non-PyTorch memory, this process has 1.61 GiB memory in use. Of the allocated memory 1.30 GiB is allocated by PyTorch, and 6.08 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)
[2024-09-19 01:24:40,701][flwr][ERROR] - Traceback (most recent call last):
  File "/home/abdenour/.local/lib/python3.10/site-packages/flwr/simulation/ray_transport/ray_client_proxy.py", line 94, in _submit_job
    out_mssg, updated_context = self.actor_pool.get_client_result(
  File "/home/abdenour/.local/lib/python3.10/site-packages/flwr/simulation/ray_transport/ray_actor.py", line 398, in get_client_result
    return self._fetch_future_result(cid)
  File "/home/abdenour/.local/lib/python3.10/site-packages/flwr/simulation/ray_transport/ray_actor.py", line 279, in _fetch_future_result
    res_cid, out_mssg, updated_context = ray.get(
  File "/home/abdenour/.local/lib/python3.10/site-packages/ray/_private/auto_init_hook.py", line 21, in auto_init_wrapper
    return fn(*args, **kwargs)
  File "/home/abdenour/.local/lib/python3.10/site-packages/ray/_private/client_mode_hook.py", line 103, in wrapper
    return func(*args, **kwargs)
  File "/home/abdenour/.local/lib/python3.10/site-packages/ray/_private/worker.py", line 2667, in get
    values, debugger_breakpoint = worker.get_objects(object_refs, timeout=timeout)
  File "/home/abdenour/.local/lib/python3.10/site-packages/ray/_private/worker.py", line 864, in get_objects
    raise value.as_instanceof_cause()
ray.exceptions.RayTaskError(ClientAppException): [36mray::ClientAppActor.run()[39m (pid=2330871, ip=115.145.172.224, actor_id=cdc8cff1bec6044f7ef6408701000000, repr=<flwr.simulation.ray_transport.ray_actor.ClientAppActor object at 0x7f75b8199750>)
  File "/home/abdenour/.local/lib/python3.10/site-packages/flwr/client/client_app.py", line 143, in __call__
    return self._call(message, context)
  File "/home/abdenour/.local/lib/python3.10/site-packages/flwr/client/client_app.py", line 126, in ffn
    out_message = handle_legacy_message_from_msgtype(
  File "/home/abdenour/.local/lib/python3.10/site-packages/flwr/client/message_handler/message_handler.py", line 129, in handle_legacy_message_from_msgtype
    fit_res = maybe_call_fit(
  File "/home/abdenour/.local/lib/python3.10/site-packages/flwr/client/client.py", line 234, in maybe_call_fit
    return client.fit(fit_ins)
  File "/home/abdenour/.local/lib/python3.10/site-packages/flwr/client/numpy_client.py", line 238, in _fit
    results = self.numpy_client.fit(parameters, ins.config)  # type: ignore
  File "/home/abdenour/fl_with_flower/fednova_vgg16/fednova_vgg16/client.py", line 69, in fit
    train(self.model, trainloader, self.optimizer, epochs=num_epochs, device=self.device)
  File "/home/abdenour/fl_with_flower/fednova_vgg16/fednova_vgg16/utils.py", line 57, in train
    loss = criterion(net(images), labels)
  File "/home/abdenour/anaconda3/envs/fl/lib/python3.10/site-packages/torch/nn/modules/module.py", line 1553, in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
  File "/home/abdenour/anaconda3/envs/fl/lib/python3.10/site-packages/torch/nn/modules/module.py", line 1562, in _call_impl
    return forward_call(*args, **kwargs)
  File "/home/abdenour/anaconda3/envs/fl/lib/python3.10/site-packages/torchvision/models/vgg.py", line 66, in forward
    x = self.features(x)
  File "/home/abdenour/anaconda3/envs/fl/lib/python3.10/site-packages/torch/nn/modules/module.py", line 1553, in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
  File "/home/abdenour/anaconda3/envs/fl/lib/python3.10/site-packages/torch/nn/modules/module.py", line 1562, in _call_impl
    return forward_call(*args, **kwargs)
  File "/home/abdenour/anaconda3/envs/fl/lib/python3.10/site-packages/torch/nn/modules/container.py", line 219, in forward
    input = module(input)
  File "/home/abdenour/anaconda3/envs/fl/lib/python3.10/site-packages/torch/nn/modules/module.py", line 1553, in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
  File "/home/abdenour/anaconda3/envs/fl/lib/python3.10/site-packages/torch/nn/modules/module.py", line 1562, in _call_impl
    return forward_call(*args, **kwargs)
  File "/home/abdenour/anaconda3/envs/fl/lib/python3.10/site-packages/torch/nn/modules/conv.py", line 458, in forward
    return self._conv_forward(input, self.weight, self.bias)
  File "/home/abdenour/anaconda3/envs/fl/lib/python3.10/site-packages/torch/nn/modules/conv.py", line 454, in _conv_forward
    return F.conv2d(input, weight, bias, self.stride,
torch.OutOfMemoryError: CUDA out of memory. Tried to allocate 784.00 MiB. GPU 0 has a total capacity of 47.33 GiB of which 8.81 MiB is free. Process 2279413 has 11.11 GiB memory in use. Process 2265701 has 2.37 GiB memory in use. Process 2279412 has 11.11 GiB memory in use. Process 2330873 has 11.11 GiB memory in use. Process 2317160 has 2.37 GiB memory in use. Process 2330872 has 6.78 GiB memory in use. Including non-PyTorch memory, this process has 816.00 MiB memory in use. Process 2330870 has 1.61 GiB memory in use. Of the allocated memory 549.92 MiB is allocated by PyTorch, and 6.08 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)

The above exception was the direct cause of the following exception:

[36mray::ClientAppActor.run()[39m (pid=2330871, ip=115.145.172.224, actor_id=cdc8cff1bec6044f7ef6408701000000, repr=<flwr.simulation.ray_transport.ray_actor.ClientAppActor object at 0x7f75b8199750>)
  File "/home/abdenour/.local/lib/python3.10/site-packages/flwr/simulation/ray_transport/ray_actor.py", line 63, in run
    raise ClientAppException(str(ex)) from ex
flwr.client.client_app.ClientAppException: 
Exception ClientAppException occurred. Message: CUDA out of memory. Tried to allocate 784.00 MiB. GPU 0 has a total capacity of 47.33 GiB of which 8.81 MiB is free. Process 2279413 has 11.11 GiB memory in use. Process 2265701 has 2.37 GiB memory in use. Process 2279412 has 11.11 GiB memory in use. Process 2330873 has 11.11 GiB memory in use. Process 2317160 has 2.37 GiB memory in use. Process 2330872 has 6.78 GiB memory in use. Including non-PyTorch memory, this process has 816.00 MiB memory in use. Process 2330870 has 1.61 GiB memory in use. Of the allocated memory 549.92 MiB is allocated by PyTorch, and 6.08 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)

[2024-09-19 01:24:40,701][flwr][ERROR] - [36mray::ClientAppActor.run()[39m (pid=2330871, ip=115.145.172.224, actor_id=cdc8cff1bec6044f7ef6408701000000, repr=<flwr.simulation.ray_transport.ray_actor.ClientAppActor object at 0x7f75b8199750>)
  File "/home/abdenour/.local/lib/python3.10/site-packages/flwr/client/client_app.py", line 143, in __call__
    return self._call(message, context)
  File "/home/abdenour/.local/lib/python3.10/site-packages/flwr/client/client_app.py", line 126, in ffn
    out_message = handle_legacy_message_from_msgtype(
  File "/home/abdenour/.local/lib/python3.10/site-packages/flwr/client/message_handler/message_handler.py", line 129, in handle_legacy_message_from_msgtype
    fit_res = maybe_call_fit(
  File "/home/abdenour/.local/lib/python3.10/site-packages/flwr/client/client.py", line 234, in maybe_call_fit
    return client.fit(fit_ins)
  File "/home/abdenour/.local/lib/python3.10/site-packages/flwr/client/numpy_client.py", line 238, in _fit
    results = self.numpy_client.fit(parameters, ins.config)  # type: ignore
  File "/home/abdenour/fl_with_flower/fednova_vgg16/fednova_vgg16/client.py", line 69, in fit
    train(self.model, trainloader, self.optimizer, epochs=num_epochs, device=self.device)
  File "/home/abdenour/fl_with_flower/fednova_vgg16/fednova_vgg16/utils.py", line 57, in train
    loss = criterion(net(images), labels)
  File "/home/abdenour/anaconda3/envs/fl/lib/python3.10/site-packages/torch/nn/modules/module.py", line 1553, in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
  File "/home/abdenour/anaconda3/envs/fl/lib/python3.10/site-packages/torch/nn/modules/module.py", line 1562, in _call_impl
    return forward_call(*args, **kwargs)
  File "/home/abdenour/anaconda3/envs/fl/lib/python3.10/site-packages/torchvision/models/vgg.py", line 66, in forward
    x = self.features(x)
  File "/home/abdenour/anaconda3/envs/fl/lib/python3.10/site-packages/torch/nn/modules/module.py", line 1553, in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
  File "/home/abdenour/anaconda3/envs/fl/lib/python3.10/site-packages/torch/nn/modules/module.py", line 1562, in _call_impl
    return forward_call(*args, **kwargs)
  File "/home/abdenour/anaconda3/envs/fl/lib/python3.10/site-packages/torch/nn/modules/container.py", line 219, in forward
    input = module(input)
  File "/home/abdenour/anaconda3/envs/fl/lib/python3.10/site-packages/torch/nn/modules/module.py", line 1553, in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
  File "/home/abdenour/anaconda3/envs/fl/lib/python3.10/site-packages/torch/nn/modules/module.py", line 1562, in _call_impl
    return forward_call(*args, **kwargs)
  File "/home/abdenour/anaconda3/envs/fl/lib/python3.10/site-packages/torch/nn/modules/conv.py", line 458, in forward
    return self._conv_forward(input, self.weight, self.bias)
  File "/home/abdenour/anaconda3/envs/fl/lib/python3.10/site-packages/torch/nn/modules/conv.py", line 454, in _conv_forward
    return F.conv2d(input, weight, bias, self.stride,
torch.OutOfMemoryError: CUDA out of memory. Tried to allocate 784.00 MiB. GPU 0 has a total capacity of 47.33 GiB of which 8.81 MiB is free. Process 2279413 has 11.11 GiB memory in use. Process 2265701 has 2.37 GiB memory in use. Process 2279412 has 11.11 GiB memory in use. Process 2330873 has 11.11 GiB memory in use. Process 2317160 has 2.37 GiB memory in use. Process 2330872 has 6.78 GiB memory in use. Including non-PyTorch memory, this process has 816.00 MiB memory in use. Process 2330870 has 1.61 GiB memory in use. Of the allocated memory 549.92 MiB is allocated by PyTorch, and 6.08 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)

The above exception was the direct cause of the following exception:

[36mray::ClientAppActor.run()[39m (pid=2330871, ip=115.145.172.224, actor_id=cdc8cff1bec6044f7ef6408701000000, repr=<flwr.simulation.ray_transport.ray_actor.ClientAppActor object at 0x7f75b8199750>)
  File "/home/abdenour/.local/lib/python3.10/site-packages/flwr/simulation/ray_transport/ray_actor.py", line 63, in run
    raise ClientAppException(str(ex)) from ex
flwr.client.client_app.ClientAppException: 
Exception ClientAppException occurred. Message: CUDA out of memory. Tried to allocate 784.00 MiB. GPU 0 has a total capacity of 47.33 GiB of which 8.81 MiB is free. Process 2279413 has 11.11 GiB memory in use. Process 2265701 has 2.37 GiB memory in use. Process 2279412 has 11.11 GiB memory in use. Process 2330873 has 11.11 GiB memory in use. Process 2317160 has 2.37 GiB memory in use. Process 2330872 has 6.78 GiB memory in use. Including non-PyTorch memory, this process has 816.00 MiB memory in use. Process 2330870 has 1.61 GiB memory in use. Of the allocated memory 549.92 MiB is allocated by PyTorch, and 6.08 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)
[2024-09-19 01:24:43,160][flwr][ERROR] - Traceback (most recent call last):
  File "/home/abdenour/.local/lib/python3.10/site-packages/flwr/simulation/ray_transport/ray_client_proxy.py", line 94, in _submit_job
    out_mssg, updated_context = self.actor_pool.get_client_result(
  File "/home/abdenour/.local/lib/python3.10/site-packages/flwr/simulation/ray_transport/ray_actor.py", line 398, in get_client_result
    return self._fetch_future_result(cid)
  File "/home/abdenour/.local/lib/python3.10/site-packages/flwr/simulation/ray_transport/ray_actor.py", line 279, in _fetch_future_result
    res_cid, out_mssg, updated_context = ray.get(
  File "/home/abdenour/.local/lib/python3.10/site-packages/ray/_private/auto_init_hook.py", line 21, in auto_init_wrapper
    return fn(*args, **kwargs)
  File "/home/abdenour/.local/lib/python3.10/site-packages/ray/_private/client_mode_hook.py", line 103, in wrapper
    return func(*args, **kwargs)
  File "/home/abdenour/.local/lib/python3.10/site-packages/ray/_private/worker.py", line 2667, in get
    values, debugger_breakpoint = worker.get_objects(object_refs, timeout=timeout)
  File "/home/abdenour/.local/lib/python3.10/site-packages/ray/_private/worker.py", line 864, in get_objects
    raise value.as_instanceof_cause()
ray.exceptions.RayTaskError(ClientAppException): [36mray::ClientAppActor.run()[39m (pid=2330872, ip=115.145.172.224, actor_id=d38096ee5352eefa91005c8e01000000, repr=<flwr.simulation.ray_transport.ray_actor.ClientAppActor object at 0x7fcf0478d750>)
  File "/home/abdenour/.local/lib/python3.10/site-packages/flwr/client/client_app.py", line 143, in __call__
    return self._call(message, context)
  File "/home/abdenour/.local/lib/python3.10/site-packages/flwr/client/client_app.py", line 126, in ffn
    out_message = handle_legacy_message_from_msgtype(
  File "/home/abdenour/.local/lib/python3.10/site-packages/flwr/client/message_handler/message_handler.py", line 129, in handle_legacy_message_from_msgtype
    fit_res = maybe_call_fit(
  File "/home/abdenour/.local/lib/python3.10/site-packages/flwr/client/client.py", line 234, in maybe_call_fit
    return client.fit(fit_ins)
  File "/home/abdenour/.local/lib/python3.10/site-packages/flwr/client/numpy_client.py", line 238, in _fit
    results = self.numpy_client.fit(parameters, ins.config)  # type: ignore
  File "/home/abdenour/fl_with_flower/fednova_vgg16/fednova_vgg16/client.py", line 69, in fit
    train(self.model, trainloader, self.optimizer, epochs=num_epochs, device=self.device)
  File "/home/abdenour/fl_with_flower/fednova_vgg16/fednova_vgg16/utils.py", line 58, in train
    loss.backward()
  File "/home/abdenour/anaconda3/envs/fl/lib/python3.10/site-packages/torch/_tensor.py", line 521, in backward
    torch.autograd.backward(
  File "/home/abdenour/anaconda3/envs/fl/lib/python3.10/site-packages/torch/autograd/__init__.py", line 289, in backward
    _engine_run_backward(
  File "/home/abdenour/anaconda3/envs/fl/lib/python3.10/site-packages/torch/autograd/graph.py", line 769, in _engine_run_backward
    return Variable._execution_engine.run_backward(  # Calls into the C++ engine to run the backward pass
torch.OutOfMemoryError: CUDA out of memory. Tried to allocate 98.00 MiB. GPU 0 has a total capacity of 47.33 GiB of which 8.81 MiB is free. Process 2279413 has 11.11 GiB memory in use. Process 2265701 has 2.37 GiB memory in use. Process 2279412 has 11.11 GiB memory in use. Process 2330873 has 11.11 GiB memory in use. Process 2317160 has 2.37 GiB memory in use. Including non-PyTorch memory, this process has 6.78 GiB memory in use. Process 2330871 has 816.00 MiB memory in use. Process 2330870 has 1.61 GiB memory in use. Of the allocated memory 6.26 GiB is allocated by PyTorch, and 199.47 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)

The above exception was the direct cause of the following exception:

[36mray::ClientAppActor.run()[39m (pid=2330872, ip=115.145.172.224, actor_id=d38096ee5352eefa91005c8e01000000, repr=<flwr.simulation.ray_transport.ray_actor.ClientAppActor object at 0x7fcf0478d750>)
  File "/home/abdenour/.local/lib/python3.10/site-packages/flwr/simulation/ray_transport/ray_actor.py", line 63, in run
    raise ClientAppException(str(ex)) from ex
flwr.client.client_app.ClientAppException: 
Exception ClientAppException occurred. Message: CUDA out of memory. Tried to allocate 98.00 MiB. GPU 0 has a total capacity of 47.33 GiB of which 8.81 MiB is free. Process 2279413 has 11.11 GiB memory in use. Process 2265701 has 2.37 GiB memory in use. Process 2279412 has 11.11 GiB memory in use. Process 2330873 has 11.11 GiB memory in use. Process 2317160 has 2.37 GiB memory in use. Including non-PyTorch memory, this process has 6.78 GiB memory in use. Process 2330871 has 816.00 MiB memory in use. Process 2330870 has 1.61 GiB memory in use. Of the allocated memory 6.26 GiB is allocated by PyTorch, and 199.47 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)

[2024-09-19 01:24:43,160][flwr][ERROR] - [36mray::ClientAppActor.run()[39m (pid=2330872, ip=115.145.172.224, actor_id=d38096ee5352eefa91005c8e01000000, repr=<flwr.simulation.ray_transport.ray_actor.ClientAppActor object at 0x7fcf0478d750>)
  File "/home/abdenour/.local/lib/python3.10/site-packages/flwr/client/client_app.py", line 143, in __call__
    return self._call(message, context)
  File "/home/abdenour/.local/lib/python3.10/site-packages/flwr/client/client_app.py", line 126, in ffn
    out_message = handle_legacy_message_from_msgtype(
  File "/home/abdenour/.local/lib/python3.10/site-packages/flwr/client/message_handler/message_handler.py", line 129, in handle_legacy_message_from_msgtype
    fit_res = maybe_call_fit(
  File "/home/abdenour/.local/lib/python3.10/site-packages/flwr/client/client.py", line 234, in maybe_call_fit
    return client.fit(fit_ins)
  File "/home/abdenour/.local/lib/python3.10/site-packages/flwr/client/numpy_client.py", line 238, in _fit
    results = self.numpy_client.fit(parameters, ins.config)  # type: ignore
  File "/home/abdenour/fl_with_flower/fednova_vgg16/fednova_vgg16/client.py", line 69, in fit
    train(self.model, trainloader, self.optimizer, epochs=num_epochs, device=self.device)
  File "/home/abdenour/fl_with_flower/fednova_vgg16/fednova_vgg16/utils.py", line 58, in train
    loss.backward()
  File "/home/abdenour/anaconda3/envs/fl/lib/python3.10/site-packages/torch/_tensor.py", line 521, in backward
    torch.autograd.backward(
  File "/home/abdenour/anaconda3/envs/fl/lib/python3.10/site-packages/torch/autograd/__init__.py", line 289, in backward
    _engine_run_backward(
  File "/home/abdenour/anaconda3/envs/fl/lib/python3.10/site-packages/torch/autograd/graph.py", line 769, in _engine_run_backward
    return Variable._execution_engine.run_backward(  # Calls into the C++ engine to run the backward pass
torch.OutOfMemoryError: CUDA out of memory. Tried to allocate 98.00 MiB. GPU 0 has a total capacity of 47.33 GiB of which 8.81 MiB is free. Process 2279413 has 11.11 GiB memory in use. Process 2265701 has 2.37 GiB memory in use. Process 2279412 has 11.11 GiB memory in use. Process 2330873 has 11.11 GiB memory in use. Process 2317160 has 2.37 GiB memory in use. Including non-PyTorch memory, this process has 6.78 GiB memory in use. Process 2330871 has 816.00 MiB memory in use. Process 2330870 has 1.61 GiB memory in use. Of the allocated memory 6.26 GiB is allocated by PyTorch, and 199.47 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)

The above exception was the direct cause of the following exception:

[36mray::ClientAppActor.run()[39m (pid=2330872, ip=115.145.172.224, actor_id=d38096ee5352eefa91005c8e01000000, repr=<flwr.simulation.ray_transport.ray_actor.ClientAppActor object at 0x7fcf0478d750>)
  File "/home/abdenour/.local/lib/python3.10/site-packages/flwr/simulation/ray_transport/ray_actor.py", line 63, in run
    raise ClientAppException(str(ex)) from ex
flwr.client.client_app.ClientAppException: 
Exception ClientAppException occurred. Message: CUDA out of memory. Tried to allocate 98.00 MiB. GPU 0 has a total capacity of 47.33 GiB of which 8.81 MiB is free. Process 2279413 has 11.11 GiB memory in use. Process 2265701 has 2.37 GiB memory in use. Process 2279412 has 11.11 GiB memory in use. Process 2330873 has 11.11 GiB memory in use. Process 2317160 has 2.37 GiB memory in use. Including non-PyTorch memory, this process has 6.78 GiB memory in use. Process 2330871 has 816.00 MiB memory in use. Process 2330870 has 1.61 GiB memory in use. Of the allocated memory 6.26 GiB is allocated by PyTorch, and 199.47 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)
[2024-09-19 01:24:43,161][flwr][INFO] - aggregate_fit: received 1 results and 4 failures
[2024-09-19 01:24:55,595][flwr][INFO] - fit progress: (3, 359.4128792285919, {'accuracy': 0.09090909090909091}, 77.00417683785781)
[2024-09-19 01:24:55,596][flwr][INFO] - configure_evaluate: strategy sampled 2 clients (out of 50)
[2024-09-19 01:24:58,950][flwr][ERROR] - Traceback (most recent call last):
  File "/home/abdenour/.local/lib/python3.10/site-packages/flwr/simulation/ray_transport/ray_client_proxy.py", line 94, in _submit_job
    out_mssg, updated_context = self.actor_pool.get_client_result(
  File "/home/abdenour/.local/lib/python3.10/site-packages/flwr/simulation/ray_transport/ray_actor.py", line 398, in get_client_result
    return self._fetch_future_result(cid)
  File "/home/abdenour/.local/lib/python3.10/site-packages/flwr/simulation/ray_transport/ray_actor.py", line 279, in _fetch_future_result
    res_cid, out_mssg, updated_context = ray.get(
  File "/home/abdenour/.local/lib/python3.10/site-packages/ray/_private/auto_init_hook.py", line 21, in auto_init_wrapper
    return fn(*args, **kwargs)
  File "/home/abdenour/.local/lib/python3.10/site-packages/ray/_private/client_mode_hook.py", line 103, in wrapper
    return func(*args, **kwargs)
  File "/home/abdenour/.local/lib/python3.10/site-packages/ray/_private/worker.py", line 2667, in get
    values, debugger_breakpoint = worker.get_objects(object_refs, timeout=timeout)
  File "/home/abdenour/.local/lib/python3.10/site-packages/ray/_private/worker.py", line 864, in get_objects
    raise value.as_instanceof_cause()
ray.exceptions.RayTaskError(ClientAppException): [36mray::ClientAppActor.run()[39m (pid=2330871, ip=115.145.172.224, actor_id=cdc8cff1bec6044f7ef6408701000000, repr=<flwr.simulation.ray_transport.ray_actor.ClientAppActor object at 0x7f75b8199750>)
  File "/home/abdenour/.local/lib/python3.10/site-packages/flwr/client/client_app.py", line 143, in __call__
    return self._call(message, context)
  File "/home/abdenour/.local/lib/python3.10/site-packages/flwr/client/client_app.py", line 126, in ffn
    out_message = handle_legacy_message_from_msgtype(
  File "/home/abdenour/.local/lib/python3.10/site-packages/flwr/client/message_handler/message_handler.py", line 136, in handle_legacy_message_from_msgtype
    evaluate_res = maybe_call_evaluate(
  File "/home/abdenour/.local/lib/python3.10/site-packages/flwr/client/client.py", line 254, in maybe_call_evaluate
    return client.evaluate(evaluate_ins)
  File "/home/abdenour/.local/lib/python3.10/site-packages/flwr/client/numpy_client.py", line 262, in _evaluate
    results = self.numpy_client.evaluate(parameters, ins.config)  # type: ignore
  File "/home/abdenour/fl_with_flower/fednova_vgg16/fednova_vgg16/client.py", line 85, in evaluate
    loss, accuracy = test(self.model, valloader, device=self.device)
  File "/home/abdenour/fl_with_flower/fednova_vgg16/fednova_vgg16/utils.py", line 71, in test
    outputs = net(images)
  File "/home/abdenour/anaconda3/envs/fl/lib/python3.10/site-packages/torch/nn/modules/module.py", line 1553, in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
  File "/home/abdenour/anaconda3/envs/fl/lib/python3.10/site-packages/torch/nn/modules/module.py", line 1562, in _call_impl
    return forward_call(*args, **kwargs)
  File "/home/abdenour/anaconda3/envs/fl/lib/python3.10/site-packages/torchvision/models/vgg.py", line 66, in forward
    x = self.features(x)
  File "/home/abdenour/anaconda3/envs/fl/lib/python3.10/site-packages/torch/nn/modules/module.py", line 1553, in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
  File "/home/abdenour/anaconda3/envs/fl/lib/python3.10/site-packages/torch/nn/modules/module.py", line 1562, in _call_impl
    return forward_call(*args, **kwargs)
  File "/home/abdenour/anaconda3/envs/fl/lib/python3.10/site-packages/torch/nn/modules/container.py", line 219, in forward
    input = module(input)
  File "/home/abdenour/anaconda3/envs/fl/lib/python3.10/site-packages/torch/nn/modules/module.py", line 1553, in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
  File "/home/abdenour/anaconda3/envs/fl/lib/python3.10/site-packages/torch/nn/modules/module.py", line 1562, in _call_impl
    return forward_call(*args, **kwargs)
  File "/home/abdenour/anaconda3/envs/fl/lib/python3.10/site-packages/torch/nn/modules/conv.py", line 458, in forward
    return self._conv_forward(input, self.weight, self.bias)
  File "/home/abdenour/anaconda3/envs/fl/lib/python3.10/site-packages/torch/nn/modules/conv.py", line 454, in _conv_forward
    return F.conv2d(input, weight, bias, self.stride,
torch.OutOfMemoryError: CUDA out of memory. Tried to allocate 282.00 MiB. GPU 0 has a total capacity of 47.33 GiB of which 6.81 MiB is free. Process 2279413 has 11.11 GiB memory in use. Process 2265701 has 2.37 GiB memory in use. Process 2279412 has 11.11 GiB memory in use. Process 2330873 has 11.11 GiB memory in use. Process 2317160 has 2.37 GiB memory in use. Process 2330872 has 6.78 GiB memory in use. Including non-PyTorch memory, this process has 816.00 MiB memory in use. Process 2330870 has 1.61 GiB memory in use. Of the allocated memory 526.37 MiB is allocated by PyTorch, and 29.63 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)

The above exception was the direct cause of the following exception:

[36mray::ClientAppActor.run()[39m (pid=2330871, ip=115.145.172.224, actor_id=cdc8cff1bec6044f7ef6408701000000, repr=<flwr.simulation.ray_transport.ray_actor.ClientAppActor object at 0x7f75b8199750>)
  File "/home/abdenour/.local/lib/python3.10/site-packages/flwr/simulation/ray_transport/ray_actor.py", line 63, in run
    raise ClientAppException(str(ex)) from ex
flwr.client.client_app.ClientAppException: 
Exception ClientAppException occurred. Message: CUDA out of memory. Tried to allocate 282.00 MiB. GPU 0 has a total capacity of 47.33 GiB of which 6.81 MiB is free. Process 2279413 has 11.11 GiB memory in use. Process 2265701 has 2.37 GiB memory in use. Process 2279412 has 11.11 GiB memory in use. Process 2330873 has 11.11 GiB memory in use. Process 2317160 has 2.37 GiB memory in use. Process 2330872 has 6.78 GiB memory in use. Including non-PyTorch memory, this process has 816.00 MiB memory in use. Process 2330870 has 1.61 GiB memory in use. Of the allocated memory 526.37 MiB is allocated by PyTorch, and 29.63 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)

[2024-09-19 01:24:58,951][flwr][ERROR] - [36mray::ClientAppActor.run()[39m (pid=2330871, ip=115.145.172.224, actor_id=cdc8cff1bec6044f7ef6408701000000, repr=<flwr.simulation.ray_transport.ray_actor.ClientAppActor object at 0x7f75b8199750>)
  File "/home/abdenour/.local/lib/python3.10/site-packages/flwr/client/client_app.py", line 143, in __call__
    return self._call(message, context)
  File "/home/abdenour/.local/lib/python3.10/site-packages/flwr/client/client_app.py", line 126, in ffn
    out_message = handle_legacy_message_from_msgtype(
  File "/home/abdenour/.local/lib/python3.10/site-packages/flwr/client/message_handler/message_handler.py", line 136, in handle_legacy_message_from_msgtype
    evaluate_res = maybe_call_evaluate(
  File "/home/abdenour/.local/lib/python3.10/site-packages/flwr/client/client.py", line 254, in maybe_call_evaluate
    return client.evaluate(evaluate_ins)
  File "/home/abdenour/.local/lib/python3.10/site-packages/flwr/client/numpy_client.py", line 262, in _evaluate
    results = self.numpy_client.evaluate(parameters, ins.config)  # type: ignore
  File "/home/abdenour/fl_with_flower/fednova_vgg16/fednova_vgg16/client.py", line 85, in evaluate
    loss, accuracy = test(self.model, valloader, device=self.device)
  File "/home/abdenour/fl_with_flower/fednova_vgg16/fednova_vgg16/utils.py", line 71, in test
    outputs = net(images)
  File "/home/abdenour/anaconda3/envs/fl/lib/python3.10/site-packages/torch/nn/modules/module.py", line 1553, in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
  File "/home/abdenour/anaconda3/envs/fl/lib/python3.10/site-packages/torch/nn/modules/module.py", line 1562, in _call_impl
    return forward_call(*args, **kwargs)
  File "/home/abdenour/anaconda3/envs/fl/lib/python3.10/site-packages/torchvision/models/vgg.py", line 66, in forward
    x = self.features(x)
  File "/home/abdenour/anaconda3/envs/fl/lib/python3.10/site-packages/torch/nn/modules/module.py", line 1553, in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
  File "/home/abdenour/anaconda3/envs/fl/lib/python3.10/site-packages/torch/nn/modules/module.py", line 1562, in _call_impl
    return forward_call(*args, **kwargs)
  File "/home/abdenour/anaconda3/envs/fl/lib/python3.10/site-packages/torch/nn/modules/container.py", line 219, in forward
    input = module(input)
  File "/home/abdenour/anaconda3/envs/fl/lib/python3.10/site-packages/torch/nn/modules/module.py", line 1553, in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
  File "/home/abdenour/anaconda3/envs/fl/lib/python3.10/site-packages/torch/nn/modules/module.py", line 1562, in _call_impl
    return forward_call(*args, **kwargs)
  File "/home/abdenour/anaconda3/envs/fl/lib/python3.10/site-packages/torch/nn/modules/conv.py", line 458, in forward
    return self._conv_forward(input, self.weight, self.bias)
  File "/home/abdenour/anaconda3/envs/fl/lib/python3.10/site-packages/torch/nn/modules/conv.py", line 454, in _conv_forward
    return F.conv2d(input, weight, bias, self.stride,
torch.OutOfMemoryError: CUDA out of memory. Tried to allocate 282.00 MiB. GPU 0 has a total capacity of 47.33 GiB of which 6.81 MiB is free. Process 2279413 has 11.11 GiB memory in use. Process 2265701 has 2.37 GiB memory in use. Process 2279412 has 11.11 GiB memory in use. Process 2330873 has 11.11 GiB memory in use. Process 2317160 has 2.37 GiB memory in use. Process 2330872 has 6.78 GiB memory in use. Including non-PyTorch memory, this process has 816.00 MiB memory in use. Process 2330870 has 1.61 GiB memory in use. Of the allocated memory 526.37 MiB is allocated by PyTorch, and 29.63 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)

The above exception was the direct cause of the following exception:

[36mray::ClientAppActor.run()[39m (pid=2330871, ip=115.145.172.224, actor_id=cdc8cff1bec6044f7ef6408701000000, repr=<flwr.simulation.ray_transport.ray_actor.ClientAppActor object at 0x7f75b8199750>)
  File "/home/abdenour/.local/lib/python3.10/site-packages/flwr/simulation/ray_transport/ray_actor.py", line 63, in run
    raise ClientAppException(str(ex)) from ex
flwr.client.client_app.ClientAppException: 
Exception ClientAppException occurred. Message: CUDA out of memory. Tried to allocate 282.00 MiB. GPU 0 has a total capacity of 47.33 GiB of which 6.81 MiB is free. Process 2279413 has 11.11 GiB memory in use. Process 2265701 has 2.37 GiB memory in use. Process 2279412 has 11.11 GiB memory in use. Process 2330873 has 11.11 GiB memory in use. Process 2317160 has 2.37 GiB memory in use. Process 2330872 has 6.78 GiB memory in use. Including non-PyTorch memory, this process has 816.00 MiB memory in use. Process 2330870 has 1.61 GiB memory in use. Of the allocated memory 526.37 MiB is allocated by PyTorch, and 29.63 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)
[2024-09-19 01:24:58,952][flwr][INFO] - aggregate_evaluate: received 1 results and 1 failures
[2024-09-19 01:24:58,953][flwr][INFO] - 
[2024-09-19 01:24:58,953][flwr][INFO] - [ROUND 4]
[2024-09-19 01:24:58,953][flwr][INFO] - configure_fit: strategy sampled 5 clients (out of 50)
[2024-09-19 01:25:03,110][flwr][ERROR] - Traceback (most recent call last):
  File "/home/abdenour/.local/lib/python3.10/site-packages/flwr/simulation/ray_transport/ray_client_proxy.py", line 94, in _submit_job
    out_mssg, updated_context = self.actor_pool.get_client_result(
  File "/home/abdenour/.local/lib/python3.10/site-packages/flwr/simulation/ray_transport/ray_actor.py", line 398, in get_client_result
    return self._fetch_future_result(cid)
  File "/home/abdenour/.local/lib/python3.10/site-packages/flwr/simulation/ray_transport/ray_actor.py", line 279, in _fetch_future_result
    res_cid, out_mssg, updated_context = ray.get(
  File "/home/abdenour/.local/lib/python3.10/site-packages/ray/_private/auto_init_hook.py", line 21, in auto_init_wrapper
    return fn(*args, **kwargs)
  File "/home/abdenour/.local/lib/python3.10/site-packages/ray/_private/client_mode_hook.py", line 103, in wrapper
    return func(*args, **kwargs)
  File "/home/abdenour/.local/lib/python3.10/site-packages/ray/_private/worker.py", line 2667, in get
    values, debugger_breakpoint = worker.get_objects(object_refs, timeout=timeout)
  File "/home/abdenour/.local/lib/python3.10/site-packages/ray/_private/worker.py", line 864, in get_objects
    raise value.as_instanceof_cause()
ray.exceptions.RayTaskError(ClientAppException): [36mray::ClientAppActor.run()[39m (pid=2330871, ip=115.145.172.224, actor_id=cdc8cff1bec6044f7ef6408701000000, repr=<flwr.simulation.ray_transport.ray_actor.ClientAppActor object at 0x7f75b8199750>)
  File "/home/abdenour/.local/lib/python3.10/site-packages/flwr/client/client_app.py", line 143, in __call__
    return self._call(message, context)
  File "/home/abdenour/.local/lib/python3.10/site-packages/flwr/client/client_app.py", line 126, in ffn
    out_message = handle_legacy_message_from_msgtype(
  File "/home/abdenour/.local/lib/python3.10/site-packages/flwr/client/message_handler/message_handler.py", line 129, in handle_legacy_message_from_msgtype
    fit_res = maybe_call_fit(
  File "/home/abdenour/.local/lib/python3.10/site-packages/flwr/client/client.py", line 234, in maybe_call_fit
    return client.fit(fit_ins)
  File "/home/abdenour/.local/lib/python3.10/site-packages/flwr/client/numpy_client.py", line 238, in _fit
    results = self.numpy_client.fit(parameters, ins.config)  # type: ignore
  File "/home/abdenour/fl_with_flower/fednova_vgg16/fednova_vgg16/client.py", line 69, in fit
    train(self.model, trainloader, self.optimizer, epochs=num_epochs, device=self.device)
  File "/home/abdenour/fl_with_flower/fednova_vgg16/fednova_vgg16/utils.py", line 57, in train
    loss = criterion(net(images), labels)
  File "/home/abdenour/anaconda3/envs/fl/lib/python3.10/site-packages/torch/nn/modules/module.py", line 1553, in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
  File "/home/abdenour/anaconda3/envs/fl/lib/python3.10/site-packages/torch/nn/modules/module.py", line 1562, in _call_impl
    return forward_call(*args, **kwargs)
  File "/home/abdenour/anaconda3/envs/fl/lib/python3.10/site-packages/torchvision/models/vgg.py", line 66, in forward
    x = self.features(x)
  File "/home/abdenour/anaconda3/envs/fl/lib/python3.10/site-packages/torch/nn/modules/module.py", line 1553, in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
  File "/home/abdenour/anaconda3/envs/fl/lib/python3.10/site-packages/torch/nn/modules/module.py", line 1562, in _call_impl
    return forward_call(*args, **kwargs)
  File "/home/abdenour/anaconda3/envs/fl/lib/python3.10/site-packages/torch/nn/modules/container.py", line 219, in forward
    input = module(input)
  File "/home/abdenour/anaconda3/envs/fl/lib/python3.10/site-packages/torch/nn/modules/module.py", line 1553, in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
  File "/home/abdenour/anaconda3/envs/fl/lib/python3.10/site-packages/torch/nn/modules/module.py", line 1562, in _call_impl
    return forward_call(*args, **kwargs)
  File "/home/abdenour/anaconda3/envs/fl/lib/python3.10/site-packages/torch/nn/modules/conv.py", line 458, in forward
    return self._conv_forward(input, self.weight, self.bias)
  File "/home/abdenour/anaconda3/envs/fl/lib/python3.10/site-packages/torch/nn/modules/conv.py", line 454, in _conv_forward
    return F.conv2d(input, weight, bias, self.stride,
torch.OutOfMemoryError: CUDA out of memory. Tried to allocate 784.00 MiB. GPU 0 has a total capacity of 47.33 GiB of which 6.81 MiB is free. Process 2279413 has 11.11 GiB memory in use. Process 2265701 has 2.37 GiB memory in use. Process 2279412 has 11.11 GiB memory in use. Process 2330873 has 11.11 GiB memory in use. Process 2317160 has 2.37 GiB memory in use. Process 2330872 has 6.78 GiB memory in use. Including non-PyTorch memory, this process has 816.00 MiB memory in use. Process 2330870 has 1.61 GiB memory in use. Of the allocated memory 549.92 MiB is allocated by PyTorch, and 6.08 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)

The above exception was the direct cause of the following exception:

[36mray::ClientAppActor.run()[39m (pid=2330871, ip=115.145.172.224, actor_id=cdc8cff1bec6044f7ef6408701000000, repr=<flwr.simulation.ray_transport.ray_actor.ClientAppActor object at 0x7f75b8199750>)
  File "/home/abdenour/.local/lib/python3.10/site-packages/flwr/simulation/ray_transport/ray_actor.py", line 63, in run
    raise ClientAppException(str(ex)) from ex
flwr.client.client_app.ClientAppException: 
Exception ClientAppException occurred. Message: CUDA out of memory. Tried to allocate 784.00 MiB. GPU 0 has a total capacity of 47.33 GiB of which 6.81 MiB is free. Process 2279413 has 11.11 GiB memory in use. Process 2265701 has 2.37 GiB memory in use. Process 2279412 has 11.11 GiB memory in use. Process 2330873 has 11.11 GiB memory in use. Process 2317160 has 2.37 GiB memory in use. Process 2330872 has 6.78 GiB memory in use. Including non-PyTorch memory, this process has 816.00 MiB memory in use. Process 2330870 has 1.61 GiB memory in use. Of the allocated memory 549.92 MiB is allocated by PyTorch, and 6.08 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)

[2024-09-19 01:25:03,110][flwr][ERROR] - [36mray::ClientAppActor.run()[39m (pid=2330871, ip=115.145.172.224, actor_id=cdc8cff1bec6044f7ef6408701000000, repr=<flwr.simulation.ray_transport.ray_actor.ClientAppActor object at 0x7f75b8199750>)
  File "/home/abdenour/.local/lib/python3.10/site-packages/flwr/client/client_app.py", line 143, in __call__
    return self._call(message, context)
  File "/home/abdenour/.local/lib/python3.10/site-packages/flwr/client/client_app.py", line 126, in ffn
    out_message = handle_legacy_message_from_msgtype(
  File "/home/abdenour/.local/lib/python3.10/site-packages/flwr/client/message_handler/message_handler.py", line 129, in handle_legacy_message_from_msgtype
    fit_res = maybe_call_fit(
  File "/home/abdenour/.local/lib/python3.10/site-packages/flwr/client/client.py", line 234, in maybe_call_fit
    return client.fit(fit_ins)
  File "/home/abdenour/.local/lib/python3.10/site-packages/flwr/client/numpy_client.py", line 238, in _fit
    results = self.numpy_client.fit(parameters, ins.config)  # type: ignore
  File "/home/abdenour/fl_with_flower/fednova_vgg16/fednova_vgg16/client.py", line 69, in fit
    train(self.model, trainloader, self.optimizer, epochs=num_epochs, device=self.device)
  File "/home/abdenour/fl_with_flower/fednova_vgg16/fednova_vgg16/utils.py", line 57, in train
    loss = criterion(net(images), labels)
  File "/home/abdenour/anaconda3/envs/fl/lib/python3.10/site-packages/torch/nn/modules/module.py", line 1553, in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
  File "/home/abdenour/anaconda3/envs/fl/lib/python3.10/site-packages/torch/nn/modules/module.py", line 1562, in _call_impl
    return forward_call(*args, **kwargs)
  File "/home/abdenour/anaconda3/envs/fl/lib/python3.10/site-packages/torchvision/models/vgg.py", line 66, in forward
    x = self.features(x)
  File "/home/abdenour/anaconda3/envs/fl/lib/python3.10/site-packages/torch/nn/modules/module.py", line 1553, in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
  File "/home/abdenour/anaconda3/envs/fl/lib/python3.10/site-packages/torch/nn/modules/module.py", line 1562, in _call_impl
    return forward_call(*args, **kwargs)
  File "/home/abdenour/anaconda3/envs/fl/lib/python3.10/site-packages/torch/nn/modules/container.py", line 219, in forward
    input = module(input)
  File "/home/abdenour/anaconda3/envs/fl/lib/python3.10/site-packages/torch/nn/modules/module.py", line 1553, in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
  File "/home/abdenour/anaconda3/envs/fl/lib/python3.10/site-packages/torch/nn/modules/module.py", line 1562, in _call_impl
    return forward_call(*args, **kwargs)
  File "/home/abdenour/anaconda3/envs/fl/lib/python3.10/site-packages/torch/nn/modules/conv.py", line 458, in forward
    return self._conv_forward(input, self.weight, self.bias)
  File "/home/abdenour/anaconda3/envs/fl/lib/python3.10/site-packages/torch/nn/modules/conv.py", line 454, in _conv_forward
    return F.conv2d(input, weight, bias, self.stride,
torch.OutOfMemoryError: CUDA out of memory. Tried to allocate 784.00 MiB. GPU 0 has a total capacity of 47.33 GiB of which 6.81 MiB is free. Process 2279413 has 11.11 GiB memory in use. Process 2265701 has 2.37 GiB memory in use. Process 2279412 has 11.11 GiB memory in use. Process 2330873 has 11.11 GiB memory in use. Process 2317160 has 2.37 GiB memory in use. Process 2330872 has 6.78 GiB memory in use. Including non-PyTorch memory, this process has 816.00 MiB memory in use. Process 2330870 has 1.61 GiB memory in use. Of the allocated memory 549.92 MiB is allocated by PyTorch, and 6.08 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)

The above exception was the direct cause of the following exception:

[36mray::ClientAppActor.run()[39m (pid=2330871, ip=115.145.172.224, actor_id=cdc8cff1bec6044f7ef6408701000000, repr=<flwr.simulation.ray_transport.ray_actor.ClientAppActor object at 0x7f75b8199750>)
  File "/home/abdenour/.local/lib/python3.10/site-packages/flwr/simulation/ray_transport/ray_actor.py", line 63, in run
    raise ClientAppException(str(ex)) from ex
flwr.client.client_app.ClientAppException: 
Exception ClientAppException occurred. Message: CUDA out of memory. Tried to allocate 784.00 MiB. GPU 0 has a total capacity of 47.33 GiB of which 6.81 MiB is free. Process 2279413 has 11.11 GiB memory in use. Process 2265701 has 2.37 GiB memory in use. Process 2279412 has 11.11 GiB memory in use. Process 2330873 has 11.11 GiB memory in use. Process 2317160 has 2.37 GiB memory in use. Process 2330872 has 6.78 GiB memory in use. Including non-PyTorch memory, this process has 816.00 MiB memory in use. Process 2330870 has 1.61 GiB memory in use. Of the allocated memory 549.92 MiB is allocated by PyTorch, and 6.08 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)
[2024-09-19 01:25:03,272][flwr][ERROR] - Traceback (most recent call last):
  File "/home/abdenour/.local/lib/python3.10/site-packages/flwr/simulation/ray_transport/ray_client_proxy.py", line 94, in _submit_job
    out_mssg, updated_context = self.actor_pool.get_client_result(
  File "/home/abdenour/.local/lib/python3.10/site-packages/flwr/simulation/ray_transport/ray_actor.py", line 398, in get_client_result
    return self._fetch_future_result(cid)
  File "/home/abdenour/.local/lib/python3.10/site-packages/flwr/simulation/ray_transport/ray_actor.py", line 279, in _fetch_future_result
    res_cid, out_mssg, updated_context = ray.get(
  File "/home/abdenour/.local/lib/python3.10/site-packages/ray/_private/auto_init_hook.py", line 21, in auto_init_wrapper
    return fn(*args, **kwargs)
  File "/home/abdenour/.local/lib/python3.10/site-packages/ray/_private/client_mode_hook.py", line 103, in wrapper
    return func(*args, **kwargs)
  File "/home/abdenour/.local/lib/python3.10/site-packages/ray/_private/worker.py", line 2667, in get
    values, debugger_breakpoint = worker.get_objects(object_refs, timeout=timeout)
  File "/home/abdenour/.local/lib/python3.10/site-packages/ray/_private/worker.py", line 864, in get_objects
    raise value.as_instanceof_cause()
ray.exceptions.RayTaskError(ClientAppException): [36mray::ClientAppActor.run()[39m (pid=2330872, ip=115.145.172.224, actor_id=d38096ee5352eefa91005c8e01000000, repr=<flwr.simulation.ray_transport.ray_actor.ClientAppActor object at 0x7fcf0478d750>)
  File "/home/abdenour/.local/lib/python3.10/site-packages/flwr/client/client_app.py", line 143, in __call__
    return self._call(message, context)
  File "/home/abdenour/.local/lib/python3.10/site-packages/flwr/client/client_app.py", line 126, in ffn
    out_message = handle_legacy_message_from_msgtype(
  File "/home/abdenour/.local/lib/python3.10/site-packages/flwr/client/message_handler/message_handler.py", line 129, in handle_legacy_message_from_msgtype
    fit_res = maybe_call_fit(
  File "/home/abdenour/.local/lib/python3.10/site-packages/flwr/client/client.py", line 234, in maybe_call_fit
    return client.fit(fit_ins)
  File "/home/abdenour/.local/lib/python3.10/site-packages/flwr/client/numpy_client.py", line 238, in _fit
    results = self.numpy_client.fit(parameters, ins.config)  # type: ignore
  File "/home/abdenour/fl_with_flower/fednova_vgg16/fednova_vgg16/client.py", line 69, in fit
    train(self.model, trainloader, self.optimizer, epochs=num_epochs, device=self.device)
  File "/home/abdenour/fl_with_flower/fednova_vgg16/fednova_vgg16/utils.py", line 58, in train
    loss.backward()
  File "/home/abdenour/anaconda3/envs/fl/lib/python3.10/site-packages/torch/_tensor.py", line 521, in backward
    torch.autograd.backward(
  File "/home/abdenour/anaconda3/envs/fl/lib/python3.10/site-packages/torch/autograd/__init__.py", line 289, in backward
    _engine_run_backward(
  File "/home/abdenour/anaconda3/envs/fl/lib/python3.10/site-packages/torch/autograd/graph.py", line 769, in _engine_run_backward
    return Variable._execution_engine.run_backward(  # Calls into the C++ engine to run the backward pass
torch.OutOfMemoryError: CUDA out of memory. Tried to allocate 98.00 MiB. GPU 0 has a total capacity of 47.33 GiB of which 6.81 MiB is free. Process 2279413 has 11.11 GiB memory in use. Process 2265701 has 2.37 GiB memory in use. Process 2279412 has 11.11 GiB memory in use. Process 2330873 has 11.11 GiB memory in use. Process 2317160 has 2.37 GiB memory in use. Including non-PyTorch memory, this process has 6.78 GiB memory in use. Process 2330871 has 816.00 MiB memory in use. Process 2330870 has 1.61 GiB memory in use. Of the allocated memory 6.26 GiB is allocated by PyTorch, and 199.47 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)

The above exception was the direct cause of the following exception:

[36mray::ClientAppActor.run()[39m (pid=2330872, ip=115.145.172.224, actor_id=d38096ee5352eefa91005c8e01000000, repr=<flwr.simulation.ray_transport.ray_actor.ClientAppActor object at 0x7fcf0478d750>)
  File "/home/abdenour/.local/lib/python3.10/site-packages/flwr/simulation/ray_transport/ray_actor.py", line 63, in run
    raise ClientAppException(str(ex)) from ex
flwr.client.client_app.ClientAppException: 
Exception ClientAppException occurred. Message: CUDA out of memory. Tried to allocate 98.00 MiB. GPU 0 has a total capacity of 47.33 GiB of which 6.81 MiB is free. Process 2279413 has 11.11 GiB memory in use. Process 2265701 has 2.37 GiB memory in use. Process 2279412 has 11.11 GiB memory in use. Process 2330873 has 11.11 GiB memory in use. Process 2317160 has 2.37 GiB memory in use. Including non-PyTorch memory, this process has 6.78 GiB memory in use. Process 2330871 has 816.00 MiB memory in use. Process 2330870 has 1.61 GiB memory in use. Of the allocated memory 6.26 GiB is allocated by PyTorch, and 199.47 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)

[2024-09-19 01:25:03,273][flwr][ERROR] - [36mray::ClientAppActor.run()[39m (pid=2330872, ip=115.145.172.224, actor_id=d38096ee5352eefa91005c8e01000000, repr=<flwr.simulation.ray_transport.ray_actor.ClientAppActor object at 0x7fcf0478d750>)
  File "/home/abdenour/.local/lib/python3.10/site-packages/flwr/client/client_app.py", line 143, in __call__
    return self._call(message, context)
  File "/home/abdenour/.local/lib/python3.10/site-packages/flwr/client/client_app.py", line 126, in ffn
    out_message = handle_legacy_message_from_msgtype(
  File "/home/abdenour/.local/lib/python3.10/site-packages/flwr/client/message_handler/message_handler.py", line 129, in handle_legacy_message_from_msgtype
    fit_res = maybe_call_fit(
  File "/home/abdenour/.local/lib/python3.10/site-packages/flwr/client/client.py", line 234, in maybe_call_fit
    return client.fit(fit_ins)
  File "/home/abdenour/.local/lib/python3.10/site-packages/flwr/client/numpy_client.py", line 238, in _fit
    results = self.numpy_client.fit(parameters, ins.config)  # type: ignore
  File "/home/abdenour/fl_with_flower/fednova_vgg16/fednova_vgg16/client.py", line 69, in fit
    train(self.model, trainloader, self.optimizer, epochs=num_epochs, device=self.device)
  File "/home/abdenour/fl_with_flower/fednova_vgg16/fednova_vgg16/utils.py", line 58, in train
    loss.backward()
  File "/home/abdenour/anaconda3/envs/fl/lib/python3.10/site-packages/torch/_tensor.py", line 521, in backward
    torch.autograd.backward(
  File "/home/abdenour/anaconda3/envs/fl/lib/python3.10/site-packages/torch/autograd/__init__.py", line 289, in backward
    _engine_run_backward(
  File "/home/abdenour/anaconda3/envs/fl/lib/python3.10/site-packages/torch/autograd/graph.py", line 769, in _engine_run_backward
    return Variable._execution_engine.run_backward(  # Calls into the C++ engine to run the backward pass
torch.OutOfMemoryError: CUDA out of memory. Tried to allocate 98.00 MiB. GPU 0 has a total capacity of 47.33 GiB of which 6.81 MiB is free. Process 2279413 has 11.11 GiB memory in use. Process 2265701 has 2.37 GiB memory in use. Process 2279412 has 11.11 GiB memory in use. Process 2330873 has 11.11 GiB memory in use. Process 2317160 has 2.37 GiB memory in use. Including non-PyTorch memory, this process has 6.78 GiB memory in use. Process 2330871 has 816.00 MiB memory in use. Process 2330870 has 1.61 GiB memory in use. Of the allocated memory 6.26 GiB is allocated by PyTorch, and 199.47 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)

The above exception was the direct cause of the following exception:

[36mray::ClientAppActor.run()[39m (pid=2330872, ip=115.145.172.224, actor_id=d38096ee5352eefa91005c8e01000000, repr=<flwr.simulation.ray_transport.ray_actor.ClientAppActor object at 0x7fcf0478d750>)
  File "/home/abdenour/.local/lib/python3.10/site-packages/flwr/simulation/ray_transport/ray_actor.py", line 63, in run
    raise ClientAppException(str(ex)) from ex
flwr.client.client_app.ClientAppException: 
Exception ClientAppException occurred. Message: CUDA out of memory. Tried to allocate 98.00 MiB. GPU 0 has a total capacity of 47.33 GiB of which 6.81 MiB is free. Process 2279413 has 11.11 GiB memory in use. Process 2265701 has 2.37 GiB memory in use. Process 2279412 has 11.11 GiB memory in use. Process 2330873 has 11.11 GiB memory in use. Process 2317160 has 2.37 GiB memory in use. Including non-PyTorch memory, this process has 6.78 GiB memory in use. Process 2330871 has 816.00 MiB memory in use. Process 2330870 has 1.61 GiB memory in use. Of the allocated memory 6.26 GiB is allocated by PyTorch, and 199.47 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)
[2024-09-19 01:25:04,219][flwr][ERROR] - Traceback (most recent call last):
  File "/home/abdenour/.local/lib/python3.10/site-packages/flwr/simulation/ray_transport/ray_client_proxy.py", line 94, in _submit_job
    out_mssg, updated_context = self.actor_pool.get_client_result(
  File "/home/abdenour/.local/lib/python3.10/site-packages/flwr/simulation/ray_transport/ray_actor.py", line 398, in get_client_result
    return self._fetch_future_result(cid)
  File "/home/abdenour/.local/lib/python3.10/site-packages/flwr/simulation/ray_transport/ray_actor.py", line 279, in _fetch_future_result
    res_cid, out_mssg, updated_context = ray.get(
  File "/home/abdenour/.local/lib/python3.10/site-packages/ray/_private/auto_init_hook.py", line 21, in auto_init_wrapper
    return fn(*args, **kwargs)
  File "/home/abdenour/.local/lib/python3.10/site-packages/ray/_private/client_mode_hook.py", line 103, in wrapper
    return func(*args, **kwargs)
  File "/home/abdenour/.local/lib/python3.10/site-packages/ray/_private/worker.py", line 2667, in get
    values, debugger_breakpoint = worker.get_objects(object_refs, timeout=timeout)
  File "/home/abdenour/.local/lib/python3.10/site-packages/ray/_private/worker.py", line 864, in get_objects
    raise value.as_instanceof_cause()
ray.exceptions.RayTaskError(ClientAppException): [36mray::ClientAppActor.run()[39m (pid=2330870, ip=115.145.172.224, actor_id=18884b649bbd3d098a6ec02b01000000, repr=<flwr.simulation.ray_transport.ray_actor.ClientAppActor object at 0x7ffb805757b0>)
  File "/home/abdenour/.local/lib/python3.10/site-packages/flwr/client/client_app.py", line 143, in __call__
    return self._call(message, context)
  File "/home/abdenour/.local/lib/python3.10/site-packages/flwr/client/client_app.py", line 126, in ffn
    out_message = handle_legacy_message_from_msgtype(
  File "/home/abdenour/.local/lib/python3.10/site-packages/flwr/client/message_handler/message_handler.py", line 129, in handle_legacy_message_from_msgtype
    fit_res = maybe_call_fit(
  File "/home/abdenour/.local/lib/python3.10/site-packages/flwr/client/client.py", line 234, in maybe_call_fit
    return client.fit(fit_ins)
  File "/home/abdenour/.local/lib/python3.10/site-packages/flwr/client/numpy_client.py", line 238, in _fit
    results = self.numpy_client.fit(parameters, ins.config)  # type: ignore
  File "/home/abdenour/fl_with_flower/fednova_vgg16/fednova_vgg16/client.py", line 69, in fit
    train(self.model, trainloader, self.optimizer, epochs=num_epochs, device=self.device)
  File "/home/abdenour/fl_with_flower/fednova_vgg16/fednova_vgg16/utils.py", line 57, in train
    loss = criterion(net(images), labels)
  File "/home/abdenour/anaconda3/envs/fl/lib/python3.10/site-packages/torch/nn/modules/module.py", line 1553, in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
  File "/home/abdenour/anaconda3/envs/fl/lib/python3.10/site-packages/torch/nn/modules/module.py", line 1562, in _call_impl
    return forward_call(*args, **kwargs)
  File "/home/abdenour/anaconda3/envs/fl/lib/python3.10/site-packages/torchvision/models/vgg.py", line 66, in forward
    x = self.features(x)
  File "/home/abdenour/anaconda3/envs/fl/lib/python3.10/site-packages/torch/nn/modules/module.py", line 1553, in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
  File "/home/abdenour/anaconda3/envs/fl/lib/python3.10/site-packages/torch/nn/modules/module.py", line 1562, in _call_impl
    return forward_call(*args, **kwargs)
  File "/home/abdenour/anaconda3/envs/fl/lib/python3.10/site-packages/torch/nn/modules/container.py", line 219, in forward
    input = module(input)
  File "/home/abdenour/anaconda3/envs/fl/lib/python3.10/site-packages/torch/nn/modules/module.py", line 1553, in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
  File "/home/abdenour/anaconda3/envs/fl/lib/python3.10/site-packages/torch/nn/modules/module.py", line 1562, in _call_impl
    return forward_call(*args, **kwargs)
  File "/home/abdenour/anaconda3/envs/fl/lib/python3.10/site-packages/torch/nn/modules/conv.py", line 458, in forward
    return self._conv_forward(input, self.weight, self.bias)
  File "/home/abdenour/anaconda3/envs/fl/lib/python3.10/site-packages/torch/nn/modules/conv.py", line 454, in _conv_forward
    return F.conv2d(input, weight, bias, self.stride,
torch.OutOfMemoryError: CUDA out of memory. Tried to allocate 784.00 MiB. GPU 0 has a total capacity of 47.33 GiB of which 6.81 MiB is free. Process 2279413 has 11.11 GiB memory in use. Process 2265701 has 2.37 GiB memory in use. Process 2279412 has 11.11 GiB memory in use. Process 2330873 has 11.11 GiB memory in use. Process 2317160 has 2.37 GiB memory in use. Process 2330872 has 6.78 GiB memory in use. Process 2330871 has 816.00 MiB memory in use. Including non-PyTorch memory, this process has 1.61 GiB memory in use. Of the allocated memory 1.30 GiB is allocated by PyTorch, and 6.08 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)

The above exception was the direct cause of the following exception:

[36mray::ClientAppActor.run()[39m (pid=2330870, ip=115.145.172.224, actor_id=18884b649bbd3d098a6ec02b01000000, repr=<flwr.simulation.ray_transport.ray_actor.ClientAppActor object at 0x7ffb805757b0>)
  File "/home/abdenour/.local/lib/python3.10/site-packages/flwr/simulation/ray_transport/ray_actor.py", line 63, in run
    raise ClientAppException(str(ex)) from ex
flwr.client.client_app.ClientAppException: 
Exception ClientAppException occurred. Message: CUDA out of memory. Tried to allocate 784.00 MiB. GPU 0 has a total capacity of 47.33 GiB of which 6.81 MiB is free. Process 2279413 has 11.11 GiB memory in use. Process 2265701 has 2.37 GiB memory in use. Process 2279412 has 11.11 GiB memory in use. Process 2330873 has 11.11 GiB memory in use. Process 2317160 has 2.37 GiB memory in use. Process 2330872 has 6.78 GiB memory in use. Process 2330871 has 816.00 MiB memory in use. Including non-PyTorch memory, this process has 1.61 GiB memory in use. Of the allocated memory 1.30 GiB is allocated by PyTorch, and 6.08 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)

[2024-09-19 01:25:04,220][flwr][ERROR] - [36mray::ClientAppActor.run()[39m (pid=2330870, ip=115.145.172.224, actor_id=18884b649bbd3d098a6ec02b01000000, repr=<flwr.simulation.ray_transport.ray_actor.ClientAppActor object at 0x7ffb805757b0>)
  File "/home/abdenour/.local/lib/python3.10/site-packages/flwr/client/client_app.py", line 143, in __call__
    return self._call(message, context)
  File "/home/abdenour/.local/lib/python3.10/site-packages/flwr/client/client_app.py", line 126, in ffn
    out_message = handle_legacy_message_from_msgtype(
  File "/home/abdenour/.local/lib/python3.10/site-packages/flwr/client/message_handler/message_handler.py", line 129, in handle_legacy_message_from_msgtype
    fit_res = maybe_call_fit(
  File "/home/abdenour/.local/lib/python3.10/site-packages/flwr/client/client.py", line 234, in maybe_call_fit
    return client.fit(fit_ins)
  File "/home/abdenour/.local/lib/python3.10/site-packages/flwr/client/numpy_client.py", line 238, in _fit
    results = self.numpy_client.fit(parameters, ins.config)  # type: ignore
  File "/home/abdenour/fl_with_flower/fednova_vgg16/fednova_vgg16/client.py", line 69, in fit
    train(self.model, trainloader, self.optimizer, epochs=num_epochs, device=self.device)
  File "/home/abdenour/fl_with_flower/fednova_vgg16/fednova_vgg16/utils.py", line 57, in train
    loss = criterion(net(images), labels)
  File "/home/abdenour/anaconda3/envs/fl/lib/python3.10/site-packages/torch/nn/modules/module.py", line 1553, in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
  File "/home/abdenour/anaconda3/envs/fl/lib/python3.10/site-packages/torch/nn/modules/module.py", line 1562, in _call_impl
    return forward_call(*args, **kwargs)
  File "/home/abdenour/anaconda3/envs/fl/lib/python3.10/site-packages/torchvision/models/vgg.py", line 66, in forward
    x = self.features(x)
  File "/home/abdenour/anaconda3/envs/fl/lib/python3.10/site-packages/torch/nn/modules/module.py", line 1553, in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
  File "/home/abdenour/anaconda3/envs/fl/lib/python3.10/site-packages/torch/nn/modules/module.py", line 1562, in _call_impl
    return forward_call(*args, **kwargs)
  File "/home/abdenour/anaconda3/envs/fl/lib/python3.10/site-packages/torch/nn/modules/container.py", line 219, in forward
    input = module(input)
  File "/home/abdenour/anaconda3/envs/fl/lib/python3.10/site-packages/torch/nn/modules/module.py", line 1553, in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
  File "/home/abdenour/anaconda3/envs/fl/lib/python3.10/site-packages/torch/nn/modules/module.py", line 1562, in _call_impl
    return forward_call(*args, **kwargs)
  File "/home/abdenour/anaconda3/envs/fl/lib/python3.10/site-packages/torch/nn/modules/conv.py", line 458, in forward
    return self._conv_forward(input, self.weight, self.bias)
  File "/home/abdenour/anaconda3/envs/fl/lib/python3.10/site-packages/torch/nn/modules/conv.py", line 454, in _conv_forward
    return F.conv2d(input, weight, bias, self.stride,
torch.OutOfMemoryError: CUDA out of memory. Tried to allocate 784.00 MiB. GPU 0 has a total capacity of 47.33 GiB of which 6.81 MiB is free. Process 2279413 has 11.11 GiB memory in use. Process 2265701 has 2.37 GiB memory in use. Process 2279412 has 11.11 GiB memory in use. Process 2330873 has 11.11 GiB memory in use. Process 2317160 has 2.37 GiB memory in use. Process 2330872 has 6.78 GiB memory in use. Process 2330871 has 816.00 MiB memory in use. Including non-PyTorch memory, this process has 1.61 GiB memory in use. Of the allocated memory 1.30 GiB is allocated by PyTorch, and 6.08 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)

The above exception was the direct cause of the following exception:

[36mray::ClientAppActor.run()[39m (pid=2330870, ip=115.145.172.224, actor_id=18884b649bbd3d098a6ec02b01000000, repr=<flwr.simulation.ray_transport.ray_actor.ClientAppActor object at 0x7ffb805757b0>)
  File "/home/abdenour/.local/lib/python3.10/site-packages/flwr/simulation/ray_transport/ray_actor.py", line 63, in run
    raise ClientAppException(str(ex)) from ex
flwr.client.client_app.ClientAppException: 
Exception ClientAppException occurred. Message: CUDA out of memory. Tried to allocate 784.00 MiB. GPU 0 has a total capacity of 47.33 GiB of which 6.81 MiB is free. Process 2279413 has 11.11 GiB memory in use. Process 2265701 has 2.37 GiB memory in use. Process 2279412 has 11.11 GiB memory in use. Process 2330873 has 11.11 GiB memory in use. Process 2317160 has 2.37 GiB memory in use. Process 2330872 has 6.78 GiB memory in use. Process 2330871 has 816.00 MiB memory in use. Including non-PyTorch memory, this process has 1.61 GiB memory in use. Of the allocated memory 1.30 GiB is allocated by PyTorch, and 6.08 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)
[2024-09-19 01:25:05,268][flwr][ERROR] - Traceback (most recent call last):
  File "/home/abdenour/.local/lib/python3.10/site-packages/flwr/simulation/ray_transport/ray_client_proxy.py", line 94, in _submit_job
    out_mssg, updated_context = self.actor_pool.get_client_result(
  File "/home/abdenour/.local/lib/python3.10/site-packages/flwr/simulation/ray_transport/ray_actor.py", line 398, in get_client_result
    return self._fetch_future_result(cid)
  File "/home/abdenour/.local/lib/python3.10/site-packages/flwr/simulation/ray_transport/ray_actor.py", line 279, in _fetch_future_result
    res_cid, out_mssg, updated_context = ray.get(
  File "/home/abdenour/.local/lib/python3.10/site-packages/ray/_private/auto_init_hook.py", line 21, in auto_init_wrapper
    return fn(*args, **kwargs)
  File "/home/abdenour/.local/lib/python3.10/site-packages/ray/_private/client_mode_hook.py", line 103, in wrapper
    return func(*args, **kwargs)
  File "/home/abdenour/.local/lib/python3.10/site-packages/ray/_private/worker.py", line 2667, in get
    values, debugger_breakpoint = worker.get_objects(object_refs, timeout=timeout)
  File "/home/abdenour/.local/lib/python3.10/site-packages/ray/_private/worker.py", line 864, in get_objects
    raise value.as_instanceof_cause()
ray.exceptions.RayTaskError(ClientAppException): [36mray::ClientAppActor.run()[39m (pid=2330871, ip=115.145.172.224, actor_id=cdc8cff1bec6044f7ef6408701000000, repr=<flwr.simulation.ray_transport.ray_actor.ClientAppActor object at 0x7f75b8199750>)
  File "/home/abdenour/.local/lib/python3.10/site-packages/flwr/client/client_app.py", line 143, in __call__
    return self._call(message, context)
  File "/home/abdenour/.local/lib/python3.10/site-packages/flwr/client/client_app.py", line 126, in ffn
    out_message = handle_legacy_message_from_msgtype(
  File "/home/abdenour/.local/lib/python3.10/site-packages/flwr/client/message_handler/message_handler.py", line 129, in handle_legacy_message_from_msgtype
    fit_res = maybe_call_fit(
  File "/home/abdenour/.local/lib/python3.10/site-packages/flwr/client/client.py", line 234, in maybe_call_fit
    return client.fit(fit_ins)
  File "/home/abdenour/.local/lib/python3.10/site-packages/flwr/client/numpy_client.py", line 238, in _fit
    results = self.numpy_client.fit(parameters, ins.config)  # type: ignore
  File "/home/abdenour/fl_with_flower/fednova_vgg16/fednova_vgg16/client.py", line 69, in fit
    train(self.model, trainloader, self.optimizer, epochs=num_epochs, device=self.device)
  File "/home/abdenour/fl_with_flower/fednova_vgg16/fednova_vgg16/utils.py", line 57, in train
    loss = criterion(net(images), labels)
  File "/home/abdenour/anaconda3/envs/fl/lib/python3.10/site-packages/torch/nn/modules/module.py", line 1553, in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
  File "/home/abdenour/anaconda3/envs/fl/lib/python3.10/site-packages/torch/nn/modules/module.py", line 1562, in _call_impl
    return forward_call(*args, **kwargs)
  File "/home/abdenour/anaconda3/envs/fl/lib/python3.10/site-packages/torchvision/models/vgg.py", line 66, in forward
    x = self.features(x)
  File "/home/abdenour/anaconda3/envs/fl/lib/python3.10/site-packages/torch/nn/modules/module.py", line 1553, in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
  File "/home/abdenour/anaconda3/envs/fl/lib/python3.10/site-packages/torch/nn/modules/module.py", line 1562, in _call_impl
    return forward_call(*args, **kwargs)
  File "/home/abdenour/anaconda3/envs/fl/lib/python3.10/site-packages/torch/nn/modules/container.py", line 219, in forward
    input = module(input)
  File "/home/abdenour/anaconda3/envs/fl/lib/python3.10/site-packages/torch/nn/modules/module.py", line 1553, in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
  File "/home/abdenour/anaconda3/envs/fl/lib/python3.10/site-packages/torch/nn/modules/module.py", line 1562, in _call_impl
    return forward_call(*args, **kwargs)
  File "/home/abdenour/anaconda3/envs/fl/lib/python3.10/site-packages/torch/nn/modules/conv.py", line 458, in forward
    return self._conv_forward(input, self.weight, self.bias)
  File "/home/abdenour/anaconda3/envs/fl/lib/python3.10/site-packages/torch/nn/modules/conv.py", line 454, in _conv_forward
    return F.conv2d(input, weight, bias, self.stride,
torch.OutOfMemoryError: CUDA out of memory. Tried to allocate 784.00 MiB. GPU 0 has a total capacity of 47.33 GiB of which 6.81 MiB is free. Process 2279413 has 11.11 GiB memory in use. Process 2265701 has 2.37 GiB memory in use. Process 2279412 has 11.11 GiB memory in use. Process 2330873 has 11.11 GiB memory in use. Process 2317160 has 2.37 GiB memory in use. Process 2330872 has 6.78 GiB memory in use. Including non-PyTorch memory, this process has 816.00 MiB memory in use. Process 2330870 has 1.61 GiB memory in use. Of the allocated memory 549.92 MiB is allocated by PyTorch, and 6.08 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)

The above exception was the direct cause of the following exception:

[36mray::ClientAppActor.run()[39m (pid=2330871, ip=115.145.172.224, actor_id=cdc8cff1bec6044f7ef6408701000000, repr=<flwr.simulation.ray_transport.ray_actor.ClientAppActor object at 0x7f75b8199750>)
  File "/home/abdenour/.local/lib/python3.10/site-packages/flwr/simulation/ray_transport/ray_actor.py", line 63, in run
    raise ClientAppException(str(ex)) from ex
flwr.client.client_app.ClientAppException: 
Exception ClientAppException occurred. Message: CUDA out of memory. Tried to allocate 784.00 MiB. GPU 0 has a total capacity of 47.33 GiB of which 6.81 MiB is free. Process 2279413 has 11.11 GiB memory in use. Process 2265701 has 2.37 GiB memory in use. Process 2279412 has 11.11 GiB memory in use. Process 2330873 has 11.11 GiB memory in use. Process 2317160 has 2.37 GiB memory in use. Process 2330872 has 6.78 GiB memory in use. Including non-PyTorch memory, this process has 816.00 MiB memory in use. Process 2330870 has 1.61 GiB memory in use. Of the allocated memory 549.92 MiB is allocated by PyTorch, and 6.08 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)

[2024-09-19 01:25:05,268][flwr][ERROR] - [36mray::ClientAppActor.run()[39m (pid=2330871, ip=115.145.172.224, actor_id=cdc8cff1bec6044f7ef6408701000000, repr=<flwr.simulation.ray_transport.ray_actor.ClientAppActor object at 0x7f75b8199750>)
  File "/home/abdenour/.local/lib/python3.10/site-packages/flwr/client/client_app.py", line 143, in __call__
    return self._call(message, context)
  File "/home/abdenour/.local/lib/python3.10/site-packages/flwr/client/client_app.py", line 126, in ffn
    out_message = handle_legacy_message_from_msgtype(
  File "/home/abdenour/.local/lib/python3.10/site-packages/flwr/client/message_handler/message_handler.py", line 129, in handle_legacy_message_from_msgtype
    fit_res = maybe_call_fit(
  File "/home/abdenour/.local/lib/python3.10/site-packages/flwr/client/client.py", line 234, in maybe_call_fit
    return client.fit(fit_ins)
  File "/home/abdenour/.local/lib/python3.10/site-packages/flwr/client/numpy_client.py", line 238, in _fit
    results = self.numpy_client.fit(parameters, ins.config)  # type: ignore
  File "/home/abdenour/fl_with_flower/fednova_vgg16/fednova_vgg16/client.py", line 69, in fit
    train(self.model, trainloader, self.optimizer, epochs=num_epochs, device=self.device)
  File "/home/abdenour/fl_with_flower/fednova_vgg16/fednova_vgg16/utils.py", line 57, in train
    loss = criterion(net(images), labels)
  File "/home/abdenour/anaconda3/envs/fl/lib/python3.10/site-packages/torch/nn/modules/module.py", line 1553, in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
  File "/home/abdenour/anaconda3/envs/fl/lib/python3.10/site-packages/torch/nn/modules/module.py", line 1562, in _call_impl
    return forward_call(*args, **kwargs)
  File "/home/abdenour/anaconda3/envs/fl/lib/python3.10/site-packages/torchvision/models/vgg.py", line 66, in forward
    x = self.features(x)
  File "/home/abdenour/anaconda3/envs/fl/lib/python3.10/site-packages/torch/nn/modules/module.py", line 1553, in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
  File "/home/abdenour/anaconda3/envs/fl/lib/python3.10/site-packages/torch/nn/modules/module.py", line 1562, in _call_impl
    return forward_call(*args, **kwargs)
  File "/home/abdenour/anaconda3/envs/fl/lib/python3.10/site-packages/torch/nn/modules/container.py", line 219, in forward
    input = module(input)
  File "/home/abdenour/anaconda3/envs/fl/lib/python3.10/site-packages/torch/nn/modules/module.py", line 1553, in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
  File "/home/abdenour/anaconda3/envs/fl/lib/python3.10/site-packages/torch/nn/modules/module.py", line 1562, in _call_impl
    return forward_call(*args, **kwargs)
  File "/home/abdenour/anaconda3/envs/fl/lib/python3.10/site-packages/torch/nn/modules/conv.py", line 458, in forward
    return self._conv_forward(input, self.weight, self.bias)
  File "/home/abdenour/anaconda3/envs/fl/lib/python3.10/site-packages/torch/nn/modules/conv.py", line 454, in _conv_forward
    return F.conv2d(input, weight, bias, self.stride,
torch.OutOfMemoryError: CUDA out of memory. Tried to allocate 784.00 MiB. GPU 0 has a total capacity of 47.33 GiB of which 6.81 MiB is free. Process 2279413 has 11.11 GiB memory in use. Process 2265701 has 2.37 GiB memory in use. Process 2279412 has 11.11 GiB memory in use. Process 2330873 has 11.11 GiB memory in use. Process 2317160 has 2.37 GiB memory in use. Process 2330872 has 6.78 GiB memory in use. Including non-PyTorch memory, this process has 816.00 MiB memory in use. Process 2330870 has 1.61 GiB memory in use. Of the allocated memory 549.92 MiB is allocated by PyTorch, and 6.08 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)

The above exception was the direct cause of the following exception:

[36mray::ClientAppActor.run()[39m (pid=2330871, ip=115.145.172.224, actor_id=cdc8cff1bec6044f7ef6408701000000, repr=<flwr.simulation.ray_transport.ray_actor.ClientAppActor object at 0x7f75b8199750>)
  File "/home/abdenour/.local/lib/python3.10/site-packages/flwr/simulation/ray_transport/ray_actor.py", line 63, in run
    raise ClientAppException(str(ex)) from ex
flwr.client.client_app.ClientAppException: 
Exception ClientAppException occurred. Message: CUDA out of memory. Tried to allocate 784.00 MiB. GPU 0 has a total capacity of 47.33 GiB of which 6.81 MiB is free. Process 2279413 has 11.11 GiB memory in use. Process 2265701 has 2.37 GiB memory in use. Process 2279412 has 11.11 GiB memory in use. Process 2330873 has 11.11 GiB memory in use. Process 2317160 has 2.37 GiB memory in use. Process 2330872 has 6.78 GiB memory in use. Including non-PyTorch memory, this process has 816.00 MiB memory in use. Process 2330870 has 1.61 GiB memory in use. Of the allocated memory 549.92 MiB is allocated by PyTorch, and 6.08 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)
[2024-09-19 01:25:11,528][flwr][INFO] - aggregate_fit: received 1 results and 4 failures
[2024-09-19 01:25:23,278][flwr][INFO] - fit progress: (4, 517.2652096748352, {'accuracy': 0.1824028061970184}, 104.68684218125418)
[2024-09-19 01:25:23,282][flwr][INFO] - configure_evaluate: strategy sampled 2 clients (out of 50)
[2024-09-19 01:25:26,383][flwr][ERROR] - Traceback (most recent call last):
  File "/home/abdenour/.local/lib/python3.10/site-packages/flwr/simulation/ray_transport/ray_client_proxy.py", line 94, in _submit_job
    out_mssg, updated_context = self.actor_pool.get_client_result(
  File "/home/abdenour/.local/lib/python3.10/site-packages/flwr/simulation/ray_transport/ray_actor.py", line 398, in get_client_result
    return self._fetch_future_result(cid)
  File "/home/abdenour/.local/lib/python3.10/site-packages/flwr/simulation/ray_transport/ray_actor.py", line 279, in _fetch_future_result
    res_cid, out_mssg, updated_context = ray.get(
  File "/home/abdenour/.local/lib/python3.10/site-packages/ray/_private/auto_init_hook.py", line 21, in auto_init_wrapper
    return fn(*args, **kwargs)
  File "/home/abdenour/.local/lib/python3.10/site-packages/ray/_private/client_mode_hook.py", line 103, in wrapper
    return func(*args, **kwargs)
  File "/home/abdenour/.local/lib/python3.10/site-packages/ray/_private/worker.py", line 2667, in get
    values, debugger_breakpoint = worker.get_objects(object_refs, timeout=timeout)
  File "/home/abdenour/.local/lib/python3.10/site-packages/ray/_private/worker.py", line 864, in get_objects
    raise value.as_instanceof_cause()
ray.exceptions.RayTaskError(ClientAppException): [36mray::ClientAppActor.run()[39m (pid=2330871, ip=115.145.172.224, actor_id=cdc8cff1bec6044f7ef6408701000000, repr=<flwr.simulation.ray_transport.ray_actor.ClientAppActor object at 0x7f75b8199750>)
  File "/home/abdenour/.local/lib/python3.10/site-packages/flwr/client/client_app.py", line 143, in __call__
    return self._call(message, context)
  File "/home/abdenour/.local/lib/python3.10/site-packages/flwr/client/client_app.py", line 126, in ffn
    out_message = handle_legacy_message_from_msgtype(
  File "/home/abdenour/.local/lib/python3.10/site-packages/flwr/client/message_handler/message_handler.py", line 136, in handle_legacy_message_from_msgtype
    evaluate_res = maybe_call_evaluate(
  File "/home/abdenour/.local/lib/python3.10/site-packages/flwr/client/client.py", line 254, in maybe_call_evaluate
    return client.evaluate(evaluate_ins)
  File "/home/abdenour/.local/lib/python3.10/site-packages/flwr/client/numpy_client.py", line 262, in _evaluate
    results = self.numpy_client.evaluate(parameters, ins.config)  # type: ignore
  File "/home/abdenour/fl_with_flower/fednova_vgg16/fednova_vgg16/client.py", line 85, in evaluate
    loss, accuracy = test(self.model, valloader, device=self.device)
  File "/home/abdenour/fl_with_flower/fednova_vgg16/fednova_vgg16/utils.py", line 71, in test
    outputs = net(images)
  File "/home/abdenour/anaconda3/envs/fl/lib/python3.10/site-packages/torch/nn/modules/module.py", line 1553, in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
  File "/home/abdenour/anaconda3/envs/fl/lib/python3.10/site-packages/torch/nn/modules/module.py", line 1562, in _call_impl
    return forward_call(*args, **kwargs)
  File "/home/abdenour/anaconda3/envs/fl/lib/python3.10/site-packages/torchvision/models/vgg.py", line 66, in forward
    x = self.features(x)
  File "/home/abdenour/anaconda3/envs/fl/lib/python3.10/site-packages/torch/nn/modules/module.py", line 1553, in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
  File "/home/abdenour/anaconda3/envs/fl/lib/python3.10/site-packages/torch/nn/modules/module.py", line 1562, in _call_impl
    return forward_call(*args, **kwargs)
  File "/home/abdenour/anaconda3/envs/fl/lib/python3.10/site-packages/torch/nn/modules/container.py", line 219, in forward
    input = module(input)
  File "/home/abdenour/anaconda3/envs/fl/lib/python3.10/site-packages/torch/nn/modules/module.py", line 1553, in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
  File "/home/abdenour/anaconda3/envs/fl/lib/python3.10/site-packages/torch/nn/modules/module.py", line 1562, in _call_impl
    return forward_call(*args, **kwargs)
  File "/home/abdenour/anaconda3/envs/fl/lib/python3.10/site-packages/torch/nn/modules/conv.py", line 458, in forward
    return self._conv_forward(input, self.weight, self.bias)
  File "/home/abdenour/anaconda3/envs/fl/lib/python3.10/site-packages/torch/nn/modules/conv.py", line 454, in _conv_forward
    return F.conv2d(input, weight, bias, self.stride,
torch.OutOfMemoryError: CUDA out of memory. Tried to allocate 246.00 MiB. GPU 0 has a total capacity of 47.33 GiB of which 6.81 MiB is free. Process 2279413 has 11.11 GiB memory in use. Process 2265701 has 2.37 GiB memory in use. Process 2279412 has 11.11 GiB memory in use. Process 2330873 has 11.11 GiB memory in use. Process 2317160 has 2.37 GiB memory in use. Process 2330872 has 6.78 GiB memory in use. Including non-PyTorch memory, this process has 816.00 MiB memory in use. Process 2330870 has 1.61 GiB memory in use. Of the allocated memory 524.65 MiB is allocated by PyTorch, and 31.35 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)

The above exception was the direct cause of the following exception:

[36mray::ClientAppActor.run()[39m (pid=2330871, ip=115.145.172.224, actor_id=cdc8cff1bec6044f7ef6408701000000, repr=<flwr.simulation.ray_transport.ray_actor.ClientAppActor object at 0x7f75b8199750>)
  File "/home/abdenour/.local/lib/python3.10/site-packages/flwr/simulation/ray_transport/ray_actor.py", line 63, in run
    raise ClientAppException(str(ex)) from ex
flwr.client.client_app.ClientAppException: 
Exception ClientAppException occurred. Message: CUDA out of memory. Tried to allocate 246.00 MiB. GPU 0 has a total capacity of 47.33 GiB of which 6.81 MiB is free. Process 2279413 has 11.11 GiB memory in use. Process 2265701 has 2.37 GiB memory in use. Process 2279412 has 11.11 GiB memory in use. Process 2330873 has 11.11 GiB memory in use. Process 2317160 has 2.37 GiB memory in use. Process 2330872 has 6.78 GiB memory in use. Including non-PyTorch memory, this process has 816.00 MiB memory in use. Process 2330870 has 1.61 GiB memory in use. Of the allocated memory 524.65 MiB is allocated by PyTorch, and 31.35 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)

[2024-09-19 01:25:26,383][flwr][ERROR] - [36mray::ClientAppActor.run()[39m (pid=2330871, ip=115.145.172.224, actor_id=cdc8cff1bec6044f7ef6408701000000, repr=<flwr.simulation.ray_transport.ray_actor.ClientAppActor object at 0x7f75b8199750>)
  File "/home/abdenour/.local/lib/python3.10/site-packages/flwr/client/client_app.py", line 143, in __call__
    return self._call(message, context)
  File "/home/abdenour/.local/lib/python3.10/site-packages/flwr/client/client_app.py", line 126, in ffn
    out_message = handle_legacy_message_from_msgtype(
  File "/home/abdenour/.local/lib/python3.10/site-packages/flwr/client/message_handler/message_handler.py", line 136, in handle_legacy_message_from_msgtype
    evaluate_res = maybe_call_evaluate(
  File "/home/abdenour/.local/lib/python3.10/site-packages/flwr/client/client.py", line 254, in maybe_call_evaluate
    return client.evaluate(evaluate_ins)
  File "/home/abdenour/.local/lib/python3.10/site-packages/flwr/client/numpy_client.py", line 262, in _evaluate
    results = self.numpy_client.evaluate(parameters, ins.config)  # type: ignore
  File "/home/abdenour/fl_with_flower/fednova_vgg16/fednova_vgg16/client.py", line 85, in evaluate
    loss, accuracy = test(self.model, valloader, device=self.device)
  File "/home/abdenour/fl_with_flower/fednova_vgg16/fednova_vgg16/utils.py", line 71, in test
    outputs = net(images)
  File "/home/abdenour/anaconda3/envs/fl/lib/python3.10/site-packages/torch/nn/modules/module.py", line 1553, in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
  File "/home/abdenour/anaconda3/envs/fl/lib/python3.10/site-packages/torch/nn/modules/module.py", line 1562, in _call_impl
    return forward_call(*args, **kwargs)
  File "/home/abdenour/anaconda3/envs/fl/lib/python3.10/site-packages/torchvision/models/vgg.py", line 66, in forward
    x = self.features(x)
  File "/home/abdenour/anaconda3/envs/fl/lib/python3.10/site-packages/torch/nn/modules/module.py", line 1553, in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
  File "/home/abdenour/anaconda3/envs/fl/lib/python3.10/site-packages/torch/nn/modules/module.py", line 1562, in _call_impl
    return forward_call(*args, **kwargs)
  File "/home/abdenour/anaconda3/envs/fl/lib/python3.10/site-packages/torch/nn/modules/container.py", line 219, in forward
    input = module(input)
  File "/home/abdenour/anaconda3/envs/fl/lib/python3.10/site-packages/torch/nn/modules/module.py", line 1553, in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
  File "/home/abdenour/anaconda3/envs/fl/lib/python3.10/site-packages/torch/nn/modules/module.py", line 1562, in _call_impl
    return forward_call(*args, **kwargs)
  File "/home/abdenour/anaconda3/envs/fl/lib/python3.10/site-packages/torch/nn/modules/conv.py", line 458, in forward
    return self._conv_forward(input, self.weight, self.bias)
  File "/home/abdenour/anaconda3/envs/fl/lib/python3.10/site-packages/torch/nn/modules/conv.py", line 454, in _conv_forward
    return F.conv2d(input, weight, bias, self.stride,
torch.OutOfMemoryError: CUDA out of memory. Tried to allocate 246.00 MiB. GPU 0 has a total capacity of 47.33 GiB of which 6.81 MiB is free. Process 2279413 has 11.11 GiB memory in use. Process 2265701 has 2.37 GiB memory in use. Process 2279412 has 11.11 GiB memory in use. Process 2330873 has 11.11 GiB memory in use. Process 2317160 has 2.37 GiB memory in use. Process 2330872 has 6.78 GiB memory in use. Including non-PyTorch memory, this process has 816.00 MiB memory in use. Process 2330870 has 1.61 GiB memory in use. Of the allocated memory 524.65 MiB is allocated by PyTorch, and 31.35 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)

The above exception was the direct cause of the following exception:

[36mray::ClientAppActor.run()[39m (pid=2330871, ip=115.145.172.224, actor_id=cdc8cff1bec6044f7ef6408701000000, repr=<flwr.simulation.ray_transport.ray_actor.ClientAppActor object at 0x7f75b8199750>)
  File "/home/abdenour/.local/lib/python3.10/site-packages/flwr/simulation/ray_transport/ray_actor.py", line 63, in run
    raise ClientAppException(str(ex)) from ex
flwr.client.client_app.ClientAppException: 
Exception ClientAppException occurred. Message: CUDA out of memory. Tried to allocate 246.00 MiB. GPU 0 has a total capacity of 47.33 GiB of which 6.81 MiB is free. Process 2279413 has 11.11 GiB memory in use. Process 2265701 has 2.37 GiB memory in use. Process 2279412 has 11.11 GiB memory in use. Process 2330873 has 11.11 GiB memory in use. Process 2317160 has 2.37 GiB memory in use. Process 2330872 has 6.78 GiB memory in use. Including non-PyTorch memory, this process has 816.00 MiB memory in use. Process 2330870 has 1.61 GiB memory in use. Of the allocated memory 524.65 MiB is allocated by PyTorch, and 31.35 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)
[2024-09-19 01:25:26,385][flwr][INFO] - aggregate_evaluate: received 1 results and 1 failures
[2024-09-19 01:25:26,385][flwr][INFO] - 
[2024-09-19 01:25:26,385][flwr][INFO] - [ROUND 5]
[2024-09-19 01:25:26,385][flwr][INFO] - configure_fit: strategy sampled 5 clients (out of 50)
[2024-09-19 01:25:30,160][flwr][ERROR] - Traceback (most recent call last):
  File "/home/abdenour/.local/lib/python3.10/site-packages/flwr/simulation/ray_transport/ray_client_proxy.py", line 94, in _submit_job
    out_mssg, updated_context = self.actor_pool.get_client_result(
  File "/home/abdenour/.local/lib/python3.10/site-packages/flwr/simulation/ray_transport/ray_actor.py", line 398, in get_client_result
    return self._fetch_future_result(cid)
  File "/home/abdenour/.local/lib/python3.10/site-packages/flwr/simulation/ray_transport/ray_actor.py", line 279, in _fetch_future_result
    res_cid, out_mssg, updated_context = ray.get(
  File "/home/abdenour/.local/lib/python3.10/site-packages/ray/_private/auto_init_hook.py", line 21, in auto_init_wrapper
    return fn(*args, **kwargs)
  File "/home/abdenour/.local/lib/python3.10/site-packages/ray/_private/client_mode_hook.py", line 103, in wrapper
    return func(*args, **kwargs)
  File "/home/abdenour/.local/lib/python3.10/site-packages/ray/_private/worker.py", line 2667, in get
    values, debugger_breakpoint = worker.get_objects(object_refs, timeout=timeout)
  File "/home/abdenour/.local/lib/python3.10/site-packages/ray/_private/worker.py", line 864, in get_objects
    raise value.as_instanceof_cause()
ray.exceptions.RayTaskError(ClientAppException): [36mray::ClientAppActor.run()[39m (pid=2330871, ip=115.145.172.224, actor_id=cdc8cff1bec6044f7ef6408701000000, repr=<flwr.simulation.ray_transport.ray_actor.ClientAppActor object at 0x7f75b8199750>)
  File "/home/abdenour/.local/lib/python3.10/site-packages/flwr/client/client_app.py", line 143, in __call__
    return self._call(message, context)
  File "/home/abdenour/.local/lib/python3.10/site-packages/flwr/client/client_app.py", line 126, in ffn
    out_message = handle_legacy_message_from_msgtype(
  File "/home/abdenour/.local/lib/python3.10/site-packages/flwr/client/message_handler/message_handler.py", line 129, in handle_legacy_message_from_msgtype
    fit_res = maybe_call_fit(
  File "/home/abdenour/.local/lib/python3.10/site-packages/flwr/client/client.py", line 234, in maybe_call_fit
    return client.fit(fit_ins)
  File "/home/abdenour/.local/lib/python3.10/site-packages/flwr/client/numpy_client.py", line 238, in _fit
    results = self.numpy_client.fit(parameters, ins.config)  # type: ignore
  File "/home/abdenour/fl_with_flower/fednova_vgg16/fednova_vgg16/client.py", line 69, in fit
    train(self.model, trainloader, self.optimizer, epochs=num_epochs, device=self.device)
  File "/home/abdenour/fl_with_flower/fednova_vgg16/fednova_vgg16/utils.py", line 57, in train
    loss = criterion(net(images), labels)
  File "/home/abdenour/anaconda3/envs/fl/lib/python3.10/site-packages/torch/nn/modules/module.py", line 1553, in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
  File "/home/abdenour/anaconda3/envs/fl/lib/python3.10/site-packages/torch/nn/modules/module.py", line 1562, in _call_impl
    return forward_call(*args, **kwargs)
  File "/home/abdenour/anaconda3/envs/fl/lib/python3.10/site-packages/torchvision/models/vgg.py", line 66, in forward
    x = self.features(x)
  File "/home/abdenour/anaconda3/envs/fl/lib/python3.10/site-packages/torch/nn/modules/module.py", line 1553, in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
  File "/home/abdenour/anaconda3/envs/fl/lib/python3.10/site-packages/torch/nn/modules/module.py", line 1562, in _call_impl
    return forward_call(*args, **kwargs)
  File "/home/abdenour/anaconda3/envs/fl/lib/python3.10/site-packages/torch/nn/modules/container.py", line 219, in forward
    input = module(input)
  File "/home/abdenour/anaconda3/envs/fl/lib/python3.10/site-packages/torch/nn/modules/module.py", line 1553, in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
  File "/home/abdenour/anaconda3/envs/fl/lib/python3.10/site-packages/torch/nn/modules/module.py", line 1562, in _call_impl
    return forward_call(*args, **kwargs)
  File "/home/abdenour/anaconda3/envs/fl/lib/python3.10/site-packages/torch/nn/modules/conv.py", line 458, in forward
    return self._conv_forward(input, self.weight, self.bias)
  File "/home/abdenour/anaconda3/envs/fl/lib/python3.10/site-packages/torch/nn/modules/conv.py", line 454, in _conv_forward
    return F.conv2d(input, weight, bias, self.stride,
torch.OutOfMemoryError: CUDA out of memory. Tried to allocate 784.00 MiB. GPU 0 has a total capacity of 47.33 GiB of which 6.81 MiB is free. Process 2279413 has 11.11 GiB memory in use. Process 2265701 has 2.37 GiB memory in use. Process 2279412 has 11.11 GiB memory in use. Process 2330873 has 11.11 GiB memory in use. Process 2317160 has 2.37 GiB memory in use. Process 2330872 has 6.78 GiB memory in use. Including non-PyTorch memory, this process has 816.00 MiB memory in use. Process 2330870 has 1.61 GiB memory in use. Of the allocated memory 549.92 MiB is allocated by PyTorch, and 6.08 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)

The above exception was the direct cause of the following exception:

[36mray::ClientAppActor.run()[39m (pid=2330871, ip=115.145.172.224, actor_id=cdc8cff1bec6044f7ef6408701000000, repr=<flwr.simulation.ray_transport.ray_actor.ClientAppActor object at 0x7f75b8199750>)
  File "/home/abdenour/.local/lib/python3.10/site-packages/flwr/simulation/ray_transport/ray_actor.py", line 63, in run
    raise ClientAppException(str(ex)) from ex
flwr.client.client_app.ClientAppException: 
Exception ClientAppException occurred. Message: CUDA out of memory. Tried to allocate 784.00 MiB. GPU 0 has a total capacity of 47.33 GiB of which 6.81 MiB is free. Process 2279413 has 11.11 GiB memory in use. Process 2265701 has 2.37 GiB memory in use. Process 2279412 has 11.11 GiB memory in use. Process 2330873 has 11.11 GiB memory in use. Process 2317160 has 2.37 GiB memory in use. Process 2330872 has 6.78 GiB memory in use. Including non-PyTorch memory, this process has 816.00 MiB memory in use. Process 2330870 has 1.61 GiB memory in use. Of the allocated memory 549.92 MiB is allocated by PyTorch, and 6.08 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)

[2024-09-19 01:25:30,160][flwr][ERROR] - [36mray::ClientAppActor.run()[39m (pid=2330871, ip=115.145.172.224, actor_id=cdc8cff1bec6044f7ef6408701000000, repr=<flwr.simulation.ray_transport.ray_actor.ClientAppActor object at 0x7f75b8199750>)
  File "/home/abdenour/.local/lib/python3.10/site-packages/flwr/client/client_app.py", line 143, in __call__
    return self._call(message, context)
  File "/home/abdenour/.local/lib/python3.10/site-packages/flwr/client/client_app.py", line 126, in ffn
    out_message = handle_legacy_message_from_msgtype(
  File "/home/abdenour/.local/lib/python3.10/site-packages/flwr/client/message_handler/message_handler.py", line 129, in handle_legacy_message_from_msgtype
    fit_res = maybe_call_fit(
  File "/home/abdenour/.local/lib/python3.10/site-packages/flwr/client/client.py", line 234, in maybe_call_fit
    return client.fit(fit_ins)
  File "/home/abdenour/.local/lib/python3.10/site-packages/flwr/client/numpy_client.py", line 238, in _fit
    results = self.numpy_client.fit(parameters, ins.config)  # type: ignore
  File "/home/abdenour/fl_with_flower/fednova_vgg16/fednova_vgg16/client.py", line 69, in fit
    train(self.model, trainloader, self.optimizer, epochs=num_epochs, device=self.device)
  File "/home/abdenour/fl_with_flower/fednova_vgg16/fednova_vgg16/utils.py", line 57, in train
    loss = criterion(net(images), labels)
  File "/home/abdenour/anaconda3/envs/fl/lib/python3.10/site-packages/torch/nn/modules/module.py", line 1553, in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
  File "/home/abdenour/anaconda3/envs/fl/lib/python3.10/site-packages/torch/nn/modules/module.py", line 1562, in _call_impl
    return forward_call(*args, **kwargs)
  File "/home/abdenour/anaconda3/envs/fl/lib/python3.10/site-packages/torchvision/models/vgg.py", line 66, in forward
    x = self.features(x)
  File "/home/abdenour/anaconda3/envs/fl/lib/python3.10/site-packages/torch/nn/modules/module.py", line 1553, in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
  File "/home/abdenour/anaconda3/envs/fl/lib/python3.10/site-packages/torch/nn/modules/module.py", line 1562, in _call_impl
    return forward_call(*args, **kwargs)
  File "/home/abdenour/anaconda3/envs/fl/lib/python3.10/site-packages/torch/nn/modules/container.py", line 219, in forward
    input = module(input)
  File "/home/abdenour/anaconda3/envs/fl/lib/python3.10/site-packages/torch/nn/modules/module.py", line 1553, in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
  File "/home/abdenour/anaconda3/envs/fl/lib/python3.10/site-packages/torch/nn/modules/module.py", line 1562, in _call_impl
    return forward_call(*args, **kwargs)
  File "/home/abdenour/anaconda3/envs/fl/lib/python3.10/site-packages/torch/nn/modules/conv.py", line 458, in forward
    return self._conv_forward(input, self.weight, self.bias)
  File "/home/abdenour/anaconda3/envs/fl/lib/python3.10/site-packages/torch/nn/modules/conv.py", line 454, in _conv_forward
    return F.conv2d(input, weight, bias, self.stride,
torch.OutOfMemoryError: CUDA out of memory. Tried to allocate 784.00 MiB. GPU 0 has a total capacity of 47.33 GiB of which 6.81 MiB is free. Process 2279413 has 11.11 GiB memory in use. Process 2265701 has 2.37 GiB memory in use. Process 2279412 has 11.11 GiB memory in use. Process 2330873 has 11.11 GiB memory in use. Process 2317160 has 2.37 GiB memory in use. Process 2330872 has 6.78 GiB memory in use. Including non-PyTorch memory, this process has 816.00 MiB memory in use. Process 2330870 has 1.61 GiB memory in use. Of the allocated memory 549.92 MiB is allocated by PyTorch, and 6.08 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)

The above exception was the direct cause of the following exception:

[36mray::ClientAppActor.run()[39m (pid=2330871, ip=115.145.172.224, actor_id=cdc8cff1bec6044f7ef6408701000000, repr=<flwr.simulation.ray_transport.ray_actor.ClientAppActor object at 0x7f75b8199750>)
  File "/home/abdenour/.local/lib/python3.10/site-packages/flwr/simulation/ray_transport/ray_actor.py", line 63, in run
    raise ClientAppException(str(ex)) from ex
flwr.client.client_app.ClientAppException: 
Exception ClientAppException occurred. Message: CUDA out of memory. Tried to allocate 784.00 MiB. GPU 0 has a total capacity of 47.33 GiB of which 6.81 MiB is free. Process 2279413 has 11.11 GiB memory in use. Process 2265701 has 2.37 GiB memory in use. Process 2279412 has 11.11 GiB memory in use. Process 2330873 has 11.11 GiB memory in use. Process 2317160 has 2.37 GiB memory in use. Process 2330872 has 6.78 GiB memory in use. Including non-PyTorch memory, this process has 816.00 MiB memory in use. Process 2330870 has 1.61 GiB memory in use. Of the allocated memory 549.92 MiB is allocated by PyTorch, and 6.08 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)
[2024-09-19 01:25:30,718][flwr][ERROR] - Traceback (most recent call last):
  File "/home/abdenour/.local/lib/python3.10/site-packages/flwr/simulation/ray_transport/ray_client_proxy.py", line 94, in _submit_job
    out_mssg, updated_context = self.actor_pool.get_client_result(
  File "/home/abdenour/.local/lib/python3.10/site-packages/flwr/simulation/ray_transport/ray_actor.py", line 398, in get_client_result
    return self._fetch_future_result(cid)
  File "/home/abdenour/.local/lib/python3.10/site-packages/flwr/simulation/ray_transport/ray_actor.py", line 279, in _fetch_future_result
    res_cid, out_mssg, updated_context = ray.get(
  File "/home/abdenour/.local/lib/python3.10/site-packages/ray/_private/auto_init_hook.py", line 21, in auto_init_wrapper
    return fn(*args, **kwargs)
  File "/home/abdenour/.local/lib/python3.10/site-packages/ray/_private/client_mode_hook.py", line 103, in wrapper
    return func(*args, **kwargs)
  File "/home/abdenour/.local/lib/python3.10/site-packages/ray/_private/worker.py", line 2667, in get
    values, debugger_breakpoint = worker.get_objects(object_refs, timeout=timeout)
  File "/home/abdenour/.local/lib/python3.10/site-packages/ray/_private/worker.py", line 864, in get_objects
    raise value.as_instanceof_cause()
ray.exceptions.RayTaskError(ClientAppException): [36mray::ClientAppActor.run()[39m (pid=2330870, ip=115.145.172.224, actor_id=18884b649bbd3d098a6ec02b01000000, repr=<flwr.simulation.ray_transport.ray_actor.ClientAppActor object at 0x7ffb805757b0>)
  File "/home/abdenour/.local/lib/python3.10/site-packages/flwr/client/client_app.py", line 143, in __call__
    return self._call(message, context)
  File "/home/abdenour/.local/lib/python3.10/site-packages/flwr/client/client_app.py", line 126, in ffn
    out_message = handle_legacy_message_from_msgtype(
  File "/home/abdenour/.local/lib/python3.10/site-packages/flwr/client/message_handler/message_handler.py", line 129, in handle_legacy_message_from_msgtype
    fit_res = maybe_call_fit(
  File "/home/abdenour/.local/lib/python3.10/site-packages/flwr/client/client.py", line 234, in maybe_call_fit
    return client.fit(fit_ins)
  File "/home/abdenour/.local/lib/python3.10/site-packages/flwr/client/numpy_client.py", line 238, in _fit
    results = self.numpy_client.fit(parameters, ins.config)  # type: ignore
  File "/home/abdenour/fl_with_flower/fednova_vgg16/fednova_vgg16/client.py", line 69, in fit
    train(self.model, trainloader, self.optimizer, epochs=num_epochs, device=self.device)
  File "/home/abdenour/fl_with_flower/fednova_vgg16/fednova_vgg16/utils.py", line 57, in train
    loss = criterion(net(images), labels)
  File "/home/abdenour/anaconda3/envs/fl/lib/python3.10/site-packages/torch/nn/modules/module.py", line 1553, in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
  File "/home/abdenour/anaconda3/envs/fl/lib/python3.10/site-packages/torch/nn/modules/module.py", line 1562, in _call_impl
    return forward_call(*args, **kwargs)
  File "/home/abdenour/anaconda3/envs/fl/lib/python3.10/site-packages/torchvision/models/vgg.py", line 66, in forward
    x = self.features(x)
  File "/home/abdenour/anaconda3/envs/fl/lib/python3.10/site-packages/torch/nn/modules/module.py", line 1553, in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
  File "/home/abdenour/anaconda3/envs/fl/lib/python3.10/site-packages/torch/nn/modules/module.py", line 1562, in _call_impl
    return forward_call(*args, **kwargs)
  File "/home/abdenour/anaconda3/envs/fl/lib/python3.10/site-packages/torch/nn/modules/container.py", line 219, in forward
    input = module(input)
  File "/home/abdenour/anaconda3/envs/fl/lib/python3.10/site-packages/torch/nn/modules/module.py", line 1553, in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
  File "/home/abdenour/anaconda3/envs/fl/lib/python3.10/site-packages/torch/nn/modules/module.py", line 1562, in _call_impl
    return forward_call(*args, **kwargs)
  File "/home/abdenour/anaconda3/envs/fl/lib/python3.10/site-packages/torch/nn/modules/conv.py", line 458, in forward
    return self._conv_forward(input, self.weight, self.bias)
  File "/home/abdenour/anaconda3/envs/fl/lib/python3.10/site-packages/torch/nn/modules/conv.py", line 454, in _conv_forward
    return F.conv2d(input, weight, bias, self.stride,
torch.OutOfMemoryError: CUDA out of memory. Tried to allocate 784.00 MiB. GPU 0 has a total capacity of 47.33 GiB of which 6.81 MiB is free. Process 2279413 has 11.11 GiB memory in use. Process 2265701 has 2.37 GiB memory in use. Process 2279412 has 11.11 GiB memory in use. Process 2330873 has 11.11 GiB memory in use. Process 2317160 has 2.37 GiB memory in use. Process 2330872 has 6.78 GiB memory in use. Process 2330871 has 816.00 MiB memory in use. Including non-PyTorch memory, this process has 1.61 GiB memory in use. Of the allocated memory 1.30 GiB is allocated by PyTorch, and 6.08 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)

The above exception was the direct cause of the following exception:

[36mray::ClientAppActor.run()[39m (pid=2330870, ip=115.145.172.224, actor_id=18884b649bbd3d098a6ec02b01000000, repr=<flwr.simulation.ray_transport.ray_actor.ClientAppActor object at 0x7ffb805757b0>)
  File "/home/abdenour/.local/lib/python3.10/site-packages/flwr/simulation/ray_transport/ray_actor.py", line 63, in run
    raise ClientAppException(str(ex)) from ex
flwr.client.client_app.ClientAppException: 
Exception ClientAppException occurred. Message: CUDA out of memory. Tried to allocate 784.00 MiB. GPU 0 has a total capacity of 47.33 GiB of which 6.81 MiB is free. Process 2279413 has 11.11 GiB memory in use. Process 2265701 has 2.37 GiB memory in use. Process 2279412 has 11.11 GiB memory in use. Process 2330873 has 11.11 GiB memory in use. Process 2317160 has 2.37 GiB memory in use. Process 2330872 has 6.78 GiB memory in use. Process 2330871 has 816.00 MiB memory in use. Including non-PyTorch memory, this process has 1.61 GiB memory in use. Of the allocated memory 1.30 GiB is allocated by PyTorch, and 6.08 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)

[2024-09-19 01:25:30,718][flwr][ERROR] - [36mray::ClientAppActor.run()[39m (pid=2330870, ip=115.145.172.224, actor_id=18884b649bbd3d098a6ec02b01000000, repr=<flwr.simulation.ray_transport.ray_actor.ClientAppActor object at 0x7ffb805757b0>)
  File "/home/abdenour/.local/lib/python3.10/site-packages/flwr/client/client_app.py", line 143, in __call__
    return self._call(message, context)
  File "/home/abdenour/.local/lib/python3.10/site-packages/flwr/client/client_app.py", line 126, in ffn
    out_message = handle_legacy_message_from_msgtype(
  File "/home/abdenour/.local/lib/python3.10/site-packages/flwr/client/message_handler/message_handler.py", line 129, in handle_legacy_message_from_msgtype
    fit_res = maybe_call_fit(
  File "/home/abdenour/.local/lib/python3.10/site-packages/flwr/client/client.py", line 234, in maybe_call_fit
    return client.fit(fit_ins)
  File "/home/abdenour/.local/lib/python3.10/site-packages/flwr/client/numpy_client.py", line 238, in _fit
    results = self.numpy_client.fit(parameters, ins.config)  # type: ignore
  File "/home/abdenour/fl_with_flower/fednova_vgg16/fednova_vgg16/client.py", line 69, in fit
    train(self.model, trainloader, self.optimizer, epochs=num_epochs, device=self.device)
  File "/home/abdenour/fl_with_flower/fednova_vgg16/fednova_vgg16/utils.py", line 57, in train
    loss = criterion(net(images), labels)
  File "/home/abdenour/anaconda3/envs/fl/lib/python3.10/site-packages/torch/nn/modules/module.py", line 1553, in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
  File "/home/abdenour/anaconda3/envs/fl/lib/python3.10/site-packages/torch/nn/modules/module.py", line 1562, in _call_impl
    return forward_call(*args, **kwargs)
  File "/home/abdenour/anaconda3/envs/fl/lib/python3.10/site-packages/torchvision/models/vgg.py", line 66, in forward
    x = self.features(x)
  File "/home/abdenour/anaconda3/envs/fl/lib/python3.10/site-packages/torch/nn/modules/module.py", line 1553, in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
  File "/home/abdenour/anaconda3/envs/fl/lib/python3.10/site-packages/torch/nn/modules/module.py", line 1562, in _call_impl
    return forward_call(*args, **kwargs)
  File "/home/abdenour/anaconda3/envs/fl/lib/python3.10/site-packages/torch/nn/modules/container.py", line 219, in forward
    input = module(input)
  File "/home/abdenour/anaconda3/envs/fl/lib/python3.10/site-packages/torch/nn/modules/module.py", line 1553, in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
  File "/home/abdenour/anaconda3/envs/fl/lib/python3.10/site-packages/torch/nn/modules/module.py", line 1562, in _call_impl
    return forward_call(*args, **kwargs)
  File "/home/abdenour/anaconda3/envs/fl/lib/python3.10/site-packages/torch/nn/modules/conv.py", line 458, in forward
    return self._conv_forward(input, self.weight, self.bias)
  File "/home/abdenour/anaconda3/envs/fl/lib/python3.10/site-packages/torch/nn/modules/conv.py", line 454, in _conv_forward
    return F.conv2d(input, weight, bias, self.stride,
torch.OutOfMemoryError: CUDA out of memory. Tried to allocate 784.00 MiB. GPU 0 has a total capacity of 47.33 GiB of which 6.81 MiB is free. Process 2279413 has 11.11 GiB memory in use. Process 2265701 has 2.37 GiB memory in use. Process 2279412 has 11.11 GiB memory in use. Process 2330873 has 11.11 GiB memory in use. Process 2317160 has 2.37 GiB memory in use. Process 2330872 has 6.78 GiB memory in use. Process 2330871 has 816.00 MiB memory in use. Including non-PyTorch memory, this process has 1.61 GiB memory in use. Of the allocated memory 1.30 GiB is allocated by PyTorch, and 6.08 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)

The above exception was the direct cause of the following exception:

[36mray::ClientAppActor.run()[39m (pid=2330870, ip=115.145.172.224, actor_id=18884b649bbd3d098a6ec02b01000000, repr=<flwr.simulation.ray_transport.ray_actor.ClientAppActor object at 0x7ffb805757b0>)
  File "/home/abdenour/.local/lib/python3.10/site-packages/flwr/simulation/ray_transport/ray_actor.py", line 63, in run
    raise ClientAppException(str(ex)) from ex
flwr.client.client_app.ClientAppException: 
Exception ClientAppException occurred. Message: CUDA out of memory. Tried to allocate 784.00 MiB. GPU 0 has a total capacity of 47.33 GiB of which 6.81 MiB is free. Process 2279413 has 11.11 GiB memory in use. Process 2265701 has 2.37 GiB memory in use. Process 2279412 has 11.11 GiB memory in use. Process 2330873 has 11.11 GiB memory in use. Process 2317160 has 2.37 GiB memory in use. Process 2330872 has 6.78 GiB memory in use. Process 2330871 has 816.00 MiB memory in use. Including non-PyTorch memory, this process has 1.61 GiB memory in use. Of the allocated memory 1.30 GiB is allocated by PyTorch, and 6.08 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)
[2024-09-19 01:25:32,174][flwr][ERROR] - Traceback (most recent call last):
  File "/home/abdenour/.local/lib/python3.10/site-packages/flwr/simulation/ray_transport/ray_client_proxy.py", line 94, in _submit_job
    out_mssg, updated_context = self.actor_pool.get_client_result(
  File "/home/abdenour/.local/lib/python3.10/site-packages/flwr/simulation/ray_transport/ray_actor.py", line 398, in get_client_result
    return self._fetch_future_result(cid)
  File "/home/abdenour/.local/lib/python3.10/site-packages/flwr/simulation/ray_transport/ray_actor.py", line 279, in _fetch_future_result
    res_cid, out_mssg, updated_context = ray.get(
  File "/home/abdenour/.local/lib/python3.10/site-packages/ray/_private/auto_init_hook.py", line 21, in auto_init_wrapper
    return fn(*args, **kwargs)
  File "/home/abdenour/.local/lib/python3.10/site-packages/ray/_private/client_mode_hook.py", line 103, in wrapper
    return func(*args, **kwargs)
  File "/home/abdenour/.local/lib/python3.10/site-packages/ray/_private/worker.py", line 2667, in get
    values, debugger_breakpoint = worker.get_objects(object_refs, timeout=timeout)
  File "/home/abdenour/.local/lib/python3.10/site-packages/ray/_private/worker.py", line 864, in get_objects
    raise value.as_instanceof_cause()
ray.exceptions.RayTaskError(ClientAppException): [36mray::ClientAppActor.run()[39m (pid=2330871, ip=115.145.172.224, actor_id=cdc8cff1bec6044f7ef6408701000000, repr=<flwr.simulation.ray_transport.ray_actor.ClientAppActor object at 0x7f75b8199750>)
  File "/home/abdenour/.local/lib/python3.10/site-packages/flwr/client/client_app.py", line 143, in __call__
    return self._call(message, context)
  File "/home/abdenour/.local/lib/python3.10/site-packages/flwr/client/client_app.py", line 126, in ffn
    out_message = handle_legacy_message_from_msgtype(
  File "/home/abdenour/.local/lib/python3.10/site-packages/flwr/client/message_handler/message_handler.py", line 129, in handle_legacy_message_from_msgtype
    fit_res = maybe_call_fit(
  File "/home/abdenour/.local/lib/python3.10/site-packages/flwr/client/client.py", line 234, in maybe_call_fit
    return client.fit(fit_ins)
  File "/home/abdenour/.local/lib/python3.10/site-packages/flwr/client/numpy_client.py", line 238, in _fit
    results = self.numpy_client.fit(parameters, ins.config)  # type: ignore
  File "/home/abdenour/fl_with_flower/fednova_vgg16/fednova_vgg16/client.py", line 69, in fit
    train(self.model, trainloader, self.optimizer, epochs=num_epochs, device=self.device)
  File "/home/abdenour/fl_with_flower/fednova_vgg16/fednova_vgg16/utils.py", line 57, in train
    loss = criterion(net(images), labels)
  File "/home/abdenour/anaconda3/envs/fl/lib/python3.10/site-packages/torch/nn/modules/module.py", line 1553, in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
  File "/home/abdenour/anaconda3/envs/fl/lib/python3.10/site-packages/torch/nn/modules/module.py", line 1562, in _call_impl
    return forward_call(*args, **kwargs)
  File "/home/abdenour/anaconda3/envs/fl/lib/python3.10/site-packages/torchvision/models/vgg.py", line 66, in forward
    x = self.features(x)
  File "/home/abdenour/anaconda3/envs/fl/lib/python3.10/site-packages/torch/nn/modules/module.py", line 1553, in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
  File "/home/abdenour/anaconda3/envs/fl/lib/python3.10/site-packages/torch/nn/modules/module.py", line 1562, in _call_impl
    return forward_call(*args, **kwargs)
  File "/home/abdenour/anaconda3/envs/fl/lib/python3.10/site-packages/torch/nn/modules/container.py", line 219, in forward
    input = module(input)
  File "/home/abdenour/anaconda3/envs/fl/lib/python3.10/site-packages/torch/nn/modules/module.py", line 1553, in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
  File "/home/abdenour/anaconda3/envs/fl/lib/python3.10/site-packages/torch/nn/modules/module.py", line 1562, in _call_impl
    return forward_call(*args, **kwargs)
  File "/home/abdenour/anaconda3/envs/fl/lib/python3.10/site-packages/torch/nn/modules/conv.py", line 458, in forward
    return self._conv_forward(input, self.weight, self.bias)
  File "/home/abdenour/anaconda3/envs/fl/lib/python3.10/site-packages/torch/nn/modules/conv.py", line 454, in _conv_forward
    return F.conv2d(input, weight, bias, self.stride,
torch.OutOfMemoryError: CUDA out of memory. Tried to allocate 784.00 MiB. GPU 0 has a total capacity of 47.33 GiB of which 2.81 MiB is free. Process 2279413 has 11.11 GiB memory in use. Process 2265701 has 2.37 GiB memory in use. Process 2279412 has 11.11 GiB memory in use. Process 2330873 has 11.11 GiB memory in use. Process 2317160 has 2.37 GiB memory in use. Process 2330872 has 6.79 GiB memory in use. Including non-PyTorch memory, this process has 816.00 MiB memory in use. Process 2330870 has 1.61 GiB memory in use. Of the allocated memory 549.92 MiB is allocated by PyTorch, and 6.08 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)

The above exception was the direct cause of the following exception:

[36mray::ClientAppActor.run()[39m (pid=2330871, ip=115.145.172.224, actor_id=cdc8cff1bec6044f7ef6408701000000, repr=<flwr.simulation.ray_transport.ray_actor.ClientAppActor object at 0x7f75b8199750>)
  File "/home/abdenour/.local/lib/python3.10/site-packages/flwr/simulation/ray_transport/ray_actor.py", line 63, in run
    raise ClientAppException(str(ex)) from ex
flwr.client.client_app.ClientAppException: 
Exception ClientAppException occurred. Message: CUDA out of memory. Tried to allocate 784.00 MiB. GPU 0 has a total capacity of 47.33 GiB of which 2.81 MiB is free. Process 2279413 has 11.11 GiB memory in use. Process 2265701 has 2.37 GiB memory in use. Process 2279412 has 11.11 GiB memory in use. Process 2330873 has 11.11 GiB memory in use. Process 2317160 has 2.37 GiB memory in use. Process 2330872 has 6.79 GiB memory in use. Including non-PyTorch memory, this process has 816.00 MiB memory in use. Process 2330870 has 1.61 GiB memory in use. Of the allocated memory 549.92 MiB is allocated by PyTorch, and 6.08 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)

[2024-09-19 01:25:32,175][flwr][ERROR] - [36mray::ClientAppActor.run()[39m (pid=2330871, ip=115.145.172.224, actor_id=cdc8cff1bec6044f7ef6408701000000, repr=<flwr.simulation.ray_transport.ray_actor.ClientAppActor object at 0x7f75b8199750>)
  File "/home/abdenour/.local/lib/python3.10/site-packages/flwr/client/client_app.py", line 143, in __call__
    return self._call(message, context)
  File "/home/abdenour/.local/lib/python3.10/site-packages/flwr/client/client_app.py", line 126, in ffn
    out_message = handle_legacy_message_from_msgtype(
  File "/home/abdenour/.local/lib/python3.10/site-packages/flwr/client/message_handler/message_handler.py", line 129, in handle_legacy_message_from_msgtype
    fit_res = maybe_call_fit(
  File "/home/abdenour/.local/lib/python3.10/site-packages/flwr/client/client.py", line 234, in maybe_call_fit
    return client.fit(fit_ins)
  File "/home/abdenour/.local/lib/python3.10/site-packages/flwr/client/numpy_client.py", line 238, in _fit
    results = self.numpy_client.fit(parameters, ins.config)  # type: ignore
  File "/home/abdenour/fl_with_flower/fednova_vgg16/fednova_vgg16/client.py", line 69, in fit
    train(self.model, trainloader, self.optimizer, epochs=num_epochs, device=self.device)
  File "/home/abdenour/fl_with_flower/fednova_vgg16/fednova_vgg16/utils.py", line 57, in train
    loss = criterion(net(images), labels)
  File "/home/abdenour/anaconda3/envs/fl/lib/python3.10/site-packages/torch/nn/modules/module.py", line 1553, in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
  File "/home/abdenour/anaconda3/envs/fl/lib/python3.10/site-packages/torch/nn/modules/module.py", line 1562, in _call_impl
    return forward_call(*args, **kwargs)
  File "/home/abdenour/anaconda3/envs/fl/lib/python3.10/site-packages/torchvision/models/vgg.py", line 66, in forward
    x = self.features(x)
  File "/home/abdenour/anaconda3/envs/fl/lib/python3.10/site-packages/torch/nn/modules/module.py", line 1553, in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
  File "/home/abdenour/anaconda3/envs/fl/lib/python3.10/site-packages/torch/nn/modules/module.py", line 1562, in _call_impl
    return forward_call(*args, **kwargs)
  File "/home/abdenour/anaconda3/envs/fl/lib/python3.10/site-packages/torch/nn/modules/container.py", line 219, in forward
    input = module(input)
  File "/home/abdenour/anaconda3/envs/fl/lib/python3.10/site-packages/torch/nn/modules/module.py", line 1553, in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
  File "/home/abdenour/anaconda3/envs/fl/lib/python3.10/site-packages/torch/nn/modules/module.py", line 1562, in _call_impl
    return forward_call(*args, **kwargs)
  File "/home/abdenour/anaconda3/envs/fl/lib/python3.10/site-packages/torch/nn/modules/conv.py", line 458, in forward
    return self._conv_forward(input, self.weight, self.bias)
  File "/home/abdenour/anaconda3/envs/fl/lib/python3.10/site-packages/torch/nn/modules/conv.py", line 454, in _conv_forward
    return F.conv2d(input, weight, bias, self.stride,
torch.OutOfMemoryError: CUDA out of memory. Tried to allocate 784.00 MiB. GPU 0 has a total capacity of 47.33 GiB of which 2.81 MiB is free. Process 2279413 has 11.11 GiB memory in use. Process 2265701 has 2.37 GiB memory in use. Process 2279412 has 11.11 GiB memory in use. Process 2330873 has 11.11 GiB memory in use. Process 2317160 has 2.37 GiB memory in use. Process 2330872 has 6.79 GiB memory in use. Including non-PyTorch memory, this process has 816.00 MiB memory in use. Process 2330870 has 1.61 GiB memory in use. Of the allocated memory 549.92 MiB is allocated by PyTorch, and 6.08 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)

The above exception was the direct cause of the following exception:

[36mray::ClientAppActor.run()[39m (pid=2330871, ip=115.145.172.224, actor_id=cdc8cff1bec6044f7ef6408701000000, repr=<flwr.simulation.ray_transport.ray_actor.ClientAppActor object at 0x7f75b8199750>)
  File "/home/abdenour/.local/lib/python3.10/site-packages/flwr/simulation/ray_transport/ray_actor.py", line 63, in run
    raise ClientAppException(str(ex)) from ex
flwr.client.client_app.ClientAppException: 
Exception ClientAppException occurred. Message: CUDA out of memory. Tried to allocate 784.00 MiB. GPU 0 has a total capacity of 47.33 GiB of which 2.81 MiB is free. Process 2279413 has 11.11 GiB memory in use. Process 2265701 has 2.37 GiB memory in use. Process 2279412 has 11.11 GiB memory in use. Process 2330873 has 11.11 GiB memory in use. Process 2317160 has 2.37 GiB memory in use. Process 2330872 has 6.79 GiB memory in use. Including non-PyTorch memory, this process has 816.00 MiB memory in use. Process 2330870 has 1.61 GiB memory in use. Of the allocated memory 549.92 MiB is allocated by PyTorch, and 6.08 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)
[2024-09-19 01:25:32,268][flwr][ERROR] - Traceback (most recent call last):
  File "/home/abdenour/.local/lib/python3.10/site-packages/flwr/simulation/ray_transport/ray_client_proxy.py", line 94, in _submit_job
    out_mssg, updated_context = self.actor_pool.get_client_result(
  File "/home/abdenour/.local/lib/python3.10/site-packages/flwr/simulation/ray_transport/ray_actor.py", line 398, in get_client_result
    return self._fetch_future_result(cid)
  File "/home/abdenour/.local/lib/python3.10/site-packages/flwr/simulation/ray_transport/ray_actor.py", line 279, in _fetch_future_result
    res_cid, out_mssg, updated_context = ray.get(
  File "/home/abdenour/.local/lib/python3.10/site-packages/ray/_private/auto_init_hook.py", line 21, in auto_init_wrapper
    return fn(*args, **kwargs)
  File "/home/abdenour/.local/lib/python3.10/site-packages/ray/_private/client_mode_hook.py", line 103, in wrapper
    return func(*args, **kwargs)
  File "/home/abdenour/.local/lib/python3.10/site-packages/ray/_private/worker.py", line 2667, in get
    values, debugger_breakpoint = worker.get_objects(object_refs, timeout=timeout)
  File "/home/abdenour/.local/lib/python3.10/site-packages/ray/_private/worker.py", line 864, in get_objects
    raise value.as_instanceof_cause()
ray.exceptions.RayTaskError(ClientAppException): [36mray::ClientAppActor.run()[39m (pid=2330872, ip=115.145.172.224, actor_id=d38096ee5352eefa91005c8e01000000, repr=<flwr.simulation.ray_transport.ray_actor.ClientAppActor object at 0x7fcf0478d750>)
  File "/home/abdenour/.local/lib/python3.10/site-packages/flwr/client/client_app.py", line 143, in __call__
    return self._call(message, context)
  File "/home/abdenour/.local/lib/python3.10/site-packages/flwr/client/client_app.py", line 126, in ffn
    out_message = handle_legacy_message_from_msgtype(
  File "/home/abdenour/.local/lib/python3.10/site-packages/flwr/client/message_handler/message_handler.py", line 129, in handle_legacy_message_from_msgtype
    fit_res = maybe_call_fit(
  File "/home/abdenour/.local/lib/python3.10/site-packages/flwr/client/client.py", line 234, in maybe_call_fit
    return client.fit(fit_ins)
  File "/home/abdenour/.local/lib/python3.10/site-packages/flwr/client/numpy_client.py", line 238, in _fit
    results = self.numpy_client.fit(parameters, ins.config)  # type: ignore
  File "/home/abdenour/fl_with_flower/fednova_vgg16/fednova_vgg16/client.py", line 69, in fit
    train(self.model, trainloader, self.optimizer, epochs=num_epochs, device=self.device)
  File "/home/abdenour/fl_with_flower/fednova_vgg16/fednova_vgg16/utils.py", line 58, in train
    loss.backward()
  File "/home/abdenour/anaconda3/envs/fl/lib/python3.10/site-packages/torch/_tensor.py", line 521, in backward
    torch.autograd.backward(
  File "/home/abdenour/anaconda3/envs/fl/lib/python3.10/site-packages/torch/autograd/__init__.py", line 289, in backward
    _engine_run_backward(
  File "/home/abdenour/anaconda3/envs/fl/lib/python3.10/site-packages/torch/autograd/graph.py", line 769, in _engine_run_backward
    return Variable._execution_engine.run_backward(  # Calls into the C++ engine to run the backward pass
torch.OutOfMemoryError: CUDA out of memory. Tried to allocate 98.00 MiB. GPU 0 has a total capacity of 47.33 GiB of which 6.81 MiB is free. Process 2279413 has 11.11 GiB memory in use. Process 2265701 has 2.37 GiB memory in use. Process 2279412 has 11.11 GiB memory in use. Process 2330873 has 11.11 GiB memory in use. Process 2317160 has 2.37 GiB memory in use. Including non-PyTorch memory, this process has 6.78 GiB memory in use. Process 2330871 has 816.00 MiB memory in use. Process 2330870 has 1.61 GiB memory in use. Of the allocated memory 6.26 GiB is allocated by PyTorch, and 199.47 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)

The above exception was the direct cause of the following exception:

[36mray::ClientAppActor.run()[39m (pid=2330872, ip=115.145.172.224, actor_id=d38096ee5352eefa91005c8e01000000, repr=<flwr.simulation.ray_transport.ray_actor.ClientAppActor object at 0x7fcf0478d750>)
  File "/home/abdenour/.local/lib/python3.10/site-packages/flwr/simulation/ray_transport/ray_actor.py", line 63, in run
    raise ClientAppException(str(ex)) from ex
flwr.client.client_app.ClientAppException: 
Exception ClientAppException occurred. Message: CUDA out of memory. Tried to allocate 98.00 MiB. GPU 0 has a total capacity of 47.33 GiB of which 6.81 MiB is free. Process 2279413 has 11.11 GiB memory in use. Process 2265701 has 2.37 GiB memory in use. Process 2279412 has 11.11 GiB memory in use. Process 2330873 has 11.11 GiB memory in use. Process 2317160 has 2.37 GiB memory in use. Including non-PyTorch memory, this process has 6.78 GiB memory in use. Process 2330871 has 816.00 MiB memory in use. Process 2330870 has 1.61 GiB memory in use. Of the allocated memory 6.26 GiB is allocated by PyTorch, and 199.47 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)

[2024-09-19 01:25:32,269][flwr][ERROR] - [36mray::ClientAppActor.run()[39m (pid=2330872, ip=115.145.172.224, actor_id=d38096ee5352eefa91005c8e01000000, repr=<flwr.simulation.ray_transport.ray_actor.ClientAppActor object at 0x7fcf0478d750>)
  File "/home/abdenour/.local/lib/python3.10/site-packages/flwr/client/client_app.py", line 143, in __call__
    return self._call(message, context)
  File "/home/abdenour/.local/lib/python3.10/site-packages/flwr/client/client_app.py", line 126, in ffn
    out_message = handle_legacy_message_from_msgtype(
  File "/home/abdenour/.local/lib/python3.10/site-packages/flwr/client/message_handler/message_handler.py", line 129, in handle_legacy_message_from_msgtype
    fit_res = maybe_call_fit(
  File "/home/abdenour/.local/lib/python3.10/site-packages/flwr/client/client.py", line 234, in maybe_call_fit
    return client.fit(fit_ins)
  File "/home/abdenour/.local/lib/python3.10/site-packages/flwr/client/numpy_client.py", line 238, in _fit
    results = self.numpy_client.fit(parameters, ins.config)  # type: ignore
  File "/home/abdenour/fl_with_flower/fednova_vgg16/fednova_vgg16/client.py", line 69, in fit
    train(self.model, trainloader, self.optimizer, epochs=num_epochs, device=self.device)
  File "/home/abdenour/fl_with_flower/fednova_vgg16/fednova_vgg16/utils.py", line 58, in train
    loss.backward()
  File "/home/abdenour/anaconda3/envs/fl/lib/python3.10/site-packages/torch/_tensor.py", line 521, in backward
    torch.autograd.backward(
  File "/home/abdenour/anaconda3/envs/fl/lib/python3.10/site-packages/torch/autograd/__init__.py", line 289, in backward
    _engine_run_backward(
  File "/home/abdenour/anaconda3/envs/fl/lib/python3.10/site-packages/torch/autograd/graph.py", line 769, in _engine_run_backward
    return Variable._execution_engine.run_backward(  # Calls into the C++ engine to run the backward pass
torch.OutOfMemoryError: CUDA out of memory. Tried to allocate 98.00 MiB. GPU 0 has a total capacity of 47.33 GiB of which 6.81 MiB is free. Process 2279413 has 11.11 GiB memory in use. Process 2265701 has 2.37 GiB memory in use. Process 2279412 has 11.11 GiB memory in use. Process 2330873 has 11.11 GiB memory in use. Process 2317160 has 2.37 GiB memory in use. Including non-PyTorch memory, this process has 6.78 GiB memory in use. Process 2330871 has 816.00 MiB memory in use. Process 2330870 has 1.61 GiB memory in use. Of the allocated memory 6.26 GiB is allocated by PyTorch, and 199.47 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)

The above exception was the direct cause of the following exception:

[36mray::ClientAppActor.run()[39m (pid=2330872, ip=115.145.172.224, actor_id=d38096ee5352eefa91005c8e01000000, repr=<flwr.simulation.ray_transport.ray_actor.ClientAppActor object at 0x7fcf0478d750>)
  File "/home/abdenour/.local/lib/python3.10/site-packages/flwr/simulation/ray_transport/ray_actor.py", line 63, in run
    raise ClientAppException(str(ex)) from ex
flwr.client.client_app.ClientAppException: 
Exception ClientAppException occurred. Message: CUDA out of memory. Tried to allocate 98.00 MiB. GPU 0 has a total capacity of 47.33 GiB of which 6.81 MiB is free. Process 2279413 has 11.11 GiB memory in use. Process 2265701 has 2.37 GiB memory in use. Process 2279412 has 11.11 GiB memory in use. Process 2330873 has 11.11 GiB memory in use. Process 2317160 has 2.37 GiB memory in use. Including non-PyTorch memory, this process has 6.78 GiB memory in use. Process 2330871 has 816.00 MiB memory in use. Process 2330870 has 1.61 GiB memory in use. Of the allocated memory 6.26 GiB is allocated by PyTorch, and 199.47 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)
[2024-09-19 01:25:34,846][flwr][INFO] - aggregate_fit: received 1 results and 4 failures
[2024-09-19 01:25:47,472][flwr][INFO] - fit progress: (5, 267.4801151752472, {'accuracy': 0.12627886582870507}, 128.88071873411536)
[2024-09-19 01:25:47,473][flwr][INFO] - configure_evaluate: strategy sampled 2 clients (out of 50)
[2024-09-19 01:25:51,081][flwr][INFO] - aggregate_evaluate: received 2 results and 0 failures
[2024-09-19 01:25:51,081][flwr][INFO] - 
[2024-09-19 01:25:51,082][flwr][INFO] - [ROUND 6]
[2024-09-19 01:25:51,082][flwr][INFO] - configure_fit: strategy sampled 5 clients (out of 50)
[2024-09-19 01:25:54,906][flwr][ERROR] - Traceback (most recent call last):
  File "/home/abdenour/.local/lib/python3.10/site-packages/flwr/simulation/ray_transport/ray_client_proxy.py", line 94, in _submit_job
    out_mssg, updated_context = self.actor_pool.get_client_result(
  File "/home/abdenour/.local/lib/python3.10/site-packages/flwr/simulation/ray_transport/ray_actor.py", line 398, in get_client_result
    return self._fetch_future_result(cid)
  File "/home/abdenour/.local/lib/python3.10/site-packages/flwr/simulation/ray_transport/ray_actor.py", line 279, in _fetch_future_result
    res_cid, out_mssg, updated_context = ray.get(
  File "/home/abdenour/.local/lib/python3.10/site-packages/ray/_private/auto_init_hook.py", line 21, in auto_init_wrapper
    return fn(*args, **kwargs)
  File "/home/abdenour/.local/lib/python3.10/site-packages/ray/_private/client_mode_hook.py", line 103, in wrapper
    return func(*args, **kwargs)
  File "/home/abdenour/.local/lib/python3.10/site-packages/ray/_private/worker.py", line 2667, in get
    values, debugger_breakpoint = worker.get_objects(object_refs, timeout=timeout)
  File "/home/abdenour/.local/lib/python3.10/site-packages/ray/_private/worker.py", line 864, in get_objects
    raise value.as_instanceof_cause()
ray.exceptions.RayTaskError(ClientAppException): [36mray::ClientAppActor.run()[39m (pid=2330872, ip=115.145.172.224, actor_id=d38096ee5352eefa91005c8e01000000, repr=<flwr.simulation.ray_transport.ray_actor.ClientAppActor object at 0x7fcf0478d750>)
  File "/home/abdenour/.local/lib/python3.10/site-packages/flwr/client/client_app.py", line 143, in __call__
    return self._call(message, context)
  File "/home/abdenour/.local/lib/python3.10/site-packages/flwr/client/client_app.py", line 126, in ffn
    out_message = handle_legacy_message_from_msgtype(
  File "/home/abdenour/.local/lib/python3.10/site-packages/flwr/client/message_handler/message_handler.py", line 129, in handle_legacy_message_from_msgtype
    fit_res = maybe_call_fit(
  File "/home/abdenour/.local/lib/python3.10/site-packages/flwr/client/client.py", line 234, in maybe_call_fit
    return client.fit(fit_ins)
  File "/home/abdenour/.local/lib/python3.10/site-packages/flwr/client/numpy_client.py", line 238, in _fit
    results = self.numpy_client.fit(parameters, ins.config)  # type: ignore
  File "/home/abdenour/fl_with_flower/fednova_vgg16/fednova_vgg16/client.py", line 69, in fit
    train(self.model, trainloader, self.optimizer, epochs=num_epochs, device=self.device)
  File "/home/abdenour/fl_with_flower/fednova_vgg16/fednova_vgg16/utils.py", line 58, in train
    loss.backward()
  File "/home/abdenour/anaconda3/envs/fl/lib/python3.10/site-packages/torch/_tensor.py", line 521, in backward
    torch.autograd.backward(
  File "/home/abdenour/anaconda3/envs/fl/lib/python3.10/site-packages/torch/autograd/__init__.py", line 289, in backward
    _engine_run_backward(
  File "/home/abdenour/anaconda3/envs/fl/lib/python3.10/site-packages/torch/autograd/graph.py", line 769, in _engine_run_backward
    return Variable._execution_engine.run_backward(  # Calls into the C++ engine to run the backward pass
torch.OutOfMemoryError: CUDA out of memory. Tried to allocate 98.00 MiB. GPU 0 has a total capacity of 47.33 GiB of which 6.81 MiB is free. Process 2279413 has 11.11 GiB memory in use. Process 2265701 has 2.37 GiB memory in use. Process 2279412 has 11.11 GiB memory in use. Process 2330873 has 11.11 GiB memory in use. Process 2317160 has 2.37 GiB memory in use. Including non-PyTorch memory, this process has 6.78 GiB memory in use. Process 2330871 has 816.00 MiB memory in use. Process 2330870 has 1.61 GiB memory in use. Of the allocated memory 6.26 GiB is allocated by PyTorch, and 199.47 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)

The above exception was the direct cause of the following exception:

[36mray::ClientAppActor.run()[39m (pid=2330872, ip=115.145.172.224, actor_id=d38096ee5352eefa91005c8e01000000, repr=<flwr.simulation.ray_transport.ray_actor.ClientAppActor object at 0x7fcf0478d750>)
  File "/home/abdenour/.local/lib/python3.10/site-packages/flwr/simulation/ray_transport/ray_actor.py", line 63, in run
    raise ClientAppException(str(ex)) from ex
flwr.client.client_app.ClientAppException: 
Exception ClientAppException occurred. Message: CUDA out of memory. Tried to allocate 98.00 MiB. GPU 0 has a total capacity of 47.33 GiB of which 6.81 MiB is free. Process 2279413 has 11.11 GiB memory in use. Process 2265701 has 2.37 GiB memory in use. Process 2279412 has 11.11 GiB memory in use. Process 2330873 has 11.11 GiB memory in use. Process 2317160 has 2.37 GiB memory in use. Including non-PyTorch memory, this process has 6.78 GiB memory in use. Process 2330871 has 816.00 MiB memory in use. Process 2330870 has 1.61 GiB memory in use. Of the allocated memory 6.26 GiB is allocated by PyTorch, and 199.47 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)

[2024-09-19 01:25:54,907][flwr][ERROR] - [36mray::ClientAppActor.run()[39m (pid=2330872, ip=115.145.172.224, actor_id=d38096ee5352eefa91005c8e01000000, repr=<flwr.simulation.ray_transport.ray_actor.ClientAppActor object at 0x7fcf0478d750>)
  File "/home/abdenour/.local/lib/python3.10/site-packages/flwr/client/client_app.py", line 143, in __call__
    return self._call(message, context)
  File "/home/abdenour/.local/lib/python3.10/site-packages/flwr/client/client_app.py", line 126, in ffn
    out_message = handle_legacy_message_from_msgtype(
  File "/home/abdenour/.local/lib/python3.10/site-packages/flwr/client/message_handler/message_handler.py", line 129, in handle_legacy_message_from_msgtype
    fit_res = maybe_call_fit(
  File "/home/abdenour/.local/lib/python3.10/site-packages/flwr/client/client.py", line 234, in maybe_call_fit
    return client.fit(fit_ins)
  File "/home/abdenour/.local/lib/python3.10/site-packages/flwr/client/numpy_client.py", line 238, in _fit
    results = self.numpy_client.fit(parameters, ins.config)  # type: ignore
  File "/home/abdenour/fl_with_flower/fednova_vgg16/fednova_vgg16/client.py", line 69, in fit
    train(self.model, trainloader, self.optimizer, epochs=num_epochs, device=self.device)
  File "/home/abdenour/fl_with_flower/fednova_vgg16/fednova_vgg16/utils.py", line 58, in train
    loss.backward()
  File "/home/abdenour/anaconda3/envs/fl/lib/python3.10/site-packages/torch/_tensor.py", line 521, in backward
    torch.autograd.backward(
  File "/home/abdenour/anaconda3/envs/fl/lib/python3.10/site-packages/torch/autograd/__init__.py", line 289, in backward
    _engine_run_backward(
  File "/home/abdenour/anaconda3/envs/fl/lib/python3.10/site-packages/torch/autograd/graph.py", line 769, in _engine_run_backward
    return Variable._execution_engine.run_backward(  # Calls into the C++ engine to run the backward pass
torch.OutOfMemoryError: CUDA out of memory. Tried to allocate 98.00 MiB. GPU 0 has a total capacity of 47.33 GiB of which 6.81 MiB is free. Process 2279413 has 11.11 GiB memory in use. Process 2265701 has 2.37 GiB memory in use. Process 2279412 has 11.11 GiB memory in use. Process 2330873 has 11.11 GiB memory in use. Process 2317160 has 2.37 GiB memory in use. Including non-PyTorch memory, this process has 6.78 GiB memory in use. Process 2330871 has 816.00 MiB memory in use. Process 2330870 has 1.61 GiB memory in use. Of the allocated memory 6.26 GiB is allocated by PyTorch, and 199.47 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)

The above exception was the direct cause of the following exception:

[36mray::ClientAppActor.run()[39m (pid=2330872, ip=115.145.172.224, actor_id=d38096ee5352eefa91005c8e01000000, repr=<flwr.simulation.ray_transport.ray_actor.ClientAppActor object at 0x7fcf0478d750>)
  File "/home/abdenour/.local/lib/python3.10/site-packages/flwr/simulation/ray_transport/ray_actor.py", line 63, in run
    raise ClientAppException(str(ex)) from ex
flwr.client.client_app.ClientAppException: 
Exception ClientAppException occurred. Message: CUDA out of memory. Tried to allocate 98.00 MiB. GPU 0 has a total capacity of 47.33 GiB of which 6.81 MiB is free. Process 2279413 has 11.11 GiB memory in use. Process 2265701 has 2.37 GiB memory in use. Process 2279412 has 11.11 GiB memory in use. Process 2330873 has 11.11 GiB memory in use. Process 2317160 has 2.37 GiB memory in use. Including non-PyTorch memory, this process has 6.78 GiB memory in use. Process 2330871 has 816.00 MiB memory in use. Process 2330870 has 1.61 GiB memory in use. Of the allocated memory 6.26 GiB is allocated by PyTorch, and 199.47 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)
[2024-09-19 01:25:55,099][flwr][ERROR] - Traceback (most recent call last):
  File "/home/abdenour/.local/lib/python3.10/site-packages/flwr/simulation/ray_transport/ray_client_proxy.py", line 94, in _submit_job
    out_mssg, updated_context = self.actor_pool.get_client_result(
  File "/home/abdenour/.local/lib/python3.10/site-packages/flwr/simulation/ray_transport/ray_actor.py", line 398, in get_client_result
    return self._fetch_future_result(cid)
  File "/home/abdenour/.local/lib/python3.10/site-packages/flwr/simulation/ray_transport/ray_actor.py", line 279, in _fetch_future_result
    res_cid, out_mssg, updated_context = ray.get(
  File "/home/abdenour/.local/lib/python3.10/site-packages/ray/_private/auto_init_hook.py", line 21, in auto_init_wrapper
    return fn(*args, **kwargs)
  File "/home/abdenour/.local/lib/python3.10/site-packages/ray/_private/client_mode_hook.py", line 103, in wrapper
    return func(*args, **kwargs)
  File "/home/abdenour/.local/lib/python3.10/site-packages/ray/_private/worker.py", line 2667, in get
    values, debugger_breakpoint = worker.get_objects(object_refs, timeout=timeout)
  File "/home/abdenour/.local/lib/python3.10/site-packages/ray/_private/worker.py", line 864, in get_objects
    raise value.as_instanceof_cause()
ray.exceptions.RayTaskError(ClientAppException): [36mray::ClientAppActor.run()[39m (pid=2330871, ip=115.145.172.224, actor_id=cdc8cff1bec6044f7ef6408701000000, repr=<flwr.simulation.ray_transport.ray_actor.ClientAppActor object at 0x7f75b8199750>)
  File "/home/abdenour/.local/lib/python3.10/site-packages/flwr/client/client_app.py", line 143, in __call__
    return self._call(message, context)
  File "/home/abdenour/.local/lib/python3.10/site-packages/flwr/client/client_app.py", line 126, in ffn
    out_message = handle_legacy_message_from_msgtype(
  File "/home/abdenour/.local/lib/python3.10/site-packages/flwr/client/message_handler/message_handler.py", line 129, in handle_legacy_message_from_msgtype
    fit_res = maybe_call_fit(
  File "/home/abdenour/.local/lib/python3.10/site-packages/flwr/client/client.py", line 234, in maybe_call_fit
    return client.fit(fit_ins)
  File "/home/abdenour/.local/lib/python3.10/site-packages/flwr/client/numpy_client.py", line 238, in _fit
    results = self.numpy_client.fit(parameters, ins.config)  # type: ignore
  File "/home/abdenour/fl_with_flower/fednova_vgg16/fednova_vgg16/client.py", line 69, in fit
    train(self.model, trainloader, self.optimizer, epochs=num_epochs, device=self.device)
  File "/home/abdenour/fl_with_flower/fednova_vgg16/fednova_vgg16/utils.py", line 57, in train
    loss = criterion(net(images), labels)
  File "/home/abdenour/anaconda3/envs/fl/lib/python3.10/site-packages/torch/nn/modules/module.py", line 1553, in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
  File "/home/abdenour/anaconda3/envs/fl/lib/python3.10/site-packages/torch/nn/modules/module.py", line 1562, in _call_impl
    return forward_call(*args, **kwargs)
  File "/home/abdenour/anaconda3/envs/fl/lib/python3.10/site-packages/torchvision/models/vgg.py", line 66, in forward
    x = self.features(x)
  File "/home/abdenour/anaconda3/envs/fl/lib/python3.10/site-packages/torch/nn/modules/module.py", line 1553, in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
  File "/home/abdenour/anaconda3/envs/fl/lib/python3.10/site-packages/torch/nn/modules/module.py", line 1562, in _call_impl
    return forward_call(*args, **kwargs)
  File "/home/abdenour/anaconda3/envs/fl/lib/python3.10/site-packages/torch/nn/modules/container.py", line 219, in forward
    input = module(input)
  File "/home/abdenour/anaconda3/envs/fl/lib/python3.10/site-packages/torch/nn/modules/module.py", line 1553, in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
  File "/home/abdenour/anaconda3/envs/fl/lib/python3.10/site-packages/torch/nn/modules/module.py", line 1562, in _call_impl
    return forward_call(*args, **kwargs)
  File "/home/abdenour/anaconda3/envs/fl/lib/python3.10/site-packages/torch/nn/modules/conv.py", line 458, in forward
    return self._conv_forward(input, self.weight, self.bias)
  File "/home/abdenour/anaconda3/envs/fl/lib/python3.10/site-packages/torch/nn/modules/conv.py", line 454, in _conv_forward
    return F.conv2d(input, weight, bias, self.stride,
torch.OutOfMemoryError: CUDA out of memory. Tried to allocate 784.00 MiB. GPU 0 has a total capacity of 47.33 GiB of which 6.81 MiB is free. Process 2279413 has 11.11 GiB memory in use. Process 2265701 has 2.37 GiB memory in use. Process 2279412 has 11.11 GiB memory in use. Process 2330873 has 11.11 GiB memory in use. Process 2317160 has 2.37 GiB memory in use. Process 2330872 has 6.78 GiB memory in use. Including non-PyTorch memory, this process has 816.00 MiB memory in use. Process 2330870 has 1.61 GiB memory in use. Of the allocated memory 549.92 MiB is allocated by PyTorch, and 6.08 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)

The above exception was the direct cause of the following exception:

[36mray::ClientAppActor.run()[39m (pid=2330871, ip=115.145.172.224, actor_id=cdc8cff1bec6044f7ef6408701000000, repr=<flwr.simulation.ray_transport.ray_actor.ClientAppActor object at 0x7f75b8199750>)
  File "/home/abdenour/.local/lib/python3.10/site-packages/flwr/simulation/ray_transport/ray_actor.py", line 63, in run
    raise ClientAppException(str(ex)) from ex
flwr.client.client_app.ClientAppException: 
Exception ClientAppException occurred. Message: CUDA out of memory. Tried to allocate 784.00 MiB. GPU 0 has a total capacity of 47.33 GiB of which 6.81 MiB is free. Process 2279413 has 11.11 GiB memory in use. Process 2265701 has 2.37 GiB memory in use. Process 2279412 has 11.11 GiB memory in use. Process 2330873 has 11.11 GiB memory in use. Process 2317160 has 2.37 GiB memory in use. Process 2330872 has 6.78 GiB memory in use. Including non-PyTorch memory, this process has 816.00 MiB memory in use. Process 2330870 has 1.61 GiB memory in use. Of the allocated memory 549.92 MiB is allocated by PyTorch, and 6.08 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)

[2024-09-19 01:25:55,100][flwr][ERROR] - [36mray::ClientAppActor.run()[39m (pid=2330871, ip=115.145.172.224, actor_id=cdc8cff1bec6044f7ef6408701000000, repr=<flwr.simulation.ray_transport.ray_actor.ClientAppActor object at 0x7f75b8199750>)
  File "/home/abdenour/.local/lib/python3.10/site-packages/flwr/client/client_app.py", line 143, in __call__
    return self._call(message, context)
  File "/home/abdenour/.local/lib/python3.10/site-packages/flwr/client/client_app.py", line 126, in ffn
    out_message = handle_legacy_message_from_msgtype(
  File "/home/abdenour/.local/lib/python3.10/site-packages/flwr/client/message_handler/message_handler.py", line 129, in handle_legacy_message_from_msgtype
    fit_res = maybe_call_fit(
  File "/home/abdenour/.local/lib/python3.10/site-packages/flwr/client/client.py", line 234, in maybe_call_fit
    return client.fit(fit_ins)
  File "/home/abdenour/.local/lib/python3.10/site-packages/flwr/client/numpy_client.py", line 238, in _fit
    results = self.numpy_client.fit(parameters, ins.config)  # type: ignore
  File "/home/abdenour/fl_with_flower/fednova_vgg16/fednova_vgg16/client.py", line 69, in fit
    train(self.model, trainloader, self.optimizer, epochs=num_epochs, device=self.device)
  File "/home/abdenour/fl_with_flower/fednova_vgg16/fednova_vgg16/utils.py", line 57, in train
    loss = criterion(net(images), labels)
  File "/home/abdenour/anaconda3/envs/fl/lib/python3.10/site-packages/torch/nn/modules/module.py", line 1553, in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
  File "/home/abdenour/anaconda3/envs/fl/lib/python3.10/site-packages/torch/nn/modules/module.py", line 1562, in _call_impl
    return forward_call(*args, **kwargs)
  File "/home/abdenour/anaconda3/envs/fl/lib/python3.10/site-packages/torchvision/models/vgg.py", line 66, in forward
    x = self.features(x)
  File "/home/abdenour/anaconda3/envs/fl/lib/python3.10/site-packages/torch/nn/modules/module.py", line 1553, in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
  File "/home/abdenour/anaconda3/envs/fl/lib/python3.10/site-packages/torch/nn/modules/module.py", line 1562, in _call_impl
    return forward_call(*args, **kwargs)
  File "/home/abdenour/anaconda3/envs/fl/lib/python3.10/site-packages/torch/nn/modules/container.py", line 219, in forward
    input = module(input)
  File "/home/abdenour/anaconda3/envs/fl/lib/python3.10/site-packages/torch/nn/modules/module.py", line 1553, in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
  File "/home/abdenour/anaconda3/envs/fl/lib/python3.10/site-packages/torch/nn/modules/module.py", line 1562, in _call_impl
    return forward_call(*args, **kwargs)
  File "/home/abdenour/anaconda3/envs/fl/lib/python3.10/site-packages/torch/nn/modules/conv.py", line 458, in forward
    return self._conv_forward(input, self.weight, self.bias)
  File "/home/abdenour/anaconda3/envs/fl/lib/python3.10/site-packages/torch/nn/modules/conv.py", line 454, in _conv_forward
    return F.conv2d(input, weight, bias, self.stride,
torch.OutOfMemoryError: CUDA out of memory. Tried to allocate 784.00 MiB. GPU 0 has a total capacity of 47.33 GiB of which 6.81 MiB is free. Process 2279413 has 11.11 GiB memory in use. Process 2265701 has 2.37 GiB memory in use. Process 2279412 has 11.11 GiB memory in use. Process 2330873 has 11.11 GiB memory in use. Process 2317160 has 2.37 GiB memory in use. Process 2330872 has 6.78 GiB memory in use. Including non-PyTorch memory, this process has 816.00 MiB memory in use. Process 2330870 has 1.61 GiB memory in use. Of the allocated memory 549.92 MiB is allocated by PyTorch, and 6.08 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)

The above exception was the direct cause of the following exception:

[36mray::ClientAppActor.run()[39m (pid=2330871, ip=115.145.172.224, actor_id=cdc8cff1bec6044f7ef6408701000000, repr=<flwr.simulation.ray_transport.ray_actor.ClientAppActor object at 0x7f75b8199750>)
  File "/home/abdenour/.local/lib/python3.10/site-packages/flwr/simulation/ray_transport/ray_actor.py", line 63, in run
    raise ClientAppException(str(ex)) from ex
flwr.client.client_app.ClientAppException: 
Exception ClientAppException occurred. Message: CUDA out of memory. Tried to allocate 784.00 MiB. GPU 0 has a total capacity of 47.33 GiB of which 6.81 MiB is free. Process 2279413 has 11.11 GiB memory in use. Process 2265701 has 2.37 GiB memory in use. Process 2279412 has 11.11 GiB memory in use. Process 2330873 has 11.11 GiB memory in use. Process 2317160 has 2.37 GiB memory in use. Process 2330872 has 6.78 GiB memory in use. Including non-PyTorch memory, this process has 816.00 MiB memory in use. Process 2330870 has 1.61 GiB memory in use. Of the allocated memory 549.92 MiB is allocated by PyTorch, and 6.08 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)
[2024-09-19 01:25:56,104][flwr][ERROR] - Traceback (most recent call last):
  File "/home/abdenour/.local/lib/python3.10/site-packages/flwr/simulation/ray_transport/ray_client_proxy.py", line 94, in _submit_job
    out_mssg, updated_context = self.actor_pool.get_client_result(
  File "/home/abdenour/.local/lib/python3.10/site-packages/flwr/simulation/ray_transport/ray_actor.py", line 398, in get_client_result
    return self._fetch_future_result(cid)
  File "/home/abdenour/.local/lib/python3.10/site-packages/flwr/simulation/ray_transport/ray_actor.py", line 279, in _fetch_future_result
    res_cid, out_mssg, updated_context = ray.get(
  File "/home/abdenour/.local/lib/python3.10/site-packages/ray/_private/auto_init_hook.py", line 21, in auto_init_wrapper
    return fn(*args, **kwargs)
  File "/home/abdenour/.local/lib/python3.10/site-packages/ray/_private/client_mode_hook.py", line 103, in wrapper
    return func(*args, **kwargs)
  File "/home/abdenour/.local/lib/python3.10/site-packages/ray/_private/worker.py", line 2667, in get
    values, debugger_breakpoint = worker.get_objects(object_refs, timeout=timeout)
  File "/home/abdenour/.local/lib/python3.10/site-packages/ray/_private/worker.py", line 864, in get_objects
    raise value.as_instanceof_cause()
ray.exceptions.RayTaskError(ClientAppException): [36mray::ClientAppActor.run()[39m (pid=2330870, ip=115.145.172.224, actor_id=18884b649bbd3d098a6ec02b01000000, repr=<flwr.simulation.ray_transport.ray_actor.ClientAppActor object at 0x7ffb805757b0>)
  File "/home/abdenour/.local/lib/python3.10/site-packages/flwr/client/client_app.py", line 143, in __call__
    return self._call(message, context)
  File "/home/abdenour/.local/lib/python3.10/site-packages/flwr/client/client_app.py", line 126, in ffn
    out_message = handle_legacy_message_from_msgtype(
  File "/home/abdenour/.local/lib/python3.10/site-packages/flwr/client/message_handler/message_handler.py", line 129, in handle_legacy_message_from_msgtype
    fit_res = maybe_call_fit(
  File "/home/abdenour/.local/lib/python3.10/site-packages/flwr/client/client.py", line 234, in maybe_call_fit
    return client.fit(fit_ins)
  File "/home/abdenour/.local/lib/python3.10/site-packages/flwr/client/numpy_client.py", line 238, in _fit
    results = self.numpy_client.fit(parameters, ins.config)  # type: ignore
  File "/home/abdenour/fl_with_flower/fednova_vgg16/fednova_vgg16/client.py", line 69, in fit
    train(self.model, trainloader, self.optimizer, epochs=num_epochs, device=self.device)
  File "/home/abdenour/fl_with_flower/fednova_vgg16/fednova_vgg16/utils.py", line 57, in train
    loss = criterion(net(images), labels)
  File "/home/abdenour/anaconda3/envs/fl/lib/python3.10/site-packages/torch/nn/modules/module.py", line 1553, in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
  File "/home/abdenour/anaconda3/envs/fl/lib/python3.10/site-packages/torch/nn/modules/module.py", line 1562, in _call_impl
    return forward_call(*args, **kwargs)
  File "/home/abdenour/anaconda3/envs/fl/lib/python3.10/site-packages/torchvision/models/vgg.py", line 66, in forward
    x = self.features(x)
  File "/home/abdenour/anaconda3/envs/fl/lib/python3.10/site-packages/torch/nn/modules/module.py", line 1553, in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
  File "/home/abdenour/anaconda3/envs/fl/lib/python3.10/site-packages/torch/nn/modules/module.py", line 1562, in _call_impl
    return forward_call(*args, **kwargs)
  File "/home/abdenour/anaconda3/envs/fl/lib/python3.10/site-packages/torch/nn/modules/container.py", line 219, in forward
    input = module(input)
  File "/home/abdenour/anaconda3/envs/fl/lib/python3.10/site-packages/torch/nn/modules/module.py", line 1553, in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
  File "/home/abdenour/anaconda3/envs/fl/lib/python3.10/site-packages/torch/nn/modules/module.py", line 1562, in _call_impl
    return forward_call(*args, **kwargs)
  File "/home/abdenour/anaconda3/envs/fl/lib/python3.10/site-packages/torch/nn/modules/conv.py", line 458, in forward
    return self._conv_forward(input, self.weight, self.bias)
  File "/home/abdenour/anaconda3/envs/fl/lib/python3.10/site-packages/torch/nn/modules/conv.py", line 454, in _conv_forward
    return F.conv2d(input, weight, bias, self.stride,
torch.OutOfMemoryError: CUDA out of memory. Tried to allocate 784.00 MiB. GPU 0 has a total capacity of 47.33 GiB of which 6.81 MiB is free. Process 2279413 has 11.11 GiB memory in use. Process 2265701 has 2.37 GiB memory in use. Process 2279412 has 11.11 GiB memory in use. Process 2330873 has 11.11 GiB memory in use. Process 2317160 has 2.37 GiB memory in use. Process 2330872 has 6.78 GiB memory in use. Process 2330871 has 816.00 MiB memory in use. Including non-PyTorch memory, this process has 1.61 GiB memory in use. Of the allocated memory 1.30 GiB is allocated by PyTorch, and 6.08 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)

The above exception was the direct cause of the following exception:

[36mray::ClientAppActor.run()[39m (pid=2330870, ip=115.145.172.224, actor_id=18884b649bbd3d098a6ec02b01000000, repr=<flwr.simulation.ray_transport.ray_actor.ClientAppActor object at 0x7ffb805757b0>)
  File "/home/abdenour/.local/lib/python3.10/site-packages/flwr/simulation/ray_transport/ray_actor.py", line 63, in run
    raise ClientAppException(str(ex)) from ex
flwr.client.client_app.ClientAppException: 
Exception ClientAppException occurred. Message: CUDA out of memory. Tried to allocate 784.00 MiB. GPU 0 has a total capacity of 47.33 GiB of which 6.81 MiB is free. Process 2279413 has 11.11 GiB memory in use. Process 2265701 has 2.37 GiB memory in use. Process 2279412 has 11.11 GiB memory in use. Process 2330873 has 11.11 GiB memory in use. Process 2317160 has 2.37 GiB memory in use. Process 2330872 has 6.78 GiB memory in use. Process 2330871 has 816.00 MiB memory in use. Including non-PyTorch memory, this process has 1.61 GiB memory in use. Of the allocated memory 1.30 GiB is allocated by PyTorch, and 6.08 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)

[2024-09-19 01:25:56,105][flwr][ERROR] - [36mray::ClientAppActor.run()[39m (pid=2330870, ip=115.145.172.224, actor_id=18884b649bbd3d098a6ec02b01000000, repr=<flwr.simulation.ray_transport.ray_actor.ClientAppActor object at 0x7ffb805757b0>)
  File "/home/abdenour/.local/lib/python3.10/site-packages/flwr/client/client_app.py", line 143, in __call__
    return self._call(message, context)
  File "/home/abdenour/.local/lib/python3.10/site-packages/flwr/client/client_app.py", line 126, in ffn
    out_message = handle_legacy_message_from_msgtype(
  File "/home/abdenour/.local/lib/python3.10/site-packages/flwr/client/message_handler/message_handler.py", line 129, in handle_legacy_message_from_msgtype
    fit_res = maybe_call_fit(
  File "/home/abdenour/.local/lib/python3.10/site-packages/flwr/client/client.py", line 234, in maybe_call_fit
    return client.fit(fit_ins)
  File "/home/abdenour/.local/lib/python3.10/site-packages/flwr/client/numpy_client.py", line 238, in _fit
    results = self.numpy_client.fit(parameters, ins.config)  # type: ignore
  File "/home/abdenour/fl_with_flower/fednova_vgg16/fednova_vgg16/client.py", line 69, in fit
    train(self.model, trainloader, self.optimizer, epochs=num_epochs, device=self.device)
  File "/home/abdenour/fl_with_flower/fednova_vgg16/fednova_vgg16/utils.py", line 57, in train
    loss = criterion(net(images), labels)
  File "/home/abdenour/anaconda3/envs/fl/lib/python3.10/site-packages/torch/nn/modules/module.py", line 1553, in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
  File "/home/abdenour/anaconda3/envs/fl/lib/python3.10/site-packages/torch/nn/modules/module.py", line 1562, in _call_impl
    return forward_call(*args, **kwargs)
  File "/home/abdenour/anaconda3/envs/fl/lib/python3.10/site-packages/torchvision/models/vgg.py", line 66, in forward
    x = self.features(x)
  File "/home/abdenour/anaconda3/envs/fl/lib/python3.10/site-packages/torch/nn/modules/module.py", line 1553, in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
  File "/home/abdenour/anaconda3/envs/fl/lib/python3.10/site-packages/torch/nn/modules/module.py", line 1562, in _call_impl
    return forward_call(*args, **kwargs)
  File "/home/abdenour/anaconda3/envs/fl/lib/python3.10/site-packages/torch/nn/modules/container.py", line 219, in forward
    input = module(input)
  File "/home/abdenour/anaconda3/envs/fl/lib/python3.10/site-packages/torch/nn/modules/module.py", line 1553, in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
  File "/home/abdenour/anaconda3/envs/fl/lib/python3.10/site-packages/torch/nn/modules/module.py", line 1562, in _call_impl
    return forward_call(*args, **kwargs)
  File "/home/abdenour/anaconda3/envs/fl/lib/python3.10/site-packages/torch/nn/modules/conv.py", line 458, in forward
    return self._conv_forward(input, self.weight, self.bias)
  File "/home/abdenour/anaconda3/envs/fl/lib/python3.10/site-packages/torch/nn/modules/conv.py", line 454, in _conv_forward
    return F.conv2d(input, weight, bias, self.stride,
torch.OutOfMemoryError: CUDA out of memory. Tried to allocate 784.00 MiB. GPU 0 has a total capacity of 47.33 GiB of which 6.81 MiB is free. Process 2279413 has 11.11 GiB memory in use. Process 2265701 has 2.37 GiB memory in use. Process 2279412 has 11.11 GiB memory in use. Process 2330873 has 11.11 GiB memory in use. Process 2317160 has 2.37 GiB memory in use. Process 2330872 has 6.78 GiB memory in use. Process 2330871 has 816.00 MiB memory in use. Including non-PyTorch memory, this process has 1.61 GiB memory in use. Of the allocated memory 1.30 GiB is allocated by PyTorch, and 6.08 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)

The above exception was the direct cause of the following exception:

[36mray::ClientAppActor.run()[39m (pid=2330870, ip=115.145.172.224, actor_id=18884b649bbd3d098a6ec02b01000000, repr=<flwr.simulation.ray_transport.ray_actor.ClientAppActor object at 0x7ffb805757b0>)
  File "/home/abdenour/.local/lib/python3.10/site-packages/flwr/simulation/ray_transport/ray_actor.py", line 63, in run
    raise ClientAppException(str(ex)) from ex
flwr.client.client_app.ClientAppException: 
Exception ClientAppException occurred. Message: CUDA out of memory. Tried to allocate 784.00 MiB. GPU 0 has a total capacity of 47.33 GiB of which 6.81 MiB is free. Process 2279413 has 11.11 GiB memory in use. Process 2265701 has 2.37 GiB memory in use. Process 2279412 has 11.11 GiB memory in use. Process 2330873 has 11.11 GiB memory in use. Process 2317160 has 2.37 GiB memory in use. Process 2330872 has 6.78 GiB memory in use. Process 2330871 has 816.00 MiB memory in use. Including non-PyTorch memory, this process has 1.61 GiB memory in use. Of the allocated memory 1.30 GiB is allocated by PyTorch, and 6.08 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)
[2024-09-19 01:25:57,306][flwr][ERROR] - Traceback (most recent call last):
  File "/home/abdenour/.local/lib/python3.10/site-packages/flwr/simulation/ray_transport/ray_client_proxy.py", line 94, in _submit_job
    out_mssg, updated_context = self.actor_pool.get_client_result(
  File "/home/abdenour/.local/lib/python3.10/site-packages/flwr/simulation/ray_transport/ray_actor.py", line 398, in get_client_result
    return self._fetch_future_result(cid)
  File "/home/abdenour/.local/lib/python3.10/site-packages/flwr/simulation/ray_transport/ray_actor.py", line 279, in _fetch_future_result
    res_cid, out_mssg, updated_context = ray.get(
  File "/home/abdenour/.local/lib/python3.10/site-packages/ray/_private/auto_init_hook.py", line 21, in auto_init_wrapper
    return fn(*args, **kwargs)
  File "/home/abdenour/.local/lib/python3.10/site-packages/ray/_private/client_mode_hook.py", line 103, in wrapper
    return func(*args, **kwargs)
  File "/home/abdenour/.local/lib/python3.10/site-packages/ray/_private/worker.py", line 2667, in get
    values, debugger_breakpoint = worker.get_objects(object_refs, timeout=timeout)
  File "/home/abdenour/.local/lib/python3.10/site-packages/ray/_private/worker.py", line 864, in get_objects
    raise value.as_instanceof_cause()
ray.exceptions.RayTaskError(ClientAppException): [36mray::ClientAppActor.run()[39m (pid=2330872, ip=115.145.172.224, actor_id=d38096ee5352eefa91005c8e01000000, repr=<flwr.simulation.ray_transport.ray_actor.ClientAppActor object at 0x7fcf0478d750>)
  File "/home/abdenour/.local/lib/python3.10/site-packages/flwr/client/client_app.py", line 143, in __call__
    return self._call(message, context)
  File "/home/abdenour/.local/lib/python3.10/site-packages/flwr/client/client_app.py", line 126, in ffn
    out_message = handle_legacy_message_from_msgtype(
  File "/home/abdenour/.local/lib/python3.10/site-packages/flwr/client/message_handler/message_handler.py", line 129, in handle_legacy_message_from_msgtype
    fit_res = maybe_call_fit(
  File "/home/abdenour/.local/lib/python3.10/site-packages/flwr/client/client.py", line 234, in maybe_call_fit
    return client.fit(fit_ins)
  File "/home/abdenour/.local/lib/python3.10/site-packages/flwr/client/numpy_client.py", line 238, in _fit
    results = self.numpy_client.fit(parameters, ins.config)  # type: ignore
  File "/home/abdenour/fl_with_flower/fednova_vgg16/fednova_vgg16/client.py", line 69, in fit
    train(self.model, trainloader, self.optimizer, epochs=num_epochs, device=self.device)
  File "/home/abdenour/fl_with_flower/fednova_vgg16/fednova_vgg16/utils.py", line 58, in train
    loss.backward()
  File "/home/abdenour/anaconda3/envs/fl/lib/python3.10/site-packages/torch/_tensor.py", line 521, in backward
    torch.autograd.backward(
  File "/home/abdenour/anaconda3/envs/fl/lib/python3.10/site-packages/torch/autograd/__init__.py", line 289, in backward
    _engine_run_backward(
  File "/home/abdenour/anaconda3/envs/fl/lib/python3.10/site-packages/torch/autograd/graph.py", line 769, in _engine_run_backward
    return Variable._execution_engine.run_backward(  # Calls into the C++ engine to run the backward pass
torch.OutOfMemoryError: CUDA out of memory. Tried to allocate 98.00 MiB. GPU 0 has a total capacity of 47.33 GiB of which 6.81 MiB is free. Process 2279413 has 11.11 GiB memory in use. Process 2265701 has 2.37 GiB memory in use. Process 2279412 has 11.11 GiB memory in use. Process 2330873 has 11.11 GiB memory in use. Process 2317160 has 2.37 GiB memory in use. Including non-PyTorch memory, this process has 6.78 GiB memory in use. Process 2330871 has 816.00 MiB memory in use. Process 2330870 has 1.61 GiB memory in use. Of the allocated memory 6.26 GiB is allocated by PyTorch, and 199.47 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)

The above exception was the direct cause of the following exception:

[36mray::ClientAppActor.run()[39m (pid=2330872, ip=115.145.172.224, actor_id=d38096ee5352eefa91005c8e01000000, repr=<flwr.simulation.ray_transport.ray_actor.ClientAppActor object at 0x7fcf0478d750>)
  File "/home/abdenour/.local/lib/python3.10/site-packages/flwr/simulation/ray_transport/ray_actor.py", line 63, in run
    raise ClientAppException(str(ex)) from ex
flwr.client.client_app.ClientAppException: 
Exception ClientAppException occurred. Message: CUDA out of memory. Tried to allocate 98.00 MiB. GPU 0 has a total capacity of 47.33 GiB of which 6.81 MiB is free. Process 2279413 has 11.11 GiB memory in use. Process 2265701 has 2.37 GiB memory in use. Process 2279412 has 11.11 GiB memory in use. Process 2330873 has 11.11 GiB memory in use. Process 2317160 has 2.37 GiB memory in use. Including non-PyTorch memory, this process has 6.78 GiB memory in use. Process 2330871 has 816.00 MiB memory in use. Process 2330870 has 1.61 GiB memory in use. Of the allocated memory 6.26 GiB is allocated by PyTorch, and 199.47 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)

[2024-09-19 01:25:57,306][flwr][ERROR] - [36mray::ClientAppActor.run()[39m (pid=2330872, ip=115.145.172.224, actor_id=d38096ee5352eefa91005c8e01000000, repr=<flwr.simulation.ray_transport.ray_actor.ClientAppActor object at 0x7fcf0478d750>)
  File "/home/abdenour/.local/lib/python3.10/site-packages/flwr/client/client_app.py", line 143, in __call__
    return self._call(message, context)
  File "/home/abdenour/.local/lib/python3.10/site-packages/flwr/client/client_app.py", line 126, in ffn
    out_message = handle_legacy_message_from_msgtype(
  File "/home/abdenour/.local/lib/python3.10/site-packages/flwr/client/message_handler/message_handler.py", line 129, in handle_legacy_message_from_msgtype
    fit_res = maybe_call_fit(
  File "/home/abdenour/.local/lib/python3.10/site-packages/flwr/client/client.py", line 234, in maybe_call_fit
    return client.fit(fit_ins)
  File "/home/abdenour/.local/lib/python3.10/site-packages/flwr/client/numpy_client.py", line 238, in _fit
    results = self.numpy_client.fit(parameters, ins.config)  # type: ignore
  File "/home/abdenour/fl_with_flower/fednova_vgg16/fednova_vgg16/client.py", line 69, in fit
    train(self.model, trainloader, self.optimizer, epochs=num_epochs, device=self.device)
  File "/home/abdenour/fl_with_flower/fednova_vgg16/fednova_vgg16/utils.py", line 58, in train
    loss.backward()
  File "/home/abdenour/anaconda3/envs/fl/lib/python3.10/site-packages/torch/_tensor.py", line 521, in backward
    torch.autograd.backward(
  File "/home/abdenour/anaconda3/envs/fl/lib/python3.10/site-packages/torch/autograd/__init__.py", line 289, in backward
    _engine_run_backward(
  File "/home/abdenour/anaconda3/envs/fl/lib/python3.10/site-packages/torch/autograd/graph.py", line 769, in _engine_run_backward
    return Variable._execution_engine.run_backward(  # Calls into the C++ engine to run the backward pass
torch.OutOfMemoryError: CUDA out of memory. Tried to allocate 98.00 MiB. GPU 0 has a total capacity of 47.33 GiB of which 6.81 MiB is free. Process 2279413 has 11.11 GiB memory in use. Process 2265701 has 2.37 GiB memory in use. Process 2279412 has 11.11 GiB memory in use. Process 2330873 has 11.11 GiB memory in use. Process 2317160 has 2.37 GiB memory in use. Including non-PyTorch memory, this process has 6.78 GiB memory in use. Process 2330871 has 816.00 MiB memory in use. Process 2330870 has 1.61 GiB memory in use. Of the allocated memory 6.26 GiB is allocated by PyTorch, and 199.47 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)

The above exception was the direct cause of the following exception:

[36mray::ClientAppActor.run()[39m (pid=2330872, ip=115.145.172.224, actor_id=d38096ee5352eefa91005c8e01000000, repr=<flwr.simulation.ray_transport.ray_actor.ClientAppActor object at 0x7fcf0478d750>)
  File "/home/abdenour/.local/lib/python3.10/site-packages/flwr/simulation/ray_transport/ray_actor.py", line 63, in run
    raise ClientAppException(str(ex)) from ex
flwr.client.client_app.ClientAppException: 
Exception ClientAppException occurred. Message: CUDA out of memory. Tried to allocate 98.00 MiB. GPU 0 has a total capacity of 47.33 GiB of which 6.81 MiB is free. Process 2279413 has 11.11 GiB memory in use. Process 2265701 has 2.37 GiB memory in use. Process 2279412 has 11.11 GiB memory in use. Process 2330873 has 11.11 GiB memory in use. Process 2317160 has 2.37 GiB memory in use. Including non-PyTorch memory, this process has 6.78 GiB memory in use. Process 2330871 has 816.00 MiB memory in use. Process 2330870 has 1.61 GiB memory in use. Of the allocated memory 6.26 GiB is allocated by PyTorch, and 199.47 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)
[2024-09-19 01:25:57,307][flwr][INFO] - aggregate_fit: received 1 results and 4 failures
[2024-09-19 01:26:09,838][flwr][INFO] - fit progress: (6, 229.61092734336853, {'accuracy': 0.0932475884244373}, 151.24692753888667)
[2024-09-19 01:26:09,838][flwr][INFO] - configure_evaluate: strategy sampled 2 clients (out of 50)
[2024-09-19 01:26:13,003][flwr][ERROR] - Traceback (most recent call last):
  File "/home/abdenour/.local/lib/python3.10/site-packages/flwr/simulation/ray_transport/ray_client_proxy.py", line 94, in _submit_job
    out_mssg, updated_context = self.actor_pool.get_client_result(
  File "/home/abdenour/.local/lib/python3.10/site-packages/flwr/simulation/ray_transport/ray_actor.py", line 398, in get_client_result
    return self._fetch_future_result(cid)
  File "/home/abdenour/.local/lib/python3.10/site-packages/flwr/simulation/ray_transport/ray_actor.py", line 279, in _fetch_future_result
    res_cid, out_mssg, updated_context = ray.get(
  File "/home/abdenour/.local/lib/python3.10/site-packages/ray/_private/auto_init_hook.py", line 21, in auto_init_wrapper
    return fn(*args, **kwargs)
  File "/home/abdenour/.local/lib/python3.10/site-packages/ray/_private/client_mode_hook.py", line 103, in wrapper
    return func(*args, **kwargs)
  File "/home/abdenour/.local/lib/python3.10/site-packages/ray/_private/worker.py", line 2667, in get
    values, debugger_breakpoint = worker.get_objects(object_refs, timeout=timeout)
  File "/home/abdenour/.local/lib/python3.10/site-packages/ray/_private/worker.py", line 864, in get_objects
    raise value.as_instanceof_cause()
ray.exceptions.RayTaskError(ClientAppException): [36mray::ClientAppActor.run()[39m (pid=2330870, ip=115.145.172.224, actor_id=18884b649bbd3d098a6ec02b01000000, repr=<flwr.simulation.ray_transport.ray_actor.ClientAppActor object at 0x7ffb805757b0>)
  File "/home/abdenour/.local/lib/python3.10/site-packages/flwr/client/client_app.py", line 143, in __call__
    return self._call(message, context)
  File "/home/abdenour/.local/lib/python3.10/site-packages/flwr/client/client_app.py", line 126, in ffn
    out_message = handle_legacy_message_from_msgtype(
  File "/home/abdenour/.local/lib/python3.10/site-packages/flwr/client/message_handler/message_handler.py", line 136, in handle_legacy_message_from_msgtype
    evaluate_res = maybe_call_evaluate(
  File "/home/abdenour/.local/lib/python3.10/site-packages/flwr/client/client.py", line 254, in maybe_call_evaluate
    return client.evaluate(evaluate_ins)
  File "/home/abdenour/.local/lib/python3.10/site-packages/flwr/client/numpy_client.py", line 262, in _evaluate
    results = self.numpy_client.evaluate(parameters, ins.config)  # type: ignore
  File "/home/abdenour/fl_with_flower/fednova_vgg16/fednova_vgg16/client.py", line 85, in evaluate
    loss, accuracy = test(self.model, valloader, device=self.device)
  File "/home/abdenour/fl_with_flower/fednova_vgg16/fednova_vgg16/utils.py", line 71, in test
    outputs = net(images)
  File "/home/abdenour/anaconda3/envs/fl/lib/python3.10/site-packages/torch/nn/modules/module.py", line 1553, in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
  File "/home/abdenour/anaconda3/envs/fl/lib/python3.10/site-packages/torch/nn/modules/module.py", line 1562, in _call_impl
    return forward_call(*args, **kwargs)
  File "/home/abdenour/anaconda3/envs/fl/lib/python3.10/site-packages/torchvision/models/vgg.py", line 69, in forward
    x = self.classifier(x)
  File "/home/abdenour/anaconda3/envs/fl/lib/python3.10/site-packages/torch/nn/modules/module.py", line 1553, in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
  File "/home/abdenour/anaconda3/envs/fl/lib/python3.10/site-packages/torch/nn/modules/module.py", line 1562, in _call_impl
    return forward_call(*args, **kwargs)
  File "/home/abdenour/anaconda3/envs/fl/lib/python3.10/site-packages/torch/nn/modules/container.py", line 219, in forward
    input = module(input)
  File "/home/abdenour/anaconda3/envs/fl/lib/python3.10/site-packages/torch/nn/modules/module.py", line 1553, in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
  File "/home/abdenour/anaconda3/envs/fl/lib/python3.10/site-packages/torch/nn/modules/module.py", line 1562, in _call_impl
    return forward_call(*args, **kwargs)
  File "/home/abdenour/anaconda3/envs/fl/lib/python3.10/site-packages/torch/nn/modules/linear.py", line 117, in forward
    return F.linear(input, self.weight, self.bias)
RuntimeError: CUDA error: CUBLAS_STATUS_ALLOC_FAILED when calling `cublasCreate(handle)`

The above exception was the direct cause of the following exception:

[36mray::ClientAppActor.run()[39m (pid=2330870, ip=115.145.172.224, actor_id=18884b649bbd3d098a6ec02b01000000, repr=<flwr.simulation.ray_transport.ray_actor.ClientAppActor object at 0x7ffb805757b0>)
  File "/home/abdenour/.local/lib/python3.10/site-packages/flwr/simulation/ray_transport/ray_actor.py", line 63, in run
    raise ClientAppException(str(ex)) from ex
flwr.client.client_app.ClientAppException: 
Exception ClientAppException occurred. Message: CUDA error: CUBLAS_STATUS_ALLOC_FAILED when calling `cublasCreate(handle)`

[2024-09-19 01:26:13,003][flwr][ERROR] - [36mray::ClientAppActor.run()[39m (pid=2330870, ip=115.145.172.224, actor_id=18884b649bbd3d098a6ec02b01000000, repr=<flwr.simulation.ray_transport.ray_actor.ClientAppActor object at 0x7ffb805757b0>)
  File "/home/abdenour/.local/lib/python3.10/site-packages/flwr/client/client_app.py", line 143, in __call__
    return self._call(message, context)
  File "/home/abdenour/.local/lib/python3.10/site-packages/flwr/client/client_app.py", line 126, in ffn
    out_message = handle_legacy_message_from_msgtype(
  File "/home/abdenour/.local/lib/python3.10/site-packages/flwr/client/message_handler/message_handler.py", line 136, in handle_legacy_message_from_msgtype
    evaluate_res = maybe_call_evaluate(
  File "/home/abdenour/.local/lib/python3.10/site-packages/flwr/client/client.py", line 254, in maybe_call_evaluate
    return client.evaluate(evaluate_ins)
  File "/home/abdenour/.local/lib/python3.10/site-packages/flwr/client/numpy_client.py", line 262, in _evaluate
    results = self.numpy_client.evaluate(parameters, ins.config)  # type: ignore
  File "/home/abdenour/fl_with_flower/fednova_vgg16/fednova_vgg16/client.py", line 85, in evaluate
    loss, accuracy = test(self.model, valloader, device=self.device)
  File "/home/abdenour/fl_with_flower/fednova_vgg16/fednova_vgg16/utils.py", line 71, in test
    outputs = net(images)
  File "/home/abdenour/anaconda3/envs/fl/lib/python3.10/site-packages/torch/nn/modules/module.py", line 1553, in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
  File "/home/abdenour/anaconda3/envs/fl/lib/python3.10/site-packages/torch/nn/modules/module.py", line 1562, in _call_impl
    return forward_call(*args, **kwargs)
  File "/home/abdenour/anaconda3/envs/fl/lib/python3.10/site-packages/torchvision/models/vgg.py", line 69, in forward
    x = self.classifier(x)
  File "/home/abdenour/anaconda3/envs/fl/lib/python3.10/site-packages/torch/nn/modules/module.py", line 1553, in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
  File "/home/abdenour/anaconda3/envs/fl/lib/python3.10/site-packages/torch/nn/modules/module.py", line 1562, in _call_impl
    return forward_call(*args, **kwargs)
  File "/home/abdenour/anaconda3/envs/fl/lib/python3.10/site-packages/torch/nn/modules/container.py", line 219, in forward
    input = module(input)
  File "/home/abdenour/anaconda3/envs/fl/lib/python3.10/site-packages/torch/nn/modules/module.py", line 1553, in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
  File "/home/abdenour/anaconda3/envs/fl/lib/python3.10/site-packages/torch/nn/modules/module.py", line 1562, in _call_impl
    return forward_call(*args, **kwargs)
  File "/home/abdenour/anaconda3/envs/fl/lib/python3.10/site-packages/torch/nn/modules/linear.py", line 117, in forward
    return F.linear(input, self.weight, self.bias)
RuntimeError: CUDA error: CUBLAS_STATUS_ALLOC_FAILED when calling `cublasCreate(handle)`

The above exception was the direct cause of the following exception:

[36mray::ClientAppActor.run()[39m (pid=2330870, ip=115.145.172.224, actor_id=18884b649bbd3d098a6ec02b01000000, repr=<flwr.simulation.ray_transport.ray_actor.ClientAppActor object at 0x7ffb805757b0>)
  File "/home/abdenour/.local/lib/python3.10/site-packages/flwr/simulation/ray_transport/ray_actor.py", line 63, in run
    raise ClientAppException(str(ex)) from ex
flwr.client.client_app.ClientAppException: 
Exception ClientAppException occurred. Message: CUDA error: CUBLAS_STATUS_ALLOC_FAILED when calling `cublasCreate(handle)`
[2024-09-19 01:26:13,003][flwr][INFO] - aggregate_evaluate: received 1 results and 1 failures
[2024-09-19 01:26:13,004][flwr][INFO] - 
[2024-09-19 01:26:13,004][flwr][INFO] - [ROUND 7]
[2024-09-19 01:26:13,004][flwr][INFO] - configure_fit: strategy sampled 5 clients (out of 50)
[2024-09-19 01:26:16,945][flwr][ERROR] - Traceback (most recent call last):
  File "/home/abdenour/.local/lib/python3.10/site-packages/flwr/simulation/ray_transport/ray_client_proxy.py", line 94, in _submit_job
    out_mssg, updated_context = self.actor_pool.get_client_result(
  File "/home/abdenour/.local/lib/python3.10/site-packages/flwr/simulation/ray_transport/ray_actor.py", line 398, in get_client_result
    return self._fetch_future_result(cid)
  File "/home/abdenour/.local/lib/python3.10/site-packages/flwr/simulation/ray_transport/ray_actor.py", line 279, in _fetch_future_result
    res_cid, out_mssg, updated_context = ray.get(
  File "/home/abdenour/.local/lib/python3.10/site-packages/ray/_private/auto_init_hook.py", line 21, in auto_init_wrapper
    return fn(*args, **kwargs)
  File "/home/abdenour/.local/lib/python3.10/site-packages/ray/_private/client_mode_hook.py", line 103, in wrapper
    return func(*args, **kwargs)
  File "/home/abdenour/.local/lib/python3.10/site-packages/ray/_private/worker.py", line 2667, in get
    values, debugger_breakpoint = worker.get_objects(object_refs, timeout=timeout)
  File "/home/abdenour/.local/lib/python3.10/site-packages/ray/_private/worker.py", line 864, in get_objects
    raise value.as_instanceof_cause()
ray.exceptions.RayTaskError(ClientAppException): [36mray::ClientAppActor.run()[39m (pid=2330870, ip=115.145.172.224, actor_id=18884b649bbd3d098a6ec02b01000000, repr=<flwr.simulation.ray_transport.ray_actor.ClientAppActor object at 0x7ffb805757b0>)
  File "/home/abdenour/.local/lib/python3.10/site-packages/flwr/client/client_app.py", line 143, in __call__
    return self._call(message, context)
  File "/home/abdenour/.local/lib/python3.10/site-packages/flwr/client/client_app.py", line 126, in ffn
    out_message = handle_legacy_message_from_msgtype(
  File "/home/abdenour/.local/lib/python3.10/site-packages/flwr/client/message_handler/message_handler.py", line 129, in handle_legacy_message_from_msgtype
    fit_res = maybe_call_fit(
  File "/home/abdenour/.local/lib/python3.10/site-packages/flwr/client/client.py", line 234, in maybe_call_fit
    return client.fit(fit_ins)
  File "/home/abdenour/.local/lib/python3.10/site-packages/flwr/client/numpy_client.py", line 238, in _fit
    results = self.numpy_client.fit(parameters, ins.config)  # type: ignore
  File "/home/abdenour/fl_with_flower/fednova_vgg16/fednova_vgg16/client.py", line 69, in fit
    train(self.model, trainloader, self.optimizer, epochs=num_epochs, device=self.device)
  File "/home/abdenour/fl_with_flower/fednova_vgg16/fednova_vgg16/utils.py", line 57, in train
    loss = criterion(net(images), labels)
  File "/home/abdenour/anaconda3/envs/fl/lib/python3.10/site-packages/torch/nn/modules/module.py", line 1553, in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
  File "/home/abdenour/anaconda3/envs/fl/lib/python3.10/site-packages/torch/nn/modules/module.py", line 1562, in _call_impl
    return forward_call(*args, **kwargs)
  File "/home/abdenour/anaconda3/envs/fl/lib/python3.10/site-packages/torchvision/models/vgg.py", line 66, in forward
    x = self.features(x)
  File "/home/abdenour/anaconda3/envs/fl/lib/python3.10/site-packages/torch/nn/modules/module.py", line 1553, in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
  File "/home/abdenour/anaconda3/envs/fl/lib/python3.10/site-packages/torch/nn/modules/module.py", line 1562, in _call_impl
    return forward_call(*args, **kwargs)
  File "/home/abdenour/anaconda3/envs/fl/lib/python3.10/site-packages/torch/nn/modules/container.py", line 219, in forward
    input = module(input)
  File "/home/abdenour/anaconda3/envs/fl/lib/python3.10/site-packages/torch/nn/modules/module.py", line 1553, in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
  File "/home/abdenour/anaconda3/envs/fl/lib/python3.10/site-packages/torch/nn/modules/module.py", line 1562, in _call_impl
    return forward_call(*args, **kwargs)
  File "/home/abdenour/anaconda3/envs/fl/lib/python3.10/site-packages/torch/nn/modules/conv.py", line 458, in forward
    return self._conv_forward(input, self.weight, self.bias)
  File "/home/abdenour/anaconda3/envs/fl/lib/python3.10/site-packages/torch/nn/modules/conv.py", line 454, in _conv_forward
    return F.conv2d(input, weight, bias, self.stride,
torch.OutOfMemoryError: CUDA out of memory. Tried to allocate 784.00 MiB. GPU 0 has a total capacity of 47.33 GiB of which 6.81 MiB is free. Process 2279413 has 11.11 GiB memory in use. Process 2265701 has 2.37 GiB memory in use. Process 2279412 has 11.11 GiB memory in use. Process 2330873 has 11.11 GiB memory in use. Process 2317160 has 2.37 GiB memory in use. Process 2330872 has 6.78 GiB memory in use. Process 2330871 has 816.00 MiB memory in use. Including non-PyTorch memory, this process has 1.61 GiB memory in use. Of the allocated memory 1.30 GiB is allocated by PyTorch, and 6.08 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)

The above exception was the direct cause of the following exception:

[36mray::ClientAppActor.run()[39m (pid=2330870, ip=115.145.172.224, actor_id=18884b649bbd3d098a6ec02b01000000, repr=<flwr.simulation.ray_transport.ray_actor.ClientAppActor object at 0x7ffb805757b0>)
  File "/home/abdenour/.local/lib/python3.10/site-packages/flwr/simulation/ray_transport/ray_actor.py", line 63, in run
    raise ClientAppException(str(ex)) from ex
flwr.client.client_app.ClientAppException: 
Exception ClientAppException occurred. Message: CUDA out of memory. Tried to allocate 784.00 MiB. GPU 0 has a total capacity of 47.33 GiB of which 6.81 MiB is free. Process 2279413 has 11.11 GiB memory in use. Process 2265701 has 2.37 GiB memory in use. Process 2279412 has 11.11 GiB memory in use. Process 2330873 has 11.11 GiB memory in use. Process 2317160 has 2.37 GiB memory in use. Process 2330872 has 6.78 GiB memory in use. Process 2330871 has 816.00 MiB memory in use. Including non-PyTorch memory, this process has 1.61 GiB memory in use. Of the allocated memory 1.30 GiB is allocated by PyTorch, and 6.08 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)

[2024-09-19 01:26:16,946][flwr][ERROR] - [36mray::ClientAppActor.run()[39m (pid=2330870, ip=115.145.172.224, actor_id=18884b649bbd3d098a6ec02b01000000, repr=<flwr.simulation.ray_transport.ray_actor.ClientAppActor object at 0x7ffb805757b0>)
  File "/home/abdenour/.local/lib/python3.10/site-packages/flwr/client/client_app.py", line 143, in __call__
    return self._call(message, context)
  File "/home/abdenour/.local/lib/python3.10/site-packages/flwr/client/client_app.py", line 126, in ffn
    out_message = handle_legacy_message_from_msgtype(
  File "/home/abdenour/.local/lib/python3.10/site-packages/flwr/client/message_handler/message_handler.py", line 129, in handle_legacy_message_from_msgtype
    fit_res = maybe_call_fit(
  File "/home/abdenour/.local/lib/python3.10/site-packages/flwr/client/client.py", line 234, in maybe_call_fit
    return client.fit(fit_ins)
  File "/home/abdenour/.local/lib/python3.10/site-packages/flwr/client/numpy_client.py", line 238, in _fit
    results = self.numpy_client.fit(parameters, ins.config)  # type: ignore
  File "/home/abdenour/fl_with_flower/fednova_vgg16/fednova_vgg16/client.py", line 69, in fit
    train(self.model, trainloader, self.optimizer, epochs=num_epochs, device=self.device)
  File "/home/abdenour/fl_with_flower/fednova_vgg16/fednova_vgg16/utils.py", line 57, in train
    loss = criterion(net(images), labels)
  File "/home/abdenour/anaconda3/envs/fl/lib/python3.10/site-packages/torch/nn/modules/module.py", line 1553, in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
  File "/home/abdenour/anaconda3/envs/fl/lib/python3.10/site-packages/torch/nn/modules/module.py", line 1562, in _call_impl
    return forward_call(*args, **kwargs)
  File "/home/abdenour/anaconda3/envs/fl/lib/python3.10/site-packages/torchvision/models/vgg.py", line 66, in forward
    x = self.features(x)
  File "/home/abdenour/anaconda3/envs/fl/lib/python3.10/site-packages/torch/nn/modules/module.py", line 1553, in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
  File "/home/abdenour/anaconda3/envs/fl/lib/python3.10/site-packages/torch/nn/modules/module.py", line 1562, in _call_impl
    return forward_call(*args, **kwargs)
  File "/home/abdenour/anaconda3/envs/fl/lib/python3.10/site-packages/torch/nn/modules/container.py", line 219, in forward
    input = module(input)
  File "/home/abdenour/anaconda3/envs/fl/lib/python3.10/site-packages/torch/nn/modules/module.py", line 1553, in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
  File "/home/abdenour/anaconda3/envs/fl/lib/python3.10/site-packages/torch/nn/modules/module.py", line 1562, in _call_impl
    return forward_call(*args, **kwargs)
  File "/home/abdenour/anaconda3/envs/fl/lib/python3.10/site-packages/torch/nn/modules/conv.py", line 458, in forward
    return self._conv_forward(input, self.weight, self.bias)
  File "/home/abdenour/anaconda3/envs/fl/lib/python3.10/site-packages/torch/nn/modules/conv.py", line 454, in _conv_forward
    return F.conv2d(input, weight, bias, self.stride,
torch.OutOfMemoryError: CUDA out of memory. Tried to allocate 784.00 MiB. GPU 0 has a total capacity of 47.33 GiB of which 6.81 MiB is free. Process 2279413 has 11.11 GiB memory in use. Process 2265701 has 2.37 GiB memory in use. Process 2279412 has 11.11 GiB memory in use. Process 2330873 has 11.11 GiB memory in use. Process 2317160 has 2.37 GiB memory in use. Process 2330872 has 6.78 GiB memory in use. Process 2330871 has 816.00 MiB memory in use. Including non-PyTorch memory, this process has 1.61 GiB memory in use. Of the allocated memory 1.30 GiB is allocated by PyTorch, and 6.08 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)

The above exception was the direct cause of the following exception:

[36mray::ClientAppActor.run()[39m (pid=2330870, ip=115.145.172.224, actor_id=18884b649bbd3d098a6ec02b01000000, repr=<flwr.simulation.ray_transport.ray_actor.ClientAppActor object at 0x7ffb805757b0>)
  File "/home/abdenour/.local/lib/python3.10/site-packages/flwr/simulation/ray_transport/ray_actor.py", line 63, in run
    raise ClientAppException(str(ex)) from ex
flwr.client.client_app.ClientAppException: 
Exception ClientAppException occurred. Message: CUDA out of memory. Tried to allocate 784.00 MiB. GPU 0 has a total capacity of 47.33 GiB of which 6.81 MiB is free. Process 2279413 has 11.11 GiB memory in use. Process 2265701 has 2.37 GiB memory in use. Process 2279412 has 11.11 GiB memory in use. Process 2330873 has 11.11 GiB memory in use. Process 2317160 has 2.37 GiB memory in use. Process 2330872 has 6.78 GiB memory in use. Process 2330871 has 816.00 MiB memory in use. Including non-PyTorch memory, this process has 1.61 GiB memory in use. Of the allocated memory 1.30 GiB is allocated by PyTorch, and 6.08 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)
[2024-09-19 01:26:16,947][flwr][ERROR] - Traceback (most recent call last):
  File "/home/abdenour/.local/lib/python3.10/site-packages/flwr/simulation/ray_transport/ray_client_proxy.py", line 94, in _submit_job
    out_mssg, updated_context = self.actor_pool.get_client_result(
  File "/home/abdenour/.local/lib/python3.10/site-packages/flwr/simulation/ray_transport/ray_actor.py", line 398, in get_client_result
    return self._fetch_future_result(cid)
  File "/home/abdenour/.local/lib/python3.10/site-packages/flwr/simulation/ray_transport/ray_actor.py", line 279, in _fetch_future_result
    res_cid, out_mssg, updated_context = ray.get(
  File "/home/abdenour/.local/lib/python3.10/site-packages/ray/_private/auto_init_hook.py", line 21, in auto_init_wrapper
    return fn(*args, **kwargs)
  File "/home/abdenour/.local/lib/python3.10/site-packages/ray/_private/client_mode_hook.py", line 103, in wrapper
    return func(*args, **kwargs)
  File "/home/abdenour/.local/lib/python3.10/site-packages/ray/_private/worker.py", line 2667, in get
    values, debugger_breakpoint = worker.get_objects(object_refs, timeout=timeout)
  File "/home/abdenour/.local/lib/python3.10/site-packages/ray/_private/worker.py", line 864, in get_objects
    raise value.as_instanceof_cause()
ray.exceptions.RayTaskError(ClientAppException): [36mray::ClientAppActor.run()[39m (pid=2330872, ip=115.145.172.224, actor_id=d38096ee5352eefa91005c8e01000000, repr=<flwr.simulation.ray_transport.ray_actor.ClientAppActor object at 0x7fcf0478d750>)
  File "/home/abdenour/.local/lib/python3.10/site-packages/flwr/client/client_app.py", line 143, in __call__
    return self._call(message, context)
  File "/home/abdenour/.local/lib/python3.10/site-packages/flwr/client/client_app.py", line 126, in ffn
    out_message = handle_legacy_message_from_msgtype(
  File "/home/abdenour/.local/lib/python3.10/site-packages/flwr/client/message_handler/message_handler.py", line 129, in handle_legacy_message_from_msgtype
    fit_res = maybe_call_fit(
  File "/home/abdenour/.local/lib/python3.10/site-packages/flwr/client/client.py", line 234, in maybe_call_fit
    return client.fit(fit_ins)
  File "/home/abdenour/.local/lib/python3.10/site-packages/flwr/client/numpy_client.py", line 238, in _fit
    results = self.numpy_client.fit(parameters, ins.config)  # type: ignore
  File "/home/abdenour/fl_with_flower/fednova_vgg16/fednova_vgg16/client.py", line 69, in fit
    train(self.model, trainloader, self.optimizer, epochs=num_epochs, device=self.device)
  File "/home/abdenour/fl_with_flower/fednova_vgg16/fednova_vgg16/utils.py", line 58, in train
    loss.backward()
  File "/home/abdenour/anaconda3/envs/fl/lib/python3.10/site-packages/torch/_tensor.py", line 521, in backward
    torch.autograd.backward(
  File "/home/abdenour/anaconda3/envs/fl/lib/python3.10/site-packages/torch/autograd/__init__.py", line 289, in backward
    _engine_run_backward(
  File "/home/abdenour/anaconda3/envs/fl/lib/python3.10/site-packages/torch/autograd/graph.py", line 769, in _engine_run_backward
    return Variable._execution_engine.run_backward(  # Calls into the C++ engine to run the backward pass
torch.OutOfMemoryError: CUDA out of memory. Tried to allocate 98.00 MiB. GPU 0 has a total capacity of 47.33 GiB of which 6.81 MiB is free. Process 2279413 has 11.11 GiB memory in use. Process 2265701 has 2.37 GiB memory in use. Process 2279412 has 11.11 GiB memory in use. Process 2330873 has 11.11 GiB memory in use. Process 2317160 has 2.37 GiB memory in use. Including non-PyTorch memory, this process has 6.78 GiB memory in use. Process 2330871 has 816.00 MiB memory in use. Process 2330870 has 1.61 GiB memory in use. Of the allocated memory 6.26 GiB is allocated by PyTorch, and 199.47 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)

The above exception was the direct cause of the following exception:

[36mray::ClientAppActor.run()[39m (pid=2330872, ip=115.145.172.224, actor_id=d38096ee5352eefa91005c8e01000000, repr=<flwr.simulation.ray_transport.ray_actor.ClientAppActor object at 0x7fcf0478d750>)
  File "/home/abdenour/.local/lib/python3.10/site-packages/flwr/simulation/ray_transport/ray_actor.py", line 63, in run
    raise ClientAppException(str(ex)) from ex
flwr.client.client_app.ClientAppException: 
Exception ClientAppException occurred. Message: CUDA out of memory. Tried to allocate 98.00 MiB. GPU 0 has a total capacity of 47.33 GiB of which 6.81 MiB is free. Process 2279413 has 11.11 GiB memory in use. Process 2265701 has 2.37 GiB memory in use. Process 2279412 has 11.11 GiB memory in use. Process 2330873 has 11.11 GiB memory in use. Process 2317160 has 2.37 GiB memory in use. Including non-PyTorch memory, this process has 6.78 GiB memory in use. Process 2330871 has 816.00 MiB memory in use. Process 2330870 has 1.61 GiB memory in use. Of the allocated memory 6.26 GiB is allocated by PyTorch, and 199.47 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)

[2024-09-19 01:26:16,947][flwr][ERROR] - [36mray::ClientAppActor.run()[39m (pid=2330872, ip=115.145.172.224, actor_id=d38096ee5352eefa91005c8e01000000, repr=<flwr.simulation.ray_transport.ray_actor.ClientAppActor object at 0x7fcf0478d750>)
  File "/home/abdenour/.local/lib/python3.10/site-packages/flwr/client/client_app.py", line 143, in __call__
    return self._call(message, context)
  File "/home/abdenour/.local/lib/python3.10/site-packages/flwr/client/client_app.py", line 126, in ffn
    out_message = handle_legacy_message_from_msgtype(
  File "/home/abdenour/.local/lib/python3.10/site-packages/flwr/client/message_handler/message_handler.py", line 129, in handle_legacy_message_from_msgtype
    fit_res = maybe_call_fit(
  File "/home/abdenour/.local/lib/python3.10/site-packages/flwr/client/client.py", line 234, in maybe_call_fit
    return client.fit(fit_ins)
  File "/home/abdenour/.local/lib/python3.10/site-packages/flwr/client/numpy_client.py", line 238, in _fit
    results = self.numpy_client.fit(parameters, ins.config)  # type: ignore
  File "/home/abdenour/fl_with_flower/fednova_vgg16/fednova_vgg16/client.py", line 69, in fit
    train(self.model, trainloader, self.optimizer, epochs=num_epochs, device=self.device)
  File "/home/abdenour/fl_with_flower/fednova_vgg16/fednova_vgg16/utils.py", line 58, in train
    loss.backward()
  File "/home/abdenour/anaconda3/envs/fl/lib/python3.10/site-packages/torch/_tensor.py", line 521, in backward
    torch.autograd.backward(
  File "/home/abdenour/anaconda3/envs/fl/lib/python3.10/site-packages/torch/autograd/__init__.py", line 289, in backward
    _engine_run_backward(
  File "/home/abdenour/anaconda3/envs/fl/lib/python3.10/site-packages/torch/autograd/graph.py", line 769, in _engine_run_backward
    return Variable._execution_engine.run_backward(  # Calls into the C++ engine to run the backward pass
torch.OutOfMemoryError: CUDA out of memory. Tried to allocate 98.00 MiB. GPU 0 has a total capacity of 47.33 GiB of which 6.81 MiB is free. Process 2279413 has 11.11 GiB memory in use. Process 2265701 has 2.37 GiB memory in use. Process 2279412 has 11.11 GiB memory in use. Process 2330873 has 11.11 GiB memory in use. Process 2317160 has 2.37 GiB memory in use. Including non-PyTorch memory, this process has 6.78 GiB memory in use. Process 2330871 has 816.00 MiB memory in use. Process 2330870 has 1.61 GiB memory in use. Of the allocated memory 6.26 GiB is allocated by PyTorch, and 199.47 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)

The above exception was the direct cause of the following exception:

[36mray::ClientAppActor.run()[39m (pid=2330872, ip=115.145.172.224, actor_id=d38096ee5352eefa91005c8e01000000, repr=<flwr.simulation.ray_transport.ray_actor.ClientAppActor object at 0x7fcf0478d750>)
  File "/home/abdenour/.local/lib/python3.10/site-packages/flwr/simulation/ray_transport/ray_actor.py", line 63, in run
    raise ClientAppException(str(ex)) from ex
flwr.client.client_app.ClientAppException: 
Exception ClientAppException occurred. Message: CUDA out of memory. Tried to allocate 98.00 MiB. GPU 0 has a total capacity of 47.33 GiB of which 6.81 MiB is free. Process 2279413 has 11.11 GiB memory in use. Process 2265701 has 2.37 GiB memory in use. Process 2279412 has 11.11 GiB memory in use. Process 2330873 has 11.11 GiB memory in use. Process 2317160 has 2.37 GiB memory in use. Including non-PyTorch memory, this process has 6.78 GiB memory in use. Process 2330871 has 816.00 MiB memory in use. Process 2330870 has 1.61 GiB memory in use. Of the allocated memory 6.26 GiB is allocated by PyTorch, and 199.47 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)
[2024-09-19 01:26:18,393][flwr][ERROR] - Traceback (most recent call last):
  File "/home/abdenour/.local/lib/python3.10/site-packages/flwr/simulation/ray_transport/ray_client_proxy.py", line 94, in _submit_job
    out_mssg, updated_context = self.actor_pool.get_client_result(
  File "/home/abdenour/.local/lib/python3.10/site-packages/flwr/simulation/ray_transport/ray_actor.py", line 398, in get_client_result
    return self._fetch_future_result(cid)
  File "/home/abdenour/.local/lib/python3.10/site-packages/flwr/simulation/ray_transport/ray_actor.py", line 279, in _fetch_future_result
    res_cid, out_mssg, updated_context = ray.get(
  File "/home/abdenour/.local/lib/python3.10/site-packages/ray/_private/auto_init_hook.py", line 21, in auto_init_wrapper
    return fn(*args, **kwargs)
  File "/home/abdenour/.local/lib/python3.10/site-packages/ray/_private/client_mode_hook.py", line 103, in wrapper
    return func(*args, **kwargs)
  File "/home/abdenour/.local/lib/python3.10/site-packages/ray/_private/worker.py", line 2667, in get
    values, debugger_breakpoint = worker.get_objects(object_refs, timeout=timeout)
  File "/home/abdenour/.local/lib/python3.10/site-packages/ray/_private/worker.py", line 864, in get_objects
    raise value.as_instanceof_cause()
ray.exceptions.RayTaskError(ClientAppException): [36mray::ClientAppActor.run()[39m (pid=2330871, ip=115.145.172.224, actor_id=cdc8cff1bec6044f7ef6408701000000, repr=<flwr.simulation.ray_transport.ray_actor.ClientAppActor object at 0x7f75b8199750>)
  File "/home/abdenour/.local/lib/python3.10/site-packages/flwr/client/client_app.py", line 143, in __call__
    return self._call(message, context)
  File "/home/abdenour/.local/lib/python3.10/site-packages/flwr/client/client_app.py", line 126, in ffn
    out_message = handle_legacy_message_from_msgtype(
  File "/home/abdenour/.local/lib/python3.10/site-packages/flwr/client/message_handler/message_handler.py", line 129, in handle_legacy_message_from_msgtype
    fit_res = maybe_call_fit(
  File "/home/abdenour/.local/lib/python3.10/site-packages/flwr/client/client.py", line 234, in maybe_call_fit
    return client.fit(fit_ins)
  File "/home/abdenour/.local/lib/python3.10/site-packages/flwr/client/numpy_client.py", line 238, in _fit
    results = self.numpy_client.fit(parameters, ins.config)  # type: ignore
  File "/home/abdenour/fl_with_flower/fednova_vgg16/fednova_vgg16/client.py", line 69, in fit
    train(self.model, trainloader, self.optimizer, epochs=num_epochs, device=self.device)
  File "/home/abdenour/fl_with_flower/fednova_vgg16/fednova_vgg16/utils.py", line 57, in train
    loss = criterion(net(images), labels)
  File "/home/abdenour/anaconda3/envs/fl/lib/python3.10/site-packages/torch/nn/modules/module.py", line 1553, in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
  File "/home/abdenour/anaconda3/envs/fl/lib/python3.10/site-packages/torch/nn/modules/module.py", line 1562, in _call_impl
    return forward_call(*args, **kwargs)
  File "/home/abdenour/anaconda3/envs/fl/lib/python3.10/site-packages/torchvision/models/vgg.py", line 66, in forward
    x = self.features(x)
  File "/home/abdenour/anaconda3/envs/fl/lib/python3.10/site-packages/torch/nn/modules/module.py", line 1553, in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
  File "/home/abdenour/anaconda3/envs/fl/lib/python3.10/site-packages/torch/nn/modules/module.py", line 1562, in _call_impl
    return forward_call(*args, **kwargs)
  File "/home/abdenour/anaconda3/envs/fl/lib/python3.10/site-packages/torch/nn/modules/container.py", line 219, in forward
    input = module(input)
  File "/home/abdenour/anaconda3/envs/fl/lib/python3.10/site-packages/torch/nn/modules/module.py", line 1553, in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
  File "/home/abdenour/anaconda3/envs/fl/lib/python3.10/site-packages/torch/nn/modules/module.py", line 1562, in _call_impl
    return forward_call(*args, **kwargs)
  File "/home/abdenour/anaconda3/envs/fl/lib/python3.10/site-packages/torch/nn/modules/conv.py", line 458, in forward
    return self._conv_forward(input, self.weight, self.bias)
  File "/home/abdenour/anaconda3/envs/fl/lib/python3.10/site-packages/torch/nn/modules/conv.py", line 454, in _conv_forward
    return F.conv2d(input, weight, bias, self.stride,
torch.OutOfMemoryError: CUDA out of memory. Tried to allocate 784.00 MiB. GPU 0 has a total capacity of 47.33 GiB of which 6.81 MiB is free. Process 2279413 has 11.11 GiB memory in use. Process 2265701 has 2.37 GiB memory in use. Process 2279412 has 11.11 GiB memory in use. Process 2330873 has 11.11 GiB memory in use. Process 2317160 has 2.37 GiB memory in use. Process 2330872 has 6.78 GiB memory in use. Including non-PyTorch memory, this process has 816.00 MiB memory in use. Process 2330870 has 1.61 GiB memory in use. Of the allocated memory 549.92 MiB is allocated by PyTorch, and 6.08 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)

The above exception was the direct cause of the following exception:

[36mray::ClientAppActor.run()[39m (pid=2330871, ip=115.145.172.224, actor_id=cdc8cff1bec6044f7ef6408701000000, repr=<flwr.simulation.ray_transport.ray_actor.ClientAppActor object at 0x7f75b8199750>)
  File "/home/abdenour/.local/lib/python3.10/site-packages/flwr/simulation/ray_transport/ray_actor.py", line 63, in run
    raise ClientAppException(str(ex)) from ex
flwr.client.client_app.ClientAppException: 
Exception ClientAppException occurred. Message: CUDA out of memory. Tried to allocate 784.00 MiB. GPU 0 has a total capacity of 47.33 GiB of which 6.81 MiB is free. Process 2279413 has 11.11 GiB memory in use. Process 2265701 has 2.37 GiB memory in use. Process 2279412 has 11.11 GiB memory in use. Process 2330873 has 11.11 GiB memory in use. Process 2317160 has 2.37 GiB memory in use. Process 2330872 has 6.78 GiB memory in use. Including non-PyTorch memory, this process has 816.00 MiB memory in use. Process 2330870 has 1.61 GiB memory in use. Of the allocated memory 549.92 MiB is allocated by PyTorch, and 6.08 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)

[2024-09-19 01:26:18,395][flwr][ERROR] - [36mray::ClientAppActor.run()[39m (pid=2330871, ip=115.145.172.224, actor_id=cdc8cff1bec6044f7ef6408701000000, repr=<flwr.simulation.ray_transport.ray_actor.ClientAppActor object at 0x7f75b8199750>)
  File "/home/abdenour/.local/lib/python3.10/site-packages/flwr/client/client_app.py", line 143, in __call__
    return self._call(message, context)
  File "/home/abdenour/.local/lib/python3.10/site-packages/flwr/client/client_app.py", line 126, in ffn
    out_message = handle_legacy_message_from_msgtype(
  File "/home/abdenour/.local/lib/python3.10/site-packages/flwr/client/message_handler/message_handler.py", line 129, in handle_legacy_message_from_msgtype
    fit_res = maybe_call_fit(
  File "/home/abdenour/.local/lib/python3.10/site-packages/flwr/client/client.py", line 234, in maybe_call_fit
    return client.fit(fit_ins)
  File "/home/abdenour/.local/lib/python3.10/site-packages/flwr/client/numpy_client.py", line 238, in _fit
    results = self.numpy_client.fit(parameters, ins.config)  # type: ignore
  File "/home/abdenour/fl_with_flower/fednova_vgg16/fednova_vgg16/client.py", line 69, in fit
    train(self.model, trainloader, self.optimizer, epochs=num_epochs, device=self.device)
  File "/home/abdenour/fl_with_flower/fednova_vgg16/fednova_vgg16/utils.py", line 57, in train
    loss = criterion(net(images), labels)
  File "/home/abdenour/anaconda3/envs/fl/lib/python3.10/site-packages/torch/nn/modules/module.py", line 1553, in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
  File "/home/abdenour/anaconda3/envs/fl/lib/python3.10/site-packages/torch/nn/modules/module.py", line 1562, in _call_impl
    return forward_call(*args, **kwargs)
  File "/home/abdenour/anaconda3/envs/fl/lib/python3.10/site-packages/torchvision/models/vgg.py", line 66, in forward
    x = self.features(x)
  File "/home/abdenour/anaconda3/envs/fl/lib/python3.10/site-packages/torch/nn/modules/module.py", line 1553, in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
  File "/home/abdenour/anaconda3/envs/fl/lib/python3.10/site-packages/torch/nn/modules/module.py", line 1562, in _call_impl
    return forward_call(*args, **kwargs)
  File "/home/abdenour/anaconda3/envs/fl/lib/python3.10/site-packages/torch/nn/modules/container.py", line 219, in forward
    input = module(input)
  File "/home/abdenour/anaconda3/envs/fl/lib/python3.10/site-packages/torch/nn/modules/module.py", line 1553, in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
  File "/home/abdenour/anaconda3/envs/fl/lib/python3.10/site-packages/torch/nn/modules/module.py", line 1562, in _call_impl
    return forward_call(*args, **kwargs)
  File "/home/abdenour/anaconda3/envs/fl/lib/python3.10/site-packages/torch/nn/modules/conv.py", line 458, in forward
    return self._conv_forward(input, self.weight, self.bias)
  File "/home/abdenour/anaconda3/envs/fl/lib/python3.10/site-packages/torch/nn/modules/conv.py", line 454, in _conv_forward
    return F.conv2d(input, weight, bias, self.stride,
torch.OutOfMemoryError: CUDA out of memory. Tried to allocate 784.00 MiB. GPU 0 has a total capacity of 47.33 GiB of which 6.81 MiB is free. Process 2279413 has 11.11 GiB memory in use. Process 2265701 has 2.37 GiB memory in use. Process 2279412 has 11.11 GiB memory in use. Process 2330873 has 11.11 GiB memory in use. Process 2317160 has 2.37 GiB memory in use. Process 2330872 has 6.78 GiB memory in use. Including non-PyTorch memory, this process has 816.00 MiB memory in use. Process 2330870 has 1.61 GiB memory in use. Of the allocated memory 549.92 MiB is allocated by PyTorch, and 6.08 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)

The above exception was the direct cause of the following exception:

[36mray::ClientAppActor.run()[39m (pid=2330871, ip=115.145.172.224, actor_id=cdc8cff1bec6044f7ef6408701000000, repr=<flwr.simulation.ray_transport.ray_actor.ClientAppActor object at 0x7f75b8199750>)
  File "/home/abdenour/.local/lib/python3.10/site-packages/flwr/simulation/ray_transport/ray_actor.py", line 63, in run
    raise ClientAppException(str(ex)) from ex
flwr.client.client_app.ClientAppException: 
Exception ClientAppException occurred. Message: CUDA out of memory. Tried to allocate 784.00 MiB. GPU 0 has a total capacity of 47.33 GiB of which 6.81 MiB is free. Process 2279413 has 11.11 GiB memory in use. Process 2265701 has 2.37 GiB memory in use. Process 2279412 has 11.11 GiB memory in use. Process 2330873 has 11.11 GiB memory in use. Process 2317160 has 2.37 GiB memory in use. Process 2330872 has 6.78 GiB memory in use. Including non-PyTorch memory, this process has 816.00 MiB memory in use. Process 2330870 has 1.61 GiB memory in use. Of the allocated memory 549.92 MiB is allocated by PyTorch, and 6.08 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)
[2024-09-19 01:26:19,101][flwr][ERROR] - Traceback (most recent call last):
  File "/home/abdenour/.local/lib/python3.10/site-packages/flwr/simulation/ray_transport/ray_client_proxy.py", line 94, in _submit_job
    out_mssg, updated_context = self.actor_pool.get_client_result(
  File "/home/abdenour/.local/lib/python3.10/site-packages/flwr/simulation/ray_transport/ray_actor.py", line 398, in get_client_result
    return self._fetch_future_result(cid)
  File "/home/abdenour/.local/lib/python3.10/site-packages/flwr/simulation/ray_transport/ray_actor.py", line 279, in _fetch_future_result
    res_cid, out_mssg, updated_context = ray.get(
  File "/home/abdenour/.local/lib/python3.10/site-packages/ray/_private/auto_init_hook.py", line 21, in auto_init_wrapper
    return fn(*args, **kwargs)
  File "/home/abdenour/.local/lib/python3.10/site-packages/ray/_private/client_mode_hook.py", line 103, in wrapper
    return func(*args, **kwargs)
  File "/home/abdenour/.local/lib/python3.10/site-packages/ray/_private/worker.py", line 2667, in get
    values, debugger_breakpoint = worker.get_objects(object_refs, timeout=timeout)
  File "/home/abdenour/.local/lib/python3.10/site-packages/ray/_private/worker.py", line 864, in get_objects
    raise value.as_instanceof_cause()
ray.exceptions.RayTaskError(ClientAppException): [36mray::ClientAppActor.run()[39m (pid=2330870, ip=115.145.172.224, actor_id=18884b649bbd3d098a6ec02b01000000, repr=<flwr.simulation.ray_transport.ray_actor.ClientAppActor object at 0x7ffb805757b0>)
  File "/home/abdenour/.local/lib/python3.10/site-packages/flwr/client/client_app.py", line 143, in __call__
    return self._call(message, context)
  File "/home/abdenour/.local/lib/python3.10/site-packages/flwr/client/client_app.py", line 126, in ffn
    out_message = handle_legacy_message_from_msgtype(
  File "/home/abdenour/.local/lib/python3.10/site-packages/flwr/client/message_handler/message_handler.py", line 129, in handle_legacy_message_from_msgtype
    fit_res = maybe_call_fit(
  File "/home/abdenour/.local/lib/python3.10/site-packages/flwr/client/client.py", line 234, in maybe_call_fit
    return client.fit(fit_ins)
  File "/home/abdenour/.local/lib/python3.10/site-packages/flwr/client/numpy_client.py", line 238, in _fit
    results = self.numpy_client.fit(parameters, ins.config)  # type: ignore
  File "/home/abdenour/fl_with_flower/fednova_vgg16/fednova_vgg16/client.py", line 69, in fit
    train(self.model, trainloader, self.optimizer, epochs=num_epochs, device=self.device)
  File "/home/abdenour/fl_with_flower/fednova_vgg16/fednova_vgg16/utils.py", line 57, in train
    loss = criterion(net(images), labels)
  File "/home/abdenour/anaconda3/envs/fl/lib/python3.10/site-packages/torch/nn/modules/module.py", line 1553, in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
  File "/home/abdenour/anaconda3/envs/fl/lib/python3.10/site-packages/torch/nn/modules/module.py", line 1562, in _call_impl
    return forward_call(*args, **kwargs)
  File "/home/abdenour/anaconda3/envs/fl/lib/python3.10/site-packages/torchvision/models/vgg.py", line 66, in forward
    x = self.features(x)
  File "/home/abdenour/anaconda3/envs/fl/lib/python3.10/site-packages/torch/nn/modules/module.py", line 1553, in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
  File "/home/abdenour/anaconda3/envs/fl/lib/python3.10/site-packages/torch/nn/modules/module.py", line 1562, in _call_impl
    return forward_call(*args, **kwargs)
  File "/home/abdenour/anaconda3/envs/fl/lib/python3.10/site-packages/torch/nn/modules/container.py", line 219, in forward
    input = module(input)
  File "/home/abdenour/anaconda3/envs/fl/lib/python3.10/site-packages/torch/nn/modules/module.py", line 1553, in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
  File "/home/abdenour/anaconda3/envs/fl/lib/python3.10/site-packages/torch/nn/modules/module.py", line 1562, in _call_impl
    return forward_call(*args, **kwargs)
  File "/home/abdenour/anaconda3/envs/fl/lib/python3.10/site-packages/torch/nn/modules/conv.py", line 458, in forward
    return self._conv_forward(input, self.weight, self.bias)
  File "/home/abdenour/anaconda3/envs/fl/lib/python3.10/site-packages/torch/nn/modules/conv.py", line 454, in _conv_forward
    return F.conv2d(input, weight, bias, self.stride,
torch.OutOfMemoryError: CUDA out of memory. Tried to allocate 784.00 MiB. GPU 0 has a total capacity of 47.33 GiB of which 6.81 MiB is free. Process 2279413 has 11.11 GiB memory in use. Process 2265701 has 2.37 GiB memory in use. Process 2279412 has 11.11 GiB memory in use. Process 2330873 has 11.11 GiB memory in use. Process 2317160 has 2.37 GiB memory in use. Process 2330872 has 6.78 GiB memory in use. Process 2330871 has 816.00 MiB memory in use. Including non-PyTorch memory, this process has 1.61 GiB memory in use. Of the allocated memory 1.30 GiB is allocated by PyTorch, and 6.08 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)

The above exception was the direct cause of the following exception:

[36mray::ClientAppActor.run()[39m (pid=2330870, ip=115.145.172.224, actor_id=18884b649bbd3d098a6ec02b01000000, repr=<flwr.simulation.ray_transport.ray_actor.ClientAppActor object at 0x7ffb805757b0>)
  File "/home/abdenour/.local/lib/python3.10/site-packages/flwr/simulation/ray_transport/ray_actor.py", line 63, in run
    raise ClientAppException(str(ex)) from ex
flwr.client.client_app.ClientAppException: 
Exception ClientAppException occurred. Message: CUDA out of memory. Tried to allocate 784.00 MiB. GPU 0 has a total capacity of 47.33 GiB of which 6.81 MiB is free. Process 2279413 has 11.11 GiB memory in use. Process 2265701 has 2.37 GiB memory in use. Process 2279412 has 11.11 GiB memory in use. Process 2330873 has 11.11 GiB memory in use. Process 2317160 has 2.37 GiB memory in use. Process 2330872 has 6.78 GiB memory in use. Process 2330871 has 816.00 MiB memory in use. Including non-PyTorch memory, this process has 1.61 GiB memory in use. Of the allocated memory 1.30 GiB is allocated by PyTorch, and 6.08 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)

[2024-09-19 01:26:19,102][flwr][ERROR] - [36mray::ClientAppActor.run()[39m (pid=2330870, ip=115.145.172.224, actor_id=18884b649bbd3d098a6ec02b01000000, repr=<flwr.simulation.ray_transport.ray_actor.ClientAppActor object at 0x7ffb805757b0>)
  File "/home/abdenour/.local/lib/python3.10/site-packages/flwr/client/client_app.py", line 143, in __call__
    return self._call(message, context)
  File "/home/abdenour/.local/lib/python3.10/site-packages/flwr/client/client_app.py", line 126, in ffn
    out_message = handle_legacy_message_from_msgtype(
  File "/home/abdenour/.local/lib/python3.10/site-packages/flwr/client/message_handler/message_handler.py", line 129, in handle_legacy_message_from_msgtype
    fit_res = maybe_call_fit(
  File "/home/abdenour/.local/lib/python3.10/site-packages/flwr/client/client.py", line 234, in maybe_call_fit
    return client.fit(fit_ins)
  File "/home/abdenour/.local/lib/python3.10/site-packages/flwr/client/numpy_client.py", line 238, in _fit
    results = self.numpy_client.fit(parameters, ins.config)  # type: ignore
  File "/home/abdenour/fl_with_flower/fednova_vgg16/fednova_vgg16/client.py", line 69, in fit
    train(self.model, trainloader, self.optimizer, epochs=num_epochs, device=self.device)
  File "/home/abdenour/fl_with_flower/fednova_vgg16/fednova_vgg16/utils.py", line 57, in train
    loss = criterion(net(images), labels)
  File "/home/abdenour/anaconda3/envs/fl/lib/python3.10/site-packages/torch/nn/modules/module.py", line 1553, in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
  File "/home/abdenour/anaconda3/envs/fl/lib/python3.10/site-packages/torch/nn/modules/module.py", line 1562, in _call_impl
    return forward_call(*args, **kwargs)
  File "/home/abdenour/anaconda3/envs/fl/lib/python3.10/site-packages/torchvision/models/vgg.py", line 66, in forward
    x = self.features(x)
  File "/home/abdenour/anaconda3/envs/fl/lib/python3.10/site-packages/torch/nn/modules/module.py", line 1553, in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
  File "/home/abdenour/anaconda3/envs/fl/lib/python3.10/site-packages/torch/nn/modules/module.py", line 1562, in _call_impl
    return forward_call(*args, **kwargs)
  File "/home/abdenour/anaconda3/envs/fl/lib/python3.10/site-packages/torch/nn/modules/container.py", line 219, in forward
    input = module(input)
  File "/home/abdenour/anaconda3/envs/fl/lib/python3.10/site-packages/torch/nn/modules/module.py", line 1553, in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
  File "/home/abdenour/anaconda3/envs/fl/lib/python3.10/site-packages/torch/nn/modules/module.py", line 1562, in _call_impl
    return forward_call(*args, **kwargs)
  File "/home/abdenour/anaconda3/envs/fl/lib/python3.10/site-packages/torch/nn/modules/conv.py", line 458, in forward
    return self._conv_forward(input, self.weight, self.bias)
  File "/home/abdenour/anaconda3/envs/fl/lib/python3.10/site-packages/torch/nn/modules/conv.py", line 454, in _conv_forward
    return F.conv2d(input, weight, bias, self.stride,
torch.OutOfMemoryError: CUDA out of memory. Tried to allocate 784.00 MiB. GPU 0 has a total capacity of 47.33 GiB of which 6.81 MiB is free. Process 2279413 has 11.11 GiB memory in use. Process 2265701 has 2.37 GiB memory in use. Process 2279412 has 11.11 GiB memory in use. Process 2330873 has 11.11 GiB memory in use. Process 2317160 has 2.37 GiB memory in use. Process 2330872 has 6.78 GiB memory in use. Process 2330871 has 816.00 MiB memory in use. Including non-PyTorch memory, this process has 1.61 GiB memory in use. Of the allocated memory 1.30 GiB is allocated by PyTorch, and 6.08 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)

The above exception was the direct cause of the following exception:

[36mray::ClientAppActor.run()[39m (pid=2330870, ip=115.145.172.224, actor_id=18884b649bbd3d098a6ec02b01000000, repr=<flwr.simulation.ray_transport.ray_actor.ClientAppActor object at 0x7ffb805757b0>)
  File "/home/abdenour/.local/lib/python3.10/site-packages/flwr/simulation/ray_transport/ray_actor.py", line 63, in run
    raise ClientAppException(str(ex)) from ex
flwr.client.client_app.ClientAppException: 
Exception ClientAppException occurred. Message: CUDA out of memory. Tried to allocate 784.00 MiB. GPU 0 has a total capacity of 47.33 GiB of which 6.81 MiB is free. Process 2279413 has 11.11 GiB memory in use. Process 2265701 has 2.37 GiB memory in use. Process 2279412 has 11.11 GiB memory in use. Process 2330873 has 11.11 GiB memory in use. Process 2317160 has 2.37 GiB memory in use. Process 2330872 has 6.78 GiB memory in use. Process 2330871 has 816.00 MiB memory in use. Including non-PyTorch memory, this process has 1.61 GiB memory in use. Of the allocated memory 1.30 GiB is allocated by PyTorch, and 6.08 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)
[2024-09-19 01:26:21,397][flwr][INFO] - aggregate_fit: received 1 results and 4 failures
[2024-09-19 01:26:32,305][flwr][INFO] - fit progress: (7, 218.05236220359802, {'accuracy': 0.17129494299912307}, 173.7141799032688)
[2024-09-19 01:26:32,306][flwr][INFO] - configure_evaluate: strategy sampled 2 clients (out of 50)
[2024-09-19 01:26:35,726][flwr][ERROR] - Traceback (most recent call last):
  File "/home/abdenour/.local/lib/python3.10/site-packages/flwr/simulation/ray_transport/ray_client_proxy.py", line 94, in _submit_job
    out_mssg, updated_context = self.actor_pool.get_client_result(
  File "/home/abdenour/.local/lib/python3.10/site-packages/flwr/simulation/ray_transport/ray_actor.py", line 398, in get_client_result
    return self._fetch_future_result(cid)
  File "/home/abdenour/.local/lib/python3.10/site-packages/flwr/simulation/ray_transport/ray_actor.py", line 279, in _fetch_future_result
    res_cid, out_mssg, updated_context = ray.get(
  File "/home/abdenour/.local/lib/python3.10/site-packages/ray/_private/auto_init_hook.py", line 21, in auto_init_wrapper
    return fn(*args, **kwargs)
  File "/home/abdenour/.local/lib/python3.10/site-packages/ray/_private/client_mode_hook.py", line 103, in wrapper
    return func(*args, **kwargs)
  File "/home/abdenour/.local/lib/python3.10/site-packages/ray/_private/worker.py", line 2667, in get
    values, debugger_breakpoint = worker.get_objects(object_refs, timeout=timeout)
  File "/home/abdenour/.local/lib/python3.10/site-packages/ray/_private/worker.py", line 864, in get_objects
    raise value.as_instanceof_cause()
ray.exceptions.RayTaskError(ClientAppException): [36mray::ClientAppActor.run()[39m (pid=2330870, ip=115.145.172.224, actor_id=18884b649bbd3d098a6ec02b01000000, repr=<flwr.simulation.ray_transport.ray_actor.ClientAppActor object at 0x7ffb805757b0>)
  File "/home/abdenour/.local/lib/python3.10/site-packages/flwr/client/client_app.py", line 143, in __call__
    return self._call(message, context)
  File "/home/abdenour/.local/lib/python3.10/site-packages/flwr/client/client_app.py", line 126, in ffn
    out_message = handle_legacy_message_from_msgtype(
  File "/home/abdenour/.local/lib/python3.10/site-packages/flwr/client/message_handler/message_handler.py", line 136, in handle_legacy_message_from_msgtype
    evaluate_res = maybe_call_evaluate(
  File "/home/abdenour/.local/lib/python3.10/site-packages/flwr/client/client.py", line 254, in maybe_call_evaluate
    return client.evaluate(evaluate_ins)
  File "/home/abdenour/.local/lib/python3.10/site-packages/flwr/client/numpy_client.py", line 262, in _evaluate
    results = self.numpy_client.evaluate(parameters, ins.config)  # type: ignore
  File "/home/abdenour/fl_with_flower/fednova_vgg16/fednova_vgg16/client.py", line 85, in evaluate
    loss, accuracy = test(self.model, valloader, device=self.device)
  File "/home/abdenour/fl_with_flower/fednova_vgg16/fednova_vgg16/utils.py", line 71, in test
    outputs = net(images)
  File "/home/abdenour/anaconda3/envs/fl/lib/python3.10/site-packages/torch/nn/modules/module.py", line 1553, in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
  File "/home/abdenour/anaconda3/envs/fl/lib/python3.10/site-packages/torch/nn/modules/module.py", line 1562, in _call_impl
    return forward_call(*args, **kwargs)
  File "/home/abdenour/anaconda3/envs/fl/lib/python3.10/site-packages/torchvision/models/vgg.py", line 66, in forward
    x = self.features(x)
  File "/home/abdenour/anaconda3/envs/fl/lib/python3.10/site-packages/torch/nn/modules/module.py", line 1553, in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
  File "/home/abdenour/anaconda3/envs/fl/lib/python3.10/site-packages/torch/nn/modules/module.py", line 1562, in _call_impl
    return forward_call(*args, **kwargs)
  File "/home/abdenour/anaconda3/envs/fl/lib/python3.10/site-packages/torch/nn/modules/container.py", line 219, in forward
    input = module(input)
  File "/home/abdenour/anaconda3/envs/fl/lib/python3.10/site-packages/torch/nn/modules/module.py", line 1553, in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
  File "/home/abdenour/anaconda3/envs/fl/lib/python3.10/site-packages/torch/nn/modules/module.py", line 1562, in _call_impl
    return forward_call(*args, **kwargs)
  File "/home/abdenour/anaconda3/envs/fl/lib/python3.10/site-packages/torch/nn/modules/conv.py", line 458, in forward
    return self._conv_forward(input, self.weight, self.bias)
  File "/home/abdenour/anaconda3/envs/fl/lib/python3.10/site-packages/torch/nn/modules/conv.py", line 454, in _conv_forward
    return F.conv2d(input, weight, bias, self.stride,
torch.OutOfMemoryError: CUDA out of memory. Tried to allocate 418.00 MiB. GPU 0 has a total capacity of 47.33 GiB of which 6.81 MiB is free. Process 2279413 has 11.11 GiB memory in use. Process 2265701 has 2.37 GiB memory in use. Process 2279412 has 11.11 GiB memory in use. Process 2330873 has 11.11 GiB memory in use. Process 2317160 has 2.37 GiB memory in use. Process 2330872 has 6.78 GiB memory in use. Process 2330871 has 816.00 MiB memory in use. Including non-PyTorch memory, this process has 1.61 GiB memory in use. Of the allocated memory 949.19 MiB is allocated by PyTorch, and 390.81 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)

The above exception was the direct cause of the following exception:

[36mray::ClientAppActor.run()[39m (pid=2330870, ip=115.145.172.224, actor_id=18884b649bbd3d098a6ec02b01000000, repr=<flwr.simulation.ray_transport.ray_actor.ClientAppActor object at 0x7ffb805757b0>)
  File "/home/abdenour/.local/lib/python3.10/site-packages/flwr/simulation/ray_transport/ray_actor.py", line 63, in run
    raise ClientAppException(str(ex)) from ex
flwr.client.client_app.ClientAppException: 
Exception ClientAppException occurred. Message: CUDA out of memory. Tried to allocate 418.00 MiB. GPU 0 has a total capacity of 47.33 GiB of which 6.81 MiB is free. Process 2279413 has 11.11 GiB memory in use. Process 2265701 has 2.37 GiB memory in use. Process 2279412 has 11.11 GiB memory in use. Process 2330873 has 11.11 GiB memory in use. Process 2317160 has 2.37 GiB memory in use. Process 2330872 has 6.78 GiB memory in use. Process 2330871 has 816.00 MiB memory in use. Including non-PyTorch memory, this process has 1.61 GiB memory in use. Of the allocated memory 949.19 MiB is allocated by PyTorch, and 390.81 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)

[2024-09-19 01:26:35,727][flwr][ERROR] - [36mray::ClientAppActor.run()[39m (pid=2330870, ip=115.145.172.224, actor_id=18884b649bbd3d098a6ec02b01000000, repr=<flwr.simulation.ray_transport.ray_actor.ClientAppActor object at 0x7ffb805757b0>)
  File "/home/abdenour/.local/lib/python3.10/site-packages/flwr/client/client_app.py", line 143, in __call__
    return self._call(message, context)
  File "/home/abdenour/.local/lib/python3.10/site-packages/flwr/client/client_app.py", line 126, in ffn
    out_message = handle_legacy_message_from_msgtype(
  File "/home/abdenour/.local/lib/python3.10/site-packages/flwr/client/message_handler/message_handler.py", line 136, in handle_legacy_message_from_msgtype
    evaluate_res = maybe_call_evaluate(
  File "/home/abdenour/.local/lib/python3.10/site-packages/flwr/client/client.py", line 254, in maybe_call_evaluate
    return client.evaluate(evaluate_ins)
  File "/home/abdenour/.local/lib/python3.10/site-packages/flwr/client/numpy_client.py", line 262, in _evaluate
    results = self.numpy_client.evaluate(parameters, ins.config)  # type: ignore
  File "/home/abdenour/fl_with_flower/fednova_vgg16/fednova_vgg16/client.py", line 85, in evaluate
    loss, accuracy = test(self.model, valloader, device=self.device)
  File "/home/abdenour/fl_with_flower/fednova_vgg16/fednova_vgg16/utils.py", line 71, in test
    outputs = net(images)
  File "/home/abdenour/anaconda3/envs/fl/lib/python3.10/site-packages/torch/nn/modules/module.py", line 1553, in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
  File "/home/abdenour/anaconda3/envs/fl/lib/python3.10/site-packages/torch/nn/modules/module.py", line 1562, in _call_impl
    return forward_call(*args, **kwargs)
  File "/home/abdenour/anaconda3/envs/fl/lib/python3.10/site-packages/torchvision/models/vgg.py", line 66, in forward
    x = self.features(x)
  File "/home/abdenour/anaconda3/envs/fl/lib/python3.10/site-packages/torch/nn/modules/module.py", line 1553, in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
  File "/home/abdenour/anaconda3/envs/fl/lib/python3.10/site-packages/torch/nn/modules/module.py", line 1562, in _call_impl
    return forward_call(*args, **kwargs)
  File "/home/abdenour/anaconda3/envs/fl/lib/python3.10/site-packages/torch/nn/modules/container.py", line 219, in forward
    input = module(input)
  File "/home/abdenour/anaconda3/envs/fl/lib/python3.10/site-packages/torch/nn/modules/module.py", line 1553, in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
  File "/home/abdenour/anaconda3/envs/fl/lib/python3.10/site-packages/torch/nn/modules/module.py", line 1562, in _call_impl
    return forward_call(*args, **kwargs)
  File "/home/abdenour/anaconda3/envs/fl/lib/python3.10/site-packages/torch/nn/modules/conv.py", line 458, in forward
    return self._conv_forward(input, self.weight, self.bias)
  File "/home/abdenour/anaconda3/envs/fl/lib/python3.10/site-packages/torch/nn/modules/conv.py", line 454, in _conv_forward
    return F.conv2d(input, weight, bias, self.stride,
torch.OutOfMemoryError: CUDA out of memory. Tried to allocate 418.00 MiB. GPU 0 has a total capacity of 47.33 GiB of which 6.81 MiB is free. Process 2279413 has 11.11 GiB memory in use. Process 2265701 has 2.37 GiB memory in use. Process 2279412 has 11.11 GiB memory in use. Process 2330873 has 11.11 GiB memory in use. Process 2317160 has 2.37 GiB memory in use. Process 2330872 has 6.78 GiB memory in use. Process 2330871 has 816.00 MiB memory in use. Including non-PyTorch memory, this process has 1.61 GiB memory in use. Of the allocated memory 949.19 MiB is allocated by PyTorch, and 390.81 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)

The above exception was the direct cause of the following exception:

[36mray::ClientAppActor.run()[39m (pid=2330870, ip=115.145.172.224, actor_id=18884b649bbd3d098a6ec02b01000000, repr=<flwr.simulation.ray_transport.ray_actor.ClientAppActor object at 0x7ffb805757b0>)
  File "/home/abdenour/.local/lib/python3.10/site-packages/flwr/simulation/ray_transport/ray_actor.py", line 63, in run
    raise ClientAppException(str(ex)) from ex
flwr.client.client_app.ClientAppException: 
Exception ClientAppException occurred. Message: CUDA out of memory. Tried to allocate 418.00 MiB. GPU 0 has a total capacity of 47.33 GiB of which 6.81 MiB is free. Process 2279413 has 11.11 GiB memory in use. Process 2265701 has 2.37 GiB memory in use. Process 2279412 has 11.11 GiB memory in use. Process 2330873 has 11.11 GiB memory in use. Process 2317160 has 2.37 GiB memory in use. Process 2330872 has 6.78 GiB memory in use. Process 2330871 has 816.00 MiB memory in use. Including non-PyTorch memory, this process has 1.61 GiB memory in use. Of the allocated memory 949.19 MiB is allocated by PyTorch, and 390.81 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)
[2024-09-19 01:26:35,727][flwr][INFO] - aggregate_evaluate: received 1 results and 1 failures
[2024-09-19 01:26:35,728][flwr][INFO] - 
[2024-09-19 01:26:35,728][flwr][INFO] - [ROUND 8]
[2024-09-19 01:26:35,728][flwr][INFO] - configure_fit: strategy sampled 5 clients (out of 50)
[2024-09-19 01:26:39,775][flwr][ERROR] - Traceback (most recent call last):
  File "/home/abdenour/.local/lib/python3.10/site-packages/flwr/simulation/ray_transport/ray_client_proxy.py", line 94, in _submit_job
    out_mssg, updated_context = self.actor_pool.get_client_result(
  File "/home/abdenour/.local/lib/python3.10/site-packages/flwr/simulation/ray_transport/ray_actor.py", line 398, in get_client_result
    return self._fetch_future_result(cid)
  File "/home/abdenour/.local/lib/python3.10/site-packages/flwr/simulation/ray_transport/ray_actor.py", line 279, in _fetch_future_result
    res_cid, out_mssg, updated_context = ray.get(
  File "/home/abdenour/.local/lib/python3.10/site-packages/ray/_private/auto_init_hook.py", line 21, in auto_init_wrapper
    return fn(*args, **kwargs)
  File "/home/abdenour/.local/lib/python3.10/site-packages/ray/_private/client_mode_hook.py", line 103, in wrapper
    return func(*args, **kwargs)
  File "/home/abdenour/.local/lib/python3.10/site-packages/ray/_private/worker.py", line 2667, in get
    values, debugger_breakpoint = worker.get_objects(object_refs, timeout=timeout)
  File "/home/abdenour/.local/lib/python3.10/site-packages/ray/_private/worker.py", line 864, in get_objects
    raise value.as_instanceof_cause()
ray.exceptions.RayTaskError(ClientAppException): [36mray::ClientAppActor.run()[39m (pid=2330870, ip=115.145.172.224, actor_id=18884b649bbd3d098a6ec02b01000000, repr=<flwr.simulation.ray_transport.ray_actor.ClientAppActor object at 0x7ffb805757b0>)
  File "/home/abdenour/.local/lib/python3.10/site-packages/flwr/client/client_app.py", line 143, in __call__
    return self._call(message, context)
  File "/home/abdenour/.local/lib/python3.10/site-packages/flwr/client/client_app.py", line 126, in ffn
    out_message = handle_legacy_message_from_msgtype(
  File "/home/abdenour/.local/lib/python3.10/site-packages/flwr/client/message_handler/message_handler.py", line 129, in handle_legacy_message_from_msgtype
    fit_res = maybe_call_fit(
  File "/home/abdenour/.local/lib/python3.10/site-packages/flwr/client/client.py", line 234, in maybe_call_fit
    return client.fit(fit_ins)
  File "/home/abdenour/.local/lib/python3.10/site-packages/flwr/client/numpy_client.py", line 238, in _fit
    results = self.numpy_client.fit(parameters, ins.config)  # type: ignore
  File "/home/abdenour/fl_with_flower/fednova_vgg16/fednova_vgg16/client.py", line 69, in fit
    train(self.model, trainloader, self.optimizer, epochs=num_epochs, device=self.device)
  File "/home/abdenour/fl_with_flower/fednova_vgg16/fednova_vgg16/utils.py", line 57, in train
    loss = criterion(net(images), labels)
  File "/home/abdenour/anaconda3/envs/fl/lib/python3.10/site-packages/torch/nn/modules/module.py", line 1553, in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
  File "/home/abdenour/anaconda3/envs/fl/lib/python3.10/site-packages/torch/nn/modules/module.py", line 1562, in _call_impl
    return forward_call(*args, **kwargs)
  File "/home/abdenour/anaconda3/envs/fl/lib/python3.10/site-packages/torchvision/models/vgg.py", line 66, in forward
    x = self.features(x)
  File "/home/abdenour/anaconda3/envs/fl/lib/python3.10/site-packages/torch/nn/modules/module.py", line 1553, in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
  File "/home/abdenour/anaconda3/envs/fl/lib/python3.10/site-packages/torch/nn/modules/module.py", line 1562, in _call_impl
    return forward_call(*args, **kwargs)
  File "/home/abdenour/anaconda3/envs/fl/lib/python3.10/site-packages/torch/nn/modules/container.py", line 219, in forward
    input = module(input)
  File "/home/abdenour/anaconda3/envs/fl/lib/python3.10/site-packages/torch/nn/modules/module.py", line 1553, in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
  File "/home/abdenour/anaconda3/envs/fl/lib/python3.10/site-packages/torch/nn/modules/module.py", line 1562, in _call_impl
    return forward_call(*args, **kwargs)
  File "/home/abdenour/anaconda3/envs/fl/lib/python3.10/site-packages/torch/nn/modules/conv.py", line 458, in forward
    return self._conv_forward(input, self.weight, self.bias)
  File "/home/abdenour/anaconda3/envs/fl/lib/python3.10/site-packages/torch/nn/modules/conv.py", line 454, in _conv_forward
    return F.conv2d(input, weight, bias, self.stride,
torch.OutOfMemoryError: CUDA out of memory. Tried to allocate 784.00 MiB. GPU 0 has a total capacity of 47.33 GiB of which 6.81 MiB is free. Process 2279413 has 11.11 GiB memory in use. Process 2265701 has 2.37 GiB memory in use. Process 2279412 has 11.11 GiB memory in use. Process 2330873 has 11.11 GiB memory in use. Process 2317160 has 2.37 GiB memory in use. Process 2330872 has 6.78 GiB memory in use. Process 2330871 has 816.00 MiB memory in use. Including non-PyTorch memory, this process has 1.61 GiB memory in use. Of the allocated memory 1.30 GiB is allocated by PyTorch, and 6.08 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)

The above exception was the direct cause of the following exception:

[36mray::ClientAppActor.run()[39m (pid=2330870, ip=115.145.172.224, actor_id=18884b649bbd3d098a6ec02b01000000, repr=<flwr.simulation.ray_transport.ray_actor.ClientAppActor object at 0x7ffb805757b0>)
  File "/home/abdenour/.local/lib/python3.10/site-packages/flwr/simulation/ray_transport/ray_actor.py", line 63, in run
    raise ClientAppException(str(ex)) from ex
flwr.client.client_app.ClientAppException: 
Exception ClientAppException occurred. Message: CUDA out of memory. Tried to allocate 784.00 MiB. GPU 0 has a total capacity of 47.33 GiB of which 6.81 MiB is free. Process 2279413 has 11.11 GiB memory in use. Process 2265701 has 2.37 GiB memory in use. Process 2279412 has 11.11 GiB memory in use. Process 2330873 has 11.11 GiB memory in use. Process 2317160 has 2.37 GiB memory in use. Process 2330872 has 6.78 GiB memory in use. Process 2330871 has 816.00 MiB memory in use. Including non-PyTorch memory, this process has 1.61 GiB memory in use. Of the allocated memory 1.30 GiB is allocated by PyTorch, and 6.08 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)

[2024-09-19 01:26:39,776][flwr][ERROR] - [36mray::ClientAppActor.run()[39m (pid=2330870, ip=115.145.172.224, actor_id=18884b649bbd3d098a6ec02b01000000, repr=<flwr.simulation.ray_transport.ray_actor.ClientAppActor object at 0x7ffb805757b0>)
  File "/home/abdenour/.local/lib/python3.10/site-packages/flwr/client/client_app.py", line 143, in __call__
    return self._call(message, context)
  File "/home/abdenour/.local/lib/python3.10/site-packages/flwr/client/client_app.py", line 126, in ffn
    out_message = handle_legacy_message_from_msgtype(
  File "/home/abdenour/.local/lib/python3.10/site-packages/flwr/client/message_handler/message_handler.py", line 129, in handle_legacy_message_from_msgtype
    fit_res = maybe_call_fit(
  File "/home/abdenour/.local/lib/python3.10/site-packages/flwr/client/client.py", line 234, in maybe_call_fit
    return client.fit(fit_ins)
  File "/home/abdenour/.local/lib/python3.10/site-packages/flwr/client/numpy_client.py", line 238, in _fit
    results = self.numpy_client.fit(parameters, ins.config)  # type: ignore
  File "/home/abdenour/fl_with_flower/fednova_vgg16/fednova_vgg16/client.py", line 69, in fit
    train(self.model, trainloader, self.optimizer, epochs=num_epochs, device=self.device)
  File "/home/abdenour/fl_with_flower/fednova_vgg16/fednova_vgg16/utils.py", line 57, in train
    loss = criterion(net(images), labels)
  File "/home/abdenour/anaconda3/envs/fl/lib/python3.10/site-packages/torch/nn/modules/module.py", line 1553, in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
  File "/home/abdenour/anaconda3/envs/fl/lib/python3.10/site-packages/torch/nn/modules/module.py", line 1562, in _call_impl
    return forward_call(*args, **kwargs)
  File "/home/abdenour/anaconda3/envs/fl/lib/python3.10/site-packages/torchvision/models/vgg.py", line 66, in forward
    x = self.features(x)
  File "/home/abdenour/anaconda3/envs/fl/lib/python3.10/site-packages/torch/nn/modules/module.py", line 1553, in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
  File "/home/abdenour/anaconda3/envs/fl/lib/python3.10/site-packages/torch/nn/modules/module.py", line 1562, in _call_impl
    return forward_call(*args, **kwargs)
  File "/home/abdenour/anaconda3/envs/fl/lib/python3.10/site-packages/torch/nn/modules/container.py", line 219, in forward
    input = module(input)
  File "/home/abdenour/anaconda3/envs/fl/lib/python3.10/site-packages/torch/nn/modules/module.py", line 1553, in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
  File "/home/abdenour/anaconda3/envs/fl/lib/python3.10/site-packages/torch/nn/modules/module.py", line 1562, in _call_impl
    return forward_call(*args, **kwargs)
  File "/home/abdenour/anaconda3/envs/fl/lib/python3.10/site-packages/torch/nn/modules/conv.py", line 458, in forward
    return self._conv_forward(input, self.weight, self.bias)
  File "/home/abdenour/anaconda3/envs/fl/lib/python3.10/site-packages/torch/nn/modules/conv.py", line 454, in _conv_forward
    return F.conv2d(input, weight, bias, self.stride,
torch.OutOfMemoryError: CUDA out of memory. Tried to allocate 784.00 MiB. GPU 0 has a total capacity of 47.33 GiB of which 6.81 MiB is free. Process 2279413 has 11.11 GiB memory in use. Process 2265701 has 2.37 GiB memory in use. Process 2279412 has 11.11 GiB memory in use. Process 2330873 has 11.11 GiB memory in use. Process 2317160 has 2.37 GiB memory in use. Process 2330872 has 6.78 GiB memory in use. Process 2330871 has 816.00 MiB memory in use. Including non-PyTorch memory, this process has 1.61 GiB memory in use. Of the allocated memory 1.30 GiB is allocated by PyTorch, and 6.08 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)

The above exception was the direct cause of the following exception:

[36mray::ClientAppActor.run()[39m (pid=2330870, ip=115.145.172.224, actor_id=18884b649bbd3d098a6ec02b01000000, repr=<flwr.simulation.ray_transport.ray_actor.ClientAppActor object at 0x7ffb805757b0>)
  File "/home/abdenour/.local/lib/python3.10/site-packages/flwr/simulation/ray_transport/ray_actor.py", line 63, in run
    raise ClientAppException(str(ex)) from ex
flwr.client.client_app.ClientAppException: 
Exception ClientAppException occurred. Message: CUDA out of memory. Tried to allocate 784.00 MiB. GPU 0 has a total capacity of 47.33 GiB of which 6.81 MiB is free. Process 2279413 has 11.11 GiB memory in use. Process 2265701 has 2.37 GiB memory in use. Process 2279412 has 11.11 GiB memory in use. Process 2330873 has 11.11 GiB memory in use. Process 2317160 has 2.37 GiB memory in use. Process 2330872 has 6.78 GiB memory in use. Process 2330871 has 816.00 MiB memory in use. Including non-PyTorch memory, this process has 1.61 GiB memory in use. Of the allocated memory 1.30 GiB is allocated by PyTorch, and 6.08 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)
[2024-09-19 01:26:39,898][flwr][ERROR] - Traceback (most recent call last):
  File "/home/abdenour/.local/lib/python3.10/site-packages/flwr/simulation/ray_transport/ray_client_proxy.py", line 94, in _submit_job
    out_mssg, updated_context = self.actor_pool.get_client_result(
  File "/home/abdenour/.local/lib/python3.10/site-packages/flwr/simulation/ray_transport/ray_actor.py", line 398, in get_client_result
    return self._fetch_future_result(cid)
  File "/home/abdenour/.local/lib/python3.10/site-packages/flwr/simulation/ray_transport/ray_actor.py", line 279, in _fetch_future_result
    res_cid, out_mssg, updated_context = ray.get(
  File "/home/abdenour/.local/lib/python3.10/site-packages/ray/_private/auto_init_hook.py", line 21, in auto_init_wrapper
    return fn(*args, **kwargs)
  File "/home/abdenour/.local/lib/python3.10/site-packages/ray/_private/client_mode_hook.py", line 103, in wrapper
    return func(*args, **kwargs)
  File "/home/abdenour/.local/lib/python3.10/site-packages/ray/_private/worker.py", line 2667, in get
    values, debugger_breakpoint = worker.get_objects(object_refs, timeout=timeout)
  File "/home/abdenour/.local/lib/python3.10/site-packages/ray/_private/worker.py", line 864, in get_objects
    raise value.as_instanceof_cause()
ray.exceptions.RayTaskError(ClientAppException): [36mray::ClientAppActor.run()[39m (pid=2330871, ip=115.145.172.224, actor_id=cdc8cff1bec6044f7ef6408701000000, repr=<flwr.simulation.ray_transport.ray_actor.ClientAppActor object at 0x7f75b8199750>)
  File "/home/abdenour/.local/lib/python3.10/site-packages/flwr/client/client_app.py", line 143, in __call__
    return self._call(message, context)
  File "/home/abdenour/.local/lib/python3.10/site-packages/flwr/client/client_app.py", line 126, in ffn
    out_message = handle_legacy_message_from_msgtype(
  File "/home/abdenour/.local/lib/python3.10/site-packages/flwr/client/message_handler/message_handler.py", line 129, in handle_legacy_message_from_msgtype
    fit_res = maybe_call_fit(
  File "/home/abdenour/.local/lib/python3.10/site-packages/flwr/client/client.py", line 234, in maybe_call_fit
    return client.fit(fit_ins)
  File "/home/abdenour/.local/lib/python3.10/site-packages/flwr/client/numpy_client.py", line 238, in _fit
    results = self.numpy_client.fit(parameters, ins.config)  # type: ignore
  File "/home/abdenour/fl_with_flower/fednova_vgg16/fednova_vgg16/client.py", line 69, in fit
    train(self.model, trainloader, self.optimizer, epochs=num_epochs, device=self.device)
  File "/home/abdenour/fl_with_flower/fednova_vgg16/fednova_vgg16/utils.py", line 57, in train
    loss = criterion(net(images), labels)
  File "/home/abdenour/anaconda3/envs/fl/lib/python3.10/site-packages/torch/nn/modules/module.py", line 1553, in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
  File "/home/abdenour/anaconda3/envs/fl/lib/python3.10/site-packages/torch/nn/modules/module.py", line 1562, in _call_impl
    return forward_call(*args, **kwargs)
  File "/home/abdenour/anaconda3/envs/fl/lib/python3.10/site-packages/torchvision/models/vgg.py", line 66, in forward
    x = self.features(x)
  File "/home/abdenour/anaconda3/envs/fl/lib/python3.10/site-packages/torch/nn/modules/module.py", line 1553, in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
  File "/home/abdenour/anaconda3/envs/fl/lib/python3.10/site-packages/torch/nn/modules/module.py", line 1562, in _call_impl
    return forward_call(*args, **kwargs)
  File "/home/abdenour/anaconda3/envs/fl/lib/python3.10/site-packages/torch/nn/modules/container.py", line 219, in forward
    input = module(input)
  File "/home/abdenour/anaconda3/envs/fl/lib/python3.10/site-packages/torch/nn/modules/module.py", line 1553, in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
  File "/home/abdenour/anaconda3/envs/fl/lib/python3.10/site-packages/torch/nn/modules/module.py", line 1562, in _call_impl
    return forward_call(*args, **kwargs)
  File "/home/abdenour/anaconda3/envs/fl/lib/python3.10/site-packages/torch/nn/modules/conv.py", line 458, in forward
    return self._conv_forward(input, self.weight, self.bias)
  File "/home/abdenour/anaconda3/envs/fl/lib/python3.10/site-packages/torch/nn/modules/conv.py", line 454, in _conv_forward
    return F.conv2d(input, weight, bias, self.stride,
torch.OutOfMemoryError: CUDA out of memory. Tried to allocate 784.00 MiB. GPU 0 has a total capacity of 47.33 GiB of which 6.81 MiB is free. Process 2279413 has 11.11 GiB memory in use. Process 2265701 has 2.37 GiB memory in use. Process 2279412 has 11.11 GiB memory in use. Process 2330873 has 11.11 GiB memory in use. Process 2317160 has 2.37 GiB memory in use. Process 2330872 has 6.78 GiB memory in use. Including non-PyTorch memory, this process has 816.00 MiB memory in use. Process 2330870 has 1.61 GiB memory in use. Of the allocated memory 549.92 MiB is allocated by PyTorch, and 6.08 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)

The above exception was the direct cause of the following exception:

[36mray::ClientAppActor.run()[39m (pid=2330871, ip=115.145.172.224, actor_id=cdc8cff1bec6044f7ef6408701000000, repr=<flwr.simulation.ray_transport.ray_actor.ClientAppActor object at 0x7f75b8199750>)
  File "/home/abdenour/.local/lib/python3.10/site-packages/flwr/simulation/ray_transport/ray_actor.py", line 63, in run
    raise ClientAppException(str(ex)) from ex
flwr.client.client_app.ClientAppException: 
Exception ClientAppException occurred. Message: CUDA out of memory. Tried to allocate 784.00 MiB. GPU 0 has a total capacity of 47.33 GiB of which 6.81 MiB is free. Process 2279413 has 11.11 GiB memory in use. Process 2265701 has 2.37 GiB memory in use. Process 2279412 has 11.11 GiB memory in use. Process 2330873 has 11.11 GiB memory in use. Process 2317160 has 2.37 GiB memory in use. Process 2330872 has 6.78 GiB memory in use. Including non-PyTorch memory, this process has 816.00 MiB memory in use. Process 2330870 has 1.61 GiB memory in use. Of the allocated memory 549.92 MiB is allocated by PyTorch, and 6.08 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)

[2024-09-19 01:26:39,898][flwr][ERROR] - [36mray::ClientAppActor.run()[39m (pid=2330871, ip=115.145.172.224, actor_id=cdc8cff1bec6044f7ef6408701000000, repr=<flwr.simulation.ray_transport.ray_actor.ClientAppActor object at 0x7f75b8199750>)
  File "/home/abdenour/.local/lib/python3.10/site-packages/flwr/client/client_app.py", line 143, in __call__
    return self._call(message, context)
  File "/home/abdenour/.local/lib/python3.10/site-packages/flwr/client/client_app.py", line 126, in ffn
    out_message = handle_legacy_message_from_msgtype(
  File "/home/abdenour/.local/lib/python3.10/site-packages/flwr/client/message_handler/message_handler.py", line 129, in handle_legacy_message_from_msgtype
    fit_res = maybe_call_fit(
  File "/home/abdenour/.local/lib/python3.10/site-packages/flwr/client/client.py", line 234, in maybe_call_fit
    return client.fit(fit_ins)
  File "/home/abdenour/.local/lib/python3.10/site-packages/flwr/client/numpy_client.py", line 238, in _fit
    results = self.numpy_client.fit(parameters, ins.config)  # type: ignore
  File "/home/abdenour/fl_with_flower/fednova_vgg16/fednova_vgg16/client.py", line 69, in fit
    train(self.model, trainloader, self.optimizer, epochs=num_epochs, device=self.device)
  File "/home/abdenour/fl_with_flower/fednova_vgg16/fednova_vgg16/utils.py", line 57, in train
    loss = criterion(net(images), labels)
  File "/home/abdenour/anaconda3/envs/fl/lib/python3.10/site-packages/torch/nn/modules/module.py", line 1553, in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
  File "/home/abdenour/anaconda3/envs/fl/lib/python3.10/site-packages/torch/nn/modules/module.py", line 1562, in _call_impl
    return forward_call(*args, **kwargs)
  File "/home/abdenour/anaconda3/envs/fl/lib/python3.10/site-packages/torchvision/models/vgg.py", line 66, in forward
    x = self.features(x)
  File "/home/abdenour/anaconda3/envs/fl/lib/python3.10/site-packages/torch/nn/modules/module.py", line 1553, in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
  File "/home/abdenour/anaconda3/envs/fl/lib/python3.10/site-packages/torch/nn/modules/module.py", line 1562, in _call_impl
    return forward_call(*args, **kwargs)
  File "/home/abdenour/anaconda3/envs/fl/lib/python3.10/site-packages/torch/nn/modules/container.py", line 219, in forward
    input = module(input)
  File "/home/abdenour/anaconda3/envs/fl/lib/python3.10/site-packages/torch/nn/modules/module.py", line 1553, in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
  File "/home/abdenour/anaconda3/envs/fl/lib/python3.10/site-packages/torch/nn/modules/module.py", line 1562, in _call_impl
    return forward_call(*args, **kwargs)
  File "/home/abdenour/anaconda3/envs/fl/lib/python3.10/site-packages/torch/nn/modules/conv.py", line 458, in forward
    return self._conv_forward(input, self.weight, self.bias)
  File "/home/abdenour/anaconda3/envs/fl/lib/python3.10/site-packages/torch/nn/modules/conv.py", line 454, in _conv_forward
    return F.conv2d(input, weight, bias, self.stride,
torch.OutOfMemoryError: CUDA out of memory. Tried to allocate 784.00 MiB. GPU 0 has a total capacity of 47.33 GiB of which 6.81 MiB is free. Process 2279413 has 11.11 GiB memory in use. Process 2265701 has 2.37 GiB memory in use. Process 2279412 has 11.11 GiB memory in use. Process 2330873 has 11.11 GiB memory in use. Process 2317160 has 2.37 GiB memory in use. Process 2330872 has 6.78 GiB memory in use. Including non-PyTorch memory, this process has 816.00 MiB memory in use. Process 2330870 has 1.61 GiB memory in use. Of the allocated memory 549.92 MiB is allocated by PyTorch, and 6.08 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)

The above exception was the direct cause of the following exception:

[36mray::ClientAppActor.run()[39m (pid=2330871, ip=115.145.172.224, actor_id=cdc8cff1bec6044f7ef6408701000000, repr=<flwr.simulation.ray_transport.ray_actor.ClientAppActor object at 0x7f75b8199750>)
  File "/home/abdenour/.local/lib/python3.10/site-packages/flwr/simulation/ray_transport/ray_actor.py", line 63, in run
    raise ClientAppException(str(ex)) from ex
flwr.client.client_app.ClientAppException: 
Exception ClientAppException occurred. Message: CUDA out of memory. Tried to allocate 784.00 MiB. GPU 0 has a total capacity of 47.33 GiB of which 6.81 MiB is free. Process 2279413 has 11.11 GiB memory in use. Process 2265701 has 2.37 GiB memory in use. Process 2279412 has 11.11 GiB memory in use. Process 2330873 has 11.11 GiB memory in use. Process 2317160 has 2.37 GiB memory in use. Process 2330872 has 6.78 GiB memory in use. Including non-PyTorch memory, this process has 816.00 MiB memory in use. Process 2330870 has 1.61 GiB memory in use. Of the allocated memory 549.92 MiB is allocated by PyTorch, and 6.08 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)
[2024-09-19 01:26:41,720][flwr][ERROR] - Traceback (most recent call last):
  File "/home/abdenour/.local/lib/python3.10/site-packages/flwr/simulation/ray_transport/ray_client_proxy.py", line 94, in _submit_job
    out_mssg, updated_context = self.actor_pool.get_client_result(
  File "/home/abdenour/.local/lib/python3.10/site-packages/flwr/simulation/ray_transport/ray_actor.py", line 398, in get_client_result
    return self._fetch_future_result(cid)
  File "/home/abdenour/.local/lib/python3.10/site-packages/flwr/simulation/ray_transport/ray_actor.py", line 279, in _fetch_future_result
    res_cid, out_mssg, updated_context = ray.get(
  File "/home/abdenour/.local/lib/python3.10/site-packages/ray/_private/auto_init_hook.py", line 21, in auto_init_wrapper
    return fn(*args, **kwargs)
  File "/home/abdenour/.local/lib/python3.10/site-packages/ray/_private/client_mode_hook.py", line 103, in wrapper
    return func(*args, **kwargs)
  File "/home/abdenour/.local/lib/python3.10/site-packages/ray/_private/worker.py", line 2667, in get
    values, debugger_breakpoint = worker.get_objects(object_refs, timeout=timeout)
  File "/home/abdenour/.local/lib/python3.10/site-packages/ray/_private/worker.py", line 864, in get_objects
    raise value.as_instanceof_cause()
ray.exceptions.RayTaskError(ClientAppException): [36mray::ClientAppActor.run()[39m (pid=2330870, ip=115.145.172.224, actor_id=18884b649bbd3d098a6ec02b01000000, repr=<flwr.simulation.ray_transport.ray_actor.ClientAppActor object at 0x7ffb805757b0>)
  File "/home/abdenour/.local/lib/python3.10/site-packages/flwr/client/client_app.py", line 143, in __call__
    return self._call(message, context)
  File "/home/abdenour/.local/lib/python3.10/site-packages/flwr/client/client_app.py", line 126, in ffn
    out_message = handle_legacy_message_from_msgtype(
  File "/home/abdenour/.local/lib/python3.10/site-packages/flwr/client/message_handler/message_handler.py", line 129, in handle_legacy_message_from_msgtype
    fit_res = maybe_call_fit(
  File "/home/abdenour/.local/lib/python3.10/site-packages/flwr/client/client.py", line 234, in maybe_call_fit
    return client.fit(fit_ins)
  File "/home/abdenour/.local/lib/python3.10/site-packages/flwr/client/numpy_client.py", line 238, in _fit
    results = self.numpy_client.fit(parameters, ins.config)  # type: ignore
  File "/home/abdenour/fl_with_flower/fednova_vgg16/fednova_vgg16/client.py", line 69, in fit
    train(self.model, trainloader, self.optimizer, epochs=num_epochs, device=self.device)
  File "/home/abdenour/fl_with_flower/fednova_vgg16/fednova_vgg16/utils.py", line 57, in train
    loss = criterion(net(images), labels)
  File "/home/abdenour/anaconda3/envs/fl/lib/python3.10/site-packages/torch/nn/modules/module.py", line 1553, in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
  File "/home/abdenour/anaconda3/envs/fl/lib/python3.10/site-packages/torch/nn/modules/module.py", line 1562, in _call_impl
    return forward_call(*args, **kwargs)
  File "/home/abdenour/anaconda3/envs/fl/lib/python3.10/site-packages/torchvision/models/vgg.py", line 66, in forward
    x = self.features(x)
  File "/home/abdenour/anaconda3/envs/fl/lib/python3.10/site-packages/torch/nn/modules/module.py", line 1553, in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
  File "/home/abdenour/anaconda3/envs/fl/lib/python3.10/site-packages/torch/nn/modules/module.py", line 1562, in _call_impl
    return forward_call(*args, **kwargs)
  File "/home/abdenour/anaconda3/envs/fl/lib/python3.10/site-packages/torch/nn/modules/container.py", line 219, in forward
    input = module(input)
  File "/home/abdenour/anaconda3/envs/fl/lib/python3.10/site-packages/torch/nn/modules/module.py", line 1553, in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
  File "/home/abdenour/anaconda3/envs/fl/lib/python3.10/site-packages/torch/nn/modules/module.py", line 1562, in _call_impl
    return forward_call(*args, **kwargs)
  File "/home/abdenour/anaconda3/envs/fl/lib/python3.10/site-packages/torch/nn/modules/conv.py", line 458, in forward
    return self._conv_forward(input, self.weight, self.bias)
  File "/home/abdenour/anaconda3/envs/fl/lib/python3.10/site-packages/torch/nn/modules/conv.py", line 454, in _conv_forward
    return F.conv2d(input, weight, bias, self.stride,
torch.OutOfMemoryError: CUDA out of memory. Tried to allocate 784.00 MiB. GPU 0 has a total capacity of 47.33 GiB of which 4.81 MiB is free. Process 2279413 has 11.11 GiB memory in use. Process 2265701 has 2.37 GiB memory in use. Process 2279412 has 11.11 GiB memory in use. Process 2330873 has 11.11 GiB memory in use. Process 2317160 has 2.37 GiB memory in use. Process 2330872 has 6.78 GiB memory in use. Process 2330871 has 816.00 MiB memory in use. Including non-PyTorch memory, this process has 1.61 GiB memory in use. Of the allocated memory 1.30 GiB is allocated by PyTorch, and 6.08 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)

The above exception was the direct cause of the following exception:

[36mray::ClientAppActor.run()[39m (pid=2330870, ip=115.145.172.224, actor_id=18884b649bbd3d098a6ec02b01000000, repr=<flwr.simulation.ray_transport.ray_actor.ClientAppActor object at 0x7ffb805757b0>)
  File "/home/abdenour/.local/lib/python3.10/site-packages/flwr/simulation/ray_transport/ray_actor.py", line 63, in run
    raise ClientAppException(str(ex)) from ex
flwr.client.client_app.ClientAppException: 
Exception ClientAppException occurred. Message: CUDA out of memory. Tried to allocate 784.00 MiB. GPU 0 has a total capacity of 47.33 GiB of which 4.81 MiB is free. Process 2279413 has 11.11 GiB memory in use. Process 2265701 has 2.37 GiB memory in use. Process 2279412 has 11.11 GiB memory in use. Process 2330873 has 11.11 GiB memory in use. Process 2317160 has 2.37 GiB memory in use. Process 2330872 has 6.78 GiB memory in use. Process 2330871 has 816.00 MiB memory in use. Including non-PyTorch memory, this process has 1.61 GiB memory in use. Of the allocated memory 1.30 GiB is allocated by PyTorch, and 6.08 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)

[2024-09-19 01:26:41,721][flwr][ERROR] - [36mray::ClientAppActor.run()[39m (pid=2330870, ip=115.145.172.224, actor_id=18884b649bbd3d098a6ec02b01000000, repr=<flwr.simulation.ray_transport.ray_actor.ClientAppActor object at 0x7ffb805757b0>)
  File "/home/abdenour/.local/lib/python3.10/site-packages/flwr/client/client_app.py", line 143, in __call__
    return self._call(message, context)
  File "/home/abdenour/.local/lib/python3.10/site-packages/flwr/client/client_app.py", line 126, in ffn
    out_message = handle_legacy_message_from_msgtype(
  File "/home/abdenour/.local/lib/python3.10/site-packages/flwr/client/message_handler/message_handler.py", line 129, in handle_legacy_message_from_msgtype
    fit_res = maybe_call_fit(
  File "/home/abdenour/.local/lib/python3.10/site-packages/flwr/client/client.py", line 234, in maybe_call_fit
    return client.fit(fit_ins)
  File "/home/abdenour/.local/lib/python3.10/site-packages/flwr/client/numpy_client.py", line 238, in _fit
    results = self.numpy_client.fit(parameters, ins.config)  # type: ignore
  File "/home/abdenour/fl_with_flower/fednova_vgg16/fednova_vgg16/client.py", line 69, in fit
    train(self.model, trainloader, self.optimizer, epochs=num_epochs, device=self.device)
  File "/home/abdenour/fl_with_flower/fednova_vgg16/fednova_vgg16/utils.py", line 57, in train
    loss = criterion(net(images), labels)
  File "/home/abdenour/anaconda3/envs/fl/lib/python3.10/site-packages/torch/nn/modules/module.py", line 1553, in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
  File "/home/abdenour/anaconda3/envs/fl/lib/python3.10/site-packages/torch/nn/modules/module.py", line 1562, in _call_impl
    return forward_call(*args, **kwargs)
  File "/home/abdenour/anaconda3/envs/fl/lib/python3.10/site-packages/torchvision/models/vgg.py", line 66, in forward
    x = self.features(x)
  File "/home/abdenour/anaconda3/envs/fl/lib/python3.10/site-packages/torch/nn/modules/module.py", line 1553, in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
  File "/home/abdenour/anaconda3/envs/fl/lib/python3.10/site-packages/torch/nn/modules/module.py", line 1562, in _call_impl
    return forward_call(*args, **kwargs)
  File "/home/abdenour/anaconda3/envs/fl/lib/python3.10/site-packages/torch/nn/modules/container.py", line 219, in forward
    input = module(input)
  File "/home/abdenour/anaconda3/envs/fl/lib/python3.10/site-packages/torch/nn/modules/module.py", line 1553, in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
  File "/home/abdenour/anaconda3/envs/fl/lib/python3.10/site-packages/torch/nn/modules/module.py", line 1562, in _call_impl
    return forward_call(*args, **kwargs)
  File "/home/abdenour/anaconda3/envs/fl/lib/python3.10/site-packages/torch/nn/modules/conv.py", line 458, in forward
    return self._conv_forward(input, self.weight, self.bias)
  File "/home/abdenour/anaconda3/envs/fl/lib/python3.10/site-packages/torch/nn/modules/conv.py", line 454, in _conv_forward
    return F.conv2d(input, weight, bias, self.stride,
torch.OutOfMemoryError: CUDA out of memory. Tried to allocate 784.00 MiB. GPU 0 has a total capacity of 47.33 GiB of which 4.81 MiB is free. Process 2279413 has 11.11 GiB memory in use. Process 2265701 has 2.37 GiB memory in use. Process 2279412 has 11.11 GiB memory in use. Process 2330873 has 11.11 GiB memory in use. Process 2317160 has 2.37 GiB memory in use. Process 2330872 has 6.78 GiB memory in use. Process 2330871 has 816.00 MiB memory in use. Including non-PyTorch memory, this process has 1.61 GiB memory in use. Of the allocated memory 1.30 GiB is allocated by PyTorch, and 6.08 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)

The above exception was the direct cause of the following exception:

[36mray::ClientAppActor.run()[39m (pid=2330870, ip=115.145.172.224, actor_id=18884b649bbd3d098a6ec02b01000000, repr=<flwr.simulation.ray_transport.ray_actor.ClientAppActor object at 0x7ffb805757b0>)
  File "/home/abdenour/.local/lib/python3.10/site-packages/flwr/simulation/ray_transport/ray_actor.py", line 63, in run
    raise ClientAppException(str(ex)) from ex
flwr.client.client_app.ClientAppException: 
Exception ClientAppException occurred. Message: CUDA out of memory. Tried to allocate 784.00 MiB. GPU 0 has a total capacity of 47.33 GiB of which 4.81 MiB is free. Process 2279413 has 11.11 GiB memory in use. Process 2265701 has 2.37 GiB memory in use. Process 2279412 has 11.11 GiB memory in use. Process 2330873 has 11.11 GiB memory in use. Process 2317160 has 2.37 GiB memory in use. Process 2330872 has 6.78 GiB memory in use. Process 2330871 has 816.00 MiB memory in use. Including non-PyTorch memory, this process has 1.61 GiB memory in use. Of the allocated memory 1.30 GiB is allocated by PyTorch, and 6.08 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)
[2024-09-19 01:26:42,096][flwr][ERROR] - Traceback (most recent call last):
  File "/home/abdenour/.local/lib/python3.10/site-packages/flwr/simulation/ray_transport/ray_client_proxy.py", line 94, in _submit_job
    out_mssg, updated_context = self.actor_pool.get_client_result(
  File "/home/abdenour/.local/lib/python3.10/site-packages/flwr/simulation/ray_transport/ray_actor.py", line 398, in get_client_result
    return self._fetch_future_result(cid)
  File "/home/abdenour/.local/lib/python3.10/site-packages/flwr/simulation/ray_transport/ray_actor.py", line 279, in _fetch_future_result
    res_cid, out_mssg, updated_context = ray.get(
  File "/home/abdenour/.local/lib/python3.10/site-packages/ray/_private/auto_init_hook.py", line 21, in auto_init_wrapper
    return fn(*args, **kwargs)
  File "/home/abdenour/.local/lib/python3.10/site-packages/ray/_private/client_mode_hook.py", line 103, in wrapper
    return func(*args, **kwargs)
  File "/home/abdenour/.local/lib/python3.10/site-packages/ray/_private/worker.py", line 2667, in get
    values, debugger_breakpoint = worker.get_objects(object_refs, timeout=timeout)
  File "/home/abdenour/.local/lib/python3.10/site-packages/ray/_private/worker.py", line 864, in get_objects
    raise value.as_instanceof_cause()
ray.exceptions.RayTaskError(ClientAppException): [36mray::ClientAppActor.run()[39m (pid=2330872, ip=115.145.172.224, actor_id=d38096ee5352eefa91005c8e01000000, repr=<flwr.simulation.ray_transport.ray_actor.ClientAppActor object at 0x7fcf0478d750>)
  File "/home/abdenour/.local/lib/python3.10/site-packages/flwr/client/client_app.py", line 143, in __call__
    return self._call(message, context)
  File "/home/abdenour/.local/lib/python3.10/site-packages/flwr/client/client_app.py", line 126, in ffn
    out_message = handle_legacy_message_from_msgtype(
  File "/home/abdenour/.local/lib/python3.10/site-packages/flwr/client/message_handler/message_handler.py", line 129, in handle_legacy_message_from_msgtype
    fit_res = maybe_call_fit(
  File "/home/abdenour/.local/lib/python3.10/site-packages/flwr/client/client.py", line 234, in maybe_call_fit
    return client.fit(fit_ins)
  File "/home/abdenour/.local/lib/python3.10/site-packages/flwr/client/numpy_client.py", line 238, in _fit
    results = self.numpy_client.fit(parameters, ins.config)  # type: ignore
  File "/home/abdenour/fl_with_flower/fednova_vgg16/fednova_vgg16/client.py", line 69, in fit
    train(self.model, trainloader, self.optimizer, epochs=num_epochs, device=self.device)
  File "/home/abdenour/fl_with_flower/fednova_vgg16/fednova_vgg16/utils.py", line 58, in train
    loss.backward()
  File "/home/abdenour/anaconda3/envs/fl/lib/python3.10/site-packages/torch/_tensor.py", line 521, in backward
    torch.autograd.backward(
  File "/home/abdenour/anaconda3/envs/fl/lib/python3.10/site-packages/torch/autograd/__init__.py", line 289, in backward
    _engine_run_backward(
  File "/home/abdenour/anaconda3/envs/fl/lib/python3.10/site-packages/torch/autograd/graph.py", line 769, in _engine_run_backward
    return Variable._execution_engine.run_backward(  # Calls into the C++ engine to run the backward pass
torch.OutOfMemoryError: CUDA out of memory. Tried to allocate 98.00 MiB. GPU 0 has a total capacity of 47.33 GiB of which 6.81 MiB is free. Process 2279413 has 11.11 GiB memory in use. Process 2265701 has 2.37 GiB memory in use. Process 2279412 has 11.11 GiB memory in use. Process 2330873 has 11.11 GiB memory in use. Process 2317160 has 2.37 GiB memory in use. Including non-PyTorch memory, this process has 6.78 GiB memory in use. Process 2330871 has 816.00 MiB memory in use. Process 2330870 has 1.61 GiB memory in use. Of the allocated memory 6.26 GiB is allocated by PyTorch, and 199.47 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)

The above exception was the direct cause of the following exception:

[36mray::ClientAppActor.run()[39m (pid=2330872, ip=115.145.172.224, actor_id=d38096ee5352eefa91005c8e01000000, repr=<flwr.simulation.ray_transport.ray_actor.ClientAppActor object at 0x7fcf0478d750>)
  File "/home/abdenour/.local/lib/python3.10/site-packages/flwr/simulation/ray_transport/ray_actor.py", line 63, in run
    raise ClientAppException(str(ex)) from ex
flwr.client.client_app.ClientAppException: 
Exception ClientAppException occurred. Message: CUDA out of memory. Tried to allocate 98.00 MiB. GPU 0 has a total capacity of 47.33 GiB of which 6.81 MiB is free. Process 2279413 has 11.11 GiB memory in use. Process 2265701 has 2.37 GiB memory in use. Process 2279412 has 11.11 GiB memory in use. Process 2330873 has 11.11 GiB memory in use. Process 2317160 has 2.37 GiB memory in use. Including non-PyTorch memory, this process has 6.78 GiB memory in use. Process 2330871 has 816.00 MiB memory in use. Process 2330870 has 1.61 GiB memory in use. Of the allocated memory 6.26 GiB is allocated by PyTorch, and 199.47 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)

[2024-09-19 01:26:42,096][flwr][ERROR] - [36mray::ClientAppActor.run()[39m (pid=2330872, ip=115.145.172.224, actor_id=d38096ee5352eefa91005c8e01000000, repr=<flwr.simulation.ray_transport.ray_actor.ClientAppActor object at 0x7fcf0478d750>)
  File "/home/abdenour/.local/lib/python3.10/site-packages/flwr/client/client_app.py", line 143, in __call__
    return self._call(message, context)
  File "/home/abdenour/.local/lib/python3.10/site-packages/flwr/client/client_app.py", line 126, in ffn
    out_message = handle_legacy_message_from_msgtype(
  File "/home/abdenour/.local/lib/python3.10/site-packages/flwr/client/message_handler/message_handler.py", line 129, in handle_legacy_message_from_msgtype
    fit_res = maybe_call_fit(
  File "/home/abdenour/.local/lib/python3.10/site-packages/flwr/client/client.py", line 234, in maybe_call_fit
    return client.fit(fit_ins)
  File "/home/abdenour/.local/lib/python3.10/site-packages/flwr/client/numpy_client.py", line 238, in _fit
    results = self.numpy_client.fit(parameters, ins.config)  # type: ignore
  File "/home/abdenour/fl_with_flower/fednova_vgg16/fednova_vgg16/client.py", line 69, in fit
    train(self.model, trainloader, self.optimizer, epochs=num_epochs, device=self.device)
  File "/home/abdenour/fl_with_flower/fednova_vgg16/fednova_vgg16/utils.py", line 58, in train
    loss.backward()
  File "/home/abdenour/anaconda3/envs/fl/lib/python3.10/site-packages/torch/_tensor.py", line 521, in backward
    torch.autograd.backward(
  File "/home/abdenour/anaconda3/envs/fl/lib/python3.10/site-packages/torch/autograd/__init__.py", line 289, in backward
    _engine_run_backward(
  File "/home/abdenour/anaconda3/envs/fl/lib/python3.10/site-packages/torch/autograd/graph.py", line 769, in _engine_run_backward
    return Variable._execution_engine.run_backward(  # Calls into the C++ engine to run the backward pass
torch.OutOfMemoryError: CUDA out of memory. Tried to allocate 98.00 MiB. GPU 0 has a total capacity of 47.33 GiB of which 6.81 MiB is free. Process 2279413 has 11.11 GiB memory in use. Process 2265701 has 2.37 GiB memory in use. Process 2279412 has 11.11 GiB memory in use. Process 2330873 has 11.11 GiB memory in use. Process 2317160 has 2.37 GiB memory in use. Including non-PyTorch memory, this process has 6.78 GiB memory in use. Process 2330871 has 816.00 MiB memory in use. Process 2330870 has 1.61 GiB memory in use. Of the allocated memory 6.26 GiB is allocated by PyTorch, and 199.47 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)

The above exception was the direct cause of the following exception:

[36mray::ClientAppActor.run()[39m (pid=2330872, ip=115.145.172.224, actor_id=d38096ee5352eefa91005c8e01000000, repr=<flwr.simulation.ray_transport.ray_actor.ClientAppActor object at 0x7fcf0478d750>)
  File "/home/abdenour/.local/lib/python3.10/site-packages/flwr/simulation/ray_transport/ray_actor.py", line 63, in run
    raise ClientAppException(str(ex)) from ex
flwr.client.client_app.ClientAppException: 
Exception ClientAppException occurred. Message: CUDA out of memory. Tried to allocate 98.00 MiB. GPU 0 has a total capacity of 47.33 GiB of which 6.81 MiB is free. Process 2279413 has 11.11 GiB memory in use. Process 2265701 has 2.37 GiB memory in use. Process 2279412 has 11.11 GiB memory in use. Process 2330873 has 11.11 GiB memory in use. Process 2317160 has 2.37 GiB memory in use. Including non-PyTorch memory, this process has 6.78 GiB memory in use. Process 2330871 has 816.00 MiB memory in use. Process 2330870 has 1.61 GiB memory in use. Of the allocated memory 6.26 GiB is allocated by PyTorch, and 199.47 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)
[2024-09-19 01:26:42,097][flwr][INFO] - aggregate_fit: received 1 results and 4 failures

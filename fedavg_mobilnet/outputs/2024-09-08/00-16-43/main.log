[2024-09-08 00:16:46,542][flwr][INFO] - Starting Flower simulation, config: num_rounds=100, no round_timeout
[2024-09-08 00:16:49,769][flwr][INFO] - Flower VCE: Ray initialized with resources: {'memory': 169159452058.0, 'CPU': 256.0, 'node:__internal_head__': 1.0, 'node:115.145.172.224': 1.0, 'object_store_memory': 76782622310.0, 'GPU': 5.0, 'accelerator_type:G': 1.0}
[2024-09-08 00:16:49,769][flwr][INFO] - Optimize your simulation with Flower VCE: https://flower.ai/docs/framework/how-to-run-simulations.html
[2024-09-08 00:16:49,769][flwr][INFO] - Flower VCE: Resources for each Virtual Client: {'num_cpus': 16, 'num_gpus': 0.25}
[2024-09-08 00:16:49,787][flwr][INFO] - Flower VCE: Creating VirtualClientEngineActorPool with 16 actors
[2024-09-08 00:16:49,787][flwr][INFO] - [INIT]
[2024-09-08 00:16:49,787][flwr][INFO] - Requesting initial parameters from one random client
[2024-09-08 00:16:54,803][flwr][ERROR] - Traceback (most recent call last):
  File "/home/abdenour/.local/lib/python3.10/site-packages/flwr/simulation/ray_transport/ray_client_proxy.py", line 94, in _submit_job
    out_mssg, updated_context = self.actor_pool.get_client_result(
  File "/home/abdenour/.local/lib/python3.10/site-packages/flwr/simulation/ray_transport/ray_actor.py", line 398, in get_client_result
    return self._fetch_future_result(cid)
  File "/home/abdenour/.local/lib/python3.10/site-packages/flwr/simulation/ray_transport/ray_actor.py", line 279, in _fetch_future_result
    res_cid, out_mssg, updated_context = ray.get(
  File "/home/abdenour/.local/lib/python3.10/site-packages/ray/_private/auto_init_hook.py", line 21, in auto_init_wrapper
    return fn(*args, **kwargs)
  File "/home/abdenour/.local/lib/python3.10/site-packages/ray/_private/client_mode_hook.py", line 103, in wrapper
    return func(*args, **kwargs)
  File "/home/abdenour/.local/lib/python3.10/site-packages/ray/_private/worker.py", line 2667, in get
    values, debugger_breakpoint = worker.get_objects(object_refs, timeout=timeout)
  File "/home/abdenour/.local/lib/python3.10/site-packages/ray/_private/worker.py", line 864, in get_objects
    raise value.as_instanceof_cause()
ray.exceptions.RayTaskError(ClientAppException): [36mray::ClientAppActor.run()[39m (pid=307327, ip=115.145.172.224, actor_id=786c9fa344367d3166d2679c01000000, repr=<flwr.simulation.ray_transport.ray_actor.ClientAppActor object at 0x7f3f50781720>)
  File "/home/abdenour/.local/lib/python3.10/site-packages/flwr/client/client_app.py", line 143, in __call__
    return self._call(message, context)
  File "/home/abdenour/.local/lib/python3.10/site-packages/flwr/client/client_app.py", line 126, in ffn
    out_message = handle_legacy_message_from_msgtype(
  File "/home/abdenour/.local/lib/python3.10/site-packages/flwr/client/message_handler/message_handler.py", line 95, in handle_legacy_message_from_msgtype
    client = client_fn(context)
  File "/home/abdenour/fl_with_flower/fedavg_mobilnet/fedavg_mobilnet/client.py", line 97, in client_fn
    return FlowerClient(trainset, valset, num_classes).to_client()
  File "/home/abdenour/fl_with_flower/fedavg_mobilnet/fedavg_mobilnet/client.py", line 26, in __init__
    self.model = self.model.to(self.device)
  File "/home/abdenour/anaconda3/envs/fl/lib/python3.10/site-packages/torch/nn/modules/module.py", line 1174, in to
    return self._apply(convert)
  File "/home/abdenour/anaconda3/envs/fl/lib/python3.10/site-packages/torch/nn/modules/module.py", line 780, in _apply
    module._apply(fn)
  File "/home/abdenour/anaconda3/envs/fl/lib/python3.10/site-packages/torch/nn/modules/module.py", line 780, in _apply
    module._apply(fn)
  File "/home/abdenour/anaconda3/envs/fl/lib/python3.10/site-packages/torch/nn/modules/module.py", line 780, in _apply
    module._apply(fn)
  File "/home/abdenour/anaconda3/envs/fl/lib/python3.10/site-packages/torch/nn/modules/module.py", line 805, in _apply
    param_applied = fn(param)
  File "/home/abdenour/anaconda3/envs/fl/lib/python3.10/site-packages/torch/nn/modules/module.py", line 1160, in convert
    return t.to(
RuntimeError: CUDA error: invalid device ordinal
CUDA kernel errors might be asynchronously reported at some other API call, so the stacktrace below might be incorrect.
For debugging consider passing CUDA_LAUNCH_BLOCKING=1
Compile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.


The above exception was the direct cause of the following exception:

[36mray::ClientAppActor.run()[39m (pid=307327, ip=115.145.172.224, actor_id=786c9fa344367d3166d2679c01000000, repr=<flwr.simulation.ray_transport.ray_actor.ClientAppActor object at 0x7f3f50781720>)
  File "/home/abdenour/.local/lib/python3.10/site-packages/flwr/simulation/ray_transport/ray_actor.py", line 63, in run
    raise ClientAppException(str(ex)) from ex
flwr.client.client_app.ClientAppException: 
Exception ClientAppException occurred. Message: CUDA error: invalid device ordinal
CUDA kernel errors might be asynchronously reported at some other API call, so the stacktrace below might be incorrect.
For debugging consider passing CUDA_LAUNCH_BLOCKING=1
Compile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.

[2024-09-08 00:16:54,804][flwr][ERROR] - [36mray::ClientAppActor.run()[39m (pid=307327, ip=115.145.172.224, actor_id=786c9fa344367d3166d2679c01000000, repr=<flwr.simulation.ray_transport.ray_actor.ClientAppActor object at 0x7f3f50781720>)
  File "/home/abdenour/.local/lib/python3.10/site-packages/flwr/client/client_app.py", line 143, in __call__
    return self._call(message, context)
  File "/home/abdenour/.local/lib/python3.10/site-packages/flwr/client/client_app.py", line 126, in ffn
    out_message = handle_legacy_message_from_msgtype(
  File "/home/abdenour/.local/lib/python3.10/site-packages/flwr/client/message_handler/message_handler.py", line 95, in handle_legacy_message_from_msgtype
    client = client_fn(context)
  File "/home/abdenour/fl_with_flower/fedavg_mobilnet/fedavg_mobilnet/client.py", line 97, in client_fn
    return FlowerClient(trainset, valset, num_classes).to_client()
  File "/home/abdenour/fl_with_flower/fedavg_mobilnet/fedavg_mobilnet/client.py", line 26, in __init__
    self.model = self.model.to(self.device)
  File "/home/abdenour/anaconda3/envs/fl/lib/python3.10/site-packages/torch/nn/modules/module.py", line 1174, in to
    return self._apply(convert)
  File "/home/abdenour/anaconda3/envs/fl/lib/python3.10/site-packages/torch/nn/modules/module.py", line 780, in _apply
    module._apply(fn)
  File "/home/abdenour/anaconda3/envs/fl/lib/python3.10/site-packages/torch/nn/modules/module.py", line 780, in _apply
    module._apply(fn)
  File "/home/abdenour/anaconda3/envs/fl/lib/python3.10/site-packages/torch/nn/modules/module.py", line 780, in _apply
    module._apply(fn)
  File "/home/abdenour/anaconda3/envs/fl/lib/python3.10/site-packages/torch/nn/modules/module.py", line 805, in _apply
    param_applied = fn(param)
  File "/home/abdenour/anaconda3/envs/fl/lib/python3.10/site-packages/torch/nn/modules/module.py", line 1160, in convert
    return t.to(
RuntimeError: CUDA error: invalid device ordinal
CUDA kernel errors might be asynchronously reported at some other API call, so the stacktrace below might be incorrect.
For debugging consider passing CUDA_LAUNCH_BLOCKING=1
Compile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.


The above exception was the direct cause of the following exception:

[36mray::ClientAppActor.run()[39m (pid=307327, ip=115.145.172.224, actor_id=786c9fa344367d3166d2679c01000000, repr=<flwr.simulation.ray_transport.ray_actor.ClientAppActor object at 0x7f3f50781720>)
  File "/home/abdenour/.local/lib/python3.10/site-packages/flwr/simulation/ray_transport/ray_actor.py", line 63, in run
    raise ClientAppException(str(ex)) from ex
flwr.client.client_app.ClientAppException: 
Exception ClientAppException occurred. Message: CUDA error: invalid device ordinal
CUDA kernel errors might be asynchronously reported at some other API call, so the stacktrace below might be incorrect.
For debugging consider passing CUDA_LAUNCH_BLOCKING=1
Compile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.
[2024-09-08 00:16:54,804][flwr][ERROR] - [36mray::ClientAppActor.run()[39m (pid=307327, ip=115.145.172.224, actor_id=786c9fa344367d3166d2679c01000000, repr=<flwr.simulation.ray_transport.ray_actor.ClientAppActor object at 0x7f3f50781720>)
  File "/home/abdenour/.local/lib/python3.10/site-packages/flwr/client/client_app.py", line 143, in __call__
    return self._call(message, context)
  File "/home/abdenour/.local/lib/python3.10/site-packages/flwr/client/client_app.py", line 126, in ffn
    out_message = handle_legacy_message_from_msgtype(
  File "/home/abdenour/.local/lib/python3.10/site-packages/flwr/client/message_handler/message_handler.py", line 95, in handle_legacy_message_from_msgtype
    client = client_fn(context)
  File "/home/abdenour/fl_with_flower/fedavg_mobilnet/fedavg_mobilnet/client.py", line 97, in client_fn
    return FlowerClient(trainset, valset, num_classes).to_client()
  File "/home/abdenour/fl_with_flower/fedavg_mobilnet/fedavg_mobilnet/client.py", line 26, in __init__
    self.model = self.model.to(self.device)
  File "/home/abdenour/anaconda3/envs/fl/lib/python3.10/site-packages/torch/nn/modules/module.py", line 1174, in to
    return self._apply(convert)
  File "/home/abdenour/anaconda3/envs/fl/lib/python3.10/site-packages/torch/nn/modules/module.py", line 780, in _apply
    module._apply(fn)
  File "/home/abdenour/anaconda3/envs/fl/lib/python3.10/site-packages/torch/nn/modules/module.py", line 780, in _apply
    module._apply(fn)
  File "/home/abdenour/anaconda3/envs/fl/lib/python3.10/site-packages/torch/nn/modules/module.py", line 780, in _apply
    module._apply(fn)
  File "/home/abdenour/anaconda3/envs/fl/lib/python3.10/site-packages/torch/nn/modules/module.py", line 805, in _apply
    param_applied = fn(param)
  File "/home/abdenour/anaconda3/envs/fl/lib/python3.10/site-packages/torch/nn/modules/module.py", line 1160, in convert
    return t.to(
RuntimeError: CUDA error: invalid device ordinal
CUDA kernel errors might be asynchronously reported at some other API call, so the stacktrace below might be incorrect.
For debugging consider passing CUDA_LAUNCH_BLOCKING=1
Compile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.


The above exception was the direct cause of the following exception:

[36mray::ClientAppActor.run()[39m (pid=307327, ip=115.145.172.224, actor_id=786c9fa344367d3166d2679c01000000, repr=<flwr.simulation.ray_transport.ray_actor.ClientAppActor object at 0x7f3f50781720>)
  File "/home/abdenour/.local/lib/python3.10/site-packages/flwr/simulation/ray_transport/ray_actor.py", line 63, in run
    raise ClientAppException(str(ex)) from ex
flwr.client.client_app.ClientAppException: 
Exception ClientAppException occurred. Message: CUDA error: invalid device ordinal
CUDA kernel errors might be asynchronously reported at some other API call, so the stacktrace below might be incorrect.
For debugging consider passing CUDA_LAUNCH_BLOCKING=1
Compile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.
[2024-09-08 00:16:54,805][flwr][ERROR] - Traceback (most recent call last):
  File "/home/abdenour/.local/lib/python3.10/site-packages/flwr/simulation/app.py", line 339, in start_simulation
    hist = run_fl(
  File "/home/abdenour/.local/lib/python3.10/site-packages/flwr/server/server.py", line 490, in run_fl
    hist, elapsed_time = server.fit(
  File "/home/abdenour/.local/lib/python3.10/site-packages/flwr/server/server.py", line 93, in fit
    self.parameters = self._get_initial_parameters(server_round=0, timeout=timeout)
  File "/home/abdenour/.local/lib/python3.10/site-packages/flwr/server/server.py", line 282, in _get_initial_parameters
    get_parameters_res = random_client.get_parameters(
  File "/home/abdenour/.local/lib/python3.10/site-packages/flwr/simulation/ray_transport/ray_client_proxy.py", line 168, in get_parameters
    message_out = self._submit_job(message, timeout)
  File "/home/abdenour/.local/lib/python3.10/site-packages/flwr/simulation/ray_transport/ray_client_proxy.py", line 108, in _submit_job
    raise ex
  File "/home/abdenour/.local/lib/python3.10/site-packages/flwr/simulation/ray_transport/ray_client_proxy.py", line 94, in _submit_job
    out_mssg, updated_context = self.actor_pool.get_client_result(
  File "/home/abdenour/.local/lib/python3.10/site-packages/flwr/simulation/ray_transport/ray_actor.py", line 398, in get_client_result
    return self._fetch_future_result(cid)
  File "/home/abdenour/.local/lib/python3.10/site-packages/flwr/simulation/ray_transport/ray_actor.py", line 279, in _fetch_future_result
    res_cid, out_mssg, updated_context = ray.get(
  File "/home/abdenour/.local/lib/python3.10/site-packages/ray/_private/auto_init_hook.py", line 21, in auto_init_wrapper
    return fn(*args, **kwargs)
  File "/home/abdenour/.local/lib/python3.10/site-packages/ray/_private/client_mode_hook.py", line 103, in wrapper
    return func(*args, **kwargs)
  File "/home/abdenour/.local/lib/python3.10/site-packages/ray/_private/worker.py", line 2667, in get
    values, debugger_breakpoint = worker.get_objects(object_refs, timeout=timeout)
  File "/home/abdenour/.local/lib/python3.10/site-packages/ray/_private/worker.py", line 864, in get_objects
    raise value.as_instanceof_cause()
ray.exceptions.RayTaskError(ClientAppException): [36mray::ClientAppActor.run()[39m (pid=307327, ip=115.145.172.224, actor_id=786c9fa344367d3166d2679c01000000, repr=<flwr.simulation.ray_transport.ray_actor.ClientAppActor object at 0x7f3f50781720>)
  File "/home/abdenour/.local/lib/python3.10/site-packages/flwr/client/client_app.py", line 143, in __call__
    return self._call(message, context)
  File "/home/abdenour/.local/lib/python3.10/site-packages/flwr/client/client_app.py", line 126, in ffn
    out_message = handle_legacy_message_from_msgtype(
  File "/home/abdenour/.local/lib/python3.10/site-packages/flwr/client/message_handler/message_handler.py", line 95, in handle_legacy_message_from_msgtype
    client = client_fn(context)
  File "/home/abdenour/fl_with_flower/fedavg_mobilnet/fedavg_mobilnet/client.py", line 97, in client_fn
    return FlowerClient(trainset, valset, num_classes).to_client()
  File "/home/abdenour/fl_with_flower/fedavg_mobilnet/fedavg_mobilnet/client.py", line 26, in __init__
    self.model = self.model.to(self.device)
  File "/home/abdenour/anaconda3/envs/fl/lib/python3.10/site-packages/torch/nn/modules/module.py", line 1174, in to
    return self._apply(convert)
  File "/home/abdenour/anaconda3/envs/fl/lib/python3.10/site-packages/torch/nn/modules/module.py", line 780, in _apply
    module._apply(fn)
  File "/home/abdenour/anaconda3/envs/fl/lib/python3.10/site-packages/torch/nn/modules/module.py", line 780, in _apply
    module._apply(fn)
  File "/home/abdenour/anaconda3/envs/fl/lib/python3.10/site-packages/torch/nn/modules/module.py", line 780, in _apply
    module._apply(fn)
  File "/home/abdenour/anaconda3/envs/fl/lib/python3.10/site-packages/torch/nn/modules/module.py", line 805, in _apply
    param_applied = fn(param)
  File "/home/abdenour/anaconda3/envs/fl/lib/python3.10/site-packages/torch/nn/modules/module.py", line 1160, in convert
    return t.to(
RuntimeError: CUDA error: invalid device ordinal
CUDA kernel errors might be asynchronously reported at some other API call, so the stacktrace below might be incorrect.
For debugging consider passing CUDA_LAUNCH_BLOCKING=1
Compile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.


The above exception was the direct cause of the following exception:

[36mray::ClientAppActor.run()[39m (pid=307327, ip=115.145.172.224, actor_id=786c9fa344367d3166d2679c01000000, repr=<flwr.simulation.ray_transport.ray_actor.ClientAppActor object at 0x7f3f50781720>)
  File "/home/abdenour/.local/lib/python3.10/site-packages/flwr/simulation/ray_transport/ray_actor.py", line 63, in run
    raise ClientAppException(str(ex)) from ex
flwr.client.client_app.ClientAppException: 
Exception ClientAppException occurred. Message: CUDA error: invalid device ordinal
CUDA kernel errors might be asynchronously reported at some other API call, so the stacktrace below might be incorrect.
For debugging consider passing CUDA_LAUNCH_BLOCKING=1
Compile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.

[2024-09-08 00:16:54,805][flwr][ERROR] - Your simulation crashed :(. This could be because of several reasons. The most common are: 
	 > Sometimes, issues in the simulation code itself can cause crashes. It's always a good idea to double-check your code for any potential bugs or inconsistencies that might be contributing to the problem. For example: 
		 - You might be using a class attribute in your clients that hasn't been defined.
		 - There could be an incorrect method call to a 3rd party library (e.g., PyTorch).
		 - The return types of methods in your clients/strategies might be incorrect.
	 > Your system couldn't fit a single VirtualClient: try lowering `client_resources`.
	 > All the actors in your pool crashed. This could be because: 
		 - You clients hit an out-of-memory (OOM) error and actors couldn't recover from it. Try launching your simulation with more generous `client_resources` setting (i.e. it seems {'num_cpus': 16, 'num_gpus': 0.25} is not enough for your run). Use fewer concurrent actors. 
		 - You were running a multi-node simulation and all worker nodes disconnected. The head node might still be alive but cannot accommodate any actor with resources: {'num_cpus': 16, 'num_gpus': 0.25}.
Take a look at the Flower simulation examples for guidance <https://flower.ai/docs/framework/how-to-run-simulations.html>.

[2024-08-13 23:31:10,450][flwr][INFO] - Starting Flower simulation, config: num_rounds=300, no round_timeout
[2024-08-13 23:31:13,372][flwr][INFO] - Flower VCE: Ray initialized with resources: {'object_store_memory': 7076173824.0, 'node:__internal_head__': 1.0, 'accelerator_type:G': 1.0, 'GPU': 1.0, 'CPU': 32.0, 'node:115.145.171.128': 1.0, 'memory': 14152347648.0}
[2024-08-13 23:31:13,372][flwr][INFO] - Optimize your simulation with Flower VCE: https://flower.ai/docs/framework/how-to-run-simulations.html
[2024-08-13 23:31:13,372][flwr][INFO] - Flower VCE: Resources for each Virtual Client: {'num_cpus': 2, 'num_gpus': 0.1}
[2024-08-13 23:31:13,382][flwr][INFO] - Flower VCE: Creating VirtualClientEngineActorPool with 10 actors
[2024-08-13 23:31:13,382][flwr][INFO] - [INIT]
[2024-08-13 23:31:13,382][flwr][INFO] - Requesting initial parameters from one random client
[2024-08-13 23:31:15,717][flwr][INFO] - Received initial parameters from one random client
[2024-08-13 23:31:15,717][flwr][INFO] - Evaluating initial global parameters
[2024-08-13 23:31:18,740][flwr][INFO] - initial parameters (loss, other metrics): 19.708263516426086, {'accuracy': 0.6298076923076923}
[2024-08-13 23:31:18,740][flwr][INFO] - 
[2024-08-13 23:31:18,740][flwr][INFO] - [ROUND 1]
[2024-08-13 23:31:18,740][flwr][INFO] - configure_fit: strategy sampled 10 clients (out of 20)
[2024-08-13 23:31:22,584][flwr][INFO] - aggregate_fit: received 10 results and 0 failures
[2024-08-13 23:31:22,662][flwr][WARNING] - No fit_metrics_aggregation_fn provided
[2024-08-13 23:31:25,385][flwr][INFO] - fit progress: (1, 9.247070968151093, {'accuracy': 0.625}, 6.645120439003222)
[2024-08-13 23:31:25,385][flwr][INFO] - configure_evaluate: strategy sampled 2 clients (out of 20)
[2024-08-13 23:31:25,571][flwr][INFO] - aggregate_evaluate: received 2 results and 0 failures
[2024-08-13 23:31:25,571][flwr][INFO] - 
[2024-08-13 23:31:25,571][flwr][INFO] - [ROUND 2]
[2024-08-13 23:31:25,571][flwr][INFO] - configure_fit: strategy sampled 10 clients (out of 20)
[2024-08-13 23:31:27,694][flwr][INFO] - aggregate_fit: received 10 results and 0 failures
[2024-08-13 23:31:30,845][flwr][INFO] - fit progress: (2, 9.014980673789978, {'accuracy': 0.625}, 12.105624274990987)
[2024-08-13 23:31:30,845][flwr][INFO] - configure_evaluate: strategy sampled 2 clients (out of 20)
[2024-08-13 23:31:31,064][flwr][INFO] - aggregate_evaluate: received 2 results and 0 failures
[2024-08-13 23:31:31,064][flwr][INFO] - 
[2024-08-13 23:31:31,064][flwr][INFO] - [ROUND 3]
[2024-08-13 23:31:31,064][flwr][INFO] - configure_fit: strategy sampled 10 clients (out of 20)
[2024-08-13 23:31:33,834][flwr][INFO] - aggregate_fit: received 10 results and 0 failures
[2024-08-13 23:31:36,960][flwr][INFO] - fit progress: (3, 8.972548604011536, {'accuracy': 0.625}, 18.220081369014224)
[2024-08-13 23:31:36,960][flwr][INFO] - configure_evaluate: strategy sampled 2 clients (out of 20)
[2024-08-13 23:31:37,257][flwr][INFO] - aggregate_evaluate: received 2 results and 0 failures
[2024-08-13 23:31:37,257][flwr][INFO] - 
[2024-08-13 23:31:37,257][flwr][INFO] - [ROUND 4]
[2024-08-13 23:31:37,257][flwr][INFO] - configure_fit: strategy sampled 10 clients (out of 20)
[2024-08-13 23:31:39,284][flwr][INFO] - aggregate_fit: received 10 results and 0 failures
[2024-08-13 23:31:42,183][flwr][INFO] - fit progress: (4, 9.09510749578476, {'accuracy': 0.625}, 23.44305821901071)
[2024-08-13 23:31:42,183][flwr][INFO] - configure_evaluate: strategy sampled 2 clients (out of 20)
[2024-08-13 23:31:42,499][flwr][INFO] - aggregate_evaluate: received 2 results and 0 failures
[2024-08-13 23:31:42,500][flwr][INFO] - 
[2024-08-13 23:31:42,500][flwr][INFO] - [ROUND 5]
[2024-08-13 23:31:42,500][flwr][INFO] - configure_fit: strategy sampled 10 clients (out of 20)
[2024-08-13 23:31:44,595][flwr][INFO] - aggregate_fit: received 10 results and 0 failures
[2024-08-13 23:31:47,389][flwr][INFO] - fit progress: (5, 8.903146803379059, {'accuracy': 0.625}, 28.649590479995823)
[2024-08-13 23:31:47,389][flwr][INFO] - configure_evaluate: strategy sampled 2 clients (out of 20)
[2024-08-13 23:31:47,788][flwr][INFO] - aggregate_evaluate: received 2 results and 0 failures
[2024-08-13 23:31:47,788][flwr][INFO] - 
[2024-08-13 23:31:47,788][flwr][INFO] - [ROUND 6]
[2024-08-13 23:31:47,789][flwr][INFO] - configure_fit: strategy sampled 10 clients (out of 20)
[2024-08-13 23:31:49,888][flwr][INFO] - aggregate_fit: received 10 results and 0 failures
[2024-08-13 23:31:52,920][flwr][INFO] - fit progress: (6, 8.827897131443024, {'accuracy': 0.625}, 34.179830943001434)
[2024-08-13 23:31:52,920][flwr][INFO] - configure_evaluate: strategy sampled 2 clients (out of 20)
[2024-08-13 23:31:53,151][flwr][INFO] - aggregate_evaluate: received 2 results and 0 failures
[2024-08-13 23:31:53,151][flwr][INFO] - 
[2024-08-13 23:31:53,151][flwr][INFO] - [ROUND 7]
[2024-08-13 23:31:53,151][flwr][INFO] - configure_fit: strategy sampled 10 clients (out of 20)
[2024-08-13 23:31:55,192][flwr][INFO] - aggregate_fit: received 10 results and 0 failures
[2024-08-13 23:31:57,943][flwr][INFO] - fit progress: (7, 8.705414533615112, {'accuracy': 0.6282051282051282}, 39.203712740010815)
[2024-08-13 23:31:57,944][flwr][INFO] - configure_evaluate: strategy sampled 2 clients (out of 20)
[2024-08-13 23:31:58,308][flwr][INFO] - aggregate_evaluate: received 2 results and 0 failures
[2024-08-13 23:31:58,308][flwr][INFO] - 
[2024-08-13 23:31:58,308][flwr][INFO] - [ROUND 8]
[2024-08-13 23:31:58,308][flwr][INFO] - configure_fit: strategy sampled 10 clients (out of 20)
[2024-08-13 23:32:00,410][flwr][INFO] - aggregate_fit: received 10 results and 0 failures
[2024-08-13 23:32:03,161][flwr][INFO] - fit progress: (8, 8.515677452087402, {'accuracy': 0.6330128205128205}, 44.421129594993545)
[2024-08-13 23:32:03,161][flwr][INFO] - configure_evaluate: strategy sampled 2 clients (out of 20)
[2024-08-13 23:32:03,554][flwr][INFO] - aggregate_evaluate: received 2 results and 0 failures
[2024-08-13 23:32:03,554][flwr][INFO] - 
[2024-08-13 23:32:03,554][flwr][INFO] - [ROUND 9]
[2024-08-13 23:32:03,554][flwr][INFO] - configure_fit: strategy sampled 10 clients (out of 20)
[2024-08-13 23:32:05,588][flwr][INFO] - aggregate_fit: received 10 results and 0 failures
[2024-08-13 23:32:08,681][flwr][INFO] - fit progress: (9, 8.332763850688934, {'accuracy': 0.6522435897435898}, 49.94105519601726)
[2024-08-13 23:32:08,681][flwr][INFO] - configure_evaluate: strategy sampled 2 clients (out of 20)
[2024-08-13 23:32:09,189][flwr][INFO] - aggregate_evaluate: received 2 results and 0 failures
[2024-08-13 23:32:09,189][flwr][INFO] - 
[2024-08-13 23:32:09,189][flwr][INFO] - [ROUND 10]
[2024-08-13 23:32:09,189][flwr][INFO] - configure_fit: strategy sampled 10 clients (out of 20)
[2024-08-13 23:32:11,316][flwr][INFO] - aggregate_fit: received 10 results and 0 failures
[2024-08-13 23:32:14,070][flwr][INFO] - fit progress: (10, 8.116403877735138, {'accuracy': 0.6698717948717948}, 55.33076642200467)
[2024-08-13 23:32:14,071][flwr][INFO] - configure_evaluate: strategy sampled 2 clients (out of 20)
[2024-08-13 23:32:14,441][flwr][INFO] - aggregate_evaluate: received 2 results and 0 failures
[2024-08-13 23:32:14,442][flwr][INFO] - 
[2024-08-13 23:32:14,442][flwr][INFO] - [ROUND 11]
[2024-08-13 23:32:14,442][flwr][INFO] - configure_fit: strategy sampled 10 clients (out of 20)
[2024-08-13 23:32:16,617][flwr][INFO] - aggregate_fit: received 10 results and 0 failures
[2024-08-13 23:32:19,980][flwr][INFO] - fit progress: (11, 8.157588243484497, {'accuracy': 0.6778846153846154}, 61.24061190200155)
[2024-08-13 23:32:19,980][flwr][INFO] - configure_evaluate: strategy sampled 2 clients (out of 20)
[2024-08-13 23:32:20,161][flwr][INFO] - aggregate_evaluate: received 2 results and 0 failures
[2024-08-13 23:32:20,161][flwr][INFO] - 
[2024-08-13 23:32:20,161][flwr][INFO] - [ROUND 12]
[2024-08-13 23:32:20,162][flwr][INFO] - configure_fit: strategy sampled 10 clients (out of 20)
[2024-08-13 23:32:22,300][flwr][INFO] - aggregate_fit: received 10 results and 0 failures
[2024-08-13 23:32:25,564][flwr][INFO] - fit progress: (12, 7.809833228588104, {'accuracy': 0.6971153846153846}, 66.82394574701902)
[2024-08-13 23:32:25,564][flwr][INFO] - configure_evaluate: strategy sampled 2 clients (out of 20)
[2024-08-13 23:32:25,799][flwr][INFO] - aggregate_evaluate: received 2 results and 0 failures
[2024-08-13 23:32:25,800][flwr][INFO] - 
[2024-08-13 23:32:25,800][flwr][INFO] - [ROUND 13]
[2024-08-13 23:32:25,800][flwr][INFO] - configure_fit: strategy sampled 10 clients (out of 20)
[2024-08-13 23:32:28,000][flwr][INFO] - aggregate_fit: received 10 results and 0 failures
[2024-08-13 23:32:31,425][flwr][INFO] - fit progress: (13, 8.08761078119278, {'accuracy': 0.6987179487179487}, 72.68524601499666)
[2024-08-13 23:32:31,425][flwr][INFO] - configure_evaluate: strategy sampled 2 clients (out of 20)
[2024-08-13 23:32:31,618][flwr][INFO] - aggregate_evaluate: received 2 results and 0 failures
[2024-08-13 23:32:31,618][flwr][INFO] - 
[2024-08-13 23:32:31,618][flwr][INFO] - [ROUND 14]
[2024-08-13 23:32:31,618][flwr][INFO] - configure_fit: strategy sampled 10 clients (out of 20)
[2024-08-13 23:32:33,631][flwr][INFO] - aggregate_fit: received 10 results and 0 failures
[2024-08-13 23:32:36,455][flwr][INFO] - fit progress: (14, 8.2101571559906, {'accuracy': 0.6939102564102564}, 77.71580062501016)
[2024-08-13 23:32:36,456][flwr][INFO] - configure_evaluate: strategy sampled 2 clients (out of 20)
[2024-08-13 23:32:36,636][flwr][INFO] - aggregate_evaluate: received 2 results and 0 failures
[2024-08-13 23:32:36,636][flwr][INFO] - 
[2024-08-13 23:32:36,636][flwr][INFO] - [ROUND 15]
[2024-08-13 23:32:36,636][flwr][INFO] - configure_fit: strategy sampled 10 clients (out of 20)
[2024-08-13 23:32:38,731][flwr][INFO] - aggregate_fit: received 10 results and 0 failures
[2024-08-13 23:32:41,711][flwr][INFO] - fit progress: (15, 8.410900950431824, {'accuracy': 0.6939102564102564}, 82.97107813699404)
[2024-08-13 23:32:41,711][flwr][INFO] - configure_evaluate: strategy sampled 2 clients (out of 20)
[2024-08-13 23:32:41,897][flwr][INFO] - aggregate_evaluate: received 2 results and 0 failures
[2024-08-13 23:32:41,897][flwr][INFO] - 
[2024-08-13 23:32:41,897][flwr][INFO] - [ROUND 16]
[2024-08-13 23:32:41,897][flwr][INFO] - configure_fit: strategy sampled 10 clients (out of 20)
[2024-08-13 23:32:43,953][flwr][INFO] - aggregate_fit: received 10 results and 0 failures
[2024-08-13 23:32:46,880][flwr][INFO] - fit progress: (16, 8.25842598080635, {'accuracy': 0.717948717948718}, 88.14055047399597)
[2024-08-13 23:32:46,880][flwr][INFO] - configure_evaluate: strategy sampled 2 clients (out of 20)
[2024-08-13 23:32:47,071][flwr][INFO] - aggregate_evaluate: received 2 results and 0 failures
[2024-08-13 23:32:47,072][flwr][INFO] - 
[2024-08-13 23:32:47,072][flwr][INFO] - [ROUND 17]
[2024-08-13 23:32:47,072][flwr][INFO] - configure_fit: strategy sampled 10 clients (out of 20)
[2024-08-13 23:32:49,140][flwr][INFO] - aggregate_fit: received 10 results and 0 failures
[2024-08-13 23:32:52,731][flwr][INFO] - fit progress: (17, 8.25343781709671, {'accuracy': 0.7227564102564102}, 93.99122331800754)
[2024-08-13 23:32:52,731][flwr][INFO] - configure_evaluate: strategy sampled 2 clients (out of 20)
[2024-08-13 23:32:52,988][flwr][INFO] - aggregate_evaluate: received 2 results and 0 failures
[2024-08-13 23:32:52,989][flwr][INFO] - 
[2024-08-13 23:32:52,989][flwr][INFO] - [ROUND 18]
[2024-08-13 23:32:52,989][flwr][INFO] - configure_fit: strategy sampled 10 clients (out of 20)
[2024-08-13 23:32:55,355][flwr][INFO] - aggregate_fit: received 10 results and 0 failures
[2024-08-13 23:32:58,901][flwr][INFO] - fit progress: (18, 7.99783393740654, {'accuracy': 0.7307692307692307}, 100.16143863799516)
[2024-08-13 23:32:58,901][flwr][INFO] - configure_evaluate: strategy sampled 2 clients (out of 20)
[2024-08-13 23:32:59,312][flwr][INFO] - aggregate_evaluate: received 2 results and 0 failures
[2024-08-13 23:32:59,312][flwr][INFO] - 
[2024-08-13 23:32:59,312][flwr][INFO] - [ROUND 19]
[2024-08-13 23:32:59,312][flwr][INFO] - configure_fit: strategy sampled 10 clients (out of 20)
[2024-08-13 23:33:01,392][flwr][INFO] - aggregate_fit: received 10 results and 0 failures
[2024-08-13 23:33:04,902][flwr][INFO] - fit progress: (19, 7.720948308706284, {'accuracy': 0.7323717948717948}, 106.16183838801226)
[2024-08-13 23:33:04,902][flwr][INFO] - configure_evaluate: strategy sampled 2 clients (out of 20)
[2024-08-13 23:33:05,091][flwr][INFO] - aggregate_evaluate: received 2 results and 0 failures
[2024-08-13 23:33:05,091][flwr][INFO] - 
[2024-08-13 23:33:05,091][flwr][INFO] - [ROUND 20]
[2024-08-13 23:33:05,091][flwr][INFO] - configure_fit: strategy sampled 10 clients (out of 20)
[2024-08-13 23:33:07,057][flwr][INFO] - aggregate_fit: received 10 results and 0 failures
[2024-08-13 23:33:09,825][flwr][INFO] - fit progress: (20, 8.009926557540894, {'accuracy': 0.7291666666666666}, 111.0857261630008)
[2024-08-13 23:33:09,826][flwr][INFO] - configure_evaluate: strategy sampled 2 clients (out of 20)
[2024-08-13 23:33:10,275][flwr][INFO] - aggregate_evaluate: received 2 results and 0 failures
[2024-08-13 23:33:10,275][flwr][INFO] - 
[2024-08-13 23:33:10,275][flwr][INFO] - [ROUND 21]
[2024-08-13 23:33:10,275][flwr][INFO] - configure_fit: strategy sampled 10 clients (out of 20)
[2024-08-13 23:33:12,392][flwr][INFO] - aggregate_fit: received 10 results and 0 failures
[2024-08-13 23:33:15,144][flwr][INFO] - fit progress: (21, 7.450685560703278, {'accuracy': 0.7516025641025641}, 116.40405782801099)
[2024-08-13 23:33:15,144][flwr][INFO] - configure_evaluate: strategy sampled 2 clients (out of 20)
[2024-08-13 23:33:15,328][flwr][INFO] - aggregate_evaluate: received 2 results and 0 failures
[2024-08-13 23:33:15,328][flwr][INFO] - 
[2024-08-13 23:33:15,328][flwr][INFO] - [ROUND 22]
[2024-08-13 23:33:15,328][flwr][INFO] - configure_fit: strategy sampled 10 clients (out of 20)
[2024-08-13 23:33:17,411][flwr][INFO] - aggregate_fit: received 10 results and 0 failures
[2024-08-13 23:33:20,182][flwr][INFO] - fit progress: (22, 7.0824103355407715, {'accuracy': 0.7660256410256411}, 121.4420510790078)
[2024-08-13 23:33:20,182][flwr][INFO] - configure_evaluate: strategy sampled 2 clients (out of 20)
[2024-08-13 23:33:20,379][flwr][INFO] - aggregate_evaluate: received 2 results and 0 failures
[2024-08-13 23:33:20,379][flwr][INFO] - 
[2024-08-13 23:33:20,379][flwr][INFO] - [ROUND 23]
[2024-08-13 23:33:20,379][flwr][INFO] - configure_fit: strategy sampled 10 clients (out of 20)
[2024-08-13 23:33:22,426][flwr][INFO] - aggregate_fit: received 10 results and 0 failures
[2024-08-13 23:33:25,659][flwr][INFO] - fit progress: (23, 8.391162514686584, {'accuracy': 0.7163461538461539}, 126.91957303101663)
[2024-08-13 23:33:25,659][flwr][INFO] - configure_evaluate: strategy sampled 2 clients (out of 20)
[2024-08-13 23:33:25,859][flwr][INFO] - aggregate_evaluate: received 2 results and 0 failures
[2024-08-13 23:33:25,859][flwr][INFO] - 
[2024-08-13 23:33:25,859][flwr][INFO] - [ROUND 24]
[2024-08-13 23:33:25,859][flwr][INFO] - configure_fit: strategy sampled 10 clients (out of 20)
[2024-08-13 23:33:27,954][flwr][INFO] - aggregate_fit: received 10 results and 0 failures
[2024-08-13 23:33:31,003][flwr][INFO] - fit progress: (24, 8.327827394008636, {'accuracy': 0.7163461538461539}, 132.26359686799697)
[2024-08-13 23:33:31,003][flwr][INFO] - configure_evaluate: strategy sampled 2 clients (out of 20)
[2024-08-13 23:33:31,369][flwr][INFO] - aggregate_evaluate: received 2 results and 0 failures
[2024-08-13 23:33:31,369][flwr][INFO] - 
[2024-08-13 23:33:31,369][flwr][INFO] - [ROUND 25]
[2024-08-13 23:33:31,369][flwr][INFO] - configure_fit: strategy sampled 10 clients (out of 20)
[2024-08-13 23:33:33,393][flwr][INFO] - aggregate_fit: received 10 results and 0 failures
[2024-08-13 23:33:36,163][flwr][INFO] - fit progress: (25, 9.101787090301514, {'accuracy': 0.6907051282051282}, 137.42302955000196)
[2024-08-13 23:33:36,163][flwr][INFO] - configure_evaluate: strategy sampled 2 clients (out of 20)
[2024-08-13 23:33:36,376][flwr][INFO] - aggregate_evaluate: received 2 results and 0 failures
[2024-08-13 23:33:36,376][flwr][INFO] - 
[2024-08-13 23:33:36,376][flwr][INFO] - [ROUND 26]
[2024-08-13 23:33:36,376][flwr][INFO] - configure_fit: strategy sampled 10 clients (out of 20)
[2024-08-13 23:33:38,387][flwr][INFO] - aggregate_fit: received 10 results and 0 failures
[2024-08-13 23:33:41,234][flwr][INFO] - fit progress: (26, 9.314217507839203, {'accuracy': 0.6891025641025641}, 142.49463212501723)
[2024-08-13 23:33:41,234][flwr][INFO] - configure_evaluate: strategy sampled 2 clients (out of 20)
[2024-08-13 23:33:41,447][flwr][INFO] - aggregate_evaluate: received 2 results and 0 failures
[2024-08-13 23:33:41,447][flwr][INFO] - 
[2024-08-13 23:33:41,447][flwr][INFO] - [ROUND 27]
[2024-08-13 23:33:41,447][flwr][INFO] - configure_fit: strategy sampled 10 clients (out of 20)
[2024-08-13 23:33:43,543][flwr][INFO] - aggregate_fit: received 10 results and 0 failures
[2024-08-13 23:33:46,371][flwr][INFO] - fit progress: (27, 7.317306816577911, {'accuracy': 0.7548076923076923}, 147.63171397900442)
[2024-08-13 23:33:46,372][flwr][INFO] - configure_evaluate: strategy sampled 2 clients (out of 20)
[2024-08-13 23:33:46,620][flwr][INFO] - aggregate_evaluate: received 2 results and 0 failures
[2024-08-13 23:33:46,620][flwr][INFO] - 
[2024-08-13 23:33:46,620][flwr][INFO] - [ROUND 28]
[2024-08-13 23:33:46,620][flwr][INFO] - configure_fit: strategy sampled 10 clients (out of 20)
[2024-08-13 23:33:48,655][flwr][INFO] - aggregate_fit: received 10 results and 0 failures
[2024-08-13 23:33:51,820][flwr][INFO] - fit progress: (28, 7.154199838638306, {'accuracy': 0.7644230769230769}, 153.08017561101587)
[2024-08-13 23:33:51,820][flwr][INFO] - configure_evaluate: strategy sampled 2 clients (out of 20)
[2024-08-13 23:33:52,012][flwr][INFO] - aggregate_evaluate: received 2 results and 0 failures
[2024-08-13 23:33:52,012][flwr][INFO] - 
[2024-08-13 23:33:52,012][flwr][INFO] - [ROUND 29]
[2024-08-13 23:33:52,012][flwr][INFO] - configure_fit: strategy sampled 10 clients (out of 20)
[2024-08-13 23:33:54,071][flwr][INFO] - aggregate_fit: received 10 results and 0 failures
[2024-08-13 23:33:56,915][flwr][INFO] - fit progress: (29, 7.988920152187347, {'accuracy': 0.7548076923076923}, 158.17551875900244)
[2024-08-13 23:33:56,915][flwr][INFO] - configure_evaluate: strategy sampled 2 clients (out of 20)
[2024-08-13 23:33:57,107][flwr][INFO] - aggregate_evaluate: received 2 results and 0 failures
[2024-08-13 23:33:57,107][flwr][INFO] - 
[2024-08-13 23:33:57,107][flwr][INFO] - [ROUND 30]
[2024-08-13 23:33:57,107][flwr][INFO] - configure_fit: strategy sampled 10 clients (out of 20)
[2024-08-13 23:33:59,105][flwr][INFO] - aggregate_fit: received 10 results and 0 failures
[2024-08-13 23:34:01,866][flwr][INFO] - fit progress: (30, 6.641378402709961, {'accuracy': 0.7852564102564102}, 163.12660392199177)
[2024-08-13 23:34:01,866][flwr][INFO] - configure_evaluate: strategy sampled 2 clients (out of 20)
[2024-08-13 23:34:02,049][flwr][INFO] - aggregate_evaluate: received 2 results and 0 failures
[2024-08-13 23:34:02,049][flwr][INFO] - 
[2024-08-13 23:34:02,049][flwr][INFO] - [ROUND 31]
[2024-08-13 23:34:02,049][flwr][INFO] - configure_fit: strategy sampled 10 clients (out of 20)
[2024-08-13 23:34:04,083][flwr][INFO] - aggregate_fit: received 10 results and 0 failures
[2024-08-13 23:34:06,866][flwr][INFO] - fit progress: (31, 7.837092816829681, {'accuracy': 0.7548076923076923}, 168.1262189699919)
[2024-08-13 23:34:06,866][flwr][INFO] - configure_evaluate: strategy sampled 2 clients (out of 20)
[2024-08-13 23:34:07,070][flwr][INFO] - aggregate_evaluate: received 2 results and 0 failures
[2024-08-13 23:34:07,070][flwr][INFO] - 
[2024-08-13 23:34:07,070][flwr][INFO] - [ROUND 32]
[2024-08-13 23:34:07,070][flwr][INFO] - configure_fit: strategy sampled 10 clients (out of 20)
[2024-08-13 23:34:09,081][flwr][INFO] - aggregate_fit: received 10 results and 0 failures
[2024-08-13 23:34:12,271][flwr][INFO] - fit progress: (32, 7.577792108058929, {'accuracy': 0.7596153846153846}, 173.5315852620115)
[2024-08-13 23:34:12,271][flwr][INFO] - configure_evaluate: strategy sampled 2 clients (out of 20)
[2024-08-13 23:34:12,469][flwr][INFO] - aggregate_evaluate: received 2 results and 0 failures
[2024-08-13 23:34:12,469][flwr][INFO] - 
[2024-08-13 23:34:12,469][flwr][INFO] - [ROUND 33]
[2024-08-13 23:34:12,469][flwr][INFO] - configure_fit: strategy sampled 10 clients (out of 20)
[2024-08-13 23:34:14,571][flwr][INFO] - aggregate_fit: received 10 results and 0 failures
[2024-08-13 23:34:17,760][flwr][INFO] - fit progress: (33, 7.5399448573589325, {'accuracy': 0.7692307692307693}, 179.02014923200477)
[2024-08-13 23:34:17,760][flwr][INFO] - configure_evaluate: strategy sampled 2 clients (out of 20)
[2024-08-13 23:34:17,957][flwr][INFO] - aggregate_evaluate: received 2 results and 0 failures
[2024-08-13 23:34:17,957][flwr][INFO] - 
[2024-08-13 23:34:17,957][flwr][INFO] - [ROUND 34]
[2024-08-13 23:34:17,957][flwr][INFO] - configure_fit: strategy sampled 10 clients (out of 20)
[2024-08-13 23:34:19,890][flwr][INFO] - aggregate_fit: received 10 results and 0 failures
[2024-08-13 23:34:23,118][flwr][INFO] - fit progress: (34, 6.864511668682098, {'accuracy': 0.7852564102564102}, 184.37872879800852)
[2024-08-13 23:34:23,119][flwr][INFO] - configure_evaluate: strategy sampled 2 clients (out of 20)
[2024-08-13 23:34:23,312][flwr][INFO] - aggregate_evaluate: received 2 results and 0 failures
[2024-08-13 23:34:23,313][flwr][INFO] - 
[2024-08-13 23:34:23,313][flwr][INFO] - [ROUND 35]
[2024-08-13 23:34:23,313][flwr][INFO] - configure_fit: strategy sampled 10 clients (out of 20)
[2024-08-13 23:34:25,412][flwr][INFO] - aggregate_fit: received 10 results and 0 failures
[2024-08-13 23:34:28,174][flwr][INFO] - fit progress: (35, 7.769441306591034, {'accuracy': 0.7516025641025641}, 189.43431522799074)
[2024-08-13 23:34:28,174][flwr][INFO] - configure_evaluate: strategy sampled 2 clients (out of 20)
[2024-08-13 23:34:28,370][flwr][INFO] - aggregate_evaluate: received 2 results and 0 failures
[2024-08-13 23:34:28,370][flwr][INFO] - 
[2024-08-13 23:34:28,371][flwr][INFO] - [ROUND 36]
[2024-08-13 23:34:28,371][flwr][INFO] - configure_fit: strategy sampled 10 clients (out of 20)
[2024-08-13 23:34:30,329][flwr][INFO] - aggregate_fit: received 10 results and 0 failures
[2024-08-13 23:34:33,309][flwr][INFO] - fit progress: (36, 7.15459805727005, {'accuracy': 0.7724358974358975}, 194.5690524810052)
[2024-08-13 23:34:33,309][flwr][INFO] - configure_evaluate: strategy sampled 2 clients (out of 20)
[2024-08-13 23:34:33,474][flwr][INFO] - aggregate_evaluate: received 2 results and 0 failures
[2024-08-13 23:34:33,474][flwr][INFO] - 
[2024-08-13 23:34:33,474][flwr][INFO] - [ROUND 37]
[2024-08-13 23:34:33,474][flwr][INFO] - configure_fit: strategy sampled 10 clients (out of 20)
[2024-08-13 23:34:35,501][flwr][INFO] - aggregate_fit: received 10 results and 0 failures
[2024-08-13 23:34:38,505][flwr][INFO] - fit progress: (37, 7.318424940109253, {'accuracy': 0.7660256410256411}, 199.7649496209924)
[2024-08-13 23:34:38,505][flwr][INFO] - configure_evaluate: strategy sampled 2 clients (out of 20)
[2024-08-13 23:34:38,695][flwr][INFO] - aggregate_evaluate: received 2 results and 0 failures
[2024-08-13 23:34:38,695][flwr][INFO] - 
[2024-08-13 23:34:38,695][flwr][INFO] - [ROUND 38]
[2024-08-13 23:34:38,695][flwr][INFO] - configure_fit: strategy sampled 10 clients (out of 20)
[2024-08-13 23:34:40,795][flwr][INFO] - aggregate_fit: received 10 results and 0 failures
[2024-08-13 23:34:43,923][flwr][INFO] - fit progress: (38, 8.092836081981659, {'accuracy': 0.7483974358974359}, 205.18341894401237)
[2024-08-13 23:34:43,923][flwr][INFO] - configure_evaluate: strategy sampled 2 clients (out of 20)
[2024-08-13 23:34:44,127][flwr][INFO] - aggregate_evaluate: received 2 results and 0 failures
[2024-08-13 23:34:44,127][flwr][INFO] - 
[2024-08-13 23:34:44,127][flwr][INFO] - [ROUND 39]
[2024-08-13 23:34:44,127][flwr][INFO] - configure_fit: strategy sampled 10 clients (out of 20)
[2024-08-13 23:34:46,083][flwr][INFO] - aggregate_fit: received 10 results and 0 failures
[2024-08-13 23:34:48,825][flwr][INFO] - fit progress: (39, 7.984121888875961, {'accuracy': 0.7548076923076923}, 210.08563808401232)
[2024-08-13 23:34:48,825][flwr][INFO] - configure_evaluate: strategy sampled 2 clients (out of 20)
[2024-08-13 23:34:49,001][flwr][INFO] - aggregate_evaluate: received 2 results and 0 failures
[2024-08-13 23:34:49,001][flwr][INFO] - 
[2024-08-13 23:34:49,001][flwr][INFO] - [ROUND 40]
[2024-08-13 23:34:49,001][flwr][INFO] - configure_fit: strategy sampled 10 clients (out of 20)
[2024-08-13 23:34:50,789][flwr][INFO] - aggregate_fit: received 10 results and 0 failures
[2024-08-13 23:34:53,852][flwr][INFO] - fit progress: (40, 7.999679803848267, {'accuracy': 0.7419871794871795}, 215.11269605500274)
[2024-08-13 23:34:53,853][flwr][INFO] - configure_evaluate: strategy sampled 2 clients (out of 20)
[2024-08-13 23:34:54,057][flwr][INFO] - aggregate_evaluate: received 2 results and 0 failures
[2024-08-13 23:34:54,057][flwr][INFO] - 
[2024-08-13 23:34:54,057][flwr][INFO] - [ROUND 41]
[2024-08-13 23:34:54,057][flwr][INFO] - configure_fit: strategy sampled 10 clients (out of 20)
[2024-08-13 23:34:56,076][flwr][INFO] - aggregate_fit: received 10 results and 0 failures
[2024-08-13 23:34:58,886][flwr][INFO] - fit progress: (41, 8.23989251255989, {'accuracy': 0.75}, 220.14640432200395)
[2024-08-13 23:34:58,886][flwr][INFO] - configure_evaluate: strategy sampled 2 clients (out of 20)
[2024-08-13 23:34:59,088][flwr][INFO] - aggregate_evaluate: received 2 results and 0 failures
[2024-08-13 23:34:59,088][flwr][INFO] - 
[2024-08-13 23:34:59,088][flwr][INFO] - [ROUND 42]
[2024-08-13 23:34:59,088][flwr][INFO] - configure_fit: strategy sampled 10 clients (out of 20)
[2024-08-13 23:35:01,228][flwr][INFO] - aggregate_fit: received 10 results and 0 failures
[2024-08-13 23:35:04,007][flwr][INFO] - fit progress: (42, 8.654605686664581, {'accuracy': 0.7387820512820513}, 225.2671475649986)
[2024-08-13 23:35:04,007][flwr][INFO] - configure_evaluate: strategy sampled 2 clients (out of 20)
[2024-08-13 23:35:04,191][flwr][INFO] - aggregate_evaluate: received 2 results and 0 failures
[2024-08-13 23:35:04,192][flwr][INFO] - 
[2024-08-13 23:35:04,192][flwr][INFO] - [ROUND 43]
[2024-08-13 23:35:04,192][flwr][INFO] - configure_fit: strategy sampled 10 clients (out of 20)
[2024-08-13 23:35:06,258][flwr][INFO] - aggregate_fit: received 10 results and 0 failures
[2024-08-13 23:35:09,213][flwr][INFO] - fit progress: (43, 8.6677166223526, {'accuracy': 0.7371794871794872}, 230.4737984270032)
[2024-08-13 23:35:09,214][flwr][INFO] - configure_evaluate: strategy sampled 2 clients (out of 20)
[2024-08-13 23:35:09,408][flwr][INFO] - aggregate_evaluate: received 2 results and 0 failures
[2024-08-13 23:35:09,408][flwr][INFO] - 
[2024-08-13 23:35:09,408][flwr][INFO] - [ROUND 44]
[2024-08-13 23:35:09,408][flwr][INFO] - configure_fit: strategy sampled 10 clients (out of 20)
[2024-08-13 23:35:11,395][flwr][INFO] - aggregate_fit: received 10 results and 0 failures
[2024-08-13 23:35:14,696][flwr][INFO] - fit progress: (44, 9.082685381174088, {'accuracy': 0.7339743589743589}, 235.9558481470158)
[2024-08-13 23:35:14,696][flwr][INFO] - configure_evaluate: strategy sampled 2 clients (out of 20)
[2024-08-13 23:35:14,987][flwr][INFO] - aggregate_evaluate: received 2 results and 0 failures
[2024-08-13 23:35:14,987][flwr][INFO] - 
[2024-08-13 23:35:14,987][flwr][INFO] - [ROUND 45]
[2024-08-13 23:35:14,987][flwr][INFO] - configure_fit: strategy sampled 10 clients (out of 20)
[2024-08-13 23:35:17,034][flwr][INFO] - aggregate_fit: received 10 results and 0 failures
[2024-08-13 23:35:20,674][flwr][INFO] - fit progress: (45, 8.411547839641571, {'accuracy': 0.7435897435897436}, 241.93446683799266)
[2024-08-13 23:35:20,674][flwr][INFO] - configure_evaluate: strategy sampled 2 clients (out of 20)
[2024-08-13 23:35:20,872][flwr][INFO] - aggregate_evaluate: received 2 results and 0 failures
[2024-08-13 23:35:20,873][flwr][INFO] - 
[2024-08-13 23:35:20,873][flwr][INFO] - [ROUND 46]
[2024-08-13 23:35:20,873][flwr][INFO] - configure_fit: strategy sampled 10 clients (out of 20)
[2024-08-13 23:35:22,835][flwr][INFO] - aggregate_fit: received 10 results and 0 failures
[2024-08-13 23:35:25,767][flwr][INFO] - fit progress: (46, 8.86949211359024, {'accuracy': 0.7339743589743589}, 247.02769834001083)
[2024-08-13 23:35:25,768][flwr][INFO] - configure_evaluate: strategy sampled 2 clients (out of 20)
[2024-08-13 23:35:25,935][flwr][INFO] - aggregate_evaluate: received 2 results and 0 failures
[2024-08-13 23:35:25,935][flwr][INFO] - 
[2024-08-13 23:35:25,935][flwr][INFO] - [ROUND 47]
[2024-08-13 23:35:25,935][flwr][INFO] - configure_fit: strategy sampled 10 clients (out of 20)
[2024-08-13 23:35:27,954][flwr][INFO] - aggregate_fit: received 10 results and 0 failures
[2024-08-13 23:35:30,809][flwr][INFO] - fit progress: (47, 6.884771674871445, {'accuracy': 0.7772435897435898}, 252.06958764599403)
[2024-08-13 23:35:30,809][flwr][INFO] - configure_evaluate: strategy sampled 2 clients (out of 20)
[2024-08-13 23:35:31,006][flwr][INFO] - aggregate_evaluate: received 2 results and 0 failures
[2024-08-13 23:35:31,006][flwr][INFO] - 
[2024-08-13 23:35:31,006][flwr][INFO] - [ROUND 48]
[2024-08-13 23:35:31,006][flwr][INFO] - configure_fit: strategy sampled 10 clients (out of 20)
[2024-08-13 23:35:33,117][flwr][INFO] - aggregate_fit: received 10 results and 0 failures
[2024-08-13 23:35:36,001][flwr][INFO] - fit progress: (48, 8.354258209466934, {'accuracy': 0.7483974358974359}, 257.261334661016)
[2024-08-13 23:35:36,001][flwr][INFO] - configure_evaluate: strategy sampled 2 clients (out of 20)
[2024-08-13 23:35:36,255][flwr][INFO] - aggregate_evaluate: received 2 results and 0 failures
[2024-08-13 23:35:36,255][flwr][INFO] - 
[2024-08-13 23:35:36,255][flwr][INFO] - [ROUND 49]
[2024-08-13 23:35:36,255][flwr][INFO] - configure_fit: strategy sampled 10 clients (out of 20)
[2024-08-13 23:35:38,245][flwr][INFO] - aggregate_fit: received 10 results and 0 failures
[2024-08-13 23:35:41,037][flwr][INFO] - fit progress: (49, 8.20971566438675, {'accuracy': 0.7467948717948718}, 262.2968699109915)
[2024-08-13 23:35:41,037][flwr][INFO] - configure_evaluate: strategy sampled 2 clients (out of 20)
[2024-08-13 23:35:41,300][flwr][INFO] - aggregate_evaluate: received 2 results and 0 failures
[2024-08-13 23:35:41,300][flwr][INFO] - 
[2024-08-13 23:35:41,300][flwr][INFO] - [ROUND 50]
[2024-08-13 23:35:41,300][flwr][INFO] - configure_fit: strategy sampled 10 clients (out of 20)
[2024-08-13 23:35:43,342][flwr][INFO] - aggregate_fit: received 10 results and 0 failures
[2024-08-13 23:35:46,791][flwr][INFO] - fit progress: (50, 7.4712788462638855, {'accuracy': 0.7564102564102564}, 268.05117469801917)
[2024-08-13 23:35:46,791][flwr][INFO] - configure_evaluate: strategy sampled 2 clients (out of 20)
[2024-08-13 23:35:47,071][flwr][INFO] - aggregate_evaluate: received 2 results and 0 failures
[2024-08-13 23:35:47,071][flwr][INFO] - 
[2024-08-13 23:35:47,071][flwr][INFO] - [ROUND 51]
[2024-08-13 23:35:47,071][flwr][INFO] - configure_fit: strategy sampled 10 clients (out of 20)
[2024-08-13 23:35:49,028][flwr][INFO] - aggregate_fit: received 10 results and 0 failures
[2024-08-13 23:35:52,554][flwr][INFO] - fit progress: (51, 8.10887685418129, {'accuracy': 0.7532051282051282}, 273.8141773279931)
[2024-08-13 23:35:52,554][flwr][INFO] - configure_evaluate: strategy sampled 2 clients (out of 20)
[2024-08-13 23:35:52,726][flwr][INFO] - aggregate_evaluate: received 2 results and 0 failures
[2024-08-13 23:35:52,726][flwr][INFO] - 
[2024-08-13 23:35:52,726][flwr][INFO] - [ROUND 52]
[2024-08-13 23:35:52,726][flwr][INFO] - configure_fit: strategy sampled 10 clients (out of 20)
[2024-08-13 23:35:54,803][flwr][INFO] - aggregate_fit: received 10 results and 0 failures
[2024-08-13 23:35:57,647][flwr][INFO] - fit progress: (52, 9.068237960338593, {'accuracy': 0.7387820512820513}, 278.9075535220036)
[2024-08-13 23:35:57,647][flwr][INFO] - configure_evaluate: strategy sampled 2 clients (out of 20)
[2024-08-13 23:35:57,835][flwr][INFO] - aggregate_evaluate: received 2 results and 0 failures
[2024-08-13 23:35:57,835][flwr][INFO] - 
[2024-08-13 23:35:57,835][flwr][INFO] - [ROUND 53]
[2024-08-13 23:35:57,835][flwr][INFO] - configure_fit: strategy sampled 10 clients (out of 20)
[2024-08-13 23:35:59,831][flwr][INFO] - aggregate_fit: received 10 results and 0 failures
[2024-08-13 23:36:02,597][flwr][INFO] - fit progress: (53, 7.481266587972641, {'accuracy': 0.7644230769230769}, 283.8571191189985)
[2024-08-13 23:36:02,597][flwr][INFO] - configure_evaluate: strategy sampled 2 clients (out of 20)
[2024-08-13 23:36:02,847][flwr][INFO] - aggregate_evaluate: received 2 results and 0 failures
[2024-08-13 23:36:02,847][flwr][INFO] - 
[2024-08-13 23:36:02,848][flwr][INFO] - [ROUND 54]
[2024-08-13 23:36:02,848][flwr][INFO] - configure_fit: strategy sampled 10 clients (out of 20)
[2024-08-13 23:36:04,987][flwr][INFO] - aggregate_fit: received 10 results and 0 failures
[2024-08-13 23:36:07,741][flwr][INFO] - fit progress: (54, 6.496564358472824, {'accuracy': 0.7996794871794872}, 289.00096472000587)
[2024-08-13 23:36:07,741][flwr][INFO] - configure_evaluate: strategy sampled 2 clients (out of 20)
[2024-08-13 23:36:07,978][flwr][INFO] - aggregate_evaluate: received 2 results and 0 failures
[2024-08-13 23:36:07,978][flwr][INFO] - 
[2024-08-13 23:36:07,978][flwr][INFO] - [ROUND 55]
[2024-08-13 23:36:07,978][flwr][INFO] - configure_fit: strategy sampled 10 clients (out of 20)
[2024-08-13 23:36:09,984][flwr][INFO] - aggregate_fit: received 10 results and 0 failures
[2024-08-13 23:36:13,570][flwr][INFO] - fit progress: (55, 8.426654160022736, {'accuracy': 0.7451923076923077}, 294.83029537301627)
[2024-08-13 23:36:13,570][flwr][INFO] - configure_evaluate: strategy sampled 2 clients (out of 20)
[2024-08-13 23:36:13,822][flwr][INFO] - aggregate_evaluate: received 2 results and 0 failures
[2024-08-13 23:36:13,822][flwr][INFO] - 
[2024-08-13 23:36:13,822][flwr][INFO] - [ROUND 56]
[2024-08-13 23:36:13,822][flwr][INFO] - configure_fit: strategy sampled 10 clients (out of 20)
[2024-08-13 23:36:15,798][flwr][INFO] - aggregate_fit: received 10 results and 0 failures
[2024-08-13 23:36:18,597][flwr][INFO] - fit progress: (56, 7.340573012828827, {'accuracy': 0.7740384615384616}, 299.85683567801607)
[2024-08-13 23:36:18,597][flwr][INFO] - configure_evaluate: strategy sampled 2 clients (out of 20)
[2024-08-13 23:36:18,820][flwr][INFO] - aggregate_evaluate: received 2 results and 0 failures
[2024-08-13 23:36:18,820][flwr][INFO] - 
[2024-08-13 23:36:18,820][flwr][INFO] - [ROUND 57]
[2024-08-13 23:36:18,820][flwr][INFO] - configure_fit: strategy sampled 10 clients (out of 20)
[2024-08-13 23:36:21,006][flwr][INFO] - aggregate_fit: received 10 results and 0 failures
[2024-08-13 23:36:23,800][flwr][INFO] - fit progress: (57, 10.547868013381958, {'accuracy': 0.7211538461538461}, 305.0607991089928)
[2024-08-13 23:36:23,801][flwr][INFO] - configure_evaluate: strategy sampled 2 clients (out of 20)
[2024-08-13 23:36:24,002][flwr][INFO] - aggregate_evaluate: received 2 results and 0 failures
[2024-08-13 23:36:24,002][flwr][INFO] - 
[2024-08-13 23:36:24,002][flwr][INFO] - [ROUND 58]
[2024-08-13 23:36:24,003][flwr][INFO] - configure_fit: strategy sampled 10 clients (out of 20)
[2024-08-13 23:36:25,915][flwr][INFO] - aggregate_fit: received 10 results and 0 failures
[2024-08-13 23:36:28,732][flwr][INFO] - fit progress: (58, 8.179313838481903, {'accuracy': 0.7596153846153846}, 309.99198069499107)
[2024-08-13 23:36:28,732][flwr][INFO] - configure_evaluate: strategy sampled 2 clients (out of 20)
[2024-08-13 23:36:28,920][flwr][INFO] - aggregate_evaluate: received 2 results and 0 failures
[2024-08-13 23:36:28,920][flwr][INFO] - 
[2024-08-13 23:36:28,920][flwr][INFO] - [ROUND 59]
[2024-08-13 23:36:28,920][flwr][INFO] - configure_fit: strategy sampled 10 clients (out of 20)
[2024-08-13 23:36:31,009][flwr][INFO] - aggregate_fit: received 10 results and 0 failures
[2024-08-13 23:36:33,864][flwr][INFO] - fit progress: (59, 9.582875967025757, {'accuracy': 0.7387820512820513}, 315.1247351100028)
[2024-08-13 23:36:33,865][flwr][INFO] - configure_evaluate: strategy sampled 2 clients (out of 20)
[2024-08-13 23:36:34,071][flwr][INFO] - aggregate_evaluate: received 2 results and 0 failures
[2024-08-13 23:36:34,071][flwr][INFO] - 
[2024-08-13 23:36:34,071][flwr][INFO] - [ROUND 60]
[2024-08-13 23:36:34,071][flwr][INFO] - configure_fit: strategy sampled 10 clients (out of 20)
[2024-08-13 23:36:36,105][flwr][INFO] - aggregate_fit: received 10 results and 0 failures
[2024-08-13 23:36:38,865][flwr][INFO] - fit progress: (60, 8.891036629676819, {'accuracy': 0.7467948717948718}, 320.12487734199385)
[2024-08-13 23:36:38,865][flwr][INFO] - configure_evaluate: strategy sampled 2 clients (out of 20)
[2024-08-13 23:36:39,053][flwr][INFO] - aggregate_evaluate: received 2 results and 0 failures
[2024-08-13 23:36:39,053][flwr][INFO] - 
[2024-08-13 23:36:39,053][flwr][INFO] - [ROUND 61]
[2024-08-13 23:36:39,053][flwr][INFO] - configure_fit: strategy sampled 10 clients (out of 20)
[2024-08-13 23:36:41,090][flwr][INFO] - aggregate_fit: received 10 results and 0 failures
[2024-08-13 23:36:43,844][flwr][INFO] - fit progress: (61, 9.029747992753983, {'accuracy': 0.7483974358974359}, 325.1044507230108)
[2024-08-13 23:36:43,844][flwr][INFO] - configure_evaluate: strategy sampled 2 clients (out of 20)
[2024-08-13 23:36:44,089][flwr][INFO] - aggregate_evaluate: received 2 results and 0 failures
[2024-08-13 23:36:44,089][flwr][INFO] - 
[2024-08-13 23:36:44,096][flwr][INFO] - [ROUND 62]
[2024-08-13 23:36:44,097][flwr][INFO] - configure_fit: strategy sampled 10 clients (out of 20)
[2024-08-13 23:36:46,199][flwr][INFO] - aggregate_fit: received 10 results and 0 failures
[2024-08-13 23:36:49,014][flwr][INFO] - fit progress: (62, 9.733731865882874, {'accuracy': 0.7403846153846154}, 330.273982632003)
[2024-08-13 23:36:49,014][flwr][INFO] - configure_evaluate: strategy sampled 2 clients (out of 20)
[2024-08-13 23:36:49,213][flwr][INFO] - aggregate_evaluate: received 2 results and 0 failures
[2024-08-13 23:36:49,213][flwr][INFO] - 
[2024-08-13 23:36:49,213][flwr][INFO] - [ROUND 63]
[2024-08-13 23:36:49,213][flwr][INFO] - configure_fit: strategy sampled 10 clients (out of 20)
[2024-08-13 23:36:51,238][flwr][INFO] - aggregate_fit: received 10 results and 0 failures
[2024-08-13 23:36:54,178][flwr][INFO] - fit progress: (63, 11.9857759475708, {'accuracy': 0.7115384615384616}, 335.4380243750056)
[2024-08-13 23:36:54,178][flwr][INFO] - configure_evaluate: strategy sampled 2 clients (out of 20)
[2024-08-13 23:36:54,383][flwr][INFO] - aggregate_evaluate: received 2 results and 0 failures
[2024-08-13 23:36:54,384][flwr][INFO] - 
[2024-08-13 23:36:54,384][flwr][INFO] - [ROUND 64]
[2024-08-13 23:36:54,384][flwr][INFO] - configure_fit: strategy sampled 10 clients (out of 20)
[2024-08-13 23:36:57,341][flwr][INFO] - aggregate_fit: received 10 results and 0 failures
[2024-08-13 23:37:00,558][flwr][INFO] - fit progress: (64, 7.99212521314621, {'accuracy': 0.7708333333333334}, 341.81797414200264)
[2024-08-13 23:37:00,558][flwr][INFO] - configure_evaluate: strategy sampled 2 clients (out of 20)
[2024-08-13 23:37:00,826][flwr][INFO] - aggregate_evaluate: received 2 results and 0 failures
[2024-08-13 23:37:00,826][flwr][INFO] - 
[2024-08-13 23:37:00,826][flwr][INFO] - [ROUND 65]
[2024-08-13 23:37:00,827][flwr][INFO] - configure_fit: strategy sampled 10 clients (out of 20)
[2024-08-13 23:37:02,844][flwr][INFO] - aggregate_fit: received 10 results and 0 failures
[2024-08-13 23:37:05,683][flwr][INFO] - fit progress: (65, 9.757590115070343, {'accuracy': 0.7419871794871795}, 346.9435978590045)
[2024-08-13 23:37:05,683][flwr][INFO] - configure_evaluate: strategy sampled 2 clients (out of 20)
[2024-08-13 23:37:05,871][flwr][INFO] - aggregate_evaluate: received 2 results and 0 failures
[2024-08-13 23:37:05,871][flwr][INFO] - 
[2024-08-13 23:37:05,871][flwr][INFO] - [ROUND 66]
[2024-08-13 23:37:05,871][flwr][INFO] - configure_fit: strategy sampled 10 clients (out of 20)
[2024-08-13 23:37:07,873][flwr][INFO] - aggregate_fit: received 10 results and 0 failures
[2024-08-13 23:37:10,662][flwr][INFO] - fit progress: (66, 9.11391830444336, {'accuracy': 0.7532051282051282}, 351.9227677920135)
[2024-08-13 23:37:10,663][flwr][INFO] - configure_evaluate: strategy sampled 2 clients (out of 20)
[2024-08-13 23:37:11,245][flwr][INFO] - aggregate_evaluate: received 2 results and 0 failures
[2024-08-13 23:37:11,245][flwr][INFO] - 
[2024-08-13 23:37:11,245][flwr][INFO] - [ROUND 67]
[2024-08-13 23:37:11,246][flwr][INFO] - configure_fit: strategy sampled 10 clients (out of 20)
[2024-08-13 23:37:13,269][flwr][INFO] - aggregate_fit: received 10 results and 0 failures
[2024-08-13 23:37:16,315][flwr][INFO] - fit progress: (67, 8.635681509971619, {'accuracy': 0.7660256410256411}, 357.57556300100987)
[2024-08-13 23:37:16,315][flwr][INFO] - configure_evaluate: strategy sampled 2 clients (out of 20)
[2024-08-13 23:37:16,516][flwr][INFO] - aggregate_evaluate: received 2 results and 0 failures
[2024-08-13 23:37:16,516][flwr][INFO] - 
[2024-08-13 23:37:16,516][flwr][INFO] - [ROUND 68]
[2024-08-13 23:37:16,516][flwr][INFO] - configure_fit: strategy sampled 10 clients (out of 20)
[2024-08-13 23:37:18,583][flwr][INFO] - aggregate_fit: received 10 results and 0 failures
[2024-08-13 23:37:21,331][flwr][INFO] - fit progress: (68, 9.25865113735199, {'accuracy': 0.7548076923076923}, 362.59165466800914)
[2024-08-13 23:37:21,331][flwr][INFO] - configure_evaluate: strategy sampled 2 clients (out of 20)
[2024-08-13 23:37:21,953][flwr][INFO] - aggregate_evaluate: received 2 results and 0 failures
[2024-08-13 23:37:21,954][flwr][INFO] - 
[2024-08-13 23:37:21,954][flwr][INFO] - [ROUND 69]
[2024-08-13 23:37:21,954][flwr][INFO] - configure_fit: strategy sampled 10 clients (out of 20)
[2024-08-13 23:37:24,007][flwr][INFO] - aggregate_fit: received 10 results and 0 failures
[2024-08-13 23:37:26,900][flwr][INFO] - fit progress: (69, 9.786803305149078, {'accuracy': 0.7467948717948718}, 368.1606649250025)
[2024-08-13 23:37:26,901][flwr][INFO] - configure_evaluate: strategy sampled 2 clients (out of 20)
[2024-08-13 23:37:27,144][flwr][INFO] - aggregate_evaluate: received 2 results and 0 failures
[2024-08-13 23:37:27,144][flwr][INFO] - 
[2024-08-13 23:37:27,144][flwr][INFO] - [ROUND 70]
[2024-08-13 23:37:27,144][flwr][INFO] - configure_fit: strategy sampled 10 clients (out of 20)
[2024-08-13 23:37:29,285][flwr][INFO] - aggregate_fit: received 10 results and 0 failures
[2024-08-13 23:37:32,101][flwr][INFO] - fit progress: (70, 7.450933784246445, {'accuracy': 0.7884615384615384}, 373.361552131013)
[2024-08-13 23:37:32,101][flwr][INFO] - configure_evaluate: strategy sampled 2 clients (out of 20)
[2024-08-13 23:37:32,375][flwr][INFO] - aggregate_evaluate: received 2 results and 0 failures
[2024-08-13 23:37:32,375][flwr][INFO] - 
[2024-08-13 23:37:32,375][flwr][INFO] - [ROUND 71]
[2024-08-13 23:37:32,376][flwr][INFO] - configure_fit: strategy sampled 10 clients (out of 20)
[2024-08-13 23:37:34,824][flwr][INFO] - aggregate_fit: received 10 results and 0 failures
[2024-08-13 23:37:37,786][flwr][INFO] - fit progress: (71, 10.31783139705658, {'accuracy': 0.7339743589743589}, 379.0462211430131)
[2024-08-13 23:37:37,786][flwr][INFO] - configure_evaluate: strategy sampled 2 clients (out of 20)
[2024-08-13 23:37:38,115][flwr][INFO] - aggregate_evaluate: received 2 results and 0 failures
[2024-08-13 23:37:38,115][flwr][INFO] - 
[2024-08-13 23:37:38,115][flwr][INFO] - [ROUND 72]
[2024-08-13 23:37:38,115][flwr][INFO] - configure_fit: strategy sampled 10 clients (out of 20)
[2024-08-13 23:37:42,120][flwr][INFO] - aggregate_fit: received 10 results and 0 failures
[2024-08-13 23:37:44,924][flwr][INFO] - fit progress: (72, 8.74988842010498, {'accuracy': 0.7724358974358975}, 386.18437373501365)
[2024-08-13 23:37:44,924][flwr][INFO] - configure_evaluate: strategy sampled 2 clients (out of 20)
[2024-08-13 23:37:45,164][flwr][INFO] - aggregate_evaluate: received 2 results and 0 failures
[2024-08-13 23:37:45,164][flwr][INFO] - 
[2024-08-13 23:37:45,164][flwr][INFO] - [ROUND 73]
[2024-08-13 23:37:45,164][flwr][INFO] - configure_fit: strategy sampled 10 clients (out of 20)
[2024-08-13 23:37:48,521][flwr][INFO] - aggregate_fit: received 10 results and 0 failures
[2024-08-13 23:37:51,580][flwr][INFO] - fit progress: (73, 8.484612077474594, {'accuracy': 0.7772435897435898}, 392.84072754200315)
[2024-08-13 23:37:51,581][flwr][INFO] - configure_evaluate: strategy sampled 2 clients (out of 20)
[2024-08-13 23:37:51,836][flwr][INFO] - aggregate_evaluate: received 2 results and 0 failures
[2024-08-13 23:37:51,836][flwr][INFO] - 
[2024-08-13 23:37:51,836][flwr][INFO] - [ROUND 74]
[2024-08-13 23:37:51,836][flwr][INFO] - configure_fit: strategy sampled 10 clients (out of 20)
[2024-08-13 23:37:53,835][flwr][INFO] - aggregate_fit: received 10 results and 0 failures
[2024-08-13 23:37:57,510][flwr][INFO] - fit progress: (74, 8.877012372016907, {'accuracy': 0.7676282051282052}, 398.7701137959957)
[2024-08-13 23:37:57,510][flwr][INFO] - configure_evaluate: strategy sampled 2 clients (out of 20)
[2024-08-13 23:37:57,932][flwr][INFO] - aggregate_evaluate: received 2 results and 0 failures
[2024-08-13 23:37:57,932][flwr][INFO] - 
[2024-08-13 23:37:57,932][flwr][INFO] - [ROUND 75]
[2024-08-13 23:37:57,932][flwr][INFO] - configure_fit: strategy sampled 10 clients (out of 20)
[2024-08-13 23:38:00,061][flwr][INFO] - aggregate_fit: received 10 results and 0 failures
[2024-08-13 23:38:03,719][flwr][INFO] - fit progress: (75, 9.352483212947845, {'accuracy': 0.7596153846153846}, 404.97891781499493)
[2024-08-13 23:38:03,719][flwr][INFO] - configure_evaluate: strategy sampled 2 clients (out of 20)
[2024-08-13 23:38:03,997][flwr][INFO] - aggregate_evaluate: received 2 results and 0 failures
[2024-08-13 23:38:03,997][flwr][INFO] - 
[2024-08-13 23:38:03,997][flwr][INFO] - [ROUND 76]
[2024-08-13 23:38:03,997][flwr][INFO] - configure_fit: strategy sampled 10 clients (out of 20)
[2024-08-13 23:38:07,122][flwr][INFO] - aggregate_fit: received 10 results and 0 failures
[2024-08-13 23:38:09,959][flwr][INFO] - fit progress: (76, 11.044700860977173, {'accuracy': 0.7339743589743589}, 411.219408984005)
[2024-08-13 23:38:09,959][flwr][INFO] - configure_evaluate: strategy sampled 2 clients (out of 20)
[2024-08-13 23:38:10,340][flwr][INFO] - aggregate_evaluate: received 2 results and 0 failures
[2024-08-13 23:38:10,340][flwr][INFO] - 
[2024-08-13 23:38:10,340][flwr][INFO] - [ROUND 77]
[2024-08-13 23:38:10,340][flwr][INFO] - configure_fit: strategy sampled 10 clients (out of 20)
[2024-08-13 23:38:12,367][flwr][INFO] - aggregate_fit: received 10 results and 0 failures
[2024-08-13 23:38:15,135][flwr][INFO] - fit progress: (77, 10.226601749658585, {'accuracy': 0.7419871794871795}, 416.39524144801544)
[2024-08-13 23:38:15,135][flwr][INFO] - configure_evaluate: strategy sampled 2 clients (out of 20)
[2024-08-13 23:38:15,396][flwr][INFO] - aggregate_evaluate: received 2 results and 0 failures
[2024-08-13 23:38:15,396][flwr][INFO] - 
[2024-08-13 23:38:15,396][flwr][INFO] - [ROUND 78]
[2024-08-13 23:38:15,396][flwr][INFO] - configure_fit: strategy sampled 10 clients (out of 20)
[2024-08-13 23:38:17,486][flwr][INFO] - aggregate_fit: received 10 results and 0 failures
[2024-08-13 23:38:20,738][flwr][INFO] - fit progress: (78, 9.102142959833145, {'accuracy': 0.7644230769230769}, 421.9984894549998)
[2024-08-13 23:38:20,738][flwr][INFO] - configure_evaluate: strategy sampled 2 clients (out of 20)
[2024-08-13 23:38:21,017][flwr][INFO] - aggregate_evaluate: received 2 results and 0 failures
[2024-08-13 23:38:21,017][flwr][INFO] - 
[2024-08-13 23:38:21,017][flwr][INFO] - [ROUND 79]
[2024-08-13 23:38:21,017][flwr][INFO] - configure_fit: strategy sampled 10 clients (out of 20)
[2024-08-13 23:38:23,031][flwr][INFO] - aggregate_fit: received 10 results and 0 failures
[2024-08-13 23:38:26,697][flwr][INFO] - fit progress: (79, 8.743759661912918, {'accuracy': 0.7708333333333334}, 427.9568429850042)
[2024-08-13 23:38:26,697][flwr][INFO] - configure_evaluate: strategy sampled 2 clients (out of 20)
[2024-08-13 23:38:26,891][flwr][INFO] - aggregate_evaluate: received 2 results and 0 failures
[2024-08-13 23:38:26,892][flwr][INFO] - 
[2024-08-13 23:38:26,892][flwr][INFO] - [ROUND 80]
[2024-08-13 23:38:26,892][flwr][INFO] - configure_fit: strategy sampled 10 clients (out of 20)
[2024-08-13 23:38:28,928][flwr][INFO] - aggregate_fit: received 10 results and 0 failures
[2024-08-13 23:38:32,572][flwr][INFO] - fit progress: (80, 9.895357191562653, {'accuracy': 0.7532051282051282}, 433.83231292199343)
[2024-08-13 23:38:32,572][flwr][INFO] - configure_evaluate: strategy sampled 2 clients (out of 20)
[2024-08-13 23:38:32,820][flwr][INFO] - aggregate_evaluate: received 2 results and 0 failures
[2024-08-13 23:38:32,820][flwr][INFO] - 
[2024-08-13 23:38:32,820][flwr][INFO] - [ROUND 81]
[2024-08-13 23:38:32,820][flwr][INFO] - configure_fit: strategy sampled 10 clients (out of 20)
[2024-08-13 23:38:34,795][flwr][INFO] - aggregate_fit: received 10 results and 0 failures
[2024-08-13 23:38:37,746][flwr][INFO] - fit progress: (81, 8.551577806472778, {'accuracy': 0.7772435897435898}, 439.0065115620091)
[2024-08-13 23:38:37,746][flwr][INFO] - configure_evaluate: strategy sampled 2 clients (out of 20)
[2024-08-13 23:38:37,928][flwr][INFO] - aggregate_evaluate: received 2 results and 0 failures
[2024-08-13 23:38:37,928][flwr][INFO] - 
[2024-08-13 23:38:37,928][flwr][INFO] - [ROUND 82]
[2024-08-13 23:38:37,928][flwr][INFO] - configure_fit: strategy sampled 10 clients (out of 20)
[2024-08-13 23:38:39,880][flwr][INFO] - aggregate_fit: received 10 results and 0 failures
[2024-08-13 23:38:42,798][flwr][INFO] - fit progress: (82, 9.991007506847382, {'accuracy': 0.7564102564102564}, 444.0586797549913)
[2024-08-13 23:38:42,799][flwr][INFO] - configure_evaluate: strategy sampled 2 clients (out of 20)
[2024-08-13 23:38:43,022][flwr][INFO] - aggregate_evaluate: received 2 results and 0 failures
[2024-08-13 23:38:43,022][flwr][INFO] - 
[2024-08-13 23:38:43,022][flwr][INFO] - [ROUND 83]
[2024-08-13 23:38:43,022][flwr][INFO] - configure_fit: strategy sampled 10 clients (out of 20)
[2024-08-13 23:38:45,027][flwr][INFO] - aggregate_fit: received 10 results and 0 failures
[2024-08-13 23:38:47,965][flwr][INFO] - fit progress: (83, 9.394631445407867, {'accuracy': 0.7644230769230769}, 449.22509045599145)
[2024-08-13 23:38:47,965][flwr][INFO] - configure_evaluate: strategy sampled 2 clients (out of 20)
[2024-08-13 23:38:48,171][flwr][INFO] - aggregate_evaluate: received 2 results and 0 failures
[2024-08-13 23:38:48,172][flwr][INFO] - 
[2024-08-13 23:38:48,172][flwr][INFO] - [ROUND 84]
[2024-08-13 23:38:48,172][flwr][INFO] - configure_fit: strategy sampled 10 clients (out of 20)
[2024-08-13 23:38:50,233][flwr][INFO] - aggregate_fit: received 10 results and 0 failures
[2024-08-13 23:38:53,114][flwr][INFO] - fit progress: (84, 8.499400317668915, {'accuracy': 0.7772435897435898}, 454.3743971340009)
[2024-08-13 23:38:53,114][flwr][INFO] - configure_evaluate: strategy sampled 2 clients (out of 20)
[2024-08-13 23:38:53,416][flwr][INFO] - aggregate_evaluate: received 2 results and 0 failures
[2024-08-13 23:38:53,416][flwr][INFO] - 
[2024-08-13 23:38:53,416][flwr][INFO] - [ROUND 85]
[2024-08-13 23:38:53,416][flwr][INFO] - configure_fit: strategy sampled 10 clients (out of 20)
[2024-08-13 23:38:55,434][flwr][INFO] - aggregate_fit: received 10 results and 0 failures
[2024-08-13 23:38:58,320][flwr][INFO] - fit progress: (85, 12.11924946308136, {'accuracy': 0.719551282051282}, 459.5800343390147)
[2024-08-13 23:38:58,320][flwr][INFO] - configure_evaluate: strategy sampled 2 clients (out of 20)
[2024-08-13 23:38:58,508][flwr][INFO] - aggregate_evaluate: received 2 results and 0 failures
[2024-08-13 23:38:58,509][flwr][INFO] - 
[2024-08-13 23:38:58,509][flwr][INFO] - [ROUND 86]
[2024-08-13 23:38:58,509][flwr][INFO] - configure_fit: strategy sampled 10 clients (out of 20)
[2024-08-13 23:39:00,543][flwr][INFO] - aggregate_fit: received 10 results and 0 failures
[2024-08-13 23:39:03,375][flwr][INFO] - fit progress: (86, 11.065840125083923, {'accuracy': 0.7371794871794872}, 464.63579710401245)
[2024-08-13 23:39:03,376][flwr][INFO] - configure_evaluate: strategy sampled 2 clients (out of 20)
[2024-08-13 23:39:03,721][flwr][INFO] - aggregate_evaluate: received 2 results and 0 failures
[2024-08-13 23:39:03,721][flwr][INFO] - 
[2024-08-13 23:39:03,722][flwr][INFO] - [ROUND 87]
[2024-08-13 23:39:03,722][flwr][INFO] - configure_fit: strategy sampled 10 clients (out of 20)
[2024-08-13 23:39:05,664][flwr][INFO] - aggregate_fit: received 10 results and 0 failures
[2024-08-13 23:39:08,446][flwr][INFO] - fit progress: (87, 11.319145560264587, {'accuracy': 0.7323717948717948}, 469.70679601901793)
[2024-08-13 23:39:08,447][flwr][INFO] - configure_evaluate: strategy sampled 2 clients (out of 20)
[2024-08-13 23:39:08,688][flwr][INFO] - aggregate_evaluate: received 2 results and 0 failures
[2024-08-13 23:39:08,688][flwr][INFO] - 
[2024-08-13 23:39:08,688][flwr][INFO] - [ROUND 88]
[2024-08-13 23:39:08,688][flwr][INFO] - configure_fit: strategy sampled 10 clients (out of 20)
[2024-08-13 23:39:10,584][flwr][INFO] - aggregate_fit: received 10 results and 0 failures
[2024-08-13 23:39:13,353][flwr][INFO] - fit progress: (88, 10.332275986671448, {'accuracy': 0.7564102564102564}, 474.61350182199385)
[2024-08-13 23:39:13,353][flwr][INFO] - configure_evaluate: strategy sampled 2 clients (out of 20)
[2024-08-13 23:39:13,558][flwr][INFO] - aggregate_evaluate: received 2 results and 0 failures
[2024-08-13 23:39:13,558][flwr][INFO] - 
[2024-08-13 23:39:13,558][flwr][INFO] - [ROUND 89]
[2024-08-13 23:39:13,558][flwr][INFO] - configure_fit: strategy sampled 10 clients (out of 20)
[2024-08-13 23:39:15,606][flwr][INFO] - aggregate_fit: received 10 results and 0 failures
[2024-08-13 23:39:18,400][flwr][INFO] - fit progress: (89, 9.872732251882553, {'accuracy': 0.7628205128205128}, 479.6601136730169)
[2024-08-13 23:39:18,400][flwr][INFO] - configure_evaluate: strategy sampled 2 clients (out of 20)
[2024-08-13 23:39:18,595][flwr][INFO] - aggregate_evaluate: received 2 results and 0 failures
[2024-08-13 23:39:18,595][flwr][INFO] - 
[2024-08-13 23:39:18,595][flwr][INFO] - [ROUND 90]
[2024-08-13 23:39:18,595][flwr][INFO] - configure_fit: strategy sampled 10 clients (out of 20)
[2024-08-13 23:39:20,664][flwr][INFO] - aggregate_fit: received 10 results and 0 failures
[2024-08-13 23:39:23,909][flwr][INFO] - fit progress: (90, 10.989163994789124, {'accuracy': 0.75}, 485.16966655800934)
[2024-08-13 23:39:23,910][flwr][INFO] - configure_evaluate: strategy sampled 2 clients (out of 20)
[2024-08-13 23:39:24,135][flwr][INFO] - aggregate_evaluate: received 2 results and 0 failures
[2024-08-13 23:39:24,135][flwr][INFO] - 
[2024-08-13 23:39:24,135][flwr][INFO] - [ROUND 91]
[2024-08-13 23:39:24,135][flwr][INFO] - configure_fit: strategy sampled 10 clients (out of 20)
[2024-08-13 23:39:26,215][flwr][INFO] - aggregate_fit: received 10 results and 0 failures
[2024-08-13 23:39:29,058][flwr][INFO] - fit progress: (91, 11.886092394590378, {'accuracy': 0.7355769230769231}, 490.31834988400806)
[2024-08-13 23:39:29,058][flwr][INFO] - configure_evaluate: strategy sampled 2 clients (out of 20)
[2024-08-13 23:39:29,249][flwr][INFO] - aggregate_evaluate: received 2 results and 0 failures
[2024-08-13 23:39:29,249][flwr][INFO] - 
[2024-08-13 23:39:29,249][flwr][INFO] - [ROUND 92]
[2024-08-13 23:39:29,249][flwr][INFO] - configure_fit: strategy sampled 10 clients (out of 20)
[2024-08-13 23:39:31,287][flwr][INFO] - aggregate_fit: received 10 results and 0 failures
[2024-08-13 23:39:34,718][flwr][INFO] - fit progress: (92, 9.50468134880066, {'accuracy': 0.7724358974358975}, 495.9781403680099)
[2024-08-13 23:39:34,718][flwr][INFO] - configure_evaluate: strategy sampled 2 clients (out of 20)
[2024-08-13 23:39:34,919][flwr][INFO] - aggregate_evaluate: received 2 results and 0 failures
[2024-08-13 23:39:34,920][flwr][INFO] - 
[2024-08-13 23:39:34,920][flwr][INFO] - [ROUND 93]
[2024-08-13 23:39:34,920][flwr][INFO] - configure_fit: strategy sampled 10 clients (out of 20)
[2024-08-13 23:39:36,999][flwr][INFO] - aggregate_fit: received 10 results and 0 failures
[2024-08-13 23:39:40,014][flwr][INFO] - fit progress: (93, 9.81183522939682, {'accuracy': 0.7676282051282052}, 501.2747022790136)
[2024-08-13 23:39:40,015][flwr][INFO] - configure_evaluate: strategy sampled 2 clients (out of 20)
[2024-08-13 23:39:40,211][flwr][INFO] - aggregate_evaluate: received 2 results and 0 failures
[2024-08-13 23:39:40,212][flwr][INFO] - 
[2024-08-13 23:39:40,212][flwr][INFO] - [ROUND 94]
[2024-08-13 23:39:40,212][flwr][INFO] - configure_fit: strategy sampled 10 clients (out of 20)
[2024-08-13 23:39:42,227][flwr][INFO] - aggregate_fit: received 10 results and 0 failures
[2024-08-13 23:39:45,764][flwr][INFO] - fit progress: (94, 12.769107282161713, {'accuracy': 0.7211538461538461}, 507.0242510550015)
[2024-08-13 23:39:45,764][flwr][INFO] - configure_evaluate: strategy sampled 2 clients (out of 20)
[2024-08-13 23:39:45,946][flwr][INFO] - aggregate_evaluate: received 2 results and 0 failures
[2024-08-13 23:39:45,946][flwr][INFO] - 
[2024-08-13 23:39:45,946][flwr][INFO] - [ROUND 95]
[2024-08-13 23:39:45,946][flwr][INFO] - configure_fit: strategy sampled 10 clients (out of 20)
[2024-08-13 23:39:47,941][flwr][INFO] - aggregate_fit: received 10 results and 0 failures
[2024-08-13 23:39:51,583][flwr][INFO] - fit progress: (95, 11.149206161499023, {'accuracy': 0.7435897435897436}, 512.8431343510165)
[2024-08-13 23:39:51,583][flwr][INFO] - configure_evaluate: strategy sampled 2 clients (out of 20)
[2024-08-13 23:39:51,787][flwr][INFO] - aggregate_evaluate: received 2 results and 0 failures
[2024-08-13 23:39:51,787][flwr][INFO] - 
[2024-08-13 23:39:51,787][flwr][INFO] - [ROUND 96]
[2024-08-13 23:39:51,787][flwr][INFO] - configure_fit: strategy sampled 10 clients (out of 20)
[2024-08-13 23:39:53,875][flwr][INFO] - aggregate_fit: received 10 results and 0 failures
[2024-08-13 23:39:57,204][flwr][INFO] - fit progress: (96, 10.55307087302208, {'accuracy': 0.75}, 518.4646653920063)
[2024-08-13 23:39:57,205][flwr][INFO] - configure_evaluate: strategy sampled 2 clients (out of 20)
[2024-08-13 23:39:57,396][flwr][INFO] - aggregate_evaluate: received 2 results and 0 failures
[2024-08-13 23:39:57,396][flwr][INFO] - 
[2024-08-13 23:39:57,396][flwr][INFO] - [ROUND 97]
[2024-08-13 23:39:57,396][flwr][INFO] - configure_fit: strategy sampled 10 clients (out of 20)
[2024-08-13 23:39:59,423][flwr][INFO] - aggregate_fit: received 10 results and 0 failures
[2024-08-13 23:40:02,333][flwr][INFO] - fit progress: (97, 14.396282315254211, {'accuracy': 0.7067307692307693}, 523.5929079420166)
[2024-08-13 23:40:02,333][flwr][INFO] - configure_evaluate: strategy sampled 2 clients (out of 20)
[2024-08-13 23:40:02,522][flwr][INFO] - aggregate_evaluate: received 2 results and 0 failures
[2024-08-13 23:40:02,522][flwr][INFO] - 
[2024-08-13 23:40:02,522][flwr][INFO] - [ROUND 98]
[2024-08-13 23:40:02,522][flwr][INFO] - configure_fit: strategy sampled 10 clients (out of 20)
[2024-08-13 23:40:04,513][flwr][INFO] - aggregate_fit: received 10 results and 0 failures
[2024-08-13 23:40:07,336][flwr][INFO] - fit progress: (98, 11.006991893053055, {'accuracy': 0.7467948717948718}, 528.5961147510097)
[2024-08-13 23:40:07,336][flwr][INFO] - configure_evaluate: strategy sampled 2 clients (out of 20)
[2024-08-13 23:40:07,515][flwr][INFO] - aggregate_evaluate: received 2 results and 0 failures
[2024-08-13 23:40:07,515][flwr][INFO] - 
[2024-08-13 23:40:07,515][flwr][INFO] - [ROUND 99]
[2024-08-13 23:40:07,515][flwr][INFO] - configure_fit: strategy sampled 10 clients (out of 20)
[2024-08-13 23:40:09,556][flwr][INFO] - aggregate_fit: received 10 results and 0 failures
[2024-08-13 23:40:12,504][flwr][INFO] - fit progress: (99, 11.5318004488945, {'accuracy': 0.7435897435897436}, 533.7641508929955)
[2024-08-13 23:40:12,504][flwr][INFO] - configure_evaluate: strategy sampled 2 clients (out of 20)
[2024-08-13 23:40:12,701][flwr][INFO] - aggregate_evaluate: received 2 results and 0 failures
[2024-08-13 23:40:12,701][flwr][INFO] - 
[2024-08-13 23:40:12,701][flwr][INFO] - [ROUND 100]
[2024-08-13 23:40:12,701][flwr][INFO] - configure_fit: strategy sampled 10 clients (out of 20)
[2024-08-13 23:40:14,777][flwr][INFO] - aggregate_fit: received 10 results and 0 failures
[2024-08-13 23:40:17,542][flwr][INFO] - fit progress: (100, 11.40353775024414, {'accuracy': 0.7467948717948718}, 538.8022930470179)
[2024-08-13 23:40:17,542][flwr][INFO] - configure_evaluate: strategy sampled 2 clients (out of 20)
[2024-08-13 23:40:17,725][flwr][INFO] - aggregate_evaluate: received 2 results and 0 failures
[2024-08-13 23:40:17,725][flwr][INFO] - 
[2024-08-13 23:40:17,725][flwr][INFO] - [ROUND 101]
[2024-08-13 23:40:17,725][flwr][INFO] - configure_fit: strategy sampled 10 clients (out of 20)
[2024-08-13 23:40:19,756][flwr][INFO] - aggregate_fit: received 10 results and 0 failures
[2024-08-13 23:40:22,704][flwr][INFO] - fit progress: (101, 10.230094909667969, {'accuracy': 0.7628205128205128}, 543.9647824440035)
[2024-08-13 23:40:22,705][flwr][INFO] - configure_evaluate: strategy sampled 2 clients (out of 20)
[2024-08-13 23:40:23,002][flwr][INFO] - aggregate_evaluate: received 2 results and 0 failures
[2024-08-13 23:40:23,002][flwr][INFO] - 
[2024-08-13 23:40:23,002][flwr][INFO] - [ROUND 102]
[2024-08-13 23:40:23,002][flwr][INFO] - configure_fit: strategy sampled 10 clients (out of 20)
[2024-08-13 23:40:25,015][flwr][INFO] - aggregate_fit: received 10 results and 0 failures
[2024-08-13 23:40:27,866][flwr][INFO] - fit progress: (102, 11.626557886600494, {'accuracy': 0.7451923076923077}, 549.1264430090087)
[2024-08-13 23:40:27,866][flwr][INFO] - configure_evaluate: strategy sampled 2 clients (out of 20)
[2024-08-13 23:40:28,084][flwr][INFO] - aggregate_evaluate: received 2 results and 0 failures
[2024-08-13 23:40:28,085][flwr][INFO] - 
[2024-08-13 23:40:28,085][flwr][INFO] - [ROUND 103]
[2024-08-13 23:40:28,085][flwr][INFO] - configure_fit: strategy sampled 10 clients (out of 20)
[2024-08-13 23:40:30,063][flwr][INFO] - aggregate_fit: received 10 results and 0 failures
[2024-08-13 23:40:33,000][flwr][INFO] - fit progress: (103, 10.144561052322388, {'accuracy': 0.7580128205128205}, 554.2607963829942)
[2024-08-13 23:40:33,001][flwr][INFO] - configure_evaluate: strategy sampled 2 clients (out of 20)
[2024-08-13 23:40:33,186][flwr][INFO] - aggregate_evaluate: received 2 results and 0 failures
[2024-08-13 23:40:33,186][flwr][INFO] - 
[2024-08-13 23:40:33,186][flwr][INFO] - [ROUND 104]
[2024-08-13 23:40:33,186][flwr][INFO] - configure_fit: strategy sampled 10 clients (out of 20)
[2024-08-13 23:40:35,147][flwr][INFO] - aggregate_fit: received 10 results and 0 failures
[2024-08-13 23:40:38,019][flwr][INFO] - fit progress: (104, 11.145703077316284, {'accuracy': 0.75}, 559.2795809349918)
[2024-08-13 23:40:38,019][flwr][INFO] - configure_evaluate: strategy sampled 2 clients (out of 20)
[2024-08-13 23:40:38,200][flwr][INFO] - aggregate_evaluate: received 2 results and 0 failures
[2024-08-13 23:40:38,201][flwr][INFO] - 
[2024-08-13 23:40:38,201][flwr][INFO] - [ROUND 105]
[2024-08-13 23:40:38,201][flwr][INFO] - configure_fit: strategy sampled 10 clients (out of 20)
[2024-08-13 23:40:40,285][flwr][INFO] - aggregate_fit: received 10 results and 0 failures
[2024-08-13 23:40:43,417][flwr][INFO] - fit progress: (105, 12.44460153579712, {'accuracy': 0.7355769230769231}, 564.6774415429973)
[2024-08-13 23:40:43,417][flwr][INFO] - configure_evaluate: strategy sampled 2 clients (out of 20)
[2024-08-13 23:40:43,624][flwr][INFO] - aggregate_evaluate: received 2 results and 0 failures
[2024-08-13 23:40:43,624][flwr][INFO] - 
[2024-08-13 23:40:43,624][flwr][INFO] - [ROUND 106]
[2024-08-13 23:40:43,624][flwr][INFO] - configure_fit: strategy sampled 10 clients (out of 20)
[2024-08-13 23:40:45,635][flwr][INFO] - aggregate_fit: received 10 results and 0 failures
[2024-08-13 23:40:48,559][flwr][INFO] - fit progress: (106, 12.332271873950958, {'accuracy': 0.7387820512820513}, 569.8195700990036)
[2024-08-13 23:40:48,559][flwr][INFO] - configure_evaluate: strategy sampled 2 clients (out of 20)
[2024-08-13 23:40:48,774][flwr][INFO] - aggregate_evaluate: received 2 results and 0 failures
[2024-08-13 23:40:48,774][flwr][INFO] - 
[2024-08-13 23:40:48,774][flwr][INFO] - [ROUND 107]
[2024-08-13 23:40:48,774][flwr][INFO] - configure_fit: strategy sampled 10 clients (out of 20)
[2024-08-13 23:40:50,876][flwr][INFO] - aggregate_fit: received 10 results and 0 failures
[2024-08-13 23:40:53,749][flwr][INFO] - fit progress: (107, 10.721769511699677, {'accuracy': 0.75}, 575.0095334890066)
[2024-08-13 23:40:53,749][flwr][INFO] - configure_evaluate: strategy sampled 2 clients (out of 20)
[2024-08-13 23:40:53,914][flwr][INFO] - aggregate_evaluate: received 2 results and 0 failures
[2024-08-13 23:40:53,914][flwr][INFO] - 
[2024-08-13 23:40:53,914][flwr][INFO] - [ROUND 108]
[2024-08-13 23:40:53,914][flwr][INFO] - configure_fit: strategy sampled 10 clients (out of 20)
[2024-08-13 23:40:55,962][flwr][INFO] - aggregate_fit: received 10 results and 0 failures
[2024-08-13 23:40:58,717][flwr][INFO] - fit progress: (108, 11.453807055950165, {'accuracy': 0.7451923076923077}, 579.977557508013)
[2024-08-13 23:40:58,717][flwr][INFO] - configure_evaluate: strategy sampled 2 clients (out of 20)
[2024-08-13 23:40:58,948][flwr][INFO] - aggregate_evaluate: received 2 results and 0 failures
[2024-08-13 23:40:58,948][flwr][INFO] - 
[2024-08-13 23:40:58,948][flwr][INFO] - [ROUND 109]
[2024-08-13 23:40:58,948][flwr][INFO] - configure_fit: strategy sampled 10 clients (out of 20)
[2024-08-13 23:41:01,014][flwr][INFO] - aggregate_fit: received 10 results and 0 failures
[2024-08-13 23:41:03,768][flwr][INFO] - fit progress: (109, 10.684684753417969, {'accuracy': 0.7596153846153846}, 585.0283581440162)
[2024-08-13 23:41:03,768][flwr][INFO] - configure_evaluate: strategy sampled 2 clients (out of 20)
[2024-08-13 23:41:03,960][flwr][INFO] - aggregate_evaluate: received 2 results and 0 failures
[2024-08-13 23:41:03,960][flwr][INFO] - 
[2024-08-13 23:41:03,960][flwr][INFO] - [ROUND 110]
[2024-08-13 23:41:03,960][flwr][INFO] - configure_fit: strategy sampled 10 clients (out of 20)
[2024-08-13 23:41:05,967][flwr][INFO] - aggregate_fit: received 10 results and 0 failures
[2024-08-13 23:41:08,888][flwr][INFO] - fit progress: (110, 11.843924760818481, {'accuracy': 0.7435897435897436}, 590.1483177170157)
[2024-08-13 23:41:08,888][flwr][INFO] - configure_evaluate: strategy sampled 2 clients (out of 20)
[2024-08-13 23:41:09,068][flwr][INFO] - aggregate_evaluate: received 2 results and 0 failures
[2024-08-13 23:41:09,068][flwr][INFO] - 
[2024-08-13 23:41:09,068][flwr][INFO] - [ROUND 111]
[2024-08-13 23:41:09,068][flwr][INFO] - configure_fit: strategy sampled 10 clients (out of 20)
[2024-08-13 23:41:11,203][flwr][INFO] - aggregate_fit: received 10 results and 0 failures
[2024-08-13 23:41:13,991][flwr][INFO] - fit progress: (111, 11.995133757591248, {'accuracy': 0.7403846153846154}, 595.2509422420117)
[2024-08-13 23:41:13,991][flwr][INFO] - configure_evaluate: strategy sampled 2 clients (out of 20)
[2024-08-13 23:41:14,168][flwr][INFO] - aggregate_evaluate: received 2 results and 0 failures
[2024-08-13 23:41:14,168][flwr][INFO] - 
[2024-08-13 23:41:14,168][flwr][INFO] - [ROUND 112]
[2024-08-13 23:41:14,168][flwr][INFO] - configure_fit: strategy sampled 10 clients (out of 20)
[2024-08-13 23:41:16,212][flwr][INFO] - aggregate_fit: received 10 results and 0 failures
[2024-08-13 23:41:18,977][flwr][INFO] - fit progress: (112, 12.83809757232666, {'accuracy': 0.7355769230769231}, 600.2371882890002)
[2024-08-13 23:41:18,977][flwr][INFO] - configure_evaluate: strategy sampled 2 clients (out of 20)
[2024-08-13 23:41:19,152][flwr][INFO] - aggregate_evaluate: received 2 results and 0 failures
[2024-08-13 23:41:19,152][flwr][INFO] - 
[2024-08-13 23:41:19,152][flwr][INFO] - [ROUND 113]
[2024-08-13 23:41:19,152][flwr][INFO] - configure_fit: strategy sampled 10 clients (out of 20)
[2024-08-13 23:41:21,152][flwr][INFO] - aggregate_fit: received 10 results and 0 failures
[2024-08-13 23:41:23,954][flwr][INFO] - fit progress: (113, 10.191668629646301, {'accuracy': 0.7660256410256411}, 605.2138791850011)
[2024-08-13 23:41:23,954][flwr][INFO] - configure_evaluate: strategy sampled 2 clients (out of 20)
[2024-08-13 23:41:24,156][flwr][INFO] - aggregate_evaluate: received 2 results and 0 failures
[2024-08-13 23:41:24,202][flwr][INFO] - 
[2024-08-13 23:41:24,202][flwr][INFO] - [ROUND 114]
[2024-08-13 23:41:24,202][flwr][INFO] - configure_fit: strategy sampled 10 clients (out of 20)
[2024-08-13 23:41:26,223][flwr][INFO] - aggregate_fit: received 10 results and 0 failures
[2024-08-13 23:41:29,061][flwr][INFO] - fit progress: (114, 10.998953700065613, {'accuracy': 0.7596153846153846}, 610.3211649119912)
[2024-08-13 23:41:29,061][flwr][INFO] - configure_evaluate: strategy sampled 2 clients (out of 20)
[2024-08-13 23:41:29,234][flwr][INFO] - aggregate_evaluate: received 2 results and 0 failures
[2024-08-13 23:41:29,234][flwr][INFO] - 
[2024-08-13 23:41:29,234][flwr][INFO] - [ROUND 115]
[2024-08-13 23:41:29,234][flwr][INFO] - configure_fit: strategy sampled 10 clients (out of 20)
[2024-08-13 23:41:31,214][flwr][INFO] - aggregate_fit: received 10 results and 0 failures
[2024-08-13 23:41:34,247][flwr][INFO] - fit progress: (115, 11.585886538028717, {'accuracy': 0.7532051282051282}, 615.5069162000145)
[2024-08-13 23:41:34,247][flwr][INFO] - configure_evaluate: strategy sampled 2 clients (out of 20)
[2024-08-13 23:41:34,465][flwr][INFO] - aggregate_evaluate: received 2 results and 0 failures
[2024-08-13 23:41:34,465][flwr][INFO] - 
[2024-08-13 23:41:34,465][flwr][INFO] - [ROUND 116]
[2024-08-13 23:41:34,465][flwr][INFO] - configure_fit: strategy sampled 10 clients (out of 20)
[2024-08-13 23:41:36,456][flwr][INFO] - aggregate_fit: received 10 results and 0 failures
[2024-08-13 23:41:39,239][flwr][INFO] - fit progress: (116, 13.108392655849457, {'accuracy': 0.7323717948717948}, 620.4993335489999)
[2024-08-13 23:41:39,239][flwr][INFO] - configure_evaluate: strategy sampled 2 clients (out of 20)
[2024-08-13 23:41:39,440][flwr][INFO] - aggregate_evaluate: received 2 results and 0 failures
[2024-08-13 23:41:39,440][flwr][INFO] - 
[2024-08-13 23:41:39,440][flwr][INFO] - [ROUND 117]
[2024-08-13 23:41:39,440][flwr][INFO] - configure_fit: strategy sampled 10 clients (out of 20)
[2024-08-13 23:41:41,550][flwr][INFO] - aggregate_fit: received 10 results and 0 failures
[2024-08-13 23:41:44,740][flwr][INFO] - fit progress: (117, 10.239876091480255, {'accuracy': 0.7660256410256411}, 626.0007487419934)
[2024-08-13 23:41:44,741][flwr][INFO] - configure_evaluate: strategy sampled 2 clients (out of 20)
[2024-08-13 23:41:44,934][flwr][INFO] - aggregate_evaluate: received 2 results and 0 failures
[2024-08-13 23:41:44,934][flwr][INFO] - 
[2024-08-13 23:41:44,934][flwr][INFO] - [ROUND 118]
[2024-08-13 23:41:44,934][flwr][INFO] - configure_fit: strategy sampled 10 clients (out of 20)
[2024-08-13 23:41:47,033][flwr][INFO] - aggregate_fit: received 10 results and 0 failures
[2024-08-13 23:41:49,981][flwr][INFO] - fit progress: (118, 10.521363586187363, {'accuracy': 0.7692307692307693}, 631.240906231018)
[2024-08-13 23:41:49,981][flwr][INFO] - configure_evaluate: strategy sampled 2 clients (out of 20)
[2024-08-13 23:41:50,175][flwr][INFO] - aggregate_evaluate: received 2 results and 0 failures
[2024-08-13 23:41:50,176][flwr][INFO] - 
[2024-08-13 23:41:50,176][flwr][INFO] - [ROUND 119]
[2024-08-13 23:41:50,176][flwr][INFO] - configure_fit: strategy sampled 10 clients (out of 20)
[2024-08-13 23:41:52,282][flwr][INFO] - aggregate_fit: received 10 results and 0 failures
[2024-08-13 23:41:55,048][flwr][INFO] - fit progress: (119, 12.382109344005585, {'accuracy': 0.7435897435897436}, 636.3084838509967)
[2024-08-13 23:41:55,048][flwr][INFO] - configure_evaluate: strategy sampled 2 clients (out of 20)
[2024-08-13 23:41:55,264][flwr][INFO] - aggregate_evaluate: received 2 results and 0 failures
[2024-08-13 23:41:55,264][flwr][INFO] - 
[2024-08-13 23:41:55,264][flwr][INFO] - [ROUND 120]
[2024-08-13 23:41:55,264][flwr][INFO] - configure_fit: strategy sampled 10 clients (out of 20)
[2024-08-13 23:41:57,298][flwr][INFO] - aggregate_fit: received 10 results and 0 failures
[2024-08-13 23:42:00,130][flwr][INFO] - fit progress: (120, 10.722600519657135, {'accuracy': 0.7628205128205128}, 641.3904509429995)
[2024-08-13 23:42:00,130][flwr][INFO] - configure_evaluate: strategy sampled 2 clients (out of 20)
[2024-08-13 23:42:00,306][flwr][INFO] - aggregate_evaluate: received 2 results and 0 failures
[2024-08-13 23:42:00,306][flwr][INFO] - 
[2024-08-13 23:42:00,306][flwr][INFO] - [ROUND 121]
[2024-08-13 23:42:00,306][flwr][INFO] - configure_fit: strategy sampled 10 clients (out of 20)
[2024-08-13 23:42:02,329][flwr][INFO] - aggregate_fit: received 10 results and 0 failures
[2024-08-13 23:42:05,100][flwr][INFO] - fit progress: (121, 12.879550814628601, {'accuracy': 0.7339743589743589}, 646.360004977003)
[2024-08-13 23:42:05,100][flwr][INFO] - configure_evaluate: strategy sampled 2 clients (out of 20)
[2024-08-13 23:42:05,266][flwr][INFO] - aggregate_evaluate: received 2 results and 0 failures
[2024-08-13 23:42:05,266][flwr][INFO] - 
[2024-08-13 23:42:05,266][flwr][INFO] - [ROUND 122]
[2024-08-13 23:42:05,266][flwr][INFO] - configure_fit: strategy sampled 10 clients (out of 20)
[2024-08-13 23:42:07,274][flwr][INFO] - aggregate_fit: received 10 results and 0 failures
[2024-08-13 23:42:10,162][flwr][INFO] - fit progress: (122, 9.67045247554779, {'accuracy': 0.7740384615384616}, 651.4226312169922)
[2024-08-13 23:42:10,162][flwr][INFO] - configure_evaluate: strategy sampled 2 clients (out of 20)
[2024-08-13 23:42:10,352][flwr][INFO] - aggregate_evaluate: received 2 results and 0 failures
[2024-08-13 23:42:10,353][flwr][INFO] - 
[2024-08-13 23:42:10,353][flwr][INFO] - [ROUND 123]
[2024-08-13 23:42:10,353][flwr][INFO] - configure_fit: strategy sampled 10 clients (out of 20)
[2024-08-13 23:42:12,499][flwr][INFO] - aggregate_fit: received 10 results and 0 failures
[2024-08-13 23:42:15,460][flwr][INFO] - fit progress: (123, 14.024156868457794, {'accuracy': 0.7243589743589743}, 656.7200696850196)
[2024-08-13 23:42:15,460][flwr][INFO] - configure_evaluate: strategy sampled 2 clients (out of 20)
[2024-08-13 23:42:15,662][flwr][INFO] - aggregate_evaluate: received 2 results and 0 failures
[2024-08-13 23:42:15,663][flwr][INFO] - 
[2024-08-13 23:42:15,663][flwr][INFO] - [ROUND 124]
[2024-08-13 23:42:15,663][flwr][INFO] - configure_fit: strategy sampled 10 clients (out of 20)
[2024-08-13 23:42:17,802][flwr][INFO] - aggregate_fit: received 10 results and 0 failures
[2024-08-13 23:42:21,210][flwr][INFO] - fit progress: (124, 11.17617005109787, {'accuracy': 0.7580128205128205}, 662.4699649889953)
[2024-08-13 23:42:21,210][flwr][INFO] - configure_evaluate: strategy sampled 2 clients (out of 20)
[2024-08-13 23:42:21,438][flwr][INFO] - aggregate_evaluate: received 2 results and 0 failures
[2024-08-13 23:42:21,439][flwr][INFO] - 
[2024-08-13 23:42:21,439][flwr][INFO] - [ROUND 125]
[2024-08-13 23:42:21,439][flwr][INFO] - configure_fit: strategy sampled 10 clients (out of 20)
[2024-08-13 23:42:23,671][flwr][INFO] - aggregate_fit: received 10 results and 0 failures
[2024-08-13 23:42:27,323][flwr][INFO] - fit progress: (125, 10.453596293926239, {'accuracy': 0.7676282051282052}, 668.583773766004)
[2024-08-13 23:42:27,324][flwr][INFO] - configure_evaluate: strategy sampled 2 clients (out of 20)
[2024-08-13 23:42:27,547][flwr][INFO] - aggregate_evaluate: received 2 results and 0 failures
[2024-08-13 23:42:27,548][flwr][INFO] - 
[2024-08-13 23:42:27,548][flwr][INFO] - [ROUND 126]
[2024-08-13 23:42:27,548][flwr][INFO] - configure_fit: strategy sampled 10 clients (out of 20)
[2024-08-13 23:42:29,703][flwr][INFO] - aggregate_fit: received 10 results and 0 failures
[2024-08-13 23:42:32,775][flwr][INFO] - fit progress: (126, 11.088509202003479, {'accuracy': 0.7660256410256411}, 674.035061852017)
[2024-08-13 23:42:32,775][flwr][INFO] - configure_evaluate: strategy sampled 2 clients (out of 20)
[2024-08-13 23:42:32,971][flwr][INFO] - aggregate_evaluate: received 2 results and 0 failures
[2024-08-13 23:42:32,971][flwr][INFO] - 
[2024-08-13 23:42:32,972][flwr][INFO] - [ROUND 127]
[2024-08-13 23:42:32,972][flwr][INFO] - configure_fit: strategy sampled 10 clients (out of 20)
[2024-08-13 23:42:35,161][flwr][INFO] - aggregate_fit: received 10 results and 0 failures
[2024-08-13 23:42:38,280][flwr][INFO] - fit progress: (127, 13.028269171714783, {'accuracy': 0.7371794871794872}, 679.5399075130117)
[2024-08-13 23:42:38,280][flwr][INFO] - configure_evaluate: strategy sampled 2 clients (out of 20)
[2024-08-13 23:42:38,468][flwr][INFO] - aggregate_evaluate: received 2 results and 0 failures
[2024-08-13 23:42:38,468][flwr][INFO] - 
[2024-08-13 23:42:38,468][flwr][INFO] - [ROUND 128]
[2024-08-13 23:42:38,468][flwr][INFO] - configure_fit: strategy sampled 10 clients (out of 20)
[2024-08-13 23:42:40,503][flwr][INFO] - aggregate_fit: received 10 results and 0 failures
[2024-08-13 23:42:44,350][flwr][INFO] - fit progress: (128, 12.831120669841766, {'accuracy': 0.7419871794871795}, 685.6100587660039)
[2024-08-13 23:42:44,350][flwr][INFO] - configure_evaluate: strategy sampled 2 clients (out of 20)
[2024-08-13 23:42:44,566][flwr][INFO] - aggregate_evaluate: received 2 results and 0 failures
[2024-08-13 23:42:44,566][flwr][INFO] - 
[2024-08-13 23:42:44,566][flwr][INFO] - [ROUND 129]
[2024-08-13 23:42:44,566][flwr][INFO] - configure_fit: strategy sampled 10 clients (out of 20)
[2024-08-13 23:42:46,685][flwr][INFO] - aggregate_fit: received 10 results and 0 failures
[2024-08-13 23:42:50,514][flwr][INFO] - fit progress: (129, 12.589567422866821, {'accuracy': 0.7451923076923077}, 691.7738158089924)
[2024-08-13 23:42:50,514][flwr][INFO] - configure_evaluate: strategy sampled 2 clients (out of 20)
[2024-08-13 23:42:50,708][flwr][INFO] - aggregate_evaluate: received 2 results and 0 failures
[2024-08-13 23:42:50,708][flwr][INFO] - 
[2024-08-13 23:42:50,708][flwr][INFO] - [ROUND 130]
[2024-08-13 23:42:50,708][flwr][INFO] - configure_fit: strategy sampled 10 clients (out of 20)
[2024-08-13 23:42:52,798][flwr][INFO] - aggregate_fit: received 10 results and 0 failures
[2024-08-13 23:42:55,805][flwr][INFO] - fit progress: (130, 13.754902720451355, {'accuracy': 0.7307692307692307}, 697.064924744016)
[2024-08-13 23:42:55,805][flwr][INFO] - configure_evaluate: strategy sampled 2 clients (out of 20)
[2024-08-13 23:42:56,015][flwr][INFO] - aggregate_evaluate: received 2 results and 0 failures
[2024-08-13 23:42:56,016][flwr][INFO] - 
[2024-08-13 23:42:56,016][flwr][INFO] - [ROUND 131]
[2024-08-13 23:42:56,016][flwr][INFO] - configure_fit: strategy sampled 10 clients (out of 20)
[2024-08-13 23:42:58,120][flwr][INFO] - aggregate_fit: received 10 results and 0 failures
[2024-08-13 23:43:01,143][flwr][INFO] - fit progress: (131, 12.347218692302704, {'accuracy': 0.7532051282051282}, 702.4034697620082)
[2024-08-13 23:43:01,143][flwr][INFO] - configure_evaluate: strategy sampled 2 clients (out of 20)
[2024-08-13 23:43:01,357][flwr][INFO] - aggregate_evaluate: received 2 results and 0 failures
[2024-08-13 23:43:01,357][flwr][INFO] - 
[2024-08-13 23:43:01,357][flwr][INFO] - [ROUND 132]
[2024-08-13 23:43:01,357][flwr][INFO] - configure_fit: strategy sampled 10 clients (out of 20)
[2024-08-13 23:43:03,448][flwr][INFO] - aggregate_fit: received 10 results and 0 failures
[2024-08-13 23:43:07,112][flwr][INFO] - fit progress: (132, 12.090193808078766, {'accuracy': 0.7564102564102564}, 708.3724936000071)
[2024-08-13 23:43:07,112][flwr][INFO] - configure_evaluate: strategy sampled 2 clients (out of 20)
[2024-08-13 23:43:07,335][flwr][INFO] - aggregate_evaluate: received 2 results and 0 failures
[2024-08-13 23:43:07,335][flwr][INFO] - 
[2024-08-13 23:43:07,335][flwr][INFO] - [ROUND 133]
[2024-08-13 23:43:07,335][flwr][INFO] - configure_fit: strategy sampled 10 clients (out of 20)
[2024-08-13 23:43:09,498][flwr][INFO] - aggregate_fit: received 10 results and 0 failures
[2024-08-13 23:43:13,374][flwr][INFO] - fit progress: (133, 10.229637801647186, {'accuracy': 0.7692307692307693}, 714.6345388500195)
[2024-08-13 23:43:13,374][flwr][INFO] - configure_evaluate: strategy sampled 2 clients (out of 20)
[2024-08-13 23:43:13,571][flwr][INFO] - aggregate_evaluate: received 2 results and 0 failures
[2024-08-13 23:43:13,571][flwr][INFO] - 
[2024-08-13 23:43:13,571][flwr][INFO] - [ROUND 134]
[2024-08-13 23:43:13,572][flwr][INFO] - configure_fit: strategy sampled 10 clients (out of 20)
[2024-08-13 23:43:15,735][flwr][INFO] - aggregate_fit: received 10 results and 0 failures
[2024-08-13 23:43:19,555][flwr][INFO] - fit progress: (134, 11.332097858190536, {'accuracy': 0.7660256410256411}, 720.8152422179992)
[2024-08-13 23:43:19,555][flwr][INFO] - configure_evaluate: strategy sampled 2 clients (out of 20)
[2024-08-13 23:43:19,801][flwr][INFO] - aggregate_evaluate: received 2 results and 0 failures
[2024-08-13 23:43:19,801][flwr][INFO] - 
[2024-08-13 23:43:19,801][flwr][INFO] - [ROUND 135]
[2024-08-13 23:43:19,801][flwr][INFO] - configure_fit: strategy sampled 10 clients (out of 20)
[2024-08-13 23:43:21,827][flwr][INFO] - aggregate_fit: received 10 results and 0 failures
[2024-08-13 23:43:25,548][flwr][INFO] - fit progress: (135, 11.382038652896881, {'accuracy': 0.7660256410256411}, 726.8081331349968)
[2024-08-13 23:43:25,548][flwr][INFO] - configure_evaluate: strategy sampled 2 clients (out of 20)
[2024-08-13 23:43:25,764][flwr][INFO] - aggregate_evaluate: received 2 results and 0 failures
[2024-08-13 23:43:25,764][flwr][INFO] - 
[2024-08-13 23:43:25,764][flwr][INFO] - [ROUND 136]
[2024-08-13 23:43:25,764][flwr][INFO] - configure_fit: strategy sampled 10 clients (out of 20)
[2024-08-13 23:43:27,910][flwr][INFO] - aggregate_fit: received 10 results and 0 failures
[2024-08-13 23:43:31,054][flwr][INFO] - fit progress: (136, 12.44033682346344, {'accuracy': 0.7483974358974359}, 732.3146446140017)
[2024-08-13 23:43:31,055][flwr][INFO] - configure_evaluate: strategy sampled 2 clients (out of 20)
[2024-08-13 23:43:31,271][flwr][INFO] - aggregate_evaluate: received 2 results and 0 failures
[2024-08-13 23:43:31,271][flwr][INFO] - 
[2024-08-13 23:43:31,271][flwr][INFO] - [ROUND 137]
[2024-08-13 23:43:31,271][flwr][INFO] - configure_fit: strategy sampled 10 clients (out of 20)
[2024-08-13 23:43:33,450][flwr][INFO] - aggregate_fit: received 10 results and 0 failures
[2024-08-13 23:43:36,820][flwr][INFO] - fit progress: (137, 13.365915060043335, {'accuracy': 0.7403846153846154}, 738.080657493003)
[2024-08-13 23:43:36,820][flwr][INFO] - configure_evaluate: strategy sampled 2 clients (out of 20)
[2024-08-13 23:43:37,042][flwr][INFO] - aggregate_evaluate: received 2 results and 0 failures
[2024-08-13 23:43:37,042][flwr][INFO] - 
[2024-08-13 23:43:37,042][flwr][INFO] - [ROUND 138]
[2024-08-13 23:43:37,042][flwr][INFO] - configure_fit: strategy sampled 10 clients (out of 20)
[2024-08-13 23:43:39,194][flwr][INFO] - aggregate_fit: received 10 results and 0 failures
[2024-08-13 23:43:42,399][flwr][INFO] - fit progress: (138, 12.782268702983856, {'accuracy': 0.7516025641025641}, 743.6591766049969)
[2024-08-13 23:43:42,399][flwr][INFO] - configure_evaluate: strategy sampled 2 clients (out of 20)
[2024-08-13 23:43:42,622][flwr][INFO] - aggregate_evaluate: received 2 results and 0 failures
[2024-08-13 23:43:42,622][flwr][INFO] - 
[2024-08-13 23:43:42,622][flwr][INFO] - [ROUND 139]
[2024-08-13 23:43:42,622][flwr][INFO] - configure_fit: strategy sampled 10 clients (out of 20)
[2024-08-13 23:43:44,776][flwr][INFO] - aggregate_fit: received 10 results and 0 failures
[2024-08-13 23:43:47,911][flwr][INFO] - fit progress: (139, 11.171699821949005, {'accuracy': 0.7660256410256411}, 749.1716229180165)
[2024-08-13 23:43:47,911][flwr][INFO] - configure_evaluate: strategy sampled 2 clients (out of 20)
[2024-08-13 23:43:48,107][flwr][INFO] - aggregate_evaluate: received 2 results and 0 failures
[2024-08-13 23:43:48,107][flwr][INFO] - 
[2024-08-13 23:43:48,107][flwr][INFO] - [ROUND 140]
[2024-08-13 23:43:48,107][flwr][INFO] - configure_fit: strategy sampled 10 clients (out of 20)
[2024-08-13 23:43:50,267][flwr][INFO] - aggregate_fit: received 10 results and 0 failures
[2024-08-13 23:43:53,577][flwr][INFO] - fit progress: (140, 11.886645555496216, {'accuracy': 0.7483974358974359}, 754.8376960309979)
[2024-08-13 23:43:53,578][flwr][INFO] - configure_evaluate: strategy sampled 2 clients (out of 20)
[2024-08-13 23:43:53,800][flwr][INFO] - aggregate_evaluate: received 2 results and 0 failures
[2024-08-13 23:43:53,800][flwr][INFO] - 
[2024-08-13 23:43:53,800][flwr][INFO] - [ROUND 141]
[2024-08-13 23:43:53,800][flwr][INFO] - configure_fit: strategy sampled 10 clients (out of 20)
[2024-08-13 23:43:55,849][flwr][INFO] - aggregate_fit: received 10 results and 0 failures
[2024-08-13 23:43:58,802][flwr][INFO] - fit progress: (141, 12.260107696056366, {'accuracy': 0.7596153846153846}, 760.0627915770165)
[2024-08-13 23:43:58,803][flwr][INFO] - configure_evaluate: strategy sampled 2 clients (out of 20)
[2024-08-13 23:43:59,010][flwr][INFO] - aggregate_evaluate: received 2 results and 0 failures
[2024-08-13 23:43:59,010][flwr][INFO] - 
[2024-08-13 23:43:59,010][flwr][INFO] - [ROUND 142]
[2024-08-13 23:43:59,010][flwr][INFO] - configure_fit: strategy sampled 10 clients (out of 20)
[2024-08-13 23:44:01,026][flwr][INFO] - aggregate_fit: received 10 results and 0 failures
[2024-08-13 23:44:04,493][flwr][INFO] - fit progress: (142, 12.149210453033447, {'accuracy': 0.7516025641025641}, 765.7537683020055)
[2024-08-13 23:44:04,494][flwr][INFO] - configure_evaluate: strategy sampled 2 clients (out of 20)
[2024-08-13 23:44:04,691][flwr][INFO] - aggregate_evaluate: received 2 results and 0 failures
[2024-08-13 23:44:04,691][flwr][INFO] - 
[2024-08-13 23:44:04,691][flwr][INFO] - [ROUND 143]
[2024-08-13 23:44:04,691][flwr][INFO] - configure_fit: strategy sampled 10 clients (out of 20)
[2024-08-13 23:44:06,741][flwr][INFO] - aggregate_fit: received 10 results and 0 failures
[2024-08-13 23:44:09,875][flwr][INFO] - fit progress: (143, 12.14543068408966, {'accuracy': 0.7580128205128205}, 771.1350807550189)
[2024-08-13 23:44:09,875][flwr][INFO] - configure_evaluate: strategy sampled 2 clients (out of 20)
[2024-08-13 23:44:10,073][flwr][INFO] - aggregate_evaluate: received 2 results and 0 failures
[2024-08-13 23:44:10,073][flwr][INFO] - 
[2024-08-13 23:44:10,073][flwr][INFO] - [ROUND 144]
[2024-08-13 23:44:10,073][flwr][INFO] - configure_fit: strategy sampled 10 clients (out of 20)
[2024-08-13 23:44:12,142][flwr][INFO] - aggregate_fit: received 10 results and 0 failures
[2024-08-13 23:44:15,101][flwr][INFO] - fit progress: (144, 13.319803833961487, {'accuracy': 0.7451923076923077}, 776.3610739700089)
[2024-08-13 23:44:15,101][flwr][INFO] - configure_evaluate: strategy sampled 2 clients (out of 20)
[2024-08-13 23:44:15,321][flwr][INFO] - aggregate_evaluate: received 2 results and 0 failures
[2024-08-13 23:44:15,321][flwr][INFO] - 
[2024-08-13 23:44:15,321][flwr][INFO] - [ROUND 145]
[2024-08-13 23:44:15,321][flwr][INFO] - configure_fit: strategy sampled 10 clients (out of 20)
[2024-08-13 23:44:17,344][flwr][INFO] - aggregate_fit: received 10 results and 0 failures
[2024-08-13 23:44:20,219][flwr][INFO] - fit progress: (145, 14.684958934783936, {'accuracy': 0.7339743589743589}, 781.4796654749953)
[2024-08-13 23:44:20,220][flwr][INFO] - configure_evaluate: strategy sampled 2 clients (out of 20)
[2024-08-13 23:44:20,400][flwr][INFO] - aggregate_evaluate: received 2 results and 0 failures
[2024-08-13 23:44:20,400][flwr][INFO] - 
[2024-08-13 23:44:20,400][flwr][INFO] - [ROUND 146]
[2024-08-13 23:44:20,400][flwr][INFO] - configure_fit: strategy sampled 10 clients (out of 20)
[2024-08-13 23:44:22,478][flwr][INFO] - aggregate_fit: received 10 results and 0 failures
[2024-08-13 23:44:25,384][flwr][INFO] - fit progress: (146, 12.8268021941185, {'accuracy': 0.7467948717948718}, 786.6441362840123)
[2024-08-13 23:44:25,384][flwr][INFO] - configure_evaluate: strategy sampled 2 clients (out of 20)
[2024-08-13 23:44:25,618][flwr][INFO] - aggregate_evaluate: received 2 results and 0 failures
[2024-08-13 23:44:25,618][flwr][INFO] - 
[2024-08-13 23:44:25,618][flwr][INFO] - [ROUND 147]
[2024-08-13 23:44:25,618][flwr][INFO] - configure_fit: strategy sampled 10 clients (out of 20)
[2024-08-13 23:44:27,698][flwr][INFO] - aggregate_fit: received 10 results and 0 failures
[2024-08-13 23:44:31,725][flwr][INFO] - fit progress: (147, 12.980101704597473, {'accuracy': 0.75}, 792.9849981839943)
[2024-08-13 23:44:31,725][flwr][INFO] - configure_evaluate: strategy sampled 2 clients (out of 20)
[2024-08-13 23:44:31,917][flwr][INFO] - aggregate_evaluate: received 2 results and 0 failures
[2024-08-13 23:44:31,918][flwr][INFO] - 
[2024-08-13 23:44:31,918][flwr][INFO] - [ROUND 148]
[2024-08-13 23:44:31,918][flwr][INFO] - configure_fit: strategy sampled 10 clients (out of 20)
[2024-08-13 23:44:34,075][flwr][INFO] - aggregate_fit: received 10 results and 0 failures
[2024-08-13 23:44:37,872][flwr][INFO] - fit progress: (148, 13.091323435306549, {'accuracy': 0.7548076923076923}, 799.1324836970016)
[2024-08-13 23:44:37,872][flwr][INFO] - configure_evaluate: strategy sampled 2 clients (out of 20)
[2024-08-13 23:44:38,079][flwr][INFO] - aggregate_evaluate: received 2 results and 0 failures
[2024-08-13 23:44:38,079][flwr][INFO] - 
[2024-08-13 23:44:38,079][flwr][INFO] - [ROUND 149]
[2024-08-13 23:44:38,080][flwr][INFO] - configure_fit: strategy sampled 10 clients (out of 20)
[2024-08-13 23:44:40,246][flwr][INFO] - aggregate_fit: received 10 results and 0 failures
[2024-08-13 23:44:43,413][flwr][INFO] - fit progress: (149, 12.776717185974121, {'accuracy': 0.7483974358974359}, 804.6737626620161)
[2024-08-13 23:44:43,414][flwr][INFO] - configure_evaluate: strategy sampled 2 clients (out of 20)
[2024-08-13 23:44:43,611][flwr][INFO] - aggregate_evaluate: received 2 results and 0 failures
[2024-08-13 23:44:43,611][flwr][INFO] - 
[2024-08-13 23:44:43,611][flwr][INFO] - [ROUND 150]
[2024-08-13 23:44:43,611][flwr][INFO] - configure_fit: strategy sampled 10 clients (out of 20)
[2024-08-13 23:44:45,883][flwr][INFO] - aggregate_fit: received 10 results and 0 failures
[2024-08-13 23:44:49,522][flwr][INFO] - fit progress: (150, 12.672921657562256, {'accuracy': 0.7516025641025641}, 810.7822237630025)
[2024-08-13 23:44:49,522][flwr][INFO] - configure_evaluate: strategy sampled 2 clients (out of 20)
[2024-08-13 23:44:49,742][flwr][INFO] - aggregate_evaluate: received 2 results and 0 failures
[2024-08-13 23:44:49,742][flwr][INFO] - 
[2024-08-13 23:44:49,742][flwr][INFO] - [ROUND 151]
[2024-08-13 23:44:49,742][flwr][INFO] - configure_fit: strategy sampled 10 clients (out of 20)
[2024-08-13 23:44:51,936][flwr][INFO] - aggregate_fit: received 10 results and 0 failures
[2024-08-13 23:44:54,957][flwr][INFO] - fit progress: (151, 13.224231839179993, {'accuracy': 0.7403846153846154}, 816.2172423280135)
[2024-08-13 23:44:54,957][flwr][INFO] - configure_evaluate: strategy sampled 2 clients (out of 20)
[2024-08-13 23:44:55,156][flwr][INFO] - aggregate_evaluate: received 2 results and 0 failures
[2024-08-13 23:44:55,156][flwr][INFO] - 
[2024-08-13 23:44:55,156][flwr][INFO] - [ROUND 152]
[2024-08-13 23:44:55,156][flwr][INFO] - configure_fit: strategy sampled 10 clients (out of 20)
[2024-08-13 23:44:57,390][flwr][INFO] - aggregate_fit: received 10 results and 0 failures
[2024-08-13 23:45:00,660][flwr][INFO] - fit progress: (152, 12.568853259086609, {'accuracy': 0.7580128205128205}, 821.9198101590155)
[2024-08-13 23:45:00,660][flwr][INFO] - configure_evaluate: strategy sampled 2 clients (out of 20)
[2024-08-13 23:45:00,875][flwr][INFO] - aggregate_evaluate: received 2 results and 0 failures
[2024-08-13 23:45:00,875][flwr][INFO] - 
[2024-08-13 23:45:00,875][flwr][INFO] - [ROUND 153]
[2024-08-13 23:45:00,875][flwr][INFO] - configure_fit: strategy sampled 10 clients (out of 20)
[2024-08-13 23:45:03,110][flwr][INFO] - aggregate_fit: received 10 results and 0 failures
[2024-08-13 23:45:06,594][flwr][INFO] - fit progress: (153, 11.924098372459412, {'accuracy': 0.7596153846153846}, 827.8542256340152)
[2024-08-13 23:45:06,594][flwr][INFO] - configure_evaluate: strategy sampled 2 clients (out of 20)
[2024-08-13 23:45:06,802][flwr][INFO] - aggregate_evaluate: received 2 results and 0 failures
[2024-08-13 23:45:06,802][flwr][INFO] - 
[2024-08-13 23:45:06,802][flwr][INFO] - [ROUND 154]
[2024-08-13 23:45:06,802][flwr][INFO] - configure_fit: strategy sampled 10 clients (out of 20)
[2024-08-13 23:45:09,046][flwr][INFO] - aggregate_fit: received 10 results and 0 failures
[2024-08-13 23:45:12,665][flwr][INFO] - fit progress: (154, 12.410275757312775, {'accuracy': 0.7580128205128205}, 833.9257491900062)
[2024-08-13 23:45:12,666][flwr][INFO] - configure_evaluate: strategy sampled 2 clients (out of 20)
[2024-08-13 23:45:12,884][flwr][INFO] - aggregate_evaluate: received 2 results and 0 failures
[2024-08-13 23:45:12,884][flwr][INFO] - 
[2024-08-13 23:45:12,884][flwr][INFO] - [ROUND 155]
[2024-08-13 23:45:12,884][flwr][INFO] - configure_fit: strategy sampled 10 clients (out of 20)
[2024-08-13 23:45:15,029][flwr][INFO] - aggregate_fit: received 10 results and 0 failures
[2024-08-13 23:45:18,721][flwr][INFO] - fit progress: (155, 11.133839011192322, {'accuracy': 0.7612179487179487}, 839.981048259011)
[2024-08-13 23:45:18,721][flwr][INFO] - configure_evaluate: strategy sampled 2 clients (out of 20)
[2024-08-13 23:45:18,987][flwr][INFO] - aggregate_evaluate: received 2 results and 0 failures
[2024-08-13 23:45:18,987][flwr][INFO] - 
[2024-08-13 23:45:18,987][flwr][INFO] - [ROUND 156]
[2024-08-13 23:45:18,987][flwr][INFO] - configure_fit: strategy sampled 10 clients (out of 20)
[2024-08-13 23:45:21,171][flwr][INFO] - aggregate_fit: received 10 results and 0 failures
[2024-08-13 23:45:24,264][flwr][INFO] - fit progress: (156, 14.082350432872772, {'accuracy': 0.7403846153846154}, 845.5246073690068)
[2024-08-13 23:45:24,265][flwr][INFO] - configure_evaluate: strategy sampled 2 clients (out of 20)
[2024-08-13 23:45:24,536][flwr][INFO] - aggregate_evaluate: received 2 results and 0 failures
[2024-08-13 23:45:24,536][flwr][INFO] - 
[2024-08-13 23:45:24,536][flwr][INFO] - [ROUND 157]
[2024-08-13 23:45:24,536][flwr][INFO] - configure_fit: strategy sampled 10 clients (out of 20)
[2024-08-13 23:45:26,688][flwr][INFO] - aggregate_fit: received 10 results and 0 failures
[2024-08-13 23:45:29,612][flwr][INFO] - fit progress: (157, 12.553157210350037, {'accuracy': 0.75}, 850.8726939980115)
[2024-08-13 23:45:29,613][flwr][INFO] - configure_evaluate: strategy sampled 2 clients (out of 20)
[2024-08-13 23:45:29,792][flwr][INFO] - aggregate_evaluate: received 2 results and 0 failures
[2024-08-13 23:45:29,792][flwr][INFO] - 
[2024-08-13 23:45:29,792][flwr][INFO] - [ROUND 158]
[2024-08-13 23:45:29,792][flwr][INFO] - configure_fit: strategy sampled 10 clients (out of 20)
[2024-08-13 23:45:31,964][flwr][INFO] - aggregate_fit: received 10 results and 0 failures
[2024-08-13 23:45:35,183][flwr][INFO] - fit progress: (158, 14.599428474903107, {'accuracy': 0.7355769230769231}, 856.443117923016)
[2024-08-13 23:45:35,183][flwr][INFO] - configure_evaluate: strategy sampled 2 clients (out of 20)
[2024-08-13 23:45:35,411][flwr][INFO] - aggregate_evaluate: received 2 results and 0 failures
[2024-08-13 23:45:35,411][flwr][INFO] - 
[2024-08-13 23:45:35,411][flwr][INFO] - [ROUND 159]
[2024-08-13 23:45:35,411][flwr][INFO] - configure_fit: strategy sampled 10 clients (out of 20)
[2024-08-13 23:45:37,624][flwr][INFO] - aggregate_fit: received 10 results and 0 failures
[2024-08-13 23:45:40,657][flwr][INFO] - fit progress: (159, 14.30335557460785, {'accuracy': 0.7371794871794872}, 861.9172284030064)
[2024-08-13 23:45:40,657][flwr][INFO] - configure_evaluate: strategy sampled 2 clients (out of 20)
[2024-08-13 23:45:40,885][flwr][INFO] - aggregate_evaluate: received 2 results and 0 failures
[2024-08-13 23:45:40,886][flwr][INFO] - 
[2024-08-13 23:45:40,886][flwr][INFO] - [ROUND 160]
[2024-08-13 23:45:40,886][flwr][INFO] - configure_fit: strategy sampled 10 clients (out of 20)
[2024-08-13 23:45:42,984][flwr][INFO] - aggregate_fit: received 10 results and 0 failures
[2024-08-13 23:45:46,601][flwr][INFO] - fit progress: (160, 13.564983785152435, {'accuracy': 0.7467948717948718}, 867.861149792996)
[2024-08-13 23:45:46,601][flwr][INFO] - configure_evaluate: strategy sampled 2 clients (out of 20)
[2024-08-13 23:45:46,833][flwr][INFO] - aggregate_evaluate: received 2 results and 0 failures
[2024-08-13 23:45:46,833][flwr][INFO] - 
[2024-08-13 23:45:46,833][flwr][INFO] - [ROUND 161]
[2024-08-13 23:45:46,833][flwr][INFO] - configure_fit: strategy sampled 10 clients (out of 20)
[2024-08-13 23:45:48,972][flwr][INFO] - aggregate_fit: received 10 results and 0 failures
[2024-08-13 23:45:52,251][flwr][INFO] - fit progress: (161, 13.136264145374298, {'accuracy': 0.7483974358974359}, 873.5117554010067)
[2024-08-13 23:45:52,252][flwr][INFO] - configure_evaluate: strategy sampled 2 clients (out of 20)
[2024-08-13 23:45:52,586][flwr][INFO] - aggregate_evaluate: received 2 results and 0 failures
[2024-08-13 23:45:52,586][flwr][INFO] - 
[2024-08-13 23:45:52,586][flwr][INFO] - [ROUND 162]
[2024-08-13 23:45:52,586][flwr][INFO] - configure_fit: strategy sampled 10 clients (out of 20)
[2024-08-13 23:45:54,752][flwr][INFO] - aggregate_fit: received 10 results and 0 failures
[2024-08-13 23:45:57,909][flwr][INFO] - fit progress: (162, 13.35452926158905, {'accuracy': 0.7451923076923077}, 879.1697136840085)
[2024-08-13 23:45:57,910][flwr][INFO] - configure_evaluate: strategy sampled 2 clients (out of 20)
[2024-08-13 23:45:58,121][flwr][INFO] - aggregate_evaluate: received 2 results and 0 failures
[2024-08-13 23:45:58,121][flwr][INFO] - 
[2024-08-13 23:45:58,121][flwr][INFO] - [ROUND 163]
[2024-08-13 23:45:58,122][flwr][INFO] - configure_fit: strategy sampled 10 clients (out of 20)
[2024-08-13 23:46:00,344][flwr][INFO] - aggregate_fit: received 10 results and 0 failures
[2024-08-13 23:46:04,186][flwr][INFO] - fit progress: (163, 13.877958357334137, {'accuracy': 0.7483974358974359}, 885.4467177969927)
[2024-08-13 23:46:04,187][flwr][INFO] - configure_evaluate: strategy sampled 2 clients (out of 20)
[2024-08-13 23:46:04,466][flwr][INFO] - aggregate_evaluate: received 2 results and 0 failures
[2024-08-13 23:46:04,466][flwr][INFO] - 
[2024-08-13 23:46:04,466][flwr][INFO] - [ROUND 164]
[2024-08-13 23:46:04,466][flwr][INFO] - configure_fit: strategy sampled 10 clients (out of 20)
[2024-08-13 23:46:06,634][flwr][INFO] - aggregate_fit: received 10 results and 0 failures
[2024-08-13 23:46:10,340][flwr][INFO] - fit progress: (164, 14.149485409259796, {'accuracy': 0.7435897435897436}, 891.5998602780164)
[2024-08-13 23:46:10,340][flwr][INFO] - configure_evaluate: strategy sampled 2 clients (out of 20)
[2024-08-13 23:46:10,530][flwr][INFO] - aggregate_evaluate: received 2 results and 0 failures
[2024-08-13 23:46:10,530][flwr][INFO] - 
[2024-08-13 23:46:10,530][flwr][INFO] - [ROUND 165]
[2024-08-13 23:46:10,530][flwr][INFO] - configure_fit: strategy sampled 10 clients (out of 20)
[2024-08-13 23:46:12,562][flwr][INFO] - aggregate_fit: received 10 results and 0 failures
[2024-08-13 23:46:16,167][flwr][INFO] - fit progress: (165, 13.386515974998474, {'accuracy': 0.7467948717948718}, 897.4271246730059)
[2024-08-13 23:46:16,167][flwr][INFO] - configure_evaluate: strategy sampled 2 clients (out of 20)
[2024-08-13 23:46:16,425][flwr][INFO] - aggregate_evaluate: received 2 results and 0 failures
[2024-08-13 23:46:16,425][flwr][INFO] - 
[2024-08-13 23:46:16,425][flwr][INFO] - [ROUND 166]
[2024-08-13 23:46:16,425][flwr][INFO] - configure_fit: strategy sampled 10 clients (out of 20)
[2024-08-13 23:46:18,541][flwr][INFO] - aggregate_fit: received 10 results and 0 failures
[2024-08-13 23:46:21,957][flwr][INFO] - fit progress: (166, 12.956559956073761, {'accuracy': 0.7532051282051282}, 903.2177982800058)
[2024-08-13 23:46:21,958][flwr][INFO] - configure_evaluate: strategy sampled 2 clients (out of 20)
[2024-08-13 23:46:22,327][flwr][INFO] - aggregate_evaluate: received 2 results and 0 failures
[2024-08-13 23:46:22,327][flwr][INFO] - 
[2024-08-13 23:46:22,327][flwr][INFO] - [ROUND 167]
[2024-08-13 23:46:22,327][flwr][INFO] - configure_fit: strategy sampled 10 clients (out of 20)
[2024-08-13 23:46:24,452][flwr][INFO] - aggregate_fit: received 10 results and 0 failures
[2024-08-13 23:46:27,778][flwr][INFO] - fit progress: (167, 13.460345387458801, {'accuracy': 0.75}, 909.0385499370168)
[2024-08-13 23:46:27,778][flwr][INFO] - configure_evaluate: strategy sampled 2 clients (out of 20)
[2024-08-13 23:46:28,045][flwr][INFO] - aggregate_evaluate: received 2 results and 0 failures
[2024-08-13 23:46:28,045][flwr][INFO] - 
[2024-08-13 23:46:28,045][flwr][INFO] - [ROUND 168]
[2024-08-13 23:46:28,045][flwr][INFO] - configure_fit: strategy sampled 10 clients (out of 20)
[2024-08-13 23:46:30,285][flwr][INFO] - aggregate_fit: received 10 results and 0 failures
[2024-08-13 23:46:33,848][flwr][INFO] - fit progress: (168, 14.44361788034439, {'accuracy': 0.7435897435897436}, 915.1080376729951)
[2024-08-13 23:46:33,848][flwr][INFO] - configure_evaluate: strategy sampled 2 clients (out of 20)
[2024-08-13 23:46:34,325][flwr][INFO] - aggregate_evaluate: received 2 results and 0 failures
[2024-08-13 23:46:34,325][flwr][INFO] - 
[2024-08-13 23:46:34,325][flwr][INFO] - [ROUND 169]
[2024-08-13 23:46:34,325][flwr][INFO] - configure_fit: strategy sampled 10 clients (out of 20)
[2024-08-13 23:46:36,447][flwr][INFO] - aggregate_fit: received 10 results and 0 failures
[2024-08-13 23:46:39,480][flwr][INFO] - fit progress: (169, 13.946899473667145, {'accuracy': 0.7467948717948718}, 920.7407997510163)
[2024-08-13 23:46:39,481][flwr][INFO] - configure_evaluate: strategy sampled 2 clients (out of 20)
[2024-08-13 23:46:39,722][flwr][INFO] - aggregate_evaluate: received 2 results and 0 failures
[2024-08-13 23:46:39,722][flwr][INFO] - 
[2024-08-13 23:46:39,722][flwr][INFO] - [ROUND 170]
[2024-08-13 23:46:39,722][flwr][INFO] - configure_fit: strategy sampled 10 clients (out of 20)
[2024-08-13 23:46:41,851][flwr][INFO] - aggregate_fit: received 10 results and 0 failures
[2024-08-13 23:46:45,166][flwr][INFO] - fit progress: (170, 15.372939467430115, {'accuracy': 0.7291666666666666}, 926.4261968200153)
[2024-08-13 23:46:45,166][flwr][INFO] - configure_evaluate: strategy sampled 2 clients (out of 20)
[2024-08-13 23:46:45,365][flwr][INFO] - aggregate_evaluate: received 2 results and 0 failures
[2024-08-13 23:46:45,365][flwr][INFO] - 
[2024-08-13 23:46:45,365][flwr][INFO] - [ROUND 171]
[2024-08-13 23:46:45,365][flwr][INFO] - configure_fit: strategy sampled 10 clients (out of 20)
[2024-08-13 23:46:47,481][flwr][INFO] - aggregate_fit: received 10 results and 0 failures
[2024-08-13 23:46:50,499][flwr][INFO] - fit progress: (171, 11.54593962430954, {'accuracy': 0.7660256410256411}, 931.7590515960183)
[2024-08-13 23:46:50,499][flwr][INFO] - configure_evaluate: strategy sampled 2 clients (out of 20)
[2024-08-13 23:46:50,895][flwr][INFO] - aggregate_evaluate: received 2 results and 0 failures
[2024-08-13 23:46:50,895][flwr][INFO] - 
[2024-08-13 23:46:50,895][flwr][INFO] - [ROUND 172]
[2024-08-13 23:46:50,895][flwr][INFO] - configure_fit: strategy sampled 10 clients (out of 20)
[2024-08-13 23:46:53,096][flwr][INFO] - aggregate_fit: received 10 results and 0 failures
[2024-08-13 23:46:56,387][flwr][INFO] - fit progress: (172, 11.56032383441925, {'accuracy': 0.7628205128205128}, 937.6475737020082)
[2024-08-13 23:46:56,387][flwr][INFO] - configure_evaluate: strategy sampled 2 clients (out of 20)
[2024-08-13 23:46:56,613][flwr][INFO] - aggregate_evaluate: received 2 results and 0 failures
[2024-08-13 23:46:56,613][flwr][INFO] - 
[2024-08-13 23:46:56,613][flwr][INFO] - [ROUND 173]
[2024-08-13 23:46:56,613][flwr][INFO] - configure_fit: strategy sampled 10 clients (out of 20)
[2024-08-13 23:46:58,988][flwr][INFO] - aggregate_fit: received 10 results and 0 failures
[2024-08-13 23:47:02,352][flwr][INFO] - fit progress: (173, 13.827125072479248, {'accuracy': 0.7548076923076923}, 943.6119699919946)
[2024-08-13 23:47:02,352][flwr][INFO] - configure_evaluate: strategy sampled 2 clients (out of 20)
[2024-08-13 23:47:02,554][flwr][INFO] - aggregate_evaluate: received 2 results and 0 failures
[2024-08-13 23:47:02,554][flwr][INFO] - 
[2024-08-13 23:47:02,554][flwr][INFO] - [ROUND 174]
[2024-08-13 23:47:02,554][flwr][INFO] - configure_fit: strategy sampled 10 clients (out of 20)
[2024-08-13 23:47:04,728][flwr][INFO] - aggregate_fit: received 10 results and 0 failures
[2024-08-13 23:47:07,692][flwr][INFO] - fit progress: (174, 13.21047180891037, {'accuracy': 0.7516025641025641}, 948.952381518)
[2024-08-13 23:47:07,692][flwr][INFO] - configure_evaluate: strategy sampled 2 clients (out of 20)
[2024-08-13 23:47:08,095][flwr][INFO] - aggregate_evaluate: received 2 results and 0 failures
[2024-08-13 23:47:08,095][flwr][INFO] - 
[2024-08-13 23:47:08,095][flwr][INFO] - [ROUND 175]
[2024-08-13 23:47:08,095][flwr][INFO] - configure_fit: strategy sampled 10 clients (out of 20)
[2024-08-13 23:47:26,744][flwr][INFO] - aggregate_fit: received 10 results and 0 failures
[2024-08-13 23:47:30,022][flwr][INFO] - fit progress: (175, 13.106289267539978, {'accuracy': 0.7548076923076923}, 971.2821751640004)
[2024-08-13 23:47:30,022][flwr][INFO] - configure_evaluate: strategy sampled 2 clients (out of 20)
[2024-08-13 23:47:39,464][flwr][INFO] - aggregate_evaluate: received 2 results and 0 failures
[2024-08-13 23:47:39,464][flwr][INFO] - 
[2024-08-13 23:47:39,464][flwr][INFO] - [ROUND 176]
[2024-08-13 23:47:39,465][flwr][INFO] - configure_fit: strategy sampled 10 clients (out of 20)
[2024-08-13 23:47:52,564][flwr][INFO] - aggregate_fit: received 10 results and 0 failures
[2024-08-13 23:47:55,732][flwr][INFO] - fit progress: (176, 12.749198079109192, {'accuracy': 0.7612179487179487}, 996.9926944379986)
[2024-08-13 23:47:55,733][flwr][INFO] - configure_evaluate: strategy sampled 2 clients (out of 20)
[2024-08-13 23:47:56,272][flwr][INFO] - aggregate_evaluate: received 2 results and 0 failures
[2024-08-13 23:47:56,272][flwr][INFO] - 
[2024-08-13 23:47:56,272][flwr][INFO] - [ROUND 177]
[2024-08-13 23:47:56,272][flwr][INFO] - configure_fit: strategy sampled 10 clients (out of 20)
[2024-08-13 23:47:59,222][flwr][INFO] - aggregate_fit: received 10 results and 0 failures
[2024-08-13 23:48:02,885][flwr][INFO] - fit progress: (177, 14.649953007698059, {'accuracy': 0.7403846153846154}, 1004.1452953059925)
[2024-08-13 23:48:02,885][flwr][INFO] - configure_evaluate: strategy sampled 2 clients (out of 20)
[2024-08-13 23:48:03,174][flwr][INFO] - aggregate_evaluate: received 2 results and 0 failures
[2024-08-13 23:48:03,174][flwr][INFO] - 
[2024-08-13 23:48:03,174][flwr][INFO] - [ROUND 178]
[2024-08-13 23:48:03,174][flwr][INFO] - configure_fit: strategy sampled 10 clients (out of 20)
[2024-08-13 23:48:14,335][flwr][INFO] - aggregate_fit: received 10 results and 0 failures
[2024-08-13 23:48:18,135][flwr][INFO] - fit progress: (178, 11.767599940299988, {'accuracy': 0.7580128205128205}, 1019.3957574779924)
[2024-08-13 23:48:18,136][flwr][INFO] - configure_evaluate: strategy sampled 2 clients (out of 20)
[2024-08-13 23:48:18,619][flwr][INFO] - aggregate_evaluate: received 2 results and 0 failures
[2024-08-13 23:48:18,619][flwr][INFO] - 
[2024-08-13 23:48:18,619][flwr][INFO] - [ROUND 179]
[2024-08-13 23:48:18,619][flwr][INFO] - configure_fit: strategy sampled 10 clients (out of 20)
[2024-08-13 23:48:21,448][flwr][INFO] - aggregate_fit: received 10 results and 0 failures
[2024-08-13 23:48:24,830][flwr][INFO] - fit progress: (179, 12.881027340888977, {'accuracy': 0.7532051282051282}, 1026.090008255007)
[2024-08-13 23:48:24,830][flwr][INFO] - configure_evaluate: strategy sampled 2 clients (out of 20)
[2024-08-13 23:48:25,316][flwr][INFO] - aggregate_evaluate: received 2 results and 0 failures
[2024-08-13 23:48:25,316][flwr][INFO] - 
[2024-08-13 23:48:25,316][flwr][INFO] - [ROUND 180]
[2024-08-13 23:48:25,316][flwr][INFO] - configure_fit: strategy sampled 10 clients (out of 20)
[2024-08-13 23:48:29,717][flwr][INFO] - aggregate_fit: received 10 results and 0 failures
[2024-08-13 23:48:32,990][flwr][INFO] - fit progress: (180, 13.485623002052307, {'accuracy': 0.7532051282051282}, 1034.2507336800045)
[2024-08-13 23:48:32,991][flwr][INFO] - configure_evaluate: strategy sampled 2 clients (out of 20)
[2024-08-13 23:48:33,603][flwr][INFO] - aggregate_evaluate: received 2 results and 0 failures
[2024-08-13 23:48:33,603][flwr][INFO] - 
[2024-08-13 23:48:33,603][flwr][INFO] - [ROUND 181]
[2024-08-13 23:48:33,604][flwr][INFO] - configure_fit: strategy sampled 10 clients (out of 20)
[2024-08-13 23:48:37,579][flwr][INFO] - aggregate_fit: received 10 results and 0 failures
[2024-08-13 23:48:40,728][flwr][INFO] - fit progress: (181, 13.61258316040039, {'accuracy': 0.7532051282051282}, 1041.9880444580049)
[2024-08-13 23:48:40,728][flwr][INFO] - configure_evaluate: strategy sampled 2 clients (out of 20)
[2024-08-13 23:48:40,941][flwr][INFO] - aggregate_evaluate: received 2 results and 0 failures
[2024-08-13 23:48:40,942][flwr][INFO] - 
[2024-08-13 23:48:40,942][flwr][INFO] - [ROUND 182]
[2024-08-13 23:48:40,942][flwr][INFO] - configure_fit: strategy sampled 10 clients (out of 20)
[2024-08-13 23:48:44,482][flwr][INFO] - aggregate_fit: received 10 results and 0 failures
[2024-08-13 23:48:47,483][flwr][INFO] - fit progress: (182, 15.166943907737732, {'accuracy': 0.7419871794871795}, 1048.7429977479915)
[2024-08-13 23:48:47,483][flwr][INFO] - configure_evaluate: strategy sampled 2 clients (out of 20)
[2024-08-13 23:48:47,743][flwr][INFO] - aggregate_evaluate: received 2 results and 0 failures
[2024-08-13 23:48:47,743][flwr][INFO] - 
[2024-08-13 23:48:47,743][flwr][INFO] - [ROUND 183]
[2024-08-13 23:48:47,743][flwr][INFO] - configure_fit: strategy sampled 10 clients (out of 20)
[2024-08-13 23:48:51,418][flwr][INFO] - aggregate_fit: received 10 results and 0 failures
[2024-08-13 23:48:54,518][flwr][INFO] - fit progress: (183, 16.270474910736084, {'accuracy': 0.7307692307692307}, 1055.7779174910102)
[2024-08-13 23:48:54,518][flwr][INFO] - configure_evaluate: strategy sampled 2 clients (out of 20)
[2024-08-13 23:48:54,812][flwr][INFO] - aggregate_evaluate: received 2 results and 0 failures
[2024-08-13 23:48:54,812][flwr][INFO] - 
[2024-08-13 23:48:54,812][flwr][INFO] - [ROUND 184]
[2024-08-13 23:48:54,812][flwr][INFO] - configure_fit: strategy sampled 10 clients (out of 20)
[2024-08-13 23:48:58,196][flwr][INFO] - aggregate_fit: received 10 results and 0 failures
[2024-08-13 23:49:01,517][flwr][INFO] - fit progress: (184, 13.998908162117004, {'accuracy': 0.7516025641025641}, 1062.777208888001)
[2024-08-13 23:49:01,517][flwr][INFO] - configure_evaluate: strategy sampled 2 clients (out of 20)
[2024-08-13 23:49:01,992][flwr][INFO] - aggregate_evaluate: received 2 results and 0 failures
[2024-08-13 23:49:01,992][flwr][INFO] - 
[2024-08-13 23:49:01,992][flwr][INFO] - [ROUND 185]
[2024-08-13 23:49:01,992][flwr][INFO] - configure_fit: strategy sampled 10 clients (out of 20)
[2024-08-13 23:49:04,517][flwr][INFO] - aggregate_fit: received 10 results and 0 failures
[2024-08-13 23:49:07,634][flwr][INFO] - fit progress: (185, 13.325047791004181, {'accuracy': 0.7564102564102564}, 1068.8942681479966)
[2024-08-13 23:49:07,634][flwr][INFO] - configure_evaluate: strategy sampled 2 clients (out of 20)
[2024-08-13 23:49:08,080][flwr][INFO] - aggregate_evaluate: received 2 results and 0 failures
[2024-08-13 23:49:08,080][flwr][INFO] - 
[2024-08-13 23:49:08,080][flwr][INFO] - [ROUND 186]
[2024-08-13 23:49:08,080][flwr][INFO] - configure_fit: strategy sampled 10 clients (out of 20)
[2024-08-13 23:49:11,869][flwr][INFO] - aggregate_fit: received 10 results and 0 failures
[2024-08-13 23:49:15,159][flwr][INFO] - fit progress: (186, 13.61717665195465, {'accuracy': 0.7548076923076923}, 1076.4190556410176)
[2024-08-13 23:49:15,159][flwr][INFO] - configure_evaluate: strategy sampled 2 clients (out of 20)
[2024-08-13 23:49:15,542][flwr][INFO] - aggregate_evaluate: received 2 results and 0 failures
[2024-08-13 23:49:15,542][flwr][INFO] - 
[2024-08-13 23:49:15,542][flwr][INFO] - [ROUND 187]
[2024-08-13 23:49:15,542][flwr][INFO] - configure_fit: strategy sampled 10 clients (out of 20)
[2024-08-13 23:49:17,664][flwr][INFO] - aggregate_fit: received 10 results and 0 failures
[2024-08-13 23:49:20,647][flwr][INFO] - fit progress: (187, 13.57814472913742, {'accuracy': 0.7564102564102564}, 1081.9073288800137)
[2024-08-13 23:49:20,647][flwr][INFO] - configure_evaluate: strategy sampled 2 clients (out of 20)
[2024-08-13 23:49:21,258][flwr][INFO] - aggregate_evaluate: received 2 results and 0 failures
[2024-08-13 23:49:21,258][flwr][INFO] - 
[2024-08-13 23:49:21,258][flwr][INFO] - [ROUND 188]
[2024-08-13 23:49:21,258][flwr][INFO] - configure_fit: strategy sampled 10 clients (out of 20)
[2024-08-13 23:49:23,777][flwr][INFO] - aggregate_fit: received 10 results and 0 failures
[2024-08-13 23:49:27,089][flwr][INFO] - fit progress: (188, 12.734549224376678, {'accuracy': 0.7612179487179487}, 1088.3489787470025)
[2024-08-13 23:49:27,089][flwr][INFO] - configure_evaluate: strategy sampled 2 clients (out of 20)
[2024-08-13 23:49:27,470][flwr][INFO] - aggregate_evaluate: received 2 results and 0 failures
[2024-08-13 23:49:27,470][flwr][INFO] - 
[2024-08-13 23:49:27,470][flwr][INFO] - [ROUND 189]
[2024-08-13 23:49:27,470][flwr][INFO] - configure_fit: strategy sampled 10 clients (out of 20)
[2024-08-13 23:49:30,052][flwr][INFO] - aggregate_fit: received 10 results and 0 failures
[2024-08-13 23:49:32,953][flwr][INFO] - fit progress: (189, 14.782073140144348, {'accuracy': 0.7483974358974359}, 1094.2130536670156)
[2024-08-13 23:49:32,953][flwr][INFO] - configure_evaluate: strategy sampled 2 clients (out of 20)
[2024-08-13 23:49:33,256][flwr][INFO] - aggregate_evaluate: received 2 results and 0 failures
[2024-08-13 23:49:33,257][flwr][INFO] - 
[2024-08-13 23:49:33,257][flwr][INFO] - [ROUND 190]
[2024-08-13 23:49:33,257][flwr][INFO] - configure_fit: strategy sampled 10 clients (out of 20)
[2024-08-13 23:49:37,825][flwr][INFO] - aggregate_fit: received 10 results and 0 failures
[2024-08-13 23:49:40,730][flwr][INFO] - fit progress: (190, 15.32398897409439, {'accuracy': 0.7435897435897436}, 1101.9907024830172)
[2024-08-13 23:49:40,731][flwr][INFO] - configure_evaluate: strategy sampled 2 clients (out of 20)
[2024-08-13 23:49:41,117][flwr][INFO] - aggregate_evaluate: received 2 results and 0 failures
[2024-08-13 23:49:41,117][flwr][INFO] - 
[2024-08-13 23:49:41,117][flwr][INFO] - [ROUND 191]
[2024-08-13 23:49:41,117][flwr][INFO] - configure_fit: strategy sampled 10 clients (out of 20)
[2024-08-13 23:49:41,414][flwr][ERROR] - Traceback (most recent call last):
  File "/home/abdenour/miniconda3/envs/fl_with_flower/lib/python3.11/site-packages/flwr/simulation/ray_transport/ray_client_proxy.py", line 94, in _submit_job
    out_mssg, updated_context = self.actor_pool.get_client_result(
                                ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/abdenour/miniconda3/envs/fl_with_flower/lib/python3.11/site-packages/flwr/simulation/ray_transport/ray_actor.py", line 398, in get_client_result
    return self._fetch_future_result(cid)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/abdenour/miniconda3/envs/fl_with_flower/lib/python3.11/site-packages/flwr/simulation/ray_transport/ray_actor.py", line 279, in _fetch_future_result
    res_cid, out_mssg, updated_context = ray.get(
                                         ^^^^^^^^
  File "/home/abdenour/miniconda3/envs/fl_with_flower/lib/python3.11/site-packages/ray/_private/auto_init_hook.py", line 21, in auto_init_wrapper
    return fn(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^
  File "/home/abdenour/miniconda3/envs/fl_with_flower/lib/python3.11/site-packages/ray/_private/client_mode_hook.py", line 103, in wrapper
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "/home/abdenour/miniconda3/envs/fl_with_flower/lib/python3.11/site-packages/ray/_private/worker.py", line 2667, in get
    values, debugger_breakpoint = worker.get_objects(object_refs, timeout=timeout)
                                  ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/abdenour/miniconda3/envs/fl_with_flower/lib/python3.11/site-packages/ray/_private/worker.py", line 866, in get_objects
    raise value
ray.exceptions.OutOfMemoryError: Task was killed due to the node running low on memory.
Memory on the node (IP: 115.145.171.128, ID: c687613ff34cdbf5097699e5c896055ef2dc44343c5ab44bd40309d5) where the task (actor ID: 85928d3f1a247800081c0e1101000000, name=ClientAppActor.__init__, pid=619614, memory used=1.73GB) was running was 29.50GB / 31.02GB (0.951049), which exceeds the memory usage threshold of 0.95. Ray killed this worker (ID: c7f644ac3cca20dffbfa1723daac4f87f04c7de03da25e7cad5fe33c) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 115.145.171.128`. To see the logs of the worker, use `ray logs worker-c7f644ac3cca20dffbfa1723daac4f87f04c7de03da25e7cad5fe33c*out -ip 115.145.171.128. Top 10 memory users:
PID	MEM(GB)	COMMAND
4942	3.49	/snap/pycharm-professional/408/jbr/bin/java -classpath /snap/pycharm-professional/408/lib/platform-l...
619616	1.80	ray::ClientAppActor.run
619615	1.78	ray::ClientAppActor
619617	1.77	ray::ClientAppActor
619611	1.73	ray::ClientAppActor
619618	1.73	ray::ClientAppActor
619614	1.73	ray::ClientAppActor
619612	1.72	ray::ClientAppActor
619619	1.70	ray::ClientAppActor
619620	1.68	ray::ClientAppActor.run
Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.

[2024-08-13 23:49:41,414][flwr][ERROR] - Traceback (most recent call last):
  File "/home/abdenour/miniconda3/envs/fl_with_flower/lib/python3.11/site-packages/flwr/simulation/ray_transport/ray_client_proxy.py", line 94, in _submit_job
    out_mssg, updated_context = self.actor_pool.get_client_result(
                                ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/abdenour/miniconda3/envs/fl_with_flower/lib/python3.11/site-packages/flwr/simulation/ray_transport/ray_actor.py", line 398, in get_client_result
    return self._fetch_future_result(cid)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/abdenour/miniconda3/envs/fl_with_flower/lib/python3.11/site-packages/flwr/simulation/ray_transport/ray_actor.py", line 279, in _fetch_future_result
    res_cid, out_mssg, updated_context = ray.get(
                                         ^^^^^^^^
  File "/home/abdenour/miniconda3/envs/fl_with_flower/lib/python3.11/site-packages/ray/_private/auto_init_hook.py", line 21, in auto_init_wrapper
    return fn(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^
  File "/home/abdenour/miniconda3/envs/fl_with_flower/lib/python3.11/site-packages/ray/_private/client_mode_hook.py", line 103, in wrapper
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "/home/abdenour/miniconda3/envs/fl_with_flower/lib/python3.11/site-packages/ray/_private/worker.py", line 2667, in get
    values, debugger_breakpoint = worker.get_objects(object_refs, timeout=timeout)
                                  ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/abdenour/miniconda3/envs/fl_with_flower/lib/python3.11/site-packages/ray/_private/worker.py", line 866, in get_objects
    raise value
ray.exceptions.OutOfMemoryError: Task was killed due to the node running low on memory.
Memory on the node (IP: 115.145.171.128, ID: c687613ff34cdbf5097699e5c896055ef2dc44343c5ab44bd40309d5) where the task (actor ID: 85928d3f1a247800081c0e1101000000, name=ClientAppActor.__init__, pid=619614, memory used=1.73GB) was running was 29.50GB / 31.02GB (0.951049), which exceeds the memory usage threshold of 0.95. Ray killed this worker (ID: c7f644ac3cca20dffbfa1723daac4f87f04c7de03da25e7cad5fe33c) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 115.145.171.128`. To see the logs of the worker, use `ray logs worker-c7f644ac3cca20dffbfa1723daac4f87f04c7de03da25e7cad5fe33c*out -ip 115.145.171.128. Top 10 memory users:
PID	MEM(GB)	COMMAND
4942	3.49	/snap/pycharm-professional/408/jbr/bin/java -classpath /snap/pycharm-professional/408/lib/platform-l...
619616	1.80	ray::ClientAppActor.run
619615	1.78	ray::ClientAppActor
619617	1.77	ray::ClientAppActor
619611	1.73	ray::ClientAppActor
619618	1.73	ray::ClientAppActor
619614	1.73	ray::ClientAppActor
619612	1.72	ray::ClientAppActor
619619	1.70	ray::ClientAppActor
619620	1.68	ray::ClientAppActor.run
Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.

[2024-08-13 23:49:41,414][flwr][ERROR] - Task was killed due to the node running low on memory.
Memory on the node (IP: 115.145.171.128, ID: c687613ff34cdbf5097699e5c896055ef2dc44343c5ab44bd40309d5) where the task (actor ID: 85928d3f1a247800081c0e1101000000, name=ClientAppActor.__init__, pid=619614, memory used=1.73GB) was running was 29.50GB / 31.02GB (0.951049), which exceeds the memory usage threshold of 0.95. Ray killed this worker (ID: c7f644ac3cca20dffbfa1723daac4f87f04c7de03da25e7cad5fe33c) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 115.145.171.128`. To see the logs of the worker, use `ray logs worker-c7f644ac3cca20dffbfa1723daac4f87f04c7de03da25e7cad5fe33c*out -ip 115.145.171.128. Top 10 memory users:
PID	MEM(GB)	COMMAND
4942	3.49	/snap/pycharm-professional/408/jbr/bin/java -classpath /snap/pycharm-professional/408/lib/platform-l...
619616	1.80	ray::ClientAppActor.run
619615	1.78	ray::ClientAppActor
619617	1.77	ray::ClientAppActor
619611	1.73	ray::ClientAppActor
619618	1.73	ray::ClientAppActor
619614	1.73	ray::ClientAppActor
619612	1.72	ray::ClientAppActor
619619	1.70	ray::ClientAppActor
619620	1.68	ray::ClientAppActor.run
Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.
[2024-08-13 23:49:41,414][flwr][ERROR] - Task was killed due to the node running low on memory.
Memory on the node (IP: 115.145.171.128, ID: c687613ff34cdbf5097699e5c896055ef2dc44343c5ab44bd40309d5) where the task (actor ID: 85928d3f1a247800081c0e1101000000, name=ClientAppActor.__init__, pid=619614, memory used=1.73GB) was running was 29.50GB / 31.02GB (0.951049), which exceeds the memory usage threshold of 0.95. Ray killed this worker (ID: c7f644ac3cca20dffbfa1723daac4f87f04c7de03da25e7cad5fe33c) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 115.145.171.128`. To see the logs of the worker, use `ray logs worker-c7f644ac3cca20dffbfa1723daac4f87f04c7de03da25e7cad5fe33c*out -ip 115.145.171.128. Top 10 memory users:
PID	MEM(GB)	COMMAND
4942	3.49	/snap/pycharm-professional/408/jbr/bin/java -classpath /snap/pycharm-professional/408/lib/platform-l...
619616	1.80	ray::ClientAppActor.run
619615	1.78	ray::ClientAppActor
619617	1.77	ray::ClientAppActor
619611	1.73	ray::ClientAppActor
619618	1.73	ray::ClientAppActor
619614	1.73	ray::ClientAppActor
619612	1.72	ray::ClientAppActor
619619	1.70	ray::ClientAppActor
619620	1.68	ray::ClientAppActor.run
Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.
[2024-08-13 23:49:44,942][flwr][INFO] - aggregate_fit: received 8 results and 2 failures
[2024-08-13 23:49:48,281][flwr][INFO] - fit progress: (191, 13.780750691890717, {'accuracy': 0.7548076923076923}, 1109.5417261760158)
[2024-08-13 23:49:48,282][flwr][INFO] - configure_evaluate: strategy sampled 2 clients (out of 20)
[2024-08-13 23:49:48,697][flwr][INFO] - aggregate_evaluate: received 2 results and 0 failures
[2024-08-13 23:49:48,697][flwr][INFO] - 
[2024-08-13 23:49:48,697][flwr][INFO] - [ROUND 192]
[2024-08-13 23:49:48,697][flwr][INFO] - configure_fit: strategy sampled 10 clients (out of 20)
[2024-08-13 23:49:48,878][flwr][ERROR] - Traceback (most recent call last):
  File "/home/abdenour/miniconda3/envs/fl_with_flower/lib/python3.11/site-packages/flwr/simulation/ray_transport/ray_client_proxy.py", line 94, in _submit_job
    out_mssg, updated_context = self.actor_pool.get_client_result(
                                ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/abdenour/miniconda3/envs/fl_with_flower/lib/python3.11/site-packages/flwr/simulation/ray_transport/ray_actor.py", line 398, in get_client_result
    return self._fetch_future_result(cid)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/abdenour/miniconda3/envs/fl_with_flower/lib/python3.11/site-packages/flwr/simulation/ray_transport/ray_actor.py", line 279, in _fetch_future_result
    res_cid, out_mssg, updated_context = ray.get(
                                         ^^^^^^^^
  File "/home/abdenour/miniconda3/envs/fl_with_flower/lib/python3.11/site-packages/ray/_private/auto_init_hook.py", line 21, in auto_init_wrapper
    return fn(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^
  File "/home/abdenour/miniconda3/envs/fl_with_flower/lib/python3.11/site-packages/ray/_private/client_mode_hook.py", line 103, in wrapper
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "/home/abdenour/miniconda3/envs/fl_with_flower/lib/python3.11/site-packages/ray/_private/worker.py", line 2667, in get
    values, debugger_breakpoint = worker.get_objects(object_refs, timeout=timeout)
                                  ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/abdenour/miniconda3/envs/fl_with_flower/lib/python3.11/site-packages/ray/_private/worker.py", line 866, in get_objects
    raise value
ray.exceptions.OutOfMemoryError: Task was killed due to the node running low on memory.
Memory on the node (IP: 115.145.171.128, ID: c687613ff34cdbf5097699e5c896055ef2dc44343c5ab44bd40309d5) where the task (actor ID: 85928d3f1a247800081c0e1101000000, name=ClientAppActor.__init__, pid=619614, memory used=1.73GB) was running was 29.50GB / 31.02GB (0.951049), which exceeds the memory usage threshold of 0.95. Ray killed this worker (ID: c7f644ac3cca20dffbfa1723daac4f87f04c7de03da25e7cad5fe33c) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 115.145.171.128`. To see the logs of the worker, use `ray logs worker-c7f644ac3cca20dffbfa1723daac4f87f04c7de03da25e7cad5fe33c*out -ip 115.145.171.128. Top 10 memory users:
PID	MEM(GB)	COMMAND
4942	3.49	/snap/pycharm-professional/408/jbr/bin/java -classpath /snap/pycharm-professional/408/lib/platform-l...
619616	1.80	ray::ClientAppActor.run
619615	1.78	ray::ClientAppActor
619617	1.77	ray::ClientAppActor
619611	1.73	ray::ClientAppActor
619618	1.73	ray::ClientAppActor
619614	1.73	ray::ClientAppActor
619612	1.72	ray::ClientAppActor
619619	1.70	ray::ClientAppActor
619620	1.68	ray::ClientAppActor.run
Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.

[2024-08-13 23:49:48,878][flwr][ERROR] - Task was killed due to the node running low on memory.
Memory on the node (IP: 115.145.171.128, ID: c687613ff34cdbf5097699e5c896055ef2dc44343c5ab44bd40309d5) where the task (actor ID: 85928d3f1a247800081c0e1101000000, name=ClientAppActor.__init__, pid=619614, memory used=1.73GB) was running was 29.50GB / 31.02GB (0.951049), which exceeds the memory usage threshold of 0.95. Ray killed this worker (ID: c7f644ac3cca20dffbfa1723daac4f87f04c7de03da25e7cad5fe33c) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 115.145.171.128`. To see the logs of the worker, use `ray logs worker-c7f644ac3cca20dffbfa1723daac4f87f04c7de03da25e7cad5fe33c*out -ip 115.145.171.128. Top 10 memory users:
PID	MEM(GB)	COMMAND
4942	3.49	/snap/pycharm-professional/408/jbr/bin/java -classpath /snap/pycharm-professional/408/lib/platform-l...
619616	1.80	ray::ClientAppActor.run
619615	1.78	ray::ClientAppActor
619617	1.77	ray::ClientAppActor
619611	1.73	ray::ClientAppActor
619618	1.73	ray::ClientAppActor
619614	1.73	ray::ClientAppActor
619612	1.72	ray::ClientAppActor
619619	1.70	ray::ClientAppActor
619620	1.68	ray::ClientAppActor.run
Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.
[2024-08-13 23:49:51,546][flwr][INFO] - aggregate_fit: received 9 results and 1 failures
[2024-08-13 23:49:54,460][flwr][INFO] - fit progress: (192, 13.567459225654602, {'accuracy': 0.7548076923076923}, 1115.7206858489953)
[2024-08-13 23:49:54,461][flwr][INFO] - configure_evaluate: strategy sampled 2 clients (out of 20)
[2024-08-13 23:49:54,779][flwr][INFO] - aggregate_evaluate: received 2 results and 0 failures
[2024-08-13 23:49:54,779][flwr][INFO] - 
[2024-08-13 23:49:54,779][flwr][INFO] - [ROUND 193]
[2024-08-13 23:49:54,779][flwr][INFO] - configure_fit: strategy sampled 10 clients (out of 20)
[2024-08-13 23:49:55,053][flwr][ERROR] - Traceback (most recent call last):
  File "/home/abdenour/miniconda3/envs/fl_with_flower/lib/python3.11/site-packages/flwr/simulation/ray_transport/ray_client_proxy.py", line 94, in _submit_job
    out_mssg, updated_context = self.actor_pool.get_client_result(
                                ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/abdenour/miniconda3/envs/fl_with_flower/lib/python3.11/site-packages/flwr/simulation/ray_transport/ray_actor.py", line 398, in get_client_result
    return self._fetch_future_result(cid)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/abdenour/miniconda3/envs/fl_with_flower/lib/python3.11/site-packages/flwr/simulation/ray_transport/ray_actor.py", line 279, in _fetch_future_result
    res_cid, out_mssg, updated_context = ray.get(
                                         ^^^^^^^^
  File "/home/abdenour/miniconda3/envs/fl_with_flower/lib/python3.11/site-packages/ray/_private/auto_init_hook.py", line 21, in auto_init_wrapper
    return fn(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^
  File "/home/abdenour/miniconda3/envs/fl_with_flower/lib/python3.11/site-packages/ray/_private/client_mode_hook.py", line 103, in wrapper
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "/home/abdenour/miniconda3/envs/fl_with_flower/lib/python3.11/site-packages/ray/_private/worker.py", line 2667, in get
    values, debugger_breakpoint = worker.get_objects(object_refs, timeout=timeout)
                                  ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/abdenour/miniconda3/envs/fl_with_flower/lib/python3.11/site-packages/ray/_private/worker.py", line 866, in get_objects
    raise value
ray.exceptions.OutOfMemoryError: Task was killed due to the node running low on memory.
Memory on the node (IP: 115.145.171.128, ID: c687613ff34cdbf5097699e5c896055ef2dc44343c5ab44bd40309d5) where the task (actor ID: 85928d3f1a247800081c0e1101000000, name=ClientAppActor.__init__, pid=619614, memory used=1.73GB) was running was 29.50GB / 31.02GB (0.951049), which exceeds the memory usage threshold of 0.95. Ray killed this worker (ID: c7f644ac3cca20dffbfa1723daac4f87f04c7de03da25e7cad5fe33c) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 115.145.171.128`. To see the logs of the worker, use `ray logs worker-c7f644ac3cca20dffbfa1723daac4f87f04c7de03da25e7cad5fe33c*out -ip 115.145.171.128. Top 10 memory users:
PID	MEM(GB)	COMMAND
4942	3.49	/snap/pycharm-professional/408/jbr/bin/java -classpath /snap/pycharm-professional/408/lib/platform-l...
619616	1.80	ray::ClientAppActor.run
619615	1.78	ray::ClientAppActor
619617	1.77	ray::ClientAppActor
619611	1.73	ray::ClientAppActor
619618	1.73	ray::ClientAppActor
619614	1.73	ray::ClientAppActor
619612	1.72	ray::ClientAppActor
619619	1.70	ray::ClientAppActor
619620	1.68	ray::ClientAppActor.run
Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.

[2024-08-13 23:49:55,053][flwr][ERROR] - Task was killed due to the node running low on memory.
Memory on the node (IP: 115.145.171.128, ID: c687613ff34cdbf5097699e5c896055ef2dc44343c5ab44bd40309d5) where the task (actor ID: 85928d3f1a247800081c0e1101000000, name=ClientAppActor.__init__, pid=619614, memory used=1.73GB) was running was 29.50GB / 31.02GB (0.951049), which exceeds the memory usage threshold of 0.95. Ray killed this worker (ID: c7f644ac3cca20dffbfa1723daac4f87f04c7de03da25e7cad5fe33c) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 115.145.171.128`. To see the logs of the worker, use `ray logs worker-c7f644ac3cca20dffbfa1723daac4f87f04c7de03da25e7cad5fe33c*out -ip 115.145.171.128. Top 10 memory users:
PID	MEM(GB)	COMMAND
4942	3.49	/snap/pycharm-professional/408/jbr/bin/java -classpath /snap/pycharm-professional/408/lib/platform-l...
619616	1.80	ray::ClientAppActor.run
619615	1.78	ray::ClientAppActor
619617	1.77	ray::ClientAppActor
619611	1.73	ray::ClientAppActor
619618	1.73	ray::ClientAppActor
619614	1.73	ray::ClientAppActor
619612	1.72	ray::ClientAppActor
619619	1.70	ray::ClientAppActor
619620	1.68	ray::ClientAppActor.run
Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.
[2024-08-13 23:49:57,128][flwr][INFO] - aggregate_fit: received 9 results and 1 failures
[2024-08-13 23:50:00,032][flwr][INFO] - fit progress: (193, 14.784509837627411, {'accuracy': 0.75}, 1121.292588701006)
[2024-08-13 23:50:00,033][flwr][INFO] - configure_evaluate: strategy sampled 2 clients (out of 20)
[2024-08-13 23:50:00,547][flwr][INFO] - aggregate_evaluate: received 2 results and 0 failures
[2024-08-13 23:50:00,547][flwr][INFO] - 
[2024-08-13 23:50:00,547][flwr][INFO] - [ROUND 194]
[2024-08-13 23:50:00,547][flwr][INFO] - configure_fit: strategy sampled 10 clients (out of 20)
[2024-08-13 23:50:00,772][flwr][ERROR] - Traceback (most recent call last):
  File "/home/abdenour/miniconda3/envs/fl_with_flower/lib/python3.11/site-packages/flwr/simulation/ray_transport/ray_client_proxy.py", line 94, in _submit_job
    out_mssg, updated_context = self.actor_pool.get_client_result(
                                ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/abdenour/miniconda3/envs/fl_with_flower/lib/python3.11/site-packages/flwr/simulation/ray_transport/ray_actor.py", line 398, in get_client_result
    return self._fetch_future_result(cid)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/abdenour/miniconda3/envs/fl_with_flower/lib/python3.11/site-packages/flwr/simulation/ray_transport/ray_actor.py", line 279, in _fetch_future_result
    res_cid, out_mssg, updated_context = ray.get(
                                         ^^^^^^^^
  File "/home/abdenour/miniconda3/envs/fl_with_flower/lib/python3.11/site-packages/ray/_private/auto_init_hook.py", line 21, in auto_init_wrapper
    return fn(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^
  File "/home/abdenour/miniconda3/envs/fl_with_flower/lib/python3.11/site-packages/ray/_private/client_mode_hook.py", line 103, in wrapper
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "/home/abdenour/miniconda3/envs/fl_with_flower/lib/python3.11/site-packages/ray/_private/worker.py", line 2667, in get
    values, debugger_breakpoint = worker.get_objects(object_refs, timeout=timeout)
                                  ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/abdenour/miniconda3/envs/fl_with_flower/lib/python3.11/site-packages/ray/_private/worker.py", line 866, in get_objects
    raise value
ray.exceptions.OutOfMemoryError: Task was killed due to the node running low on memory.
Memory on the node (IP: 115.145.171.128, ID: c687613ff34cdbf5097699e5c896055ef2dc44343c5ab44bd40309d5) where the task (actor ID: 85928d3f1a247800081c0e1101000000, name=ClientAppActor.__init__, pid=619614, memory used=1.73GB) was running was 29.50GB / 31.02GB (0.951049), which exceeds the memory usage threshold of 0.95. Ray killed this worker (ID: c7f644ac3cca20dffbfa1723daac4f87f04c7de03da25e7cad5fe33c) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 115.145.171.128`. To see the logs of the worker, use `ray logs worker-c7f644ac3cca20dffbfa1723daac4f87f04c7de03da25e7cad5fe33c*out -ip 115.145.171.128. Top 10 memory users:
PID	MEM(GB)	COMMAND
4942	3.49	/snap/pycharm-professional/408/jbr/bin/java -classpath /snap/pycharm-professional/408/lib/platform-l...
619616	1.80	ray::ClientAppActor.run
619615	1.78	ray::ClientAppActor
619617	1.77	ray::ClientAppActor
619611	1.73	ray::ClientAppActor
619618	1.73	ray::ClientAppActor
619614	1.73	ray::ClientAppActor
619612	1.72	ray::ClientAppActor
619619	1.70	ray::ClientAppActor
619620	1.68	ray::ClientAppActor.run
Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.

[2024-08-13 23:50:00,773][flwr][ERROR] - Task was killed due to the node running low on memory.
Memory on the node (IP: 115.145.171.128, ID: c687613ff34cdbf5097699e5c896055ef2dc44343c5ab44bd40309d5) where the task (actor ID: 85928d3f1a247800081c0e1101000000, name=ClientAppActor.__init__, pid=619614, memory used=1.73GB) was running was 29.50GB / 31.02GB (0.951049), which exceeds the memory usage threshold of 0.95. Ray killed this worker (ID: c7f644ac3cca20dffbfa1723daac4f87f04c7de03da25e7cad5fe33c) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 115.145.171.128`. To see the logs of the worker, use `ray logs worker-c7f644ac3cca20dffbfa1723daac4f87f04c7de03da25e7cad5fe33c*out -ip 115.145.171.128. Top 10 memory users:
PID	MEM(GB)	COMMAND
4942	3.49	/snap/pycharm-professional/408/jbr/bin/java -classpath /snap/pycharm-professional/408/lib/platform-l...
619616	1.80	ray::ClientAppActor.run
619615	1.78	ray::ClientAppActor
619617	1.77	ray::ClientAppActor
619611	1.73	ray::ClientAppActor
619618	1.73	ray::ClientAppActor
619614	1.73	ray::ClientAppActor
619612	1.72	ray::ClientAppActor
619619	1.70	ray::ClientAppActor
619620	1.68	ray::ClientAppActor.run
Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.
[2024-08-13 23:50:02,677][flwr][INFO] - aggregate_fit: received 9 results and 1 failures
[2024-08-13 23:50:05,694][flwr][INFO] - fit progress: (194, 15.68291687965393, {'accuracy': 0.7419871794871795}, 1126.9538927770045)
[2024-08-13 23:50:05,694][flwr][INFO] - configure_evaluate: strategy sampled 2 clients (out of 20)
[2024-08-13 23:50:06,162][flwr][INFO] - aggregate_evaluate: received 2 results and 0 failures
[2024-08-13 23:50:06,163][flwr][INFO] - 
[2024-08-13 23:50:06,163][flwr][INFO] - [ROUND 195]
[2024-08-13 23:50:06,163][flwr][INFO] - configure_fit: strategy sampled 10 clients (out of 20)
[2024-08-13 23:50:06,337][flwr][ERROR] - Traceback (most recent call last):
  File "/home/abdenour/miniconda3/envs/fl_with_flower/lib/python3.11/site-packages/flwr/simulation/ray_transport/ray_client_proxy.py", line 94, in _submit_job
    out_mssg, updated_context = self.actor_pool.get_client_result(
                                ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/abdenour/miniconda3/envs/fl_with_flower/lib/python3.11/site-packages/flwr/simulation/ray_transport/ray_actor.py", line 398, in get_client_result
    return self._fetch_future_result(cid)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/abdenour/miniconda3/envs/fl_with_flower/lib/python3.11/site-packages/flwr/simulation/ray_transport/ray_actor.py", line 279, in _fetch_future_result
    res_cid, out_mssg, updated_context = ray.get(
                                         ^^^^^^^^
  File "/home/abdenour/miniconda3/envs/fl_with_flower/lib/python3.11/site-packages/ray/_private/auto_init_hook.py", line 21, in auto_init_wrapper
    return fn(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^
  File "/home/abdenour/miniconda3/envs/fl_with_flower/lib/python3.11/site-packages/ray/_private/client_mode_hook.py", line 103, in wrapper
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "/home/abdenour/miniconda3/envs/fl_with_flower/lib/python3.11/site-packages/ray/_private/worker.py", line 2667, in get
    values, debugger_breakpoint = worker.get_objects(object_refs, timeout=timeout)
                                  ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/abdenour/miniconda3/envs/fl_with_flower/lib/python3.11/site-packages/ray/_private/worker.py", line 866, in get_objects
    raise value
ray.exceptions.OutOfMemoryError: Task was killed due to the node running low on memory.
Memory on the node (IP: 115.145.171.128, ID: c687613ff34cdbf5097699e5c896055ef2dc44343c5ab44bd40309d5) where the task (actor ID: 85928d3f1a247800081c0e1101000000, name=ClientAppActor.__init__, pid=619614, memory used=1.73GB) was running was 29.50GB / 31.02GB (0.951049), which exceeds the memory usage threshold of 0.95. Ray killed this worker (ID: c7f644ac3cca20dffbfa1723daac4f87f04c7de03da25e7cad5fe33c) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 115.145.171.128`. To see the logs of the worker, use `ray logs worker-c7f644ac3cca20dffbfa1723daac4f87f04c7de03da25e7cad5fe33c*out -ip 115.145.171.128. Top 10 memory users:
PID	MEM(GB)	COMMAND
4942	3.49	/snap/pycharm-professional/408/jbr/bin/java -classpath /snap/pycharm-professional/408/lib/platform-l...
619616	1.80	ray::ClientAppActor.run
619615	1.78	ray::ClientAppActor
619617	1.77	ray::ClientAppActor
619611	1.73	ray::ClientAppActor
619618	1.73	ray::ClientAppActor
619614	1.73	ray::ClientAppActor
619612	1.72	ray::ClientAppActor
619619	1.70	ray::ClientAppActor
619620	1.68	ray::ClientAppActor.run
Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.

[2024-08-13 23:50:06,337][flwr][ERROR] - Task was killed due to the node running low on memory.
Memory on the node (IP: 115.145.171.128, ID: c687613ff34cdbf5097699e5c896055ef2dc44343c5ab44bd40309d5) where the task (actor ID: 85928d3f1a247800081c0e1101000000, name=ClientAppActor.__init__, pid=619614, memory used=1.73GB) was running was 29.50GB / 31.02GB (0.951049), which exceeds the memory usage threshold of 0.95. Ray killed this worker (ID: c7f644ac3cca20dffbfa1723daac4f87f04c7de03da25e7cad5fe33c) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 115.145.171.128`. To see the logs of the worker, use `ray logs worker-c7f644ac3cca20dffbfa1723daac4f87f04c7de03da25e7cad5fe33c*out -ip 115.145.171.128. Top 10 memory users:
PID	MEM(GB)	COMMAND
4942	3.49	/snap/pycharm-professional/408/jbr/bin/java -classpath /snap/pycharm-professional/408/lib/platform-l...
619616	1.80	ray::ClientAppActor.run
619615	1.78	ray::ClientAppActor
619617	1.77	ray::ClientAppActor
619611	1.73	ray::ClientAppActor
619618	1.73	ray::ClientAppActor
619614	1.73	ray::ClientAppActor
619612	1.72	ray::ClientAppActor
619619	1.70	ray::ClientAppActor
619620	1.68	ray::ClientAppActor.run
Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.
[2024-08-13 23:50:08,239][flwr][INFO] - aggregate_fit: received 9 results and 1 failures
[2024-08-13 23:50:11,826][flwr][INFO] - fit progress: (195, 12.837655484676361, {'accuracy': 0.7548076923076923}, 1133.0862333149998)
[2024-08-13 23:50:11,826][flwr][INFO] - configure_evaluate: strategy sampled 2 clients (out of 20)
[2024-08-13 23:50:12,124][flwr][INFO] - aggregate_evaluate: received 2 results and 0 failures
[2024-08-13 23:50:12,124][flwr][INFO] - 
[2024-08-13 23:50:12,124][flwr][INFO] - [ROUND 196]
[2024-08-13 23:50:12,124][flwr][INFO] - configure_fit: strategy sampled 10 clients (out of 20)
[2024-08-13 23:50:12,294][flwr][ERROR] - Traceback (most recent call last):
  File "/home/abdenour/miniconda3/envs/fl_with_flower/lib/python3.11/site-packages/flwr/simulation/ray_transport/ray_client_proxy.py", line 94, in _submit_job
    out_mssg, updated_context = self.actor_pool.get_client_result(
                                ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/abdenour/miniconda3/envs/fl_with_flower/lib/python3.11/site-packages/flwr/simulation/ray_transport/ray_actor.py", line 398, in get_client_result
    return self._fetch_future_result(cid)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/abdenour/miniconda3/envs/fl_with_flower/lib/python3.11/site-packages/flwr/simulation/ray_transport/ray_actor.py", line 279, in _fetch_future_result
    res_cid, out_mssg, updated_context = ray.get(
                                         ^^^^^^^^
  File "/home/abdenour/miniconda3/envs/fl_with_flower/lib/python3.11/site-packages/ray/_private/auto_init_hook.py", line 21, in auto_init_wrapper
    return fn(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^
  File "/home/abdenour/miniconda3/envs/fl_with_flower/lib/python3.11/site-packages/ray/_private/client_mode_hook.py", line 103, in wrapper
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "/home/abdenour/miniconda3/envs/fl_with_flower/lib/python3.11/site-packages/ray/_private/worker.py", line 2667, in get
    values, debugger_breakpoint = worker.get_objects(object_refs, timeout=timeout)
                                  ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/abdenour/miniconda3/envs/fl_with_flower/lib/python3.11/site-packages/ray/_private/worker.py", line 866, in get_objects
    raise value
ray.exceptions.OutOfMemoryError: Task was killed due to the node running low on memory.
Memory on the node (IP: 115.145.171.128, ID: c687613ff34cdbf5097699e5c896055ef2dc44343c5ab44bd40309d5) where the task (actor ID: 85928d3f1a247800081c0e1101000000, name=ClientAppActor.__init__, pid=619614, memory used=1.73GB) was running was 29.50GB / 31.02GB (0.951049), which exceeds the memory usage threshold of 0.95. Ray killed this worker (ID: c7f644ac3cca20dffbfa1723daac4f87f04c7de03da25e7cad5fe33c) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 115.145.171.128`. To see the logs of the worker, use `ray logs worker-c7f644ac3cca20dffbfa1723daac4f87f04c7de03da25e7cad5fe33c*out -ip 115.145.171.128. Top 10 memory users:
PID	MEM(GB)	COMMAND
4942	3.49	/snap/pycharm-professional/408/jbr/bin/java -classpath /snap/pycharm-professional/408/lib/platform-l...
619616	1.80	ray::ClientAppActor.run
619615	1.78	ray::ClientAppActor
619617	1.77	ray::ClientAppActor
619611	1.73	ray::ClientAppActor
619618	1.73	ray::ClientAppActor
619614	1.73	ray::ClientAppActor
619612	1.72	ray::ClientAppActor
619619	1.70	ray::ClientAppActor
619620	1.68	ray::ClientAppActor.run
Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.

[2024-08-13 23:50:12,295][flwr][ERROR] - Task was killed due to the node running low on memory.
Memory on the node (IP: 115.145.171.128, ID: c687613ff34cdbf5097699e5c896055ef2dc44343c5ab44bd40309d5) where the task (actor ID: 85928d3f1a247800081c0e1101000000, name=ClientAppActor.__init__, pid=619614, memory used=1.73GB) was running was 29.50GB / 31.02GB (0.951049), which exceeds the memory usage threshold of 0.95. Ray killed this worker (ID: c7f644ac3cca20dffbfa1723daac4f87f04c7de03da25e7cad5fe33c) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 115.145.171.128`. To see the logs of the worker, use `ray logs worker-c7f644ac3cca20dffbfa1723daac4f87f04c7de03da25e7cad5fe33c*out -ip 115.145.171.128. Top 10 memory users:
PID	MEM(GB)	COMMAND
4942	3.49	/snap/pycharm-professional/408/jbr/bin/java -classpath /snap/pycharm-professional/408/lib/platform-l...
619616	1.80	ray::ClientAppActor.run
619615	1.78	ray::ClientAppActor
619617	1.77	ray::ClientAppActor
619611	1.73	ray::ClientAppActor
619618	1.73	ray::ClientAppActor
619614	1.73	ray::ClientAppActor
619612	1.72	ray::ClientAppActor
619619	1.70	ray::ClientAppActor
619620	1.68	ray::ClientAppActor.run
Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.
[2024-08-13 23:50:14,071][flwr][INFO] - aggregate_fit: received 9 results and 1 failures
[2024-08-13 23:50:17,451][flwr][INFO] - fit progress: (196, 14.54332959651947, {'accuracy': 0.7532051282051282}, 1138.7116920519911)
[2024-08-13 23:50:17,452][flwr][INFO] - configure_evaluate: strategy sampled 2 clients (out of 20)
[2024-08-13 23:50:17,656][flwr][INFO] - aggregate_evaluate: received 2 results and 0 failures
[2024-08-13 23:50:17,656][flwr][INFO] - 
[2024-08-13 23:50:17,656][flwr][INFO] - [ROUND 197]
[2024-08-13 23:50:17,656][flwr][INFO] - configure_fit: strategy sampled 10 clients (out of 20)
[2024-08-13 23:50:17,848][flwr][ERROR] - Traceback (most recent call last):
  File "/home/abdenour/miniconda3/envs/fl_with_flower/lib/python3.11/site-packages/flwr/simulation/ray_transport/ray_client_proxy.py", line 94, in _submit_job
    out_mssg, updated_context = self.actor_pool.get_client_result(
                                ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/abdenour/miniconda3/envs/fl_with_flower/lib/python3.11/site-packages/flwr/simulation/ray_transport/ray_actor.py", line 398, in get_client_result
    return self._fetch_future_result(cid)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/abdenour/miniconda3/envs/fl_with_flower/lib/python3.11/site-packages/flwr/simulation/ray_transport/ray_actor.py", line 279, in _fetch_future_result
    res_cid, out_mssg, updated_context = ray.get(
                                         ^^^^^^^^
  File "/home/abdenour/miniconda3/envs/fl_with_flower/lib/python3.11/site-packages/ray/_private/auto_init_hook.py", line 21, in auto_init_wrapper
    return fn(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^
  File "/home/abdenour/miniconda3/envs/fl_with_flower/lib/python3.11/site-packages/ray/_private/client_mode_hook.py", line 103, in wrapper
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "/home/abdenour/miniconda3/envs/fl_with_flower/lib/python3.11/site-packages/ray/_private/worker.py", line 2667, in get
    values, debugger_breakpoint = worker.get_objects(object_refs, timeout=timeout)
                                  ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/abdenour/miniconda3/envs/fl_with_flower/lib/python3.11/site-packages/ray/_private/worker.py", line 866, in get_objects
    raise value
ray.exceptions.OutOfMemoryError: Task was killed due to the node running low on memory.
Memory on the node (IP: 115.145.171.128, ID: c687613ff34cdbf5097699e5c896055ef2dc44343c5ab44bd40309d5) where the task (actor ID: 85928d3f1a247800081c0e1101000000, name=ClientAppActor.__init__, pid=619614, memory used=1.73GB) was running was 29.50GB / 31.02GB (0.951049), which exceeds the memory usage threshold of 0.95. Ray killed this worker (ID: c7f644ac3cca20dffbfa1723daac4f87f04c7de03da25e7cad5fe33c) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 115.145.171.128`. To see the logs of the worker, use `ray logs worker-c7f644ac3cca20dffbfa1723daac4f87f04c7de03da25e7cad5fe33c*out -ip 115.145.171.128. Top 10 memory users:
PID	MEM(GB)	COMMAND
4942	3.49	/snap/pycharm-professional/408/jbr/bin/java -classpath /snap/pycharm-professional/408/lib/platform-l...
619616	1.80	ray::ClientAppActor.run
619615	1.78	ray::ClientAppActor
619617	1.77	ray::ClientAppActor
619611	1.73	ray::ClientAppActor
619618	1.73	ray::ClientAppActor
619614	1.73	ray::ClientAppActor
619612	1.72	ray::ClientAppActor
619619	1.70	ray::ClientAppActor
619620	1.68	ray::ClientAppActor.run
Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.

[2024-08-13 23:50:17,848][flwr][ERROR] - Task was killed due to the node running low on memory.
Memory on the node (IP: 115.145.171.128, ID: c687613ff34cdbf5097699e5c896055ef2dc44343c5ab44bd40309d5) where the task (actor ID: 85928d3f1a247800081c0e1101000000, name=ClientAppActor.__init__, pid=619614, memory used=1.73GB) was running was 29.50GB / 31.02GB (0.951049), which exceeds the memory usage threshold of 0.95. Ray killed this worker (ID: c7f644ac3cca20dffbfa1723daac4f87f04c7de03da25e7cad5fe33c) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 115.145.171.128`. To see the logs of the worker, use `ray logs worker-c7f644ac3cca20dffbfa1723daac4f87f04c7de03da25e7cad5fe33c*out -ip 115.145.171.128. Top 10 memory users:
PID	MEM(GB)	COMMAND
4942	3.49	/snap/pycharm-professional/408/jbr/bin/java -classpath /snap/pycharm-professional/408/lib/platform-l...
619616	1.80	ray::ClientAppActor.run
619615	1.78	ray::ClientAppActor
619617	1.77	ray::ClientAppActor
619611	1.73	ray::ClientAppActor
619618	1.73	ray::ClientAppActor
619614	1.73	ray::ClientAppActor
619612	1.72	ray::ClientAppActor
619619	1.70	ray::ClientAppActor
619620	1.68	ray::ClientAppActor.run
Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.
[2024-08-13 23:50:19,714][flwr][INFO] - aggregate_fit: received 9 results and 1 failures
[2024-08-13 23:50:23,445][flwr][INFO] - fit progress: (197, 12.827079772949219, {'accuracy': 0.7532051282051282}, 1144.705292610015)
[2024-08-13 23:50:23,445][flwr][INFO] - configure_evaluate: strategy sampled 2 clients (out of 20)
[2024-08-13 23:50:23,671][flwr][INFO] - aggregate_evaluate: received 2 results and 0 failures
[2024-08-13 23:50:23,671][flwr][INFO] - 
[2024-08-13 23:50:23,671][flwr][INFO] - [ROUND 198]
[2024-08-13 23:50:23,671][flwr][INFO] - configure_fit: strategy sampled 10 clients (out of 20)
[2024-08-13 23:50:23,838][flwr][ERROR] - Traceback (most recent call last):
  File "/home/abdenour/miniconda3/envs/fl_with_flower/lib/python3.11/site-packages/flwr/simulation/ray_transport/ray_client_proxy.py", line 94, in _submit_job
    out_mssg, updated_context = self.actor_pool.get_client_result(
                                ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/abdenour/miniconda3/envs/fl_with_flower/lib/python3.11/site-packages/flwr/simulation/ray_transport/ray_actor.py", line 398, in get_client_result
    return self._fetch_future_result(cid)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/abdenour/miniconda3/envs/fl_with_flower/lib/python3.11/site-packages/flwr/simulation/ray_transport/ray_actor.py", line 279, in _fetch_future_result
    res_cid, out_mssg, updated_context = ray.get(
                                         ^^^^^^^^
  File "/home/abdenour/miniconda3/envs/fl_with_flower/lib/python3.11/site-packages/ray/_private/auto_init_hook.py", line 21, in auto_init_wrapper
    return fn(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^
  File "/home/abdenour/miniconda3/envs/fl_with_flower/lib/python3.11/site-packages/ray/_private/client_mode_hook.py", line 103, in wrapper
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "/home/abdenour/miniconda3/envs/fl_with_flower/lib/python3.11/site-packages/ray/_private/worker.py", line 2667, in get
    values, debugger_breakpoint = worker.get_objects(object_refs, timeout=timeout)
                                  ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/abdenour/miniconda3/envs/fl_with_flower/lib/python3.11/site-packages/ray/_private/worker.py", line 866, in get_objects
    raise value
ray.exceptions.OutOfMemoryError: Task was killed due to the node running low on memory.
Memory on the node (IP: 115.145.171.128, ID: c687613ff34cdbf5097699e5c896055ef2dc44343c5ab44bd40309d5) where the task (actor ID: 85928d3f1a247800081c0e1101000000, name=ClientAppActor.__init__, pid=619614, memory used=1.73GB) was running was 29.50GB / 31.02GB (0.951049), which exceeds the memory usage threshold of 0.95. Ray killed this worker (ID: c7f644ac3cca20dffbfa1723daac4f87f04c7de03da25e7cad5fe33c) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 115.145.171.128`. To see the logs of the worker, use `ray logs worker-c7f644ac3cca20dffbfa1723daac4f87f04c7de03da25e7cad5fe33c*out -ip 115.145.171.128. Top 10 memory users:
PID	MEM(GB)	COMMAND
4942	3.49	/snap/pycharm-professional/408/jbr/bin/java -classpath /snap/pycharm-professional/408/lib/platform-l...
619616	1.80	ray::ClientAppActor.run
619615	1.78	ray::ClientAppActor
619617	1.77	ray::ClientAppActor
619611	1.73	ray::ClientAppActor
619618	1.73	ray::ClientAppActor
619614	1.73	ray::ClientAppActor
619612	1.72	ray::ClientAppActor
619619	1.70	ray::ClientAppActor
619620	1.68	ray::ClientAppActor.run
Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.

[2024-08-13 23:50:23,838][flwr][ERROR] - Task was killed due to the node running low on memory.
Memory on the node (IP: 115.145.171.128, ID: c687613ff34cdbf5097699e5c896055ef2dc44343c5ab44bd40309d5) where the task (actor ID: 85928d3f1a247800081c0e1101000000, name=ClientAppActor.__init__, pid=619614, memory used=1.73GB) was running was 29.50GB / 31.02GB (0.951049), which exceeds the memory usage threshold of 0.95. Ray killed this worker (ID: c7f644ac3cca20dffbfa1723daac4f87f04c7de03da25e7cad5fe33c) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 115.145.171.128`. To see the logs of the worker, use `ray logs worker-c7f644ac3cca20dffbfa1723daac4f87f04c7de03da25e7cad5fe33c*out -ip 115.145.171.128. Top 10 memory users:
PID	MEM(GB)	COMMAND
4942	3.49	/snap/pycharm-professional/408/jbr/bin/java -classpath /snap/pycharm-professional/408/lib/platform-l...
619616	1.80	ray::ClientAppActor.run
619615	1.78	ray::ClientAppActor
619617	1.77	ray::ClientAppActor
619611	1.73	ray::ClientAppActor
619618	1.73	ray::ClientAppActor
619614	1.73	ray::ClientAppActor
619612	1.72	ray::ClientAppActor
619619	1.70	ray::ClientAppActor
619620	1.68	ray::ClientAppActor.run
Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.
[2024-08-13 23:50:25,662][flwr][INFO] - aggregate_fit: received 9 results and 1 failures
[2024-08-13 23:50:28,813][flwr][INFO] - fit progress: (198, 12.739644050598145, {'accuracy': 0.7628205128205128}, 1150.0728376590123)
[2024-08-13 23:50:28,813][flwr][INFO] - configure_evaluate: strategy sampled 2 clients (out of 20)
[2024-08-13 23:50:29,231][flwr][INFO] - aggregate_evaluate: received 2 results and 0 failures
[2024-08-13 23:50:29,231][flwr][INFO] - 
[2024-08-13 23:50:29,231][flwr][INFO] - [ROUND 199]
[2024-08-13 23:50:29,231][flwr][INFO] - configure_fit: strategy sampled 10 clients (out of 20)
[2024-08-13 23:50:29,405][flwr][ERROR] - Traceback (most recent call last):
  File "/home/abdenour/miniconda3/envs/fl_with_flower/lib/python3.11/site-packages/flwr/simulation/ray_transport/ray_client_proxy.py", line 94, in _submit_job
    out_mssg, updated_context = self.actor_pool.get_client_result(
                                ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/abdenour/miniconda3/envs/fl_with_flower/lib/python3.11/site-packages/flwr/simulation/ray_transport/ray_actor.py", line 398, in get_client_result
    return self._fetch_future_result(cid)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/abdenour/miniconda3/envs/fl_with_flower/lib/python3.11/site-packages/flwr/simulation/ray_transport/ray_actor.py", line 279, in _fetch_future_result
    res_cid, out_mssg, updated_context = ray.get(
                                         ^^^^^^^^
  File "/home/abdenour/miniconda3/envs/fl_with_flower/lib/python3.11/site-packages/ray/_private/auto_init_hook.py", line 21, in auto_init_wrapper
    return fn(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^
  File "/home/abdenour/miniconda3/envs/fl_with_flower/lib/python3.11/site-packages/ray/_private/client_mode_hook.py", line 103, in wrapper
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "/home/abdenour/miniconda3/envs/fl_with_flower/lib/python3.11/site-packages/ray/_private/worker.py", line 2667, in get
    values, debugger_breakpoint = worker.get_objects(object_refs, timeout=timeout)
                                  ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/abdenour/miniconda3/envs/fl_with_flower/lib/python3.11/site-packages/ray/_private/worker.py", line 866, in get_objects
    raise value
ray.exceptions.OutOfMemoryError: Task was killed due to the node running low on memory.
Memory on the node (IP: 115.145.171.128, ID: c687613ff34cdbf5097699e5c896055ef2dc44343c5ab44bd40309d5) where the task (actor ID: 85928d3f1a247800081c0e1101000000, name=ClientAppActor.__init__, pid=619614, memory used=1.73GB) was running was 29.50GB / 31.02GB (0.951049), which exceeds the memory usage threshold of 0.95. Ray killed this worker (ID: c7f644ac3cca20dffbfa1723daac4f87f04c7de03da25e7cad5fe33c) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 115.145.171.128`. To see the logs of the worker, use `ray logs worker-c7f644ac3cca20dffbfa1723daac4f87f04c7de03da25e7cad5fe33c*out -ip 115.145.171.128. Top 10 memory users:
PID	MEM(GB)	COMMAND
4942	3.49	/snap/pycharm-professional/408/jbr/bin/java -classpath /snap/pycharm-professional/408/lib/platform-l...
619616	1.80	ray::ClientAppActor.run
619615	1.78	ray::ClientAppActor
619617	1.77	ray::ClientAppActor
619611	1.73	ray::ClientAppActor
619618	1.73	ray::ClientAppActor
619614	1.73	ray::ClientAppActor
619612	1.72	ray::ClientAppActor
619619	1.70	ray::ClientAppActor
619620	1.68	ray::ClientAppActor.run
Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.

[2024-08-13 23:50:29,405][flwr][ERROR] - Task was killed due to the node running low on memory.
Memory on the node (IP: 115.145.171.128, ID: c687613ff34cdbf5097699e5c896055ef2dc44343c5ab44bd40309d5) where the task (actor ID: 85928d3f1a247800081c0e1101000000, name=ClientAppActor.__init__, pid=619614, memory used=1.73GB) was running was 29.50GB / 31.02GB (0.951049), which exceeds the memory usage threshold of 0.95. Ray killed this worker (ID: c7f644ac3cca20dffbfa1723daac4f87f04c7de03da25e7cad5fe33c) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 115.145.171.128`. To see the logs of the worker, use `ray logs worker-c7f644ac3cca20dffbfa1723daac4f87f04c7de03da25e7cad5fe33c*out -ip 115.145.171.128. Top 10 memory users:
PID	MEM(GB)	COMMAND
4942	3.49	/snap/pycharm-professional/408/jbr/bin/java -classpath /snap/pycharm-professional/408/lib/platform-l...
619616	1.80	ray::ClientAppActor.run
619615	1.78	ray::ClientAppActor
619617	1.77	ray::ClientAppActor
619611	1.73	ray::ClientAppActor
619618	1.73	ray::ClientAppActor
619614	1.73	ray::ClientAppActor
619612	1.72	ray::ClientAppActor
619619	1.70	ray::ClientAppActor
619620	1.68	ray::ClientAppActor.run
Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.
[2024-08-13 23:50:31,287][flwr][INFO] - aggregate_fit: received 9 results and 1 failures
[2024-08-13 23:50:34,346][flwr][INFO] - fit progress: (199, 15.992461919784546, {'accuracy': 0.7387820512820513}, 1155.6065416900092)
[2024-08-13 23:50:34,346][flwr][INFO] - configure_evaluate: strategy sampled 2 clients (out of 20)
[2024-08-13 23:50:34,543][flwr][INFO] - aggregate_evaluate: received 2 results and 0 failures
[2024-08-13 23:50:34,543][flwr][INFO] - 
[2024-08-13 23:50:34,543][flwr][INFO] - [ROUND 200]
[2024-08-13 23:50:34,543][flwr][INFO] - configure_fit: strategy sampled 10 clients (out of 20)
[2024-08-13 23:50:34,729][flwr][ERROR] - Traceback (most recent call last):
  File "/home/abdenour/miniconda3/envs/fl_with_flower/lib/python3.11/site-packages/flwr/simulation/ray_transport/ray_client_proxy.py", line 94, in _submit_job
    out_mssg, updated_context = self.actor_pool.get_client_result(
                                ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/abdenour/miniconda3/envs/fl_with_flower/lib/python3.11/site-packages/flwr/simulation/ray_transport/ray_actor.py", line 398, in get_client_result
    return self._fetch_future_result(cid)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/abdenour/miniconda3/envs/fl_with_flower/lib/python3.11/site-packages/flwr/simulation/ray_transport/ray_actor.py", line 279, in _fetch_future_result
    res_cid, out_mssg, updated_context = ray.get(
                                         ^^^^^^^^
  File "/home/abdenour/miniconda3/envs/fl_with_flower/lib/python3.11/site-packages/ray/_private/auto_init_hook.py", line 21, in auto_init_wrapper
    return fn(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^
  File "/home/abdenour/miniconda3/envs/fl_with_flower/lib/python3.11/site-packages/ray/_private/client_mode_hook.py", line 103, in wrapper
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "/home/abdenour/miniconda3/envs/fl_with_flower/lib/python3.11/site-packages/ray/_private/worker.py", line 2667, in get
    values, debugger_breakpoint = worker.get_objects(object_refs, timeout=timeout)
                                  ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/abdenour/miniconda3/envs/fl_with_flower/lib/python3.11/site-packages/ray/_private/worker.py", line 866, in get_objects
    raise value
ray.exceptions.OutOfMemoryError: Task was killed due to the node running low on memory.
Memory on the node (IP: 115.145.171.128, ID: c687613ff34cdbf5097699e5c896055ef2dc44343c5ab44bd40309d5) where the task (actor ID: 85928d3f1a247800081c0e1101000000, name=ClientAppActor.__init__, pid=619614, memory used=1.73GB) was running was 29.50GB / 31.02GB (0.951049), which exceeds the memory usage threshold of 0.95. Ray killed this worker (ID: c7f644ac3cca20dffbfa1723daac4f87f04c7de03da25e7cad5fe33c) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 115.145.171.128`. To see the logs of the worker, use `ray logs worker-c7f644ac3cca20dffbfa1723daac4f87f04c7de03da25e7cad5fe33c*out -ip 115.145.171.128. Top 10 memory users:
PID	MEM(GB)	COMMAND
4942	3.49	/snap/pycharm-professional/408/jbr/bin/java -classpath /snap/pycharm-professional/408/lib/platform-l...
619616	1.80	ray::ClientAppActor.run
619615	1.78	ray::ClientAppActor
619617	1.77	ray::ClientAppActor
619611	1.73	ray::ClientAppActor
619618	1.73	ray::ClientAppActor
619614	1.73	ray::ClientAppActor
619612	1.72	ray::ClientAppActor
619619	1.70	ray::ClientAppActor
619620	1.68	ray::ClientAppActor.run
Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.

[2024-08-13 23:50:34,729][flwr][ERROR] - Task was killed due to the node running low on memory.
Memory on the node (IP: 115.145.171.128, ID: c687613ff34cdbf5097699e5c896055ef2dc44343c5ab44bd40309d5) where the task (actor ID: 85928d3f1a247800081c0e1101000000, name=ClientAppActor.__init__, pid=619614, memory used=1.73GB) was running was 29.50GB / 31.02GB (0.951049), which exceeds the memory usage threshold of 0.95. Ray killed this worker (ID: c7f644ac3cca20dffbfa1723daac4f87f04c7de03da25e7cad5fe33c) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 115.145.171.128`. To see the logs of the worker, use `ray logs worker-c7f644ac3cca20dffbfa1723daac4f87f04c7de03da25e7cad5fe33c*out -ip 115.145.171.128. Top 10 memory users:
PID	MEM(GB)	COMMAND
4942	3.49	/snap/pycharm-professional/408/jbr/bin/java -classpath /snap/pycharm-professional/408/lib/platform-l...
619616	1.80	ray::ClientAppActor.run
619615	1.78	ray::ClientAppActor
619617	1.77	ray::ClientAppActor
619611	1.73	ray::ClientAppActor
619618	1.73	ray::ClientAppActor
619614	1.73	ray::ClientAppActor
619612	1.72	ray::ClientAppActor
619619	1.70	ray::ClientAppActor
619620	1.68	ray::ClientAppActor.run
Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.
[2024-08-13 23:50:36,511][flwr][INFO] - aggregate_fit: received 9 results and 1 failures
[2024-08-13 23:50:39,646][flwr][INFO] - fit progress: (200, 15.587242364883423, {'accuracy': 0.7467948717948718}, 1160.9060849889938)
[2024-08-13 23:50:39,646][flwr][INFO] - configure_evaluate: strategy sampled 2 clients (out of 20)
[2024-08-13 23:50:39,845][flwr][INFO] - aggregate_evaluate: received 2 results and 0 failures
[2024-08-13 23:50:39,845][flwr][INFO] - 
[2024-08-13 23:50:39,845][flwr][INFO] - [ROUND 201]
[2024-08-13 23:50:39,845][flwr][INFO] - configure_fit: strategy sampled 10 clients (out of 20)
[2024-08-13 23:50:40,019][flwr][ERROR] - Traceback (most recent call last):
  File "/home/abdenour/miniconda3/envs/fl_with_flower/lib/python3.11/site-packages/flwr/simulation/ray_transport/ray_client_proxy.py", line 94, in _submit_job
    out_mssg, updated_context = self.actor_pool.get_client_result(
                                ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/abdenour/miniconda3/envs/fl_with_flower/lib/python3.11/site-packages/flwr/simulation/ray_transport/ray_actor.py", line 398, in get_client_result
    return self._fetch_future_result(cid)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/abdenour/miniconda3/envs/fl_with_flower/lib/python3.11/site-packages/flwr/simulation/ray_transport/ray_actor.py", line 279, in _fetch_future_result
    res_cid, out_mssg, updated_context = ray.get(
                                         ^^^^^^^^
  File "/home/abdenour/miniconda3/envs/fl_with_flower/lib/python3.11/site-packages/ray/_private/auto_init_hook.py", line 21, in auto_init_wrapper
    return fn(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^
  File "/home/abdenour/miniconda3/envs/fl_with_flower/lib/python3.11/site-packages/ray/_private/client_mode_hook.py", line 103, in wrapper
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "/home/abdenour/miniconda3/envs/fl_with_flower/lib/python3.11/site-packages/ray/_private/worker.py", line 2667, in get
    values, debugger_breakpoint = worker.get_objects(object_refs, timeout=timeout)
                                  ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/abdenour/miniconda3/envs/fl_with_flower/lib/python3.11/site-packages/ray/_private/worker.py", line 866, in get_objects
    raise value
ray.exceptions.OutOfMemoryError: Task was killed due to the node running low on memory.
Memory on the node (IP: 115.145.171.128, ID: c687613ff34cdbf5097699e5c896055ef2dc44343c5ab44bd40309d5) where the task (actor ID: 85928d3f1a247800081c0e1101000000, name=ClientAppActor.__init__, pid=619614, memory used=1.73GB) was running was 29.50GB / 31.02GB (0.951049), which exceeds the memory usage threshold of 0.95. Ray killed this worker (ID: c7f644ac3cca20dffbfa1723daac4f87f04c7de03da25e7cad5fe33c) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 115.145.171.128`. To see the logs of the worker, use `ray logs worker-c7f644ac3cca20dffbfa1723daac4f87f04c7de03da25e7cad5fe33c*out -ip 115.145.171.128. Top 10 memory users:
PID	MEM(GB)	COMMAND
4942	3.49	/snap/pycharm-professional/408/jbr/bin/java -classpath /snap/pycharm-professional/408/lib/platform-l...
619616	1.80	ray::ClientAppActor.run
619615	1.78	ray::ClientAppActor
619617	1.77	ray::ClientAppActor
619611	1.73	ray::ClientAppActor
619618	1.73	ray::ClientAppActor
619614	1.73	ray::ClientAppActor
619612	1.72	ray::ClientAppActor
619619	1.70	ray::ClientAppActor
619620	1.68	ray::ClientAppActor.run
Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.

[2024-08-13 23:50:40,019][flwr][ERROR] - Task was killed due to the node running low on memory.
Memory on the node (IP: 115.145.171.128, ID: c687613ff34cdbf5097699e5c896055ef2dc44343c5ab44bd40309d5) where the task (actor ID: 85928d3f1a247800081c0e1101000000, name=ClientAppActor.__init__, pid=619614, memory used=1.73GB) was running was 29.50GB / 31.02GB (0.951049), which exceeds the memory usage threshold of 0.95. Ray killed this worker (ID: c7f644ac3cca20dffbfa1723daac4f87f04c7de03da25e7cad5fe33c) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 115.145.171.128`. To see the logs of the worker, use `ray logs worker-c7f644ac3cca20dffbfa1723daac4f87f04c7de03da25e7cad5fe33c*out -ip 115.145.171.128. Top 10 memory users:
PID	MEM(GB)	COMMAND
4942	3.49	/snap/pycharm-professional/408/jbr/bin/java -classpath /snap/pycharm-professional/408/lib/platform-l...
619616	1.80	ray::ClientAppActor.run
619615	1.78	ray::ClientAppActor
619617	1.77	ray::ClientAppActor
619611	1.73	ray::ClientAppActor
619618	1.73	ray::ClientAppActor
619614	1.73	ray::ClientAppActor
619612	1.72	ray::ClientAppActor
619619	1.70	ray::ClientAppActor
619620	1.68	ray::ClientAppActor.run
Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.
[2024-08-13 23:50:41,915][flwr][INFO] - aggregate_fit: received 9 results and 1 failures
[2024-08-13 23:50:45,124][flwr][INFO] - fit progress: (201, 14.418637156486511, {'accuracy': 0.7451923076923077}, 1166.3839732579945)
[2024-08-13 23:50:45,124][flwr][INFO] - configure_evaluate: strategy sampled 2 clients (out of 20)
[2024-08-13 23:50:45,471][flwr][INFO] - aggregate_evaluate: received 2 results and 0 failures
[2024-08-13 23:50:45,471][flwr][INFO] - 
[2024-08-13 23:50:45,471][flwr][INFO] - [ROUND 202]
[2024-08-13 23:50:45,471][flwr][INFO] - configure_fit: strategy sampled 10 clients (out of 20)
[2024-08-13 23:50:45,639][flwr][ERROR] - Traceback (most recent call last):
  File "/home/abdenour/miniconda3/envs/fl_with_flower/lib/python3.11/site-packages/flwr/simulation/ray_transport/ray_client_proxy.py", line 94, in _submit_job
    out_mssg, updated_context = self.actor_pool.get_client_result(
                                ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/abdenour/miniconda3/envs/fl_with_flower/lib/python3.11/site-packages/flwr/simulation/ray_transport/ray_actor.py", line 398, in get_client_result
    return self._fetch_future_result(cid)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/abdenour/miniconda3/envs/fl_with_flower/lib/python3.11/site-packages/flwr/simulation/ray_transport/ray_actor.py", line 279, in _fetch_future_result
    res_cid, out_mssg, updated_context = ray.get(
                                         ^^^^^^^^
  File "/home/abdenour/miniconda3/envs/fl_with_flower/lib/python3.11/site-packages/ray/_private/auto_init_hook.py", line 21, in auto_init_wrapper
    return fn(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^
  File "/home/abdenour/miniconda3/envs/fl_with_flower/lib/python3.11/site-packages/ray/_private/client_mode_hook.py", line 103, in wrapper
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "/home/abdenour/miniconda3/envs/fl_with_flower/lib/python3.11/site-packages/ray/_private/worker.py", line 2667, in get
    values, debugger_breakpoint = worker.get_objects(object_refs, timeout=timeout)
                                  ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/abdenour/miniconda3/envs/fl_with_flower/lib/python3.11/site-packages/ray/_private/worker.py", line 866, in get_objects
    raise value
ray.exceptions.OutOfMemoryError: Task was killed due to the node running low on memory.
Memory on the node (IP: 115.145.171.128, ID: c687613ff34cdbf5097699e5c896055ef2dc44343c5ab44bd40309d5) where the task (actor ID: 85928d3f1a247800081c0e1101000000, name=ClientAppActor.__init__, pid=619614, memory used=1.73GB) was running was 29.50GB / 31.02GB (0.951049), which exceeds the memory usage threshold of 0.95. Ray killed this worker (ID: c7f644ac3cca20dffbfa1723daac4f87f04c7de03da25e7cad5fe33c) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 115.145.171.128`. To see the logs of the worker, use `ray logs worker-c7f644ac3cca20dffbfa1723daac4f87f04c7de03da25e7cad5fe33c*out -ip 115.145.171.128. Top 10 memory users:
PID	MEM(GB)	COMMAND
4942	3.49	/snap/pycharm-professional/408/jbr/bin/java -classpath /snap/pycharm-professional/408/lib/platform-l...
619616	1.80	ray::ClientAppActor.run
619615	1.78	ray::ClientAppActor
619617	1.77	ray::ClientAppActor
619611	1.73	ray::ClientAppActor
619618	1.73	ray::ClientAppActor
619614	1.73	ray::ClientAppActor
619612	1.72	ray::ClientAppActor
619619	1.70	ray::ClientAppActor
619620	1.68	ray::ClientAppActor.run
Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.

[2024-08-13 23:50:45,639][flwr][ERROR] - Task was killed due to the node running low on memory.
Memory on the node (IP: 115.145.171.128, ID: c687613ff34cdbf5097699e5c896055ef2dc44343c5ab44bd40309d5) where the task (actor ID: 85928d3f1a247800081c0e1101000000, name=ClientAppActor.__init__, pid=619614, memory used=1.73GB) was running was 29.50GB / 31.02GB (0.951049), which exceeds the memory usage threshold of 0.95. Ray killed this worker (ID: c7f644ac3cca20dffbfa1723daac4f87f04c7de03da25e7cad5fe33c) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 115.145.171.128`. To see the logs of the worker, use `ray logs worker-c7f644ac3cca20dffbfa1723daac4f87f04c7de03da25e7cad5fe33c*out -ip 115.145.171.128. Top 10 memory users:
PID	MEM(GB)	COMMAND
4942	3.49	/snap/pycharm-professional/408/jbr/bin/java -classpath /snap/pycharm-professional/408/lib/platform-l...
619616	1.80	ray::ClientAppActor.run
619615	1.78	ray::ClientAppActor
619617	1.77	ray::ClientAppActor
619611	1.73	ray::ClientAppActor
619618	1.73	ray::ClientAppActor
619614	1.73	ray::ClientAppActor
619612	1.72	ray::ClientAppActor
619619	1.70	ray::ClientAppActor
619620	1.68	ray::ClientAppActor.run
Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.
[2024-08-13 23:50:47,484][flwr][INFO] - aggregate_fit: received 9 results and 1 failures
[2024-08-13 23:50:50,755][flwr][INFO] - fit progress: (202, 15.153498828411102, {'accuracy': 0.7516025641025641}, 1172.015611996001)
[2024-08-13 23:50:50,755][flwr][INFO] - configure_evaluate: strategy sampled 2 clients (out of 20)
[2024-08-13 23:50:51,002][flwr][INFO] - aggregate_evaluate: received 2 results and 0 failures
[2024-08-13 23:50:51,002][flwr][INFO] - 
[2024-08-13 23:50:51,002][flwr][INFO] - [ROUND 203]
[2024-08-13 23:50:51,002][flwr][INFO] - configure_fit: strategy sampled 10 clients (out of 20)
[2024-08-13 23:50:51,171][flwr][ERROR] - Traceback (most recent call last):
  File "/home/abdenour/miniconda3/envs/fl_with_flower/lib/python3.11/site-packages/flwr/simulation/ray_transport/ray_client_proxy.py", line 94, in _submit_job
    out_mssg, updated_context = self.actor_pool.get_client_result(
                                ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/abdenour/miniconda3/envs/fl_with_flower/lib/python3.11/site-packages/flwr/simulation/ray_transport/ray_actor.py", line 398, in get_client_result
    return self._fetch_future_result(cid)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/abdenour/miniconda3/envs/fl_with_flower/lib/python3.11/site-packages/flwr/simulation/ray_transport/ray_actor.py", line 279, in _fetch_future_result
    res_cid, out_mssg, updated_context = ray.get(
                                         ^^^^^^^^
  File "/home/abdenour/miniconda3/envs/fl_with_flower/lib/python3.11/site-packages/ray/_private/auto_init_hook.py", line 21, in auto_init_wrapper
    return fn(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^
  File "/home/abdenour/miniconda3/envs/fl_with_flower/lib/python3.11/site-packages/ray/_private/client_mode_hook.py", line 103, in wrapper
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "/home/abdenour/miniconda3/envs/fl_with_flower/lib/python3.11/site-packages/ray/_private/worker.py", line 2667, in get
    values, debugger_breakpoint = worker.get_objects(object_refs, timeout=timeout)
                                  ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/abdenour/miniconda3/envs/fl_with_flower/lib/python3.11/site-packages/ray/_private/worker.py", line 866, in get_objects
    raise value
ray.exceptions.OutOfMemoryError: Task was killed due to the node running low on memory.
Memory on the node (IP: 115.145.171.128, ID: c687613ff34cdbf5097699e5c896055ef2dc44343c5ab44bd40309d5) where the task (actor ID: 85928d3f1a247800081c0e1101000000, name=ClientAppActor.__init__, pid=619614, memory used=1.73GB) was running was 29.50GB / 31.02GB (0.951049), which exceeds the memory usage threshold of 0.95. Ray killed this worker (ID: c7f644ac3cca20dffbfa1723daac4f87f04c7de03da25e7cad5fe33c) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 115.145.171.128`. To see the logs of the worker, use `ray logs worker-c7f644ac3cca20dffbfa1723daac4f87f04c7de03da25e7cad5fe33c*out -ip 115.145.171.128. Top 10 memory users:
PID	MEM(GB)	COMMAND
4942	3.49	/snap/pycharm-professional/408/jbr/bin/java -classpath /snap/pycharm-professional/408/lib/platform-l...
619616	1.80	ray::ClientAppActor.run
619615	1.78	ray::ClientAppActor
619617	1.77	ray::ClientAppActor
619611	1.73	ray::ClientAppActor
619618	1.73	ray::ClientAppActor
619614	1.73	ray::ClientAppActor
619612	1.72	ray::ClientAppActor
619619	1.70	ray::ClientAppActor
619620	1.68	ray::ClientAppActor.run
Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.

[2024-08-13 23:50:51,171][flwr][ERROR] - Task was killed due to the node running low on memory.
Memory on the node (IP: 115.145.171.128, ID: c687613ff34cdbf5097699e5c896055ef2dc44343c5ab44bd40309d5) where the task (actor ID: 85928d3f1a247800081c0e1101000000, name=ClientAppActor.__init__, pid=619614, memory used=1.73GB) was running was 29.50GB / 31.02GB (0.951049), which exceeds the memory usage threshold of 0.95. Ray killed this worker (ID: c7f644ac3cca20dffbfa1723daac4f87f04c7de03da25e7cad5fe33c) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 115.145.171.128`. To see the logs of the worker, use `ray logs worker-c7f644ac3cca20dffbfa1723daac4f87f04c7de03da25e7cad5fe33c*out -ip 115.145.171.128. Top 10 memory users:
PID	MEM(GB)	COMMAND
4942	3.49	/snap/pycharm-professional/408/jbr/bin/java -classpath /snap/pycharm-professional/408/lib/platform-l...
619616	1.80	ray::ClientAppActor.run
619615	1.78	ray::ClientAppActor
619617	1.77	ray::ClientAppActor
619611	1.73	ray::ClientAppActor
619618	1.73	ray::ClientAppActor
619614	1.73	ray::ClientAppActor
619612	1.72	ray::ClientAppActor
619619	1.70	ray::ClientAppActor
619620	1.68	ray::ClientAppActor.run
Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.
[2024-08-13 23:50:53,070][flwr][INFO] - aggregate_fit: received 9 results and 1 failures
[2024-08-13 23:50:55,909][flwr][INFO] - fit progress: (203, 15.243372201919556, {'accuracy': 0.75}, 1177.1690940900007)
[2024-08-13 23:50:55,909][flwr][INFO] - configure_evaluate: strategy sampled 2 clients (out of 20)
[2024-08-13 23:50:56,168][flwr][INFO] - aggregate_evaluate: received 2 results and 0 failures
[2024-08-13 23:50:56,168][flwr][INFO] - 
[2024-08-13 23:50:56,168][flwr][INFO] - [ROUND 204]
[2024-08-13 23:50:56,168][flwr][INFO] - configure_fit: strategy sampled 10 clients (out of 20)
[2024-08-13 23:50:56,332][flwr][ERROR] - Traceback (most recent call last):
  File "/home/abdenour/miniconda3/envs/fl_with_flower/lib/python3.11/site-packages/flwr/simulation/ray_transport/ray_client_proxy.py", line 94, in _submit_job
    out_mssg, updated_context = self.actor_pool.get_client_result(
                                ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/abdenour/miniconda3/envs/fl_with_flower/lib/python3.11/site-packages/flwr/simulation/ray_transport/ray_actor.py", line 398, in get_client_result
    return self._fetch_future_result(cid)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/abdenour/miniconda3/envs/fl_with_flower/lib/python3.11/site-packages/flwr/simulation/ray_transport/ray_actor.py", line 279, in _fetch_future_result
    res_cid, out_mssg, updated_context = ray.get(
                                         ^^^^^^^^
  File "/home/abdenour/miniconda3/envs/fl_with_flower/lib/python3.11/site-packages/ray/_private/auto_init_hook.py", line 21, in auto_init_wrapper
    return fn(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^
  File "/home/abdenour/miniconda3/envs/fl_with_flower/lib/python3.11/site-packages/ray/_private/client_mode_hook.py", line 103, in wrapper
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "/home/abdenour/miniconda3/envs/fl_with_flower/lib/python3.11/site-packages/ray/_private/worker.py", line 2667, in get
    values, debugger_breakpoint = worker.get_objects(object_refs, timeout=timeout)
                                  ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/abdenour/miniconda3/envs/fl_with_flower/lib/python3.11/site-packages/ray/_private/worker.py", line 866, in get_objects
    raise value
ray.exceptions.OutOfMemoryError: Task was killed due to the node running low on memory.
Memory on the node (IP: 115.145.171.128, ID: c687613ff34cdbf5097699e5c896055ef2dc44343c5ab44bd40309d5) where the task (actor ID: 85928d3f1a247800081c0e1101000000, name=ClientAppActor.__init__, pid=619614, memory used=1.73GB) was running was 29.50GB / 31.02GB (0.951049), which exceeds the memory usage threshold of 0.95. Ray killed this worker (ID: c7f644ac3cca20dffbfa1723daac4f87f04c7de03da25e7cad5fe33c) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 115.145.171.128`. To see the logs of the worker, use `ray logs worker-c7f644ac3cca20dffbfa1723daac4f87f04c7de03da25e7cad5fe33c*out -ip 115.145.171.128. Top 10 memory users:
PID	MEM(GB)	COMMAND
4942	3.49	/snap/pycharm-professional/408/jbr/bin/java -classpath /snap/pycharm-professional/408/lib/platform-l...
619616	1.80	ray::ClientAppActor.run
619615	1.78	ray::ClientAppActor
619617	1.77	ray::ClientAppActor
619611	1.73	ray::ClientAppActor
619618	1.73	ray::ClientAppActor
619614	1.73	ray::ClientAppActor
619612	1.72	ray::ClientAppActor
619619	1.70	ray::ClientAppActor
619620	1.68	ray::ClientAppActor.run
Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.

[2024-08-13 23:50:56,332][flwr][ERROR] - Task was killed due to the node running low on memory.
Memory on the node (IP: 115.145.171.128, ID: c687613ff34cdbf5097699e5c896055ef2dc44343c5ab44bd40309d5) where the task (actor ID: 85928d3f1a247800081c0e1101000000, name=ClientAppActor.__init__, pid=619614, memory used=1.73GB) was running was 29.50GB / 31.02GB (0.951049), which exceeds the memory usage threshold of 0.95. Ray killed this worker (ID: c7f644ac3cca20dffbfa1723daac4f87f04c7de03da25e7cad5fe33c) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 115.145.171.128`. To see the logs of the worker, use `ray logs worker-c7f644ac3cca20dffbfa1723daac4f87f04c7de03da25e7cad5fe33c*out -ip 115.145.171.128. Top 10 memory users:
PID	MEM(GB)	COMMAND
4942	3.49	/snap/pycharm-professional/408/jbr/bin/java -classpath /snap/pycharm-professional/408/lib/platform-l...
619616	1.80	ray::ClientAppActor.run
619615	1.78	ray::ClientAppActor
619617	1.77	ray::ClientAppActor
619611	1.73	ray::ClientAppActor
619618	1.73	ray::ClientAppActor
619614	1.73	ray::ClientAppActor
619612	1.72	ray::ClientAppActor
619619	1.70	ray::ClientAppActor
619620	1.68	ray::ClientAppActor.run
Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.
[2024-08-13 23:50:58,194][flwr][INFO] - aggregate_fit: received 9 results and 1 failures
[2024-08-13 23:51:01,009][flwr][INFO] - fit progress: (204, 17.69785350561142, {'accuracy': 0.7323717948717948}, 1182.269568290998)
[2024-08-13 23:51:01,009][flwr][INFO] - configure_evaluate: strategy sampled 2 clients (out of 20)
[2024-08-13 23:51:01,236][flwr][INFO] - aggregate_evaluate: received 2 results and 0 failures
[2024-08-13 23:51:01,236][flwr][INFO] - 
[2024-08-13 23:51:01,236][flwr][INFO] - [ROUND 205]
[2024-08-13 23:51:01,236][flwr][INFO] - configure_fit: strategy sampled 10 clients (out of 20)
[2024-08-13 23:51:01,401][flwr][ERROR] - Traceback (most recent call last):
  File "/home/abdenour/miniconda3/envs/fl_with_flower/lib/python3.11/site-packages/flwr/simulation/ray_transport/ray_client_proxy.py", line 94, in _submit_job
    out_mssg, updated_context = self.actor_pool.get_client_result(
                                ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/abdenour/miniconda3/envs/fl_with_flower/lib/python3.11/site-packages/flwr/simulation/ray_transport/ray_actor.py", line 398, in get_client_result
    return self._fetch_future_result(cid)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/abdenour/miniconda3/envs/fl_with_flower/lib/python3.11/site-packages/flwr/simulation/ray_transport/ray_actor.py", line 279, in _fetch_future_result
    res_cid, out_mssg, updated_context = ray.get(
                                         ^^^^^^^^
  File "/home/abdenour/miniconda3/envs/fl_with_flower/lib/python3.11/site-packages/ray/_private/auto_init_hook.py", line 21, in auto_init_wrapper
    return fn(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^
  File "/home/abdenour/miniconda3/envs/fl_with_flower/lib/python3.11/site-packages/ray/_private/client_mode_hook.py", line 103, in wrapper
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "/home/abdenour/miniconda3/envs/fl_with_flower/lib/python3.11/site-packages/ray/_private/worker.py", line 2667, in get
    values, debugger_breakpoint = worker.get_objects(object_refs, timeout=timeout)
                                  ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/abdenour/miniconda3/envs/fl_with_flower/lib/python3.11/site-packages/ray/_private/worker.py", line 866, in get_objects
    raise value
ray.exceptions.OutOfMemoryError: Task was killed due to the node running low on memory.
Memory on the node (IP: 115.145.171.128, ID: c687613ff34cdbf5097699e5c896055ef2dc44343c5ab44bd40309d5) where the task (actor ID: 85928d3f1a247800081c0e1101000000, name=ClientAppActor.__init__, pid=619614, memory used=1.73GB) was running was 29.50GB / 31.02GB (0.951049), which exceeds the memory usage threshold of 0.95. Ray killed this worker (ID: c7f644ac3cca20dffbfa1723daac4f87f04c7de03da25e7cad5fe33c) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 115.145.171.128`. To see the logs of the worker, use `ray logs worker-c7f644ac3cca20dffbfa1723daac4f87f04c7de03da25e7cad5fe33c*out -ip 115.145.171.128. Top 10 memory users:
PID	MEM(GB)	COMMAND
4942	3.49	/snap/pycharm-professional/408/jbr/bin/java -classpath /snap/pycharm-professional/408/lib/platform-l...
619616	1.80	ray::ClientAppActor.run
619615	1.78	ray::ClientAppActor
619617	1.77	ray::ClientAppActor
619611	1.73	ray::ClientAppActor
619618	1.73	ray::ClientAppActor
619614	1.73	ray::ClientAppActor
619612	1.72	ray::ClientAppActor
619619	1.70	ray::ClientAppActor
619620	1.68	ray::ClientAppActor.run
Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.

[2024-08-13 23:51:01,402][flwr][ERROR] - Task was killed due to the node running low on memory.
Memory on the node (IP: 115.145.171.128, ID: c687613ff34cdbf5097699e5c896055ef2dc44343c5ab44bd40309d5) where the task (actor ID: 85928d3f1a247800081c0e1101000000, name=ClientAppActor.__init__, pid=619614, memory used=1.73GB) was running was 29.50GB / 31.02GB (0.951049), which exceeds the memory usage threshold of 0.95. Ray killed this worker (ID: c7f644ac3cca20dffbfa1723daac4f87f04c7de03da25e7cad5fe33c) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 115.145.171.128`. To see the logs of the worker, use `ray logs worker-c7f644ac3cca20dffbfa1723daac4f87f04c7de03da25e7cad5fe33c*out -ip 115.145.171.128. Top 10 memory users:
PID	MEM(GB)	COMMAND
4942	3.49	/snap/pycharm-professional/408/jbr/bin/java -classpath /snap/pycharm-professional/408/lib/platform-l...
619616	1.80	ray::ClientAppActor.run
619615	1.78	ray::ClientAppActor
619617	1.77	ray::ClientAppActor
619611	1.73	ray::ClientAppActor
619618	1.73	ray::ClientAppActor
619614	1.73	ray::ClientAppActor
619612	1.72	ray::ClientAppActor
619619	1.70	ray::ClientAppActor
619620	1.68	ray::ClientAppActor.run
Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.
[2024-08-13 23:51:03,199][flwr][INFO] - aggregate_fit: received 9 results and 1 failures
[2024-08-13 23:51:06,226][flwr][INFO] - fit progress: (205, 13.600517630577087, {'accuracy': 0.7548076923076923}, 1187.4862911709934)
[2024-08-13 23:51:06,226][flwr][INFO] - configure_evaluate: strategy sampled 2 clients (out of 20)
[2024-08-13 23:51:06,510][flwr][INFO] - aggregate_evaluate: received 2 results and 0 failures
[2024-08-13 23:51:06,510][flwr][INFO] - 
[2024-08-13 23:51:06,510][flwr][INFO] - [ROUND 206]
[2024-08-13 23:51:06,510][flwr][INFO] - configure_fit: strategy sampled 10 clients (out of 20)
[2024-08-13 23:51:06,707][flwr][ERROR] - Traceback (most recent call last):
  File "/home/abdenour/miniconda3/envs/fl_with_flower/lib/python3.11/site-packages/flwr/simulation/ray_transport/ray_client_proxy.py", line 94, in _submit_job
    out_mssg, updated_context = self.actor_pool.get_client_result(
                                ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/abdenour/miniconda3/envs/fl_with_flower/lib/python3.11/site-packages/flwr/simulation/ray_transport/ray_actor.py", line 398, in get_client_result
    return self._fetch_future_result(cid)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/abdenour/miniconda3/envs/fl_with_flower/lib/python3.11/site-packages/flwr/simulation/ray_transport/ray_actor.py", line 279, in _fetch_future_result
    res_cid, out_mssg, updated_context = ray.get(
                                         ^^^^^^^^
  File "/home/abdenour/miniconda3/envs/fl_with_flower/lib/python3.11/site-packages/ray/_private/auto_init_hook.py", line 21, in auto_init_wrapper
    return fn(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^
  File "/home/abdenour/miniconda3/envs/fl_with_flower/lib/python3.11/site-packages/ray/_private/client_mode_hook.py", line 103, in wrapper
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "/home/abdenour/miniconda3/envs/fl_with_flower/lib/python3.11/site-packages/ray/_private/worker.py", line 2667, in get
    values, debugger_breakpoint = worker.get_objects(object_refs, timeout=timeout)
                                  ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/abdenour/miniconda3/envs/fl_with_flower/lib/python3.11/site-packages/ray/_private/worker.py", line 866, in get_objects
    raise value
ray.exceptions.OutOfMemoryError: Task was killed due to the node running low on memory.
Memory on the node (IP: 115.145.171.128, ID: c687613ff34cdbf5097699e5c896055ef2dc44343c5ab44bd40309d5) where the task (actor ID: 85928d3f1a247800081c0e1101000000, name=ClientAppActor.__init__, pid=619614, memory used=1.73GB) was running was 29.50GB / 31.02GB (0.951049), which exceeds the memory usage threshold of 0.95. Ray killed this worker (ID: c7f644ac3cca20dffbfa1723daac4f87f04c7de03da25e7cad5fe33c) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 115.145.171.128`. To see the logs of the worker, use `ray logs worker-c7f644ac3cca20dffbfa1723daac4f87f04c7de03da25e7cad5fe33c*out -ip 115.145.171.128. Top 10 memory users:
PID	MEM(GB)	COMMAND
4942	3.49	/snap/pycharm-professional/408/jbr/bin/java -classpath /snap/pycharm-professional/408/lib/platform-l...
619616	1.80	ray::ClientAppActor.run
619615	1.78	ray::ClientAppActor
619617	1.77	ray::ClientAppActor
619611	1.73	ray::ClientAppActor
619618	1.73	ray::ClientAppActor
619614	1.73	ray::ClientAppActor
619612	1.72	ray::ClientAppActor
619619	1.70	ray::ClientAppActor
619620	1.68	ray::ClientAppActor.run
Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.

[2024-08-13 23:51:06,707][flwr][ERROR] - Task was killed due to the node running low on memory.
Memory on the node (IP: 115.145.171.128, ID: c687613ff34cdbf5097699e5c896055ef2dc44343c5ab44bd40309d5) where the task (actor ID: 85928d3f1a247800081c0e1101000000, name=ClientAppActor.__init__, pid=619614, memory used=1.73GB) was running was 29.50GB / 31.02GB (0.951049), which exceeds the memory usage threshold of 0.95. Ray killed this worker (ID: c7f644ac3cca20dffbfa1723daac4f87f04c7de03da25e7cad5fe33c) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 115.145.171.128`. To see the logs of the worker, use `ray logs worker-c7f644ac3cca20dffbfa1723daac4f87f04c7de03da25e7cad5fe33c*out -ip 115.145.171.128. Top 10 memory users:
PID	MEM(GB)	COMMAND
4942	3.49	/snap/pycharm-professional/408/jbr/bin/java -classpath /snap/pycharm-professional/408/lib/platform-l...
619616	1.80	ray::ClientAppActor.run
619615	1.78	ray::ClientAppActor
619617	1.77	ray::ClientAppActor
619611	1.73	ray::ClientAppActor
619618	1.73	ray::ClientAppActor
619614	1.73	ray::ClientAppActor
619612	1.72	ray::ClientAppActor
619619	1.70	ray::ClientAppActor
619620	1.68	ray::ClientAppActor.run
Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.
[2024-08-13 23:51:08,575][flwr][INFO] - aggregate_fit: received 9 results and 1 failures
[2024-08-13 23:51:11,710][flwr][INFO] - fit progress: (206, 14.250674307346344, {'accuracy': 0.7580128205128205}, 1192.970478673)
[2024-08-13 23:51:11,710][flwr][INFO] - configure_evaluate: strategy sampled 2 clients (out of 20)
[2024-08-13 23:51:11,889][flwr][INFO] - aggregate_evaluate: received 2 results and 0 failures
[2024-08-13 23:51:11,890][flwr][INFO] - 
[2024-08-13 23:51:11,890][flwr][INFO] - [ROUND 207]
[2024-08-13 23:51:11,890][flwr][INFO] - configure_fit: strategy sampled 10 clients (out of 20)
[2024-08-13 23:51:12,047][flwr][ERROR] - Traceback (most recent call last):
  File "/home/abdenour/miniconda3/envs/fl_with_flower/lib/python3.11/site-packages/flwr/simulation/ray_transport/ray_client_proxy.py", line 94, in _submit_job
    out_mssg, updated_context = self.actor_pool.get_client_result(
                                ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/abdenour/miniconda3/envs/fl_with_flower/lib/python3.11/site-packages/flwr/simulation/ray_transport/ray_actor.py", line 398, in get_client_result
    return self._fetch_future_result(cid)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/abdenour/miniconda3/envs/fl_with_flower/lib/python3.11/site-packages/flwr/simulation/ray_transport/ray_actor.py", line 279, in _fetch_future_result
    res_cid, out_mssg, updated_context = ray.get(
                                         ^^^^^^^^
  File "/home/abdenour/miniconda3/envs/fl_with_flower/lib/python3.11/site-packages/ray/_private/auto_init_hook.py", line 21, in auto_init_wrapper
    return fn(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^
  File "/home/abdenour/miniconda3/envs/fl_with_flower/lib/python3.11/site-packages/ray/_private/client_mode_hook.py", line 103, in wrapper
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "/home/abdenour/miniconda3/envs/fl_with_flower/lib/python3.11/site-packages/ray/_private/worker.py", line 2667, in get
    values, debugger_breakpoint = worker.get_objects(object_refs, timeout=timeout)
                                  ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/abdenour/miniconda3/envs/fl_with_flower/lib/python3.11/site-packages/ray/_private/worker.py", line 866, in get_objects
    raise value
ray.exceptions.OutOfMemoryError: Task was killed due to the node running low on memory.
Memory on the node (IP: 115.145.171.128, ID: c687613ff34cdbf5097699e5c896055ef2dc44343c5ab44bd40309d5) where the task (actor ID: 85928d3f1a247800081c0e1101000000, name=ClientAppActor.__init__, pid=619614, memory used=1.73GB) was running was 29.50GB / 31.02GB (0.951049), which exceeds the memory usage threshold of 0.95. Ray killed this worker (ID: c7f644ac3cca20dffbfa1723daac4f87f04c7de03da25e7cad5fe33c) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 115.145.171.128`. To see the logs of the worker, use `ray logs worker-c7f644ac3cca20dffbfa1723daac4f87f04c7de03da25e7cad5fe33c*out -ip 115.145.171.128. Top 10 memory users:
PID	MEM(GB)	COMMAND
4942	3.49	/snap/pycharm-professional/408/jbr/bin/java -classpath /snap/pycharm-professional/408/lib/platform-l...
619616	1.80	ray::ClientAppActor.run
619615	1.78	ray::ClientAppActor
619617	1.77	ray::ClientAppActor
619611	1.73	ray::ClientAppActor
619618	1.73	ray::ClientAppActor
619614	1.73	ray::ClientAppActor
619612	1.72	ray::ClientAppActor
619619	1.70	ray::ClientAppActor
619620	1.68	ray::ClientAppActor.run
Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.

[2024-08-13 23:51:12,048][flwr][ERROR] - Task was killed due to the node running low on memory.
Memory on the node (IP: 115.145.171.128, ID: c687613ff34cdbf5097699e5c896055ef2dc44343c5ab44bd40309d5) where the task (actor ID: 85928d3f1a247800081c0e1101000000, name=ClientAppActor.__init__, pid=619614, memory used=1.73GB) was running was 29.50GB / 31.02GB (0.951049), which exceeds the memory usage threshold of 0.95. Ray killed this worker (ID: c7f644ac3cca20dffbfa1723daac4f87f04c7de03da25e7cad5fe33c) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 115.145.171.128`. To see the logs of the worker, use `ray logs worker-c7f644ac3cca20dffbfa1723daac4f87f04c7de03da25e7cad5fe33c*out -ip 115.145.171.128. Top 10 memory users:
PID	MEM(GB)	COMMAND
4942	3.49	/snap/pycharm-professional/408/jbr/bin/java -classpath /snap/pycharm-professional/408/lib/platform-l...
619616	1.80	ray::ClientAppActor.run
619615	1.78	ray::ClientAppActor
619617	1.77	ray::ClientAppActor
619611	1.73	ray::ClientAppActor
619618	1.73	ray::ClientAppActor
619614	1.73	ray::ClientAppActor
619612	1.72	ray::ClientAppActor
619619	1.70	ray::ClientAppActor
619620	1.68	ray::ClientAppActor.run
Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.
[2024-08-13 23:51:13,951][flwr][INFO] - aggregate_fit: received 9 results and 1 failures
[2024-08-13 23:51:16,927][flwr][INFO] - fit progress: (207, 15.61570006608963, {'accuracy': 0.7451923076923077}, 1198.1876515109907)
[2024-08-13 23:51:16,928][flwr][INFO] - configure_evaluate: strategy sampled 2 clients (out of 20)
[2024-08-13 23:51:17,337][flwr][INFO] - aggregate_evaluate: received 2 results and 0 failures
[2024-08-13 23:51:17,337][flwr][INFO] - 
[2024-08-13 23:51:17,337][flwr][INFO] - [ROUND 208]
[2024-08-13 23:51:17,337][flwr][INFO] - configure_fit: strategy sampled 10 clients (out of 20)
[2024-08-13 23:51:17,527][flwr][ERROR] - Traceback (most recent call last):
  File "/home/abdenour/miniconda3/envs/fl_with_flower/lib/python3.11/site-packages/flwr/simulation/ray_transport/ray_client_proxy.py", line 94, in _submit_job
    out_mssg, updated_context = self.actor_pool.get_client_result(
                                ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/abdenour/miniconda3/envs/fl_with_flower/lib/python3.11/site-packages/flwr/simulation/ray_transport/ray_actor.py", line 398, in get_client_result
    return self._fetch_future_result(cid)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/abdenour/miniconda3/envs/fl_with_flower/lib/python3.11/site-packages/flwr/simulation/ray_transport/ray_actor.py", line 279, in _fetch_future_result
    res_cid, out_mssg, updated_context = ray.get(
                                         ^^^^^^^^
  File "/home/abdenour/miniconda3/envs/fl_with_flower/lib/python3.11/site-packages/ray/_private/auto_init_hook.py", line 21, in auto_init_wrapper
    return fn(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^
  File "/home/abdenour/miniconda3/envs/fl_with_flower/lib/python3.11/site-packages/ray/_private/client_mode_hook.py", line 103, in wrapper
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "/home/abdenour/miniconda3/envs/fl_with_flower/lib/python3.11/site-packages/ray/_private/worker.py", line 2667, in get
    values, debugger_breakpoint = worker.get_objects(object_refs, timeout=timeout)
                                  ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/abdenour/miniconda3/envs/fl_with_flower/lib/python3.11/site-packages/ray/_private/worker.py", line 866, in get_objects
    raise value
ray.exceptions.OutOfMemoryError: Task was killed due to the node running low on memory.
Memory on the node (IP: 115.145.171.128, ID: c687613ff34cdbf5097699e5c896055ef2dc44343c5ab44bd40309d5) where the task (actor ID: 85928d3f1a247800081c0e1101000000, name=ClientAppActor.__init__, pid=619614, memory used=1.73GB) was running was 29.50GB / 31.02GB (0.951049), which exceeds the memory usage threshold of 0.95. Ray killed this worker (ID: c7f644ac3cca20dffbfa1723daac4f87f04c7de03da25e7cad5fe33c) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 115.145.171.128`. To see the logs of the worker, use `ray logs worker-c7f644ac3cca20dffbfa1723daac4f87f04c7de03da25e7cad5fe33c*out -ip 115.145.171.128. Top 10 memory users:
PID	MEM(GB)	COMMAND
4942	3.49	/snap/pycharm-professional/408/jbr/bin/java -classpath /snap/pycharm-professional/408/lib/platform-l...
619616	1.80	ray::ClientAppActor.run
619615	1.78	ray::ClientAppActor
619617	1.77	ray::ClientAppActor
619611	1.73	ray::ClientAppActor
619618	1.73	ray::ClientAppActor
619614	1.73	ray::ClientAppActor
619612	1.72	ray::ClientAppActor
619619	1.70	ray::ClientAppActor
619620	1.68	ray::ClientAppActor.run
Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.

[2024-08-13 23:51:17,527][flwr][ERROR] - Task was killed due to the node running low on memory.
Memory on the node (IP: 115.145.171.128, ID: c687613ff34cdbf5097699e5c896055ef2dc44343c5ab44bd40309d5) where the task (actor ID: 85928d3f1a247800081c0e1101000000, name=ClientAppActor.__init__, pid=619614, memory used=1.73GB) was running was 29.50GB / 31.02GB (0.951049), which exceeds the memory usage threshold of 0.95. Ray killed this worker (ID: c7f644ac3cca20dffbfa1723daac4f87f04c7de03da25e7cad5fe33c) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 115.145.171.128`. To see the logs of the worker, use `ray logs worker-c7f644ac3cca20dffbfa1723daac4f87f04c7de03da25e7cad5fe33c*out -ip 115.145.171.128. Top 10 memory users:
PID	MEM(GB)	COMMAND
4942	3.49	/snap/pycharm-professional/408/jbr/bin/java -classpath /snap/pycharm-professional/408/lib/platform-l...
619616	1.80	ray::ClientAppActor.run
619615	1.78	ray::ClientAppActor
619617	1.77	ray::ClientAppActor
619611	1.73	ray::ClientAppActor
619618	1.73	ray::ClientAppActor
619614	1.73	ray::ClientAppActor
619612	1.72	ray::ClientAppActor
619619	1.70	ray::ClientAppActor
619620	1.68	ray::ClientAppActor.run
Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.
[2024-08-13 23:51:19,284][flwr][INFO] - aggregate_fit: received 9 results and 1 failures
[2024-08-13 23:51:22,122][flwr][INFO] - fit progress: (208, 14.277549088001251, {'accuracy': 0.7564102564102564}, 1203.3823296060145)
[2024-08-13 23:51:22,122][flwr][INFO] - configure_evaluate: strategy sampled 2 clients (out of 20)
[2024-08-13 23:51:22,377][flwr][INFO] - aggregate_evaluate: received 2 results and 0 failures
[2024-08-13 23:51:22,377][flwr][INFO] - 
[2024-08-13 23:51:22,377][flwr][INFO] - [ROUND 209]
[2024-08-13 23:51:22,377][flwr][INFO] - configure_fit: strategy sampled 10 clients (out of 20)
[2024-08-13 23:51:22,539][flwr][ERROR] - Traceback (most recent call last):
  File "/home/abdenour/miniconda3/envs/fl_with_flower/lib/python3.11/site-packages/flwr/simulation/ray_transport/ray_client_proxy.py", line 94, in _submit_job
    out_mssg, updated_context = self.actor_pool.get_client_result(
                                ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/abdenour/miniconda3/envs/fl_with_flower/lib/python3.11/site-packages/flwr/simulation/ray_transport/ray_actor.py", line 398, in get_client_result
    return self._fetch_future_result(cid)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/abdenour/miniconda3/envs/fl_with_flower/lib/python3.11/site-packages/flwr/simulation/ray_transport/ray_actor.py", line 279, in _fetch_future_result
    res_cid, out_mssg, updated_context = ray.get(
                                         ^^^^^^^^
  File "/home/abdenour/miniconda3/envs/fl_with_flower/lib/python3.11/site-packages/ray/_private/auto_init_hook.py", line 21, in auto_init_wrapper
    return fn(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^
  File "/home/abdenour/miniconda3/envs/fl_with_flower/lib/python3.11/site-packages/ray/_private/client_mode_hook.py", line 103, in wrapper
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "/home/abdenour/miniconda3/envs/fl_with_flower/lib/python3.11/site-packages/ray/_private/worker.py", line 2667, in get
    values, debugger_breakpoint = worker.get_objects(object_refs, timeout=timeout)
                                  ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/abdenour/miniconda3/envs/fl_with_flower/lib/python3.11/site-packages/ray/_private/worker.py", line 866, in get_objects
    raise value
ray.exceptions.OutOfMemoryError: Task was killed due to the node running low on memory.
Memory on the node (IP: 115.145.171.128, ID: c687613ff34cdbf5097699e5c896055ef2dc44343c5ab44bd40309d5) where the task (actor ID: 85928d3f1a247800081c0e1101000000, name=ClientAppActor.__init__, pid=619614, memory used=1.73GB) was running was 29.50GB / 31.02GB (0.951049), which exceeds the memory usage threshold of 0.95. Ray killed this worker (ID: c7f644ac3cca20dffbfa1723daac4f87f04c7de03da25e7cad5fe33c) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 115.145.171.128`. To see the logs of the worker, use `ray logs worker-c7f644ac3cca20dffbfa1723daac4f87f04c7de03da25e7cad5fe33c*out -ip 115.145.171.128. Top 10 memory users:
PID	MEM(GB)	COMMAND
4942	3.49	/snap/pycharm-professional/408/jbr/bin/java -classpath /snap/pycharm-professional/408/lib/platform-l...
619616	1.80	ray::ClientAppActor.run
619615	1.78	ray::ClientAppActor
619617	1.77	ray::ClientAppActor
619611	1.73	ray::ClientAppActor
619618	1.73	ray::ClientAppActor
619614	1.73	ray::ClientAppActor
619612	1.72	ray::ClientAppActor
619619	1.70	ray::ClientAppActor
619620	1.68	ray::ClientAppActor.run
Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.

[2024-08-13 23:51:22,539][flwr][ERROR] - Task was killed due to the node running low on memory.
Memory on the node (IP: 115.145.171.128, ID: c687613ff34cdbf5097699e5c896055ef2dc44343c5ab44bd40309d5) where the task (actor ID: 85928d3f1a247800081c0e1101000000, name=ClientAppActor.__init__, pid=619614, memory used=1.73GB) was running was 29.50GB / 31.02GB (0.951049), which exceeds the memory usage threshold of 0.95. Ray killed this worker (ID: c7f644ac3cca20dffbfa1723daac4f87f04c7de03da25e7cad5fe33c) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 115.145.171.128`. To see the logs of the worker, use `ray logs worker-c7f644ac3cca20dffbfa1723daac4f87f04c7de03da25e7cad5fe33c*out -ip 115.145.171.128. Top 10 memory users:
PID	MEM(GB)	COMMAND
4942	3.49	/snap/pycharm-professional/408/jbr/bin/java -classpath /snap/pycharm-professional/408/lib/platform-l...
619616	1.80	ray::ClientAppActor.run
619615	1.78	ray::ClientAppActor
619617	1.77	ray::ClientAppActor
619611	1.73	ray::ClientAppActor
619618	1.73	ray::ClientAppActor
619614	1.73	ray::ClientAppActor
619612	1.72	ray::ClientAppActor
619619	1.70	ray::ClientAppActor
619620	1.68	ray::ClientAppActor.run
Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.
[2024-08-13 23:51:24,293][flwr][INFO] - aggregate_fit: received 9 results and 1 failures
[2024-08-13 23:51:27,444][flwr][INFO] - fit progress: (209, 15.682195544242859, {'accuracy': 0.7467948717948718}, 1208.704164677998)
[2024-08-13 23:51:27,444][flwr][INFO] - configure_evaluate: strategy sampled 2 clients (out of 20)
[2024-08-13 23:51:27,647][flwr][INFO] - aggregate_evaluate: received 2 results and 0 failures
[2024-08-13 23:51:27,647][flwr][INFO] - 
[2024-08-13 23:51:27,647][flwr][INFO] - [ROUND 210]
[2024-08-13 23:51:27,647][flwr][INFO] - configure_fit: strategy sampled 10 clients (out of 20)
[2024-08-13 23:51:27,806][flwr][ERROR] - Traceback (most recent call last):
  File "/home/abdenour/miniconda3/envs/fl_with_flower/lib/python3.11/site-packages/flwr/simulation/ray_transport/ray_client_proxy.py", line 94, in _submit_job
    out_mssg, updated_context = self.actor_pool.get_client_result(
                                ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/abdenour/miniconda3/envs/fl_with_flower/lib/python3.11/site-packages/flwr/simulation/ray_transport/ray_actor.py", line 398, in get_client_result
    return self._fetch_future_result(cid)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/abdenour/miniconda3/envs/fl_with_flower/lib/python3.11/site-packages/flwr/simulation/ray_transport/ray_actor.py", line 279, in _fetch_future_result
    res_cid, out_mssg, updated_context = ray.get(
                                         ^^^^^^^^
  File "/home/abdenour/miniconda3/envs/fl_with_flower/lib/python3.11/site-packages/ray/_private/auto_init_hook.py", line 21, in auto_init_wrapper
    return fn(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^
  File "/home/abdenour/miniconda3/envs/fl_with_flower/lib/python3.11/site-packages/ray/_private/client_mode_hook.py", line 103, in wrapper
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "/home/abdenour/miniconda3/envs/fl_with_flower/lib/python3.11/site-packages/ray/_private/worker.py", line 2667, in get
    values, debugger_breakpoint = worker.get_objects(object_refs, timeout=timeout)
                                  ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/abdenour/miniconda3/envs/fl_with_flower/lib/python3.11/site-packages/ray/_private/worker.py", line 866, in get_objects
    raise value
ray.exceptions.OutOfMemoryError: Task was killed due to the node running low on memory.
Memory on the node (IP: 115.145.171.128, ID: c687613ff34cdbf5097699e5c896055ef2dc44343c5ab44bd40309d5) where the task (actor ID: 85928d3f1a247800081c0e1101000000, name=ClientAppActor.__init__, pid=619614, memory used=1.73GB) was running was 29.50GB / 31.02GB (0.951049), which exceeds the memory usage threshold of 0.95. Ray killed this worker (ID: c7f644ac3cca20dffbfa1723daac4f87f04c7de03da25e7cad5fe33c) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 115.145.171.128`. To see the logs of the worker, use `ray logs worker-c7f644ac3cca20dffbfa1723daac4f87f04c7de03da25e7cad5fe33c*out -ip 115.145.171.128. Top 10 memory users:
PID	MEM(GB)	COMMAND
4942	3.49	/snap/pycharm-professional/408/jbr/bin/java -classpath /snap/pycharm-professional/408/lib/platform-l...
619616	1.80	ray::ClientAppActor.run
619615	1.78	ray::ClientAppActor
619617	1.77	ray::ClientAppActor
619611	1.73	ray::ClientAppActor
619618	1.73	ray::ClientAppActor
619614	1.73	ray::ClientAppActor
619612	1.72	ray::ClientAppActor
619619	1.70	ray::ClientAppActor
619620	1.68	ray::ClientAppActor.run
Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.

[2024-08-13 23:51:27,806][flwr][ERROR] - Task was killed due to the node running low on memory.
Memory on the node (IP: 115.145.171.128, ID: c687613ff34cdbf5097699e5c896055ef2dc44343c5ab44bd40309d5) where the task (actor ID: 85928d3f1a247800081c0e1101000000, name=ClientAppActor.__init__, pid=619614, memory used=1.73GB) was running was 29.50GB / 31.02GB (0.951049), which exceeds the memory usage threshold of 0.95. Ray killed this worker (ID: c7f644ac3cca20dffbfa1723daac4f87f04c7de03da25e7cad5fe33c) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 115.145.171.128`. To see the logs of the worker, use `ray logs worker-c7f644ac3cca20dffbfa1723daac4f87f04c7de03da25e7cad5fe33c*out -ip 115.145.171.128. Top 10 memory users:
PID	MEM(GB)	COMMAND
4942	3.49	/snap/pycharm-professional/408/jbr/bin/java -classpath /snap/pycharm-professional/408/lib/platform-l...
619616	1.80	ray::ClientAppActor.run
619615	1.78	ray::ClientAppActor
619617	1.77	ray::ClientAppActor
619611	1.73	ray::ClientAppActor
619618	1.73	ray::ClientAppActor
619614	1.73	ray::ClientAppActor
619612	1.72	ray::ClientAppActor
619619	1.70	ray::ClientAppActor
619620	1.68	ray::ClientAppActor.run
Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.
[2024-08-13 23:51:29,700][flwr][INFO] - aggregate_fit: received 9 results and 1 failures
[2024-08-13 23:51:32,909][flwr][INFO] - fit progress: (210, 15.852089405059814, {'accuracy': 0.7467948717948718}, 1214.16889898901)
[2024-08-13 23:51:32,909][flwr][INFO] - configure_evaluate: strategy sampled 2 clients (out of 20)
[2024-08-13 23:51:33,406][flwr][INFO] - aggregate_evaluate: received 2 results and 0 failures
[2024-08-13 23:51:33,406][flwr][INFO] - 
[2024-08-13 23:51:33,407][flwr][INFO] - [ROUND 211]
[2024-08-13 23:51:33,407][flwr][INFO] - configure_fit: strategy sampled 10 clients (out of 20)
[2024-08-13 23:51:33,561][flwr][ERROR] - Traceback (most recent call last):
  File "/home/abdenour/miniconda3/envs/fl_with_flower/lib/python3.11/site-packages/flwr/simulation/ray_transport/ray_client_proxy.py", line 94, in _submit_job
    out_mssg, updated_context = self.actor_pool.get_client_result(
                                ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/abdenour/miniconda3/envs/fl_with_flower/lib/python3.11/site-packages/flwr/simulation/ray_transport/ray_actor.py", line 398, in get_client_result
    return self._fetch_future_result(cid)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/abdenour/miniconda3/envs/fl_with_flower/lib/python3.11/site-packages/flwr/simulation/ray_transport/ray_actor.py", line 279, in _fetch_future_result
    res_cid, out_mssg, updated_context = ray.get(
                                         ^^^^^^^^
  File "/home/abdenour/miniconda3/envs/fl_with_flower/lib/python3.11/site-packages/ray/_private/auto_init_hook.py", line 21, in auto_init_wrapper
    return fn(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^
  File "/home/abdenour/miniconda3/envs/fl_with_flower/lib/python3.11/site-packages/ray/_private/client_mode_hook.py", line 103, in wrapper
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "/home/abdenour/miniconda3/envs/fl_with_flower/lib/python3.11/site-packages/ray/_private/worker.py", line 2667, in get
    values, debugger_breakpoint = worker.get_objects(object_refs, timeout=timeout)
                                  ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/abdenour/miniconda3/envs/fl_with_flower/lib/python3.11/site-packages/ray/_private/worker.py", line 866, in get_objects
    raise value
ray.exceptions.OutOfMemoryError: Task was killed due to the node running low on memory.
Memory on the node (IP: 115.145.171.128, ID: c687613ff34cdbf5097699e5c896055ef2dc44343c5ab44bd40309d5) where the task (actor ID: 85928d3f1a247800081c0e1101000000, name=ClientAppActor.__init__, pid=619614, memory used=1.73GB) was running was 29.50GB / 31.02GB (0.951049), which exceeds the memory usage threshold of 0.95. Ray killed this worker (ID: c7f644ac3cca20dffbfa1723daac4f87f04c7de03da25e7cad5fe33c) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 115.145.171.128`. To see the logs of the worker, use `ray logs worker-c7f644ac3cca20dffbfa1723daac4f87f04c7de03da25e7cad5fe33c*out -ip 115.145.171.128. Top 10 memory users:
PID	MEM(GB)	COMMAND
4942	3.49	/snap/pycharm-professional/408/jbr/bin/java -classpath /snap/pycharm-professional/408/lib/platform-l...
619616	1.80	ray::ClientAppActor.run
619615	1.78	ray::ClientAppActor
619617	1.77	ray::ClientAppActor
619611	1.73	ray::ClientAppActor
619618	1.73	ray::ClientAppActor
619614	1.73	ray::ClientAppActor
619612	1.72	ray::ClientAppActor
619619	1.70	ray::ClientAppActor
619620	1.68	ray::ClientAppActor.run
Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.

[2024-08-13 23:51:33,561][flwr][ERROR] - Task was killed due to the node running low on memory.
Memory on the node (IP: 115.145.171.128, ID: c687613ff34cdbf5097699e5c896055ef2dc44343c5ab44bd40309d5) where the task (actor ID: 85928d3f1a247800081c0e1101000000, name=ClientAppActor.__init__, pid=619614, memory used=1.73GB) was running was 29.50GB / 31.02GB (0.951049), which exceeds the memory usage threshold of 0.95. Ray killed this worker (ID: c7f644ac3cca20dffbfa1723daac4f87f04c7de03da25e7cad5fe33c) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 115.145.171.128`. To see the logs of the worker, use `ray logs worker-c7f644ac3cca20dffbfa1723daac4f87f04c7de03da25e7cad5fe33c*out -ip 115.145.171.128. Top 10 memory users:
PID	MEM(GB)	COMMAND
4942	3.49	/snap/pycharm-professional/408/jbr/bin/java -classpath /snap/pycharm-professional/408/lib/platform-l...
619616	1.80	ray::ClientAppActor.run
619615	1.78	ray::ClientAppActor
619617	1.77	ray::ClientAppActor
619611	1.73	ray::ClientAppActor
619618	1.73	ray::ClientAppActor
619614	1.73	ray::ClientAppActor
619612	1.72	ray::ClientAppActor
619619	1.70	ray::ClientAppActor
619620	1.68	ray::ClientAppActor.run
Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.
[2024-08-13 23:51:35,459][flwr][INFO] - aggregate_fit: received 9 results and 1 failures
[2024-08-13 23:51:38,826][flwr][INFO] - fit progress: (211, 15.789223849773407, {'accuracy': 0.7451923076923077}, 1220.0866677720041)
[2024-08-13 23:51:38,826][flwr][INFO] - configure_evaluate: strategy sampled 2 clients (out of 20)
[2024-08-13 23:51:39,025][flwr][INFO] - aggregate_evaluate: received 2 results and 0 failures
[2024-08-13 23:51:39,025][flwr][INFO] - 
[2024-08-13 23:51:39,025][flwr][INFO] - [ROUND 212]
[2024-08-13 23:51:39,025][flwr][INFO] - configure_fit: strategy sampled 10 clients (out of 20)
[2024-08-13 23:51:39,221][flwr][ERROR] - Traceback (most recent call last):
  File "/home/abdenour/miniconda3/envs/fl_with_flower/lib/python3.11/site-packages/flwr/simulation/ray_transport/ray_client_proxy.py", line 94, in _submit_job
    out_mssg, updated_context = self.actor_pool.get_client_result(
                                ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/abdenour/miniconda3/envs/fl_with_flower/lib/python3.11/site-packages/flwr/simulation/ray_transport/ray_actor.py", line 398, in get_client_result
    return self._fetch_future_result(cid)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/abdenour/miniconda3/envs/fl_with_flower/lib/python3.11/site-packages/flwr/simulation/ray_transport/ray_actor.py", line 279, in _fetch_future_result
    res_cid, out_mssg, updated_context = ray.get(
                                         ^^^^^^^^
  File "/home/abdenour/miniconda3/envs/fl_with_flower/lib/python3.11/site-packages/ray/_private/auto_init_hook.py", line 21, in auto_init_wrapper
    return fn(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^
  File "/home/abdenour/miniconda3/envs/fl_with_flower/lib/python3.11/site-packages/ray/_private/client_mode_hook.py", line 103, in wrapper
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "/home/abdenour/miniconda3/envs/fl_with_flower/lib/python3.11/site-packages/ray/_private/worker.py", line 2667, in get
    values, debugger_breakpoint = worker.get_objects(object_refs, timeout=timeout)
                                  ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/abdenour/miniconda3/envs/fl_with_flower/lib/python3.11/site-packages/ray/_private/worker.py", line 866, in get_objects
    raise value
ray.exceptions.OutOfMemoryError: Task was killed due to the node running low on memory.
Memory on the node (IP: 115.145.171.128, ID: c687613ff34cdbf5097699e5c896055ef2dc44343c5ab44bd40309d5) where the task (actor ID: 85928d3f1a247800081c0e1101000000, name=ClientAppActor.__init__, pid=619614, memory used=1.73GB) was running was 29.50GB / 31.02GB (0.951049), which exceeds the memory usage threshold of 0.95. Ray killed this worker (ID: c7f644ac3cca20dffbfa1723daac4f87f04c7de03da25e7cad5fe33c) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 115.145.171.128`. To see the logs of the worker, use `ray logs worker-c7f644ac3cca20dffbfa1723daac4f87f04c7de03da25e7cad5fe33c*out -ip 115.145.171.128. Top 10 memory users:
PID	MEM(GB)	COMMAND
4942	3.49	/snap/pycharm-professional/408/jbr/bin/java -classpath /snap/pycharm-professional/408/lib/platform-l...
619616	1.80	ray::ClientAppActor.run
619615	1.78	ray::ClientAppActor
619617	1.77	ray::ClientAppActor
619611	1.73	ray::ClientAppActor
619618	1.73	ray::ClientAppActor
619614	1.73	ray::ClientAppActor
619612	1.72	ray::ClientAppActor
619619	1.70	ray::ClientAppActor
619620	1.68	ray::ClientAppActor.run
Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.

[2024-08-13 23:51:39,221][flwr][ERROR] - Task was killed due to the node running low on memory.
Memory on the node (IP: 115.145.171.128, ID: c687613ff34cdbf5097699e5c896055ef2dc44343c5ab44bd40309d5) where the task (actor ID: 85928d3f1a247800081c0e1101000000, name=ClientAppActor.__init__, pid=619614, memory used=1.73GB) was running was 29.50GB / 31.02GB (0.951049), which exceeds the memory usage threshold of 0.95. Ray killed this worker (ID: c7f644ac3cca20dffbfa1723daac4f87f04c7de03da25e7cad5fe33c) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 115.145.171.128`. To see the logs of the worker, use `ray logs worker-c7f644ac3cca20dffbfa1723daac4f87f04c7de03da25e7cad5fe33c*out -ip 115.145.171.128. Top 10 memory users:
PID	MEM(GB)	COMMAND
4942	3.49	/snap/pycharm-professional/408/jbr/bin/java -classpath /snap/pycharm-professional/408/lib/platform-l...
619616	1.80	ray::ClientAppActor.run
619615	1.78	ray::ClientAppActor
619617	1.77	ray::ClientAppActor
619611	1.73	ray::ClientAppActor
619618	1.73	ray::ClientAppActor
619614	1.73	ray::ClientAppActor
619612	1.72	ray::ClientAppActor
619619	1.70	ray::ClientAppActor
619620	1.68	ray::ClientAppActor.run
Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.
[2024-08-13 23:51:41,161][flwr][INFO] - aggregate_fit: received 9 results and 1 failures
[2024-08-13 23:51:44,211][flwr][INFO] - fit progress: (212, 13.746719896793365, {'accuracy': 0.7548076923076923}, 1225.4709358949913)
[2024-08-13 23:51:44,211][flwr][INFO] - configure_evaluate: strategy sampled 2 clients (out of 20)
[2024-08-13 23:51:44,463][flwr][INFO] - aggregate_evaluate: received 2 results and 0 failures
[2024-08-13 23:51:44,463][flwr][INFO] - 
[2024-08-13 23:51:44,463][flwr][INFO] - [ROUND 213]
[2024-08-13 23:51:44,463][flwr][INFO] - configure_fit: strategy sampled 10 clients (out of 20)
[2024-08-13 23:51:44,628][flwr][ERROR] - Traceback (most recent call last):
  File "/home/abdenour/miniconda3/envs/fl_with_flower/lib/python3.11/site-packages/flwr/simulation/ray_transport/ray_client_proxy.py", line 94, in _submit_job
    out_mssg, updated_context = self.actor_pool.get_client_result(
                                ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/abdenour/miniconda3/envs/fl_with_flower/lib/python3.11/site-packages/flwr/simulation/ray_transport/ray_actor.py", line 398, in get_client_result
    return self._fetch_future_result(cid)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/abdenour/miniconda3/envs/fl_with_flower/lib/python3.11/site-packages/flwr/simulation/ray_transport/ray_actor.py", line 279, in _fetch_future_result
    res_cid, out_mssg, updated_context = ray.get(
                                         ^^^^^^^^
  File "/home/abdenour/miniconda3/envs/fl_with_flower/lib/python3.11/site-packages/ray/_private/auto_init_hook.py", line 21, in auto_init_wrapper
    return fn(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^
  File "/home/abdenour/miniconda3/envs/fl_with_flower/lib/python3.11/site-packages/ray/_private/client_mode_hook.py", line 103, in wrapper
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "/home/abdenour/miniconda3/envs/fl_with_flower/lib/python3.11/site-packages/ray/_private/worker.py", line 2667, in get
    values, debugger_breakpoint = worker.get_objects(object_refs, timeout=timeout)
                                  ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/abdenour/miniconda3/envs/fl_with_flower/lib/python3.11/site-packages/ray/_private/worker.py", line 866, in get_objects
    raise value
ray.exceptions.OutOfMemoryError: Task was killed due to the node running low on memory.
Memory on the node (IP: 115.145.171.128, ID: c687613ff34cdbf5097699e5c896055ef2dc44343c5ab44bd40309d5) where the task (actor ID: 85928d3f1a247800081c0e1101000000, name=ClientAppActor.__init__, pid=619614, memory used=1.73GB) was running was 29.50GB / 31.02GB (0.951049), which exceeds the memory usage threshold of 0.95. Ray killed this worker (ID: c7f644ac3cca20dffbfa1723daac4f87f04c7de03da25e7cad5fe33c) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 115.145.171.128`. To see the logs of the worker, use `ray logs worker-c7f644ac3cca20dffbfa1723daac4f87f04c7de03da25e7cad5fe33c*out -ip 115.145.171.128. Top 10 memory users:
PID	MEM(GB)	COMMAND
4942	3.49	/snap/pycharm-professional/408/jbr/bin/java -classpath /snap/pycharm-professional/408/lib/platform-l...
619616	1.80	ray::ClientAppActor.run
619615	1.78	ray::ClientAppActor
619617	1.77	ray::ClientAppActor
619611	1.73	ray::ClientAppActor
619618	1.73	ray::ClientAppActor
619614	1.73	ray::ClientAppActor
619612	1.72	ray::ClientAppActor
619619	1.70	ray::ClientAppActor
619620	1.68	ray::ClientAppActor.run
Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.

[2024-08-13 23:51:44,628][flwr][ERROR] - Task was killed due to the node running low on memory.
Memory on the node (IP: 115.145.171.128, ID: c687613ff34cdbf5097699e5c896055ef2dc44343c5ab44bd40309d5) where the task (actor ID: 85928d3f1a247800081c0e1101000000, name=ClientAppActor.__init__, pid=619614, memory used=1.73GB) was running was 29.50GB / 31.02GB (0.951049), which exceeds the memory usage threshold of 0.95. Ray killed this worker (ID: c7f644ac3cca20dffbfa1723daac4f87f04c7de03da25e7cad5fe33c) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 115.145.171.128`. To see the logs of the worker, use `ray logs worker-c7f644ac3cca20dffbfa1723daac4f87f04c7de03da25e7cad5fe33c*out -ip 115.145.171.128. Top 10 memory users:
PID	MEM(GB)	COMMAND
4942	3.49	/snap/pycharm-professional/408/jbr/bin/java -classpath /snap/pycharm-professional/408/lib/platform-l...
619616	1.80	ray::ClientAppActor.run
619615	1.78	ray::ClientAppActor
619617	1.77	ray::ClientAppActor
619611	1.73	ray::ClientAppActor
619618	1.73	ray::ClientAppActor
619614	1.73	ray::ClientAppActor
619612	1.72	ray::ClientAppActor
619619	1.70	ray::ClientAppActor
619620	1.68	ray::ClientAppActor.run
Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.
[2024-08-13 23:51:46,503][flwr][INFO] - aggregate_fit: received 9 results and 1 failures
[2024-08-13 23:51:49,593][flwr][INFO] - fit progress: (213, 15.387958765029907, {'accuracy': 0.75}, 1230.8528246420028)
[2024-08-13 23:51:49,593][flwr][INFO] - configure_evaluate: strategy sampled 2 clients (out of 20)
[2024-08-13 23:51:49,889][flwr][INFO] - aggregate_evaluate: received 2 results and 0 failures
[2024-08-13 23:51:49,889][flwr][INFO] - 
[2024-08-13 23:51:49,889][flwr][INFO] - [ROUND 214]
[2024-08-13 23:51:49,889][flwr][INFO] - configure_fit: strategy sampled 10 clients (out of 20)
[2024-08-13 23:51:50,065][flwr][ERROR] - Traceback (most recent call last):
  File "/home/abdenour/miniconda3/envs/fl_with_flower/lib/python3.11/site-packages/flwr/simulation/ray_transport/ray_client_proxy.py", line 94, in _submit_job
    out_mssg, updated_context = self.actor_pool.get_client_result(
                                ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/abdenour/miniconda3/envs/fl_with_flower/lib/python3.11/site-packages/flwr/simulation/ray_transport/ray_actor.py", line 398, in get_client_result
    return self._fetch_future_result(cid)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/abdenour/miniconda3/envs/fl_with_flower/lib/python3.11/site-packages/flwr/simulation/ray_transport/ray_actor.py", line 279, in _fetch_future_result
    res_cid, out_mssg, updated_context = ray.get(
                                         ^^^^^^^^
  File "/home/abdenour/miniconda3/envs/fl_with_flower/lib/python3.11/site-packages/ray/_private/auto_init_hook.py", line 21, in auto_init_wrapper
    return fn(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^
  File "/home/abdenour/miniconda3/envs/fl_with_flower/lib/python3.11/site-packages/ray/_private/client_mode_hook.py", line 103, in wrapper
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "/home/abdenour/miniconda3/envs/fl_with_flower/lib/python3.11/site-packages/ray/_private/worker.py", line 2667, in get
    values, debugger_breakpoint = worker.get_objects(object_refs, timeout=timeout)
                                  ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/abdenour/miniconda3/envs/fl_with_flower/lib/python3.11/site-packages/ray/_private/worker.py", line 866, in get_objects
    raise value
ray.exceptions.OutOfMemoryError: Task was killed due to the node running low on memory.
Memory on the node (IP: 115.145.171.128, ID: c687613ff34cdbf5097699e5c896055ef2dc44343c5ab44bd40309d5) where the task (actor ID: 85928d3f1a247800081c0e1101000000, name=ClientAppActor.__init__, pid=619614, memory used=1.73GB) was running was 29.50GB / 31.02GB (0.951049), which exceeds the memory usage threshold of 0.95. Ray killed this worker (ID: c7f644ac3cca20dffbfa1723daac4f87f04c7de03da25e7cad5fe33c) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 115.145.171.128`. To see the logs of the worker, use `ray logs worker-c7f644ac3cca20dffbfa1723daac4f87f04c7de03da25e7cad5fe33c*out -ip 115.145.171.128. Top 10 memory users:
PID	MEM(GB)	COMMAND
4942	3.49	/snap/pycharm-professional/408/jbr/bin/java -classpath /snap/pycharm-professional/408/lib/platform-l...
619616	1.80	ray::ClientAppActor.run
619615	1.78	ray::ClientAppActor
619617	1.77	ray::ClientAppActor
619611	1.73	ray::ClientAppActor
619618	1.73	ray::ClientAppActor
619614	1.73	ray::ClientAppActor
619612	1.72	ray::ClientAppActor
619619	1.70	ray::ClientAppActor
619620	1.68	ray::ClientAppActor.run
Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.

[2024-08-13 23:51:50,065][flwr][ERROR] - Task was killed due to the node running low on memory.
Memory on the node (IP: 115.145.171.128, ID: c687613ff34cdbf5097699e5c896055ef2dc44343c5ab44bd40309d5) where the task (actor ID: 85928d3f1a247800081c0e1101000000, name=ClientAppActor.__init__, pid=619614, memory used=1.73GB) was running was 29.50GB / 31.02GB (0.951049), which exceeds the memory usage threshold of 0.95. Ray killed this worker (ID: c7f644ac3cca20dffbfa1723daac4f87f04c7de03da25e7cad5fe33c) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 115.145.171.128`. To see the logs of the worker, use `ray logs worker-c7f644ac3cca20dffbfa1723daac4f87f04c7de03da25e7cad5fe33c*out -ip 115.145.171.128. Top 10 memory users:
PID	MEM(GB)	COMMAND
4942	3.49	/snap/pycharm-professional/408/jbr/bin/java -classpath /snap/pycharm-professional/408/lib/platform-l...
619616	1.80	ray::ClientAppActor.run
619615	1.78	ray::ClientAppActor
619617	1.77	ray::ClientAppActor
619611	1.73	ray::ClientAppActor
619618	1.73	ray::ClientAppActor
619614	1.73	ray::ClientAppActor
619612	1.72	ray::ClientAppActor
619619	1.70	ray::ClientAppActor
619620	1.68	ray::ClientAppActor.run
Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.
[2024-08-13 23:51:52,174][flwr][INFO] - aggregate_fit: received 9 results and 1 failures
[2024-08-13 23:51:55,114][flwr][INFO] - fit progress: (214, 13.251238763332367, {'accuracy': 0.7580128205128205}, 1236.3739042770176)
[2024-08-13 23:51:55,114][flwr][INFO] - configure_evaluate: strategy sampled 2 clients (out of 20)
[2024-08-13 23:51:55,524][flwr][INFO] - aggregate_evaluate: received 2 results and 0 failures
[2024-08-13 23:51:55,524][flwr][INFO] - 
[2024-08-13 23:51:55,524][flwr][INFO] - [ROUND 215]
[2024-08-13 23:51:55,524][flwr][INFO] - configure_fit: strategy sampled 10 clients (out of 20)
[2024-08-13 23:51:55,677][flwr][ERROR] - Traceback (most recent call last):
  File "/home/abdenour/miniconda3/envs/fl_with_flower/lib/python3.11/site-packages/flwr/simulation/ray_transport/ray_client_proxy.py", line 94, in _submit_job
    out_mssg, updated_context = self.actor_pool.get_client_result(
                                ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/abdenour/miniconda3/envs/fl_with_flower/lib/python3.11/site-packages/flwr/simulation/ray_transport/ray_actor.py", line 398, in get_client_result
    return self._fetch_future_result(cid)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/abdenour/miniconda3/envs/fl_with_flower/lib/python3.11/site-packages/flwr/simulation/ray_transport/ray_actor.py", line 279, in _fetch_future_result
    res_cid, out_mssg, updated_context = ray.get(
                                         ^^^^^^^^
  File "/home/abdenour/miniconda3/envs/fl_with_flower/lib/python3.11/site-packages/ray/_private/auto_init_hook.py", line 21, in auto_init_wrapper
    return fn(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^
  File "/home/abdenour/miniconda3/envs/fl_with_flower/lib/python3.11/site-packages/ray/_private/client_mode_hook.py", line 103, in wrapper
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "/home/abdenour/miniconda3/envs/fl_with_flower/lib/python3.11/site-packages/ray/_private/worker.py", line 2667, in get
    values, debugger_breakpoint = worker.get_objects(object_refs, timeout=timeout)
                                  ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/abdenour/miniconda3/envs/fl_with_flower/lib/python3.11/site-packages/ray/_private/worker.py", line 866, in get_objects
    raise value
ray.exceptions.OutOfMemoryError: Task was killed due to the node running low on memory.
Memory on the node (IP: 115.145.171.128, ID: c687613ff34cdbf5097699e5c896055ef2dc44343c5ab44bd40309d5) where the task (actor ID: 85928d3f1a247800081c0e1101000000, name=ClientAppActor.__init__, pid=619614, memory used=1.73GB) was running was 29.50GB / 31.02GB (0.951049), which exceeds the memory usage threshold of 0.95. Ray killed this worker (ID: c7f644ac3cca20dffbfa1723daac4f87f04c7de03da25e7cad5fe33c) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 115.145.171.128`. To see the logs of the worker, use `ray logs worker-c7f644ac3cca20dffbfa1723daac4f87f04c7de03da25e7cad5fe33c*out -ip 115.145.171.128. Top 10 memory users:
PID	MEM(GB)	COMMAND
4942	3.49	/snap/pycharm-professional/408/jbr/bin/java -classpath /snap/pycharm-professional/408/lib/platform-l...
619616	1.80	ray::ClientAppActor.run
619615	1.78	ray::ClientAppActor
619617	1.77	ray::ClientAppActor
619611	1.73	ray::ClientAppActor
619618	1.73	ray::ClientAppActor
619614	1.73	ray::ClientAppActor
619612	1.72	ray::ClientAppActor
619619	1.70	ray::ClientAppActor
619620	1.68	ray::ClientAppActor.run
Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.

[2024-08-13 23:51:55,677][flwr][ERROR] - Task was killed due to the node running low on memory.
Memory on the node (IP: 115.145.171.128, ID: c687613ff34cdbf5097699e5c896055ef2dc44343c5ab44bd40309d5) where the task (actor ID: 85928d3f1a247800081c0e1101000000, name=ClientAppActor.__init__, pid=619614, memory used=1.73GB) was running was 29.50GB / 31.02GB (0.951049), which exceeds the memory usage threshold of 0.95. Ray killed this worker (ID: c7f644ac3cca20dffbfa1723daac4f87f04c7de03da25e7cad5fe33c) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 115.145.171.128`. To see the logs of the worker, use `ray logs worker-c7f644ac3cca20dffbfa1723daac4f87f04c7de03da25e7cad5fe33c*out -ip 115.145.171.128. Top 10 memory users:
PID	MEM(GB)	COMMAND
4942	3.49	/snap/pycharm-professional/408/jbr/bin/java -classpath /snap/pycharm-professional/408/lib/platform-l...
619616	1.80	ray::ClientAppActor.run
619615	1.78	ray::ClientAppActor
619617	1.77	ray::ClientAppActor
619611	1.73	ray::ClientAppActor
619618	1.73	ray::ClientAppActor
619614	1.73	ray::ClientAppActor
619612	1.72	ray::ClientAppActor
619619	1.70	ray::ClientAppActor
619620	1.68	ray::ClientAppActor.run
Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.
[2024-08-13 23:51:59,494][flwr][INFO] - aggregate_fit: received 9 results and 1 failures
[2024-08-13 23:52:02,878][flwr][INFO] - fit progress: (215, 13.718591690063477, {'accuracy': 0.7596153846153846}, 1244.1385483700142)
[2024-08-13 23:52:02,878][flwr][INFO] - configure_evaluate: strategy sampled 2 clients (out of 20)
[2024-08-13 23:52:03,214][flwr][INFO] - aggregate_evaluate: received 2 results and 0 failures
[2024-08-13 23:52:03,214][flwr][INFO] - 
[2024-08-13 23:52:03,214][flwr][INFO] - [ROUND 216]
[2024-08-13 23:52:03,214][flwr][INFO] - configure_fit: strategy sampled 10 clients (out of 20)
[2024-08-13 23:52:03,374][flwr][ERROR] - Traceback (most recent call last):
  File "/home/abdenour/miniconda3/envs/fl_with_flower/lib/python3.11/site-packages/flwr/simulation/ray_transport/ray_client_proxy.py", line 94, in _submit_job
    out_mssg, updated_context = self.actor_pool.get_client_result(
                                ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/abdenour/miniconda3/envs/fl_with_flower/lib/python3.11/site-packages/flwr/simulation/ray_transport/ray_actor.py", line 398, in get_client_result
    return self._fetch_future_result(cid)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/abdenour/miniconda3/envs/fl_with_flower/lib/python3.11/site-packages/flwr/simulation/ray_transport/ray_actor.py", line 279, in _fetch_future_result
    res_cid, out_mssg, updated_context = ray.get(
                                         ^^^^^^^^
  File "/home/abdenour/miniconda3/envs/fl_with_flower/lib/python3.11/site-packages/ray/_private/auto_init_hook.py", line 21, in auto_init_wrapper
    return fn(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^
  File "/home/abdenour/miniconda3/envs/fl_with_flower/lib/python3.11/site-packages/ray/_private/client_mode_hook.py", line 103, in wrapper
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "/home/abdenour/miniconda3/envs/fl_with_flower/lib/python3.11/site-packages/ray/_private/worker.py", line 2667, in get
    values, debugger_breakpoint = worker.get_objects(object_refs, timeout=timeout)
                                  ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/abdenour/miniconda3/envs/fl_with_flower/lib/python3.11/site-packages/ray/_private/worker.py", line 866, in get_objects
    raise value
ray.exceptions.OutOfMemoryError: Task was killed due to the node running low on memory.
Memory on the node (IP: 115.145.171.128, ID: c687613ff34cdbf5097699e5c896055ef2dc44343c5ab44bd40309d5) where the task (actor ID: 85928d3f1a247800081c0e1101000000, name=ClientAppActor.__init__, pid=619614, memory used=1.73GB) was running was 29.50GB / 31.02GB (0.951049), which exceeds the memory usage threshold of 0.95. Ray killed this worker (ID: c7f644ac3cca20dffbfa1723daac4f87f04c7de03da25e7cad5fe33c) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 115.145.171.128`. To see the logs of the worker, use `ray logs worker-c7f644ac3cca20dffbfa1723daac4f87f04c7de03da25e7cad5fe33c*out -ip 115.145.171.128. Top 10 memory users:
PID	MEM(GB)	COMMAND
4942	3.49	/snap/pycharm-professional/408/jbr/bin/java -classpath /snap/pycharm-professional/408/lib/platform-l...
619616	1.80	ray::ClientAppActor.run
619615	1.78	ray::ClientAppActor
619617	1.77	ray::ClientAppActor
619611	1.73	ray::ClientAppActor
619618	1.73	ray::ClientAppActor
619614	1.73	ray::ClientAppActor
619612	1.72	ray::ClientAppActor
619619	1.70	ray::ClientAppActor
619620	1.68	ray::ClientAppActor.run
Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.

[2024-08-13 23:52:03,374][flwr][ERROR] - Task was killed due to the node running low on memory.
Memory on the node (IP: 115.145.171.128, ID: c687613ff34cdbf5097699e5c896055ef2dc44343c5ab44bd40309d5) where the task (actor ID: 85928d3f1a247800081c0e1101000000, name=ClientAppActor.__init__, pid=619614, memory used=1.73GB) was running was 29.50GB / 31.02GB (0.951049), which exceeds the memory usage threshold of 0.95. Ray killed this worker (ID: c7f644ac3cca20dffbfa1723daac4f87f04c7de03da25e7cad5fe33c) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 115.145.171.128`. To see the logs of the worker, use `ray logs worker-c7f644ac3cca20dffbfa1723daac4f87f04c7de03da25e7cad5fe33c*out -ip 115.145.171.128. Top 10 memory users:
PID	MEM(GB)	COMMAND
4942	3.49	/snap/pycharm-professional/408/jbr/bin/java -classpath /snap/pycharm-professional/408/lib/platform-l...
619616	1.80	ray::ClientAppActor.run
619615	1.78	ray::ClientAppActor
619617	1.77	ray::ClientAppActor
619611	1.73	ray::ClientAppActor
619618	1.73	ray::ClientAppActor
619614	1.73	ray::ClientAppActor
619612	1.72	ray::ClientAppActor
619619	1.70	ray::ClientAppActor
619620	1.68	ray::ClientAppActor.run
Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.
[2024-08-13 23:52:06,040][flwr][INFO] - aggregate_fit: received 9 results and 1 failures
[2024-08-13 23:52:09,350][flwr][INFO] - fit progress: (216, 14.615563869476318, {'accuracy': 0.7548076923076923}, 1250.610584153008)
[2024-08-13 23:52:09,350][flwr][INFO] - configure_evaluate: strategy sampled 2 clients (out of 20)
[2024-08-13 23:52:09,747][flwr][INFO] - aggregate_evaluate: received 2 results and 0 failures
[2024-08-13 23:52:09,747][flwr][INFO] - 
[2024-08-13 23:52:09,747][flwr][INFO] - [ROUND 217]
[2024-08-13 23:52:09,747][flwr][INFO] - configure_fit: strategy sampled 10 clients (out of 20)
[2024-08-13 23:52:09,919][flwr][ERROR] - Traceback (most recent call last):
  File "/home/abdenour/miniconda3/envs/fl_with_flower/lib/python3.11/site-packages/flwr/simulation/ray_transport/ray_client_proxy.py", line 94, in _submit_job
    out_mssg, updated_context = self.actor_pool.get_client_result(
                                ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/abdenour/miniconda3/envs/fl_with_flower/lib/python3.11/site-packages/flwr/simulation/ray_transport/ray_actor.py", line 398, in get_client_result
    return self._fetch_future_result(cid)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/abdenour/miniconda3/envs/fl_with_flower/lib/python3.11/site-packages/flwr/simulation/ray_transport/ray_actor.py", line 279, in _fetch_future_result
    res_cid, out_mssg, updated_context = ray.get(
                                         ^^^^^^^^
  File "/home/abdenour/miniconda3/envs/fl_with_flower/lib/python3.11/site-packages/ray/_private/auto_init_hook.py", line 21, in auto_init_wrapper
    return fn(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^
  File "/home/abdenour/miniconda3/envs/fl_with_flower/lib/python3.11/site-packages/ray/_private/client_mode_hook.py", line 103, in wrapper
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "/home/abdenour/miniconda3/envs/fl_with_flower/lib/python3.11/site-packages/ray/_private/worker.py", line 2667, in get
    values, debugger_breakpoint = worker.get_objects(object_refs, timeout=timeout)
                                  ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/abdenour/miniconda3/envs/fl_with_flower/lib/python3.11/site-packages/ray/_private/worker.py", line 866, in get_objects
    raise value
ray.exceptions.OutOfMemoryError: Task was killed due to the node running low on memory.
Memory on the node (IP: 115.145.171.128, ID: c687613ff34cdbf5097699e5c896055ef2dc44343c5ab44bd40309d5) where the task (actor ID: 85928d3f1a247800081c0e1101000000, name=ClientAppActor.__init__, pid=619614, memory used=1.73GB) was running was 29.50GB / 31.02GB (0.951049), which exceeds the memory usage threshold of 0.95. Ray killed this worker (ID: c7f644ac3cca20dffbfa1723daac4f87f04c7de03da25e7cad5fe33c) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 115.145.171.128`. To see the logs of the worker, use `ray logs worker-c7f644ac3cca20dffbfa1723daac4f87f04c7de03da25e7cad5fe33c*out -ip 115.145.171.128. Top 10 memory users:
PID	MEM(GB)	COMMAND
4942	3.49	/snap/pycharm-professional/408/jbr/bin/java -classpath /snap/pycharm-professional/408/lib/platform-l...
619616	1.80	ray::ClientAppActor.run
619615	1.78	ray::ClientAppActor
619617	1.77	ray::ClientAppActor
619611	1.73	ray::ClientAppActor
619618	1.73	ray::ClientAppActor
619614	1.73	ray::ClientAppActor
619612	1.72	ray::ClientAppActor
619619	1.70	ray::ClientAppActor
619620	1.68	ray::ClientAppActor.run
Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.

[2024-08-13 23:52:09,919][flwr][ERROR] - Task was killed due to the node running low on memory.
Memory on the node (IP: 115.145.171.128, ID: c687613ff34cdbf5097699e5c896055ef2dc44343c5ab44bd40309d5) where the task (actor ID: 85928d3f1a247800081c0e1101000000, name=ClientAppActor.__init__, pid=619614, memory used=1.73GB) was running was 29.50GB / 31.02GB (0.951049), which exceeds the memory usage threshold of 0.95. Ray killed this worker (ID: c7f644ac3cca20dffbfa1723daac4f87f04c7de03da25e7cad5fe33c) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 115.145.171.128`. To see the logs of the worker, use `ray logs worker-c7f644ac3cca20dffbfa1723daac4f87f04c7de03da25e7cad5fe33c*out -ip 115.145.171.128. Top 10 memory users:
PID	MEM(GB)	COMMAND
4942	3.49	/snap/pycharm-professional/408/jbr/bin/java -classpath /snap/pycharm-professional/408/lib/platform-l...
619616	1.80	ray::ClientAppActor.run
619615	1.78	ray::ClientAppActor
619617	1.77	ray::ClientAppActor
619611	1.73	ray::ClientAppActor
619618	1.73	ray::ClientAppActor
619614	1.73	ray::ClientAppActor
619612	1.72	ray::ClientAppActor
619619	1.70	ray::ClientAppActor
619620	1.68	ray::ClientAppActor.run
Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.
[2024-08-13 23:52:13,562][flwr][INFO] - aggregate_fit: received 9 results and 1 failures
[2024-08-13 23:52:16,781][flwr][INFO] - fit progress: (217, 17.944170832633972, {'accuracy': 0.7339743589743589}, 1258.0411017500155)
[2024-08-13 23:52:16,781][flwr][INFO] - configure_evaluate: strategy sampled 2 clients (out of 20)
[2024-08-13 23:52:17,008][flwr][INFO] - aggregate_evaluate: received 2 results and 0 failures
[2024-08-13 23:52:17,008][flwr][INFO] - 
[2024-08-13 23:52:17,008][flwr][INFO] - [ROUND 218]
[2024-08-13 23:52:17,008][flwr][INFO] - configure_fit: strategy sampled 10 clients (out of 20)
[2024-08-13 23:52:17,317][flwr][ERROR] - Traceback (most recent call last):
  File "/home/abdenour/miniconda3/envs/fl_with_flower/lib/python3.11/site-packages/flwr/simulation/ray_transport/ray_client_proxy.py", line 94, in _submit_job
    out_mssg, updated_context = self.actor_pool.get_client_result(
                                ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/abdenour/miniconda3/envs/fl_with_flower/lib/python3.11/site-packages/flwr/simulation/ray_transport/ray_actor.py", line 398, in get_client_result
    return self._fetch_future_result(cid)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/abdenour/miniconda3/envs/fl_with_flower/lib/python3.11/site-packages/flwr/simulation/ray_transport/ray_actor.py", line 279, in _fetch_future_result
    res_cid, out_mssg, updated_context = ray.get(
                                         ^^^^^^^^
  File "/home/abdenour/miniconda3/envs/fl_with_flower/lib/python3.11/site-packages/ray/_private/auto_init_hook.py", line 21, in auto_init_wrapper
    return fn(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^
  File "/home/abdenour/miniconda3/envs/fl_with_flower/lib/python3.11/site-packages/ray/_private/client_mode_hook.py", line 103, in wrapper
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "/home/abdenour/miniconda3/envs/fl_with_flower/lib/python3.11/site-packages/ray/_private/worker.py", line 2667, in get
    values, debugger_breakpoint = worker.get_objects(object_refs, timeout=timeout)
                                  ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/abdenour/miniconda3/envs/fl_with_flower/lib/python3.11/site-packages/ray/_private/worker.py", line 866, in get_objects
    raise value
ray.exceptions.OutOfMemoryError: Task was killed due to the node running low on memory.
Memory on the node (IP: 115.145.171.128, ID: c687613ff34cdbf5097699e5c896055ef2dc44343c5ab44bd40309d5) where the task (actor ID: 85928d3f1a247800081c0e1101000000, name=ClientAppActor.__init__, pid=619614, memory used=1.73GB) was running was 29.50GB / 31.02GB (0.951049), which exceeds the memory usage threshold of 0.95. Ray killed this worker (ID: c7f644ac3cca20dffbfa1723daac4f87f04c7de03da25e7cad5fe33c) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 115.145.171.128`. To see the logs of the worker, use `ray logs worker-c7f644ac3cca20dffbfa1723daac4f87f04c7de03da25e7cad5fe33c*out -ip 115.145.171.128. Top 10 memory users:
PID	MEM(GB)	COMMAND
4942	3.49	/snap/pycharm-professional/408/jbr/bin/java -classpath /snap/pycharm-professional/408/lib/platform-l...
619616	1.80	ray::ClientAppActor.run
619615	1.78	ray::ClientAppActor
619617	1.77	ray::ClientAppActor
619611	1.73	ray::ClientAppActor
619618	1.73	ray::ClientAppActor
619614	1.73	ray::ClientAppActor
619612	1.72	ray::ClientAppActor
619619	1.70	ray::ClientAppActor
619620	1.68	ray::ClientAppActor.run
Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.

[2024-08-13 23:52:17,317][flwr][ERROR] - Task was killed due to the node running low on memory.
Memory on the node (IP: 115.145.171.128, ID: c687613ff34cdbf5097699e5c896055ef2dc44343c5ab44bd40309d5) where the task (actor ID: 85928d3f1a247800081c0e1101000000, name=ClientAppActor.__init__, pid=619614, memory used=1.73GB) was running was 29.50GB / 31.02GB (0.951049), which exceeds the memory usage threshold of 0.95. Ray killed this worker (ID: c7f644ac3cca20dffbfa1723daac4f87f04c7de03da25e7cad5fe33c) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 115.145.171.128`. To see the logs of the worker, use `ray logs worker-c7f644ac3cca20dffbfa1723daac4f87f04c7de03da25e7cad5fe33c*out -ip 115.145.171.128. Top 10 memory users:
PID	MEM(GB)	COMMAND
4942	3.49	/snap/pycharm-professional/408/jbr/bin/java -classpath /snap/pycharm-professional/408/lib/platform-l...
619616	1.80	ray::ClientAppActor.run
619615	1.78	ray::ClientAppActor
619617	1.77	ray::ClientAppActor
619611	1.73	ray::ClientAppActor
619618	1.73	ray::ClientAppActor
619614	1.73	ray::ClientAppActor
619612	1.72	ray::ClientAppActor
619619	1.70	ray::ClientAppActor
619620	1.68	ray::ClientAppActor.run
Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.
[2024-08-13 23:52:21,847][flwr][INFO] - aggregate_fit: received 9 results and 1 failures
[2024-08-13 23:52:25,019][flwr][INFO] - fit progress: (218, 15.447046935558319, {'accuracy': 0.75}, 1266.2794387820177)
[2024-08-13 23:52:25,019][flwr][INFO] - configure_evaluate: strategy sampled 2 clients (out of 20)
[2024-08-13 23:52:25,711][flwr][INFO] - aggregate_evaluate: received 2 results and 0 failures
[2024-08-13 23:52:25,711][flwr][INFO] - 
[2024-08-13 23:52:25,711][flwr][INFO] - [ROUND 219]
[2024-08-13 23:52:25,711][flwr][INFO] - configure_fit: strategy sampled 10 clients (out of 20)
[2024-08-13 23:52:25,846][flwr][ERROR] - Traceback (most recent call last):
  File "/home/abdenour/miniconda3/envs/fl_with_flower/lib/python3.11/site-packages/flwr/simulation/ray_transport/ray_client_proxy.py", line 94, in _submit_job
    out_mssg, updated_context = self.actor_pool.get_client_result(
                                ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/abdenour/miniconda3/envs/fl_with_flower/lib/python3.11/site-packages/flwr/simulation/ray_transport/ray_actor.py", line 398, in get_client_result
    return self._fetch_future_result(cid)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/abdenour/miniconda3/envs/fl_with_flower/lib/python3.11/site-packages/flwr/simulation/ray_transport/ray_actor.py", line 279, in _fetch_future_result
    res_cid, out_mssg, updated_context = ray.get(
                                         ^^^^^^^^
  File "/home/abdenour/miniconda3/envs/fl_with_flower/lib/python3.11/site-packages/ray/_private/auto_init_hook.py", line 21, in auto_init_wrapper
    return fn(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^
  File "/home/abdenour/miniconda3/envs/fl_with_flower/lib/python3.11/site-packages/ray/_private/client_mode_hook.py", line 103, in wrapper
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "/home/abdenour/miniconda3/envs/fl_with_flower/lib/python3.11/site-packages/ray/_private/worker.py", line 2667, in get
    values, debugger_breakpoint = worker.get_objects(object_refs, timeout=timeout)
                                  ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/abdenour/miniconda3/envs/fl_with_flower/lib/python3.11/site-packages/ray/_private/worker.py", line 866, in get_objects
    raise value
ray.exceptions.OutOfMemoryError: Task was killed due to the node running low on memory.
Memory on the node (IP: 115.145.171.128, ID: c687613ff34cdbf5097699e5c896055ef2dc44343c5ab44bd40309d5) where the task (actor ID: 85928d3f1a247800081c0e1101000000, name=ClientAppActor.__init__, pid=619614, memory used=1.73GB) was running was 29.50GB / 31.02GB (0.951049), which exceeds the memory usage threshold of 0.95. Ray killed this worker (ID: c7f644ac3cca20dffbfa1723daac4f87f04c7de03da25e7cad5fe33c) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 115.145.171.128`. To see the logs of the worker, use `ray logs worker-c7f644ac3cca20dffbfa1723daac4f87f04c7de03da25e7cad5fe33c*out -ip 115.145.171.128. Top 10 memory users:
PID	MEM(GB)	COMMAND
4942	3.49	/snap/pycharm-professional/408/jbr/bin/java -classpath /snap/pycharm-professional/408/lib/platform-l...
619616	1.80	ray::ClientAppActor.run
619615	1.78	ray::ClientAppActor
619617	1.77	ray::ClientAppActor
619611	1.73	ray::ClientAppActor
619618	1.73	ray::ClientAppActor
619614	1.73	ray::ClientAppActor
619612	1.72	ray::ClientAppActor
619619	1.70	ray::ClientAppActor
619620	1.68	ray::ClientAppActor.run
Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.

[2024-08-13 23:52:25,846][flwr][ERROR] - Task was killed due to the node running low on memory.
Memory on the node (IP: 115.145.171.128, ID: c687613ff34cdbf5097699e5c896055ef2dc44343c5ab44bd40309d5) where the task (actor ID: 85928d3f1a247800081c0e1101000000, name=ClientAppActor.__init__, pid=619614, memory used=1.73GB) was running was 29.50GB / 31.02GB (0.951049), which exceeds the memory usage threshold of 0.95. Ray killed this worker (ID: c7f644ac3cca20dffbfa1723daac4f87f04c7de03da25e7cad5fe33c) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 115.145.171.128`. To see the logs of the worker, use `ray logs worker-c7f644ac3cca20dffbfa1723daac4f87f04c7de03da25e7cad5fe33c*out -ip 115.145.171.128. Top 10 memory users:
PID	MEM(GB)	COMMAND
4942	3.49	/snap/pycharm-professional/408/jbr/bin/java -classpath /snap/pycharm-professional/408/lib/platform-l...
619616	1.80	ray::ClientAppActor.run
619615	1.78	ray::ClientAppActor
619617	1.77	ray::ClientAppActor
619611	1.73	ray::ClientAppActor
619618	1.73	ray::ClientAppActor
619614	1.73	ray::ClientAppActor
619612	1.72	ray::ClientAppActor
619619	1.70	ray::ClientAppActor
619620	1.68	ray::ClientAppActor.run
Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.
[2024-08-13 23:52:29,449][flwr][INFO] - aggregate_fit: received 9 results and 1 failures
[2024-08-13 23:52:32,438][flwr][INFO] - fit progress: (219, 14.304011225700378, {'accuracy': 0.7516025641025641}, 1273.6984535160009)
[2024-08-13 23:52:32,438][flwr][INFO] - configure_evaluate: strategy sampled 2 clients (out of 20)
[2024-08-13 23:52:32,942][flwr][INFO] - aggregate_evaluate: received 2 results and 0 failures
[2024-08-13 23:52:32,942][flwr][INFO] - 
[2024-08-13 23:52:32,942][flwr][INFO] - [ROUND 220]
[2024-08-13 23:52:32,942][flwr][INFO] - configure_fit: strategy sampled 10 clients (out of 20)
[2024-08-13 23:52:33,118][flwr][ERROR] - Traceback (most recent call last):
  File "/home/abdenour/miniconda3/envs/fl_with_flower/lib/python3.11/site-packages/flwr/simulation/ray_transport/ray_client_proxy.py", line 94, in _submit_job
    out_mssg, updated_context = self.actor_pool.get_client_result(
                                ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/abdenour/miniconda3/envs/fl_with_flower/lib/python3.11/site-packages/flwr/simulation/ray_transport/ray_actor.py", line 398, in get_client_result
    return self._fetch_future_result(cid)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/abdenour/miniconda3/envs/fl_with_flower/lib/python3.11/site-packages/flwr/simulation/ray_transport/ray_actor.py", line 279, in _fetch_future_result
    res_cid, out_mssg, updated_context = ray.get(
                                         ^^^^^^^^
  File "/home/abdenour/miniconda3/envs/fl_with_flower/lib/python3.11/site-packages/ray/_private/auto_init_hook.py", line 21, in auto_init_wrapper
    return fn(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^
  File "/home/abdenour/miniconda3/envs/fl_with_flower/lib/python3.11/site-packages/ray/_private/client_mode_hook.py", line 103, in wrapper
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "/home/abdenour/miniconda3/envs/fl_with_flower/lib/python3.11/site-packages/ray/_private/worker.py", line 2667, in get
    values, debugger_breakpoint = worker.get_objects(object_refs, timeout=timeout)
                                  ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/abdenour/miniconda3/envs/fl_with_flower/lib/python3.11/site-packages/ray/_private/worker.py", line 866, in get_objects
    raise value
ray.exceptions.OutOfMemoryError: Task was killed due to the node running low on memory.
Memory on the node (IP: 115.145.171.128, ID: c687613ff34cdbf5097699e5c896055ef2dc44343c5ab44bd40309d5) where the task (actor ID: 85928d3f1a247800081c0e1101000000, name=ClientAppActor.__init__, pid=619614, memory used=1.73GB) was running was 29.50GB / 31.02GB (0.951049), which exceeds the memory usage threshold of 0.95. Ray killed this worker (ID: c7f644ac3cca20dffbfa1723daac4f87f04c7de03da25e7cad5fe33c) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 115.145.171.128`. To see the logs of the worker, use `ray logs worker-c7f644ac3cca20dffbfa1723daac4f87f04c7de03da25e7cad5fe33c*out -ip 115.145.171.128. Top 10 memory users:
PID	MEM(GB)	COMMAND
4942	3.49	/snap/pycharm-professional/408/jbr/bin/java -classpath /snap/pycharm-professional/408/lib/platform-l...
619616	1.80	ray::ClientAppActor.run
619615	1.78	ray::ClientAppActor
619617	1.77	ray::ClientAppActor
619611	1.73	ray::ClientAppActor
619618	1.73	ray::ClientAppActor
619614	1.73	ray::ClientAppActor
619612	1.72	ray::ClientAppActor
619619	1.70	ray::ClientAppActor
619620	1.68	ray::ClientAppActor.run
Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.

[2024-08-13 23:52:33,118][flwr][ERROR] - Task was killed due to the node running low on memory.
Memory on the node (IP: 115.145.171.128, ID: c687613ff34cdbf5097699e5c896055ef2dc44343c5ab44bd40309d5) where the task (actor ID: 85928d3f1a247800081c0e1101000000, name=ClientAppActor.__init__, pid=619614, memory used=1.73GB) was running was 29.50GB / 31.02GB (0.951049), which exceeds the memory usage threshold of 0.95. Ray killed this worker (ID: c7f644ac3cca20dffbfa1723daac4f87f04c7de03da25e7cad5fe33c) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 115.145.171.128`. To see the logs of the worker, use `ray logs worker-c7f644ac3cca20dffbfa1723daac4f87f04c7de03da25e7cad5fe33c*out -ip 115.145.171.128. Top 10 memory users:
PID	MEM(GB)	COMMAND
4942	3.49	/snap/pycharm-professional/408/jbr/bin/java -classpath /snap/pycharm-professional/408/lib/platform-l...
619616	1.80	ray::ClientAppActor.run
619615	1.78	ray::ClientAppActor
619617	1.77	ray::ClientAppActor
619611	1.73	ray::ClientAppActor
619618	1.73	ray::ClientAppActor
619614	1.73	ray::ClientAppActor
619612	1.72	ray::ClientAppActor
619619	1.70	ray::ClientAppActor
619620	1.68	ray::ClientAppActor.run
Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.
[2024-08-13 23:52:35,566][flwr][INFO] - aggregate_fit: received 9 results and 1 failures
[2024-08-13 23:52:38,627][flwr][INFO] - fit progress: (220, 12.830143511295319, {'accuracy': 0.7548076923076923}, 1279.886904612009)
[2024-08-13 23:52:38,627][flwr][INFO] - configure_evaluate: strategy sampled 2 clients (out of 20)
[2024-08-13 23:52:38,874][flwr][INFO] - aggregate_evaluate: received 2 results and 0 failures
[2024-08-13 23:52:38,874][flwr][INFO] - 
[2024-08-13 23:52:38,874][flwr][INFO] - [ROUND 221]
[2024-08-13 23:52:38,874][flwr][INFO] - configure_fit: strategy sampled 10 clients (out of 20)
[2024-08-13 23:52:39,111][flwr][ERROR] - Traceback (most recent call last):
  File "/home/abdenour/miniconda3/envs/fl_with_flower/lib/python3.11/site-packages/flwr/simulation/ray_transport/ray_client_proxy.py", line 94, in _submit_job
    out_mssg, updated_context = self.actor_pool.get_client_result(
                                ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/abdenour/miniconda3/envs/fl_with_flower/lib/python3.11/site-packages/flwr/simulation/ray_transport/ray_actor.py", line 398, in get_client_result
    return self._fetch_future_result(cid)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/abdenour/miniconda3/envs/fl_with_flower/lib/python3.11/site-packages/flwr/simulation/ray_transport/ray_actor.py", line 279, in _fetch_future_result
    res_cid, out_mssg, updated_context = ray.get(
                                         ^^^^^^^^
  File "/home/abdenour/miniconda3/envs/fl_with_flower/lib/python3.11/site-packages/ray/_private/auto_init_hook.py", line 21, in auto_init_wrapper
    return fn(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^
  File "/home/abdenour/miniconda3/envs/fl_with_flower/lib/python3.11/site-packages/ray/_private/client_mode_hook.py", line 103, in wrapper
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "/home/abdenour/miniconda3/envs/fl_with_flower/lib/python3.11/site-packages/ray/_private/worker.py", line 2667, in get
    values, debugger_breakpoint = worker.get_objects(object_refs, timeout=timeout)
                                  ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/abdenour/miniconda3/envs/fl_with_flower/lib/python3.11/site-packages/ray/_private/worker.py", line 866, in get_objects
    raise value
ray.exceptions.OutOfMemoryError: Task was killed due to the node running low on memory.
Memory on the node (IP: 115.145.171.128, ID: c687613ff34cdbf5097699e5c896055ef2dc44343c5ab44bd40309d5) where the task (actor ID: 52365bb7997979e5a4c7ddbd01000000, name=ClientAppActor.__init__, pid=619618, memory used=1.84GB) was running was 29.49GB / 31.02GB (0.950707), which exceeds the memory usage threshold of 0.95. Ray killed this worker (ID: 7e48cd46c72821b17be65b05a4e61b5846ba998a57d76db44ca4f8fa) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 115.145.171.128`. To see the logs of the worker, use `ray logs worker-7e48cd46c72821b17be65b05a4e61b5846ba998a57d76db44ca4f8fa*out -ip 115.145.171.128. Top 10 memory users:
PID	MEM(GB)	COMMAND
4942	3.48	/snap/pycharm-professional/408/jbr/bin/java -classpath /snap/pycharm-professional/408/lib/platform-l...
616443	2.14	/home/abdenour/miniconda3/envs/fl_with_flower/bin/python /home/abdenour/PycharmProjects/fl_with_flow...
619615	1.90	ray::ClientAppActor
619616	1.88	ray::ClientAppActor
619611	1.87	ray::ClientAppActor
619617	1.86	ray::ClientAppActor
619620	1.85	ray::ClientAppActor
619618	1.84	ray::ClientAppActor
619613	1.82	ray::ClientAppActor
619619	1.80	ray::ClientAppActor
Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.

[2024-08-13 23:52:39,111][flwr][ERROR] - Task was killed due to the node running low on memory.
Memory on the node (IP: 115.145.171.128, ID: c687613ff34cdbf5097699e5c896055ef2dc44343c5ab44bd40309d5) where the task (actor ID: 52365bb7997979e5a4c7ddbd01000000, name=ClientAppActor.__init__, pid=619618, memory used=1.84GB) was running was 29.49GB / 31.02GB (0.950707), which exceeds the memory usage threshold of 0.95. Ray killed this worker (ID: 7e48cd46c72821b17be65b05a4e61b5846ba998a57d76db44ca4f8fa) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 115.145.171.128`. To see the logs of the worker, use `ray logs worker-7e48cd46c72821b17be65b05a4e61b5846ba998a57d76db44ca4f8fa*out -ip 115.145.171.128. Top 10 memory users:
PID	MEM(GB)	COMMAND
4942	3.48	/snap/pycharm-professional/408/jbr/bin/java -classpath /snap/pycharm-professional/408/lib/platform-l...
616443	2.14	/home/abdenour/miniconda3/envs/fl_with_flower/bin/python /home/abdenour/PycharmProjects/fl_with_flow...
619615	1.90	ray::ClientAppActor
619616	1.88	ray::ClientAppActor
619611	1.87	ray::ClientAppActor
619617	1.86	ray::ClientAppActor
619620	1.85	ray::ClientAppActor
619618	1.84	ray::ClientAppActor
619613	1.82	ray::ClientAppActor
619619	1.80	ray::ClientAppActor
Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.
[2024-08-13 23:52:39,119][flwr][ERROR] - Traceback (most recent call last):
  File "/home/abdenour/miniconda3/envs/fl_with_flower/lib/python3.11/site-packages/flwr/simulation/ray_transport/ray_client_proxy.py", line 94, in _submit_job
    out_mssg, updated_context = self.actor_pool.get_client_result(
                                ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/abdenour/miniconda3/envs/fl_with_flower/lib/python3.11/site-packages/flwr/simulation/ray_transport/ray_actor.py", line 398, in get_client_result
    return self._fetch_future_result(cid)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/abdenour/miniconda3/envs/fl_with_flower/lib/python3.11/site-packages/flwr/simulation/ray_transport/ray_actor.py", line 279, in _fetch_future_result
    res_cid, out_mssg, updated_context = ray.get(
                                         ^^^^^^^^
  File "/home/abdenour/miniconda3/envs/fl_with_flower/lib/python3.11/site-packages/ray/_private/auto_init_hook.py", line 21, in auto_init_wrapper
    return fn(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^
  File "/home/abdenour/miniconda3/envs/fl_with_flower/lib/python3.11/site-packages/ray/_private/client_mode_hook.py", line 103, in wrapper
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "/home/abdenour/miniconda3/envs/fl_with_flower/lib/python3.11/site-packages/ray/_private/worker.py", line 2667, in get
    values, debugger_breakpoint = worker.get_objects(object_refs, timeout=timeout)
                                  ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/abdenour/miniconda3/envs/fl_with_flower/lib/python3.11/site-packages/ray/_private/worker.py", line 866, in get_objects
    raise value
ray.exceptions.OutOfMemoryError: Task was killed due to the node running low on memory.
Memory on the node (IP: 115.145.171.128, ID: c687613ff34cdbf5097699e5c896055ef2dc44343c5ab44bd40309d5) where the task (actor ID: 52365bb7997979e5a4c7ddbd01000000, name=ClientAppActor.__init__, pid=619618, memory used=1.84GB) was running was 29.49GB / 31.02GB (0.950707), which exceeds the memory usage threshold of 0.95. Ray killed this worker (ID: 7e48cd46c72821b17be65b05a4e61b5846ba998a57d76db44ca4f8fa) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 115.145.171.128`. To see the logs of the worker, use `ray logs worker-7e48cd46c72821b17be65b05a4e61b5846ba998a57d76db44ca4f8fa*out -ip 115.145.171.128. Top 10 memory users:
PID	MEM(GB)	COMMAND
4942	3.48	/snap/pycharm-professional/408/jbr/bin/java -classpath /snap/pycharm-professional/408/lib/platform-l...
616443	2.14	/home/abdenour/miniconda3/envs/fl_with_flower/bin/python /home/abdenour/PycharmProjects/fl_with_flow...
619615	1.90	ray::ClientAppActor
619616	1.88	ray::ClientAppActor
619611	1.87	ray::ClientAppActor
619617	1.86	ray::ClientAppActor
619620	1.85	ray::ClientAppActor
619618	1.84	ray::ClientAppActor
619613	1.82	ray::ClientAppActor
619619	1.80	ray::ClientAppActor
Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.

[2024-08-13 23:52:39,119][flwr][ERROR] - Task was killed due to the node running low on memory.
Memory on the node (IP: 115.145.171.128, ID: c687613ff34cdbf5097699e5c896055ef2dc44343c5ab44bd40309d5) where the task (actor ID: 52365bb7997979e5a4c7ddbd01000000, name=ClientAppActor.__init__, pid=619618, memory used=1.84GB) was running was 29.49GB / 31.02GB (0.950707), which exceeds the memory usage threshold of 0.95. Ray killed this worker (ID: 7e48cd46c72821b17be65b05a4e61b5846ba998a57d76db44ca4f8fa) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 115.145.171.128`. To see the logs of the worker, use `ray logs worker-7e48cd46c72821b17be65b05a4e61b5846ba998a57d76db44ca4f8fa*out -ip 115.145.171.128. Top 10 memory users:
PID	MEM(GB)	COMMAND
4942	3.48	/snap/pycharm-professional/408/jbr/bin/java -classpath /snap/pycharm-professional/408/lib/platform-l...
616443	2.14	/home/abdenour/miniconda3/envs/fl_with_flower/bin/python /home/abdenour/PycharmProjects/fl_with_flow...
619615	1.90	ray::ClientAppActor
619616	1.88	ray::ClientAppActor
619611	1.87	ray::ClientAppActor
619617	1.86	ray::ClientAppActor
619620	1.85	ray::ClientAppActor
619618	1.84	ray::ClientAppActor
619613	1.82	ray::ClientAppActor
619619	1.80	ray::ClientAppActor
Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.
[2024-08-13 23:52:41,923][flwr][INFO] - aggregate_fit: received 8 results and 2 failures
[2024-08-13 23:52:45,153][flwr][INFO] - fit progress: (221, 15.216641783714294, {'accuracy': 0.75}, 1286.4135785770195)
[2024-08-13 23:52:45,153][flwr][INFO] - configure_evaluate: strategy sampled 2 clients (out of 20)
[2024-08-13 23:52:45,828][flwr][INFO] - aggregate_evaluate: received 2 results and 0 failures
[2024-08-13 23:52:45,828][flwr][INFO] - 
[2024-08-13 23:52:45,828][flwr][INFO] - [ROUND 222]
[2024-08-13 23:52:45,828][flwr][INFO] - configure_fit: strategy sampled 10 clients (out of 20)
[2024-08-13 23:52:45,996][flwr][ERROR] - Traceback (most recent call last):
  File "/home/abdenour/miniconda3/envs/fl_with_flower/lib/python3.11/site-packages/flwr/simulation/ray_transport/ray_client_proxy.py", line 94, in _submit_job
    out_mssg, updated_context = self.actor_pool.get_client_result(
                                ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/abdenour/miniconda3/envs/fl_with_flower/lib/python3.11/site-packages/flwr/simulation/ray_transport/ray_actor.py", line 398, in get_client_result
    return self._fetch_future_result(cid)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/abdenour/miniconda3/envs/fl_with_flower/lib/python3.11/site-packages/flwr/simulation/ray_transport/ray_actor.py", line 279, in _fetch_future_result
    res_cid, out_mssg, updated_context = ray.get(
                                         ^^^^^^^^
  File "/home/abdenour/miniconda3/envs/fl_with_flower/lib/python3.11/site-packages/ray/_private/auto_init_hook.py", line 21, in auto_init_wrapper
    return fn(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^
  File "/home/abdenour/miniconda3/envs/fl_with_flower/lib/python3.11/site-packages/ray/_private/client_mode_hook.py", line 103, in wrapper
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "/home/abdenour/miniconda3/envs/fl_with_flower/lib/python3.11/site-packages/ray/_private/worker.py", line 2667, in get
    values, debugger_breakpoint = worker.get_objects(object_refs, timeout=timeout)
                                  ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/abdenour/miniconda3/envs/fl_with_flower/lib/python3.11/site-packages/ray/_private/worker.py", line 866, in get_objects
    raise value
ray.exceptions.OutOfMemoryError: Task was killed due to the node running low on memory.
Memory on the node (IP: 115.145.171.128, ID: c687613ff34cdbf5097699e5c896055ef2dc44343c5ab44bd40309d5) where the task (actor ID: 52365bb7997979e5a4c7ddbd01000000, name=ClientAppActor.__init__, pid=619618, memory used=1.84GB) was running was 29.49GB / 31.02GB (0.950707), which exceeds the memory usage threshold of 0.95. Ray killed this worker (ID: 7e48cd46c72821b17be65b05a4e61b5846ba998a57d76db44ca4f8fa) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 115.145.171.128`. To see the logs of the worker, use `ray logs worker-7e48cd46c72821b17be65b05a4e61b5846ba998a57d76db44ca4f8fa*out -ip 115.145.171.128. Top 10 memory users:
PID	MEM(GB)	COMMAND
4942	3.48	/snap/pycharm-professional/408/jbr/bin/java -classpath /snap/pycharm-professional/408/lib/platform-l...
616443	2.14	/home/abdenour/miniconda3/envs/fl_with_flower/bin/python /home/abdenour/PycharmProjects/fl_with_flow...
619615	1.90	ray::ClientAppActor
619616	1.88	ray::ClientAppActor
619611	1.87	ray::ClientAppActor
619617	1.86	ray::ClientAppActor
619620	1.85	ray::ClientAppActor
619618	1.84	ray::ClientAppActor
619613	1.82	ray::ClientAppActor
619619	1.80	ray::ClientAppActor
Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.

[2024-08-13 23:52:45,997][flwr][ERROR] - Traceback (most recent call last):
  File "/home/abdenour/miniconda3/envs/fl_with_flower/lib/python3.11/site-packages/flwr/simulation/ray_transport/ray_client_proxy.py", line 94, in _submit_job
    out_mssg, updated_context = self.actor_pool.get_client_result(
                                ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/abdenour/miniconda3/envs/fl_with_flower/lib/python3.11/site-packages/flwr/simulation/ray_transport/ray_actor.py", line 398, in get_client_result
    return self._fetch_future_result(cid)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/abdenour/miniconda3/envs/fl_with_flower/lib/python3.11/site-packages/flwr/simulation/ray_transport/ray_actor.py", line 279, in _fetch_future_result
    res_cid, out_mssg, updated_context = ray.get(
                                         ^^^^^^^^
  File "/home/abdenour/miniconda3/envs/fl_with_flower/lib/python3.11/site-packages/ray/_private/auto_init_hook.py", line 21, in auto_init_wrapper
    return fn(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^
  File "/home/abdenour/miniconda3/envs/fl_with_flower/lib/python3.11/site-packages/ray/_private/client_mode_hook.py", line 103, in wrapper
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "/home/abdenour/miniconda3/envs/fl_with_flower/lib/python3.11/site-packages/ray/_private/worker.py", line 2667, in get
    values, debugger_breakpoint = worker.get_objects(object_refs, timeout=timeout)
                                  ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/abdenour/miniconda3/envs/fl_with_flower/lib/python3.11/site-packages/ray/_private/worker.py", line 866, in get_objects
    raise value
ray.exceptions.OutOfMemoryError: Task was killed due to the node running low on memory.
Memory on the node (IP: 115.145.171.128, ID: c687613ff34cdbf5097699e5c896055ef2dc44343c5ab44bd40309d5) where the task (actor ID: 85928d3f1a247800081c0e1101000000, name=ClientAppActor.__init__, pid=619614, memory used=1.73GB) was running was 29.50GB / 31.02GB (0.951049), which exceeds the memory usage threshold of 0.95. Ray killed this worker (ID: c7f644ac3cca20dffbfa1723daac4f87f04c7de03da25e7cad5fe33c) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 115.145.171.128`. To see the logs of the worker, use `ray logs worker-c7f644ac3cca20dffbfa1723daac4f87f04c7de03da25e7cad5fe33c*out -ip 115.145.171.128. Top 10 memory users:
PID	MEM(GB)	COMMAND
4942	3.49	/snap/pycharm-professional/408/jbr/bin/java -classpath /snap/pycharm-professional/408/lib/platform-l...
619616	1.80	ray::ClientAppActor.run
619615	1.78	ray::ClientAppActor
619617	1.77	ray::ClientAppActor
619611	1.73	ray::ClientAppActor
619618	1.73	ray::ClientAppActor
619614	1.73	ray::ClientAppActor
619612	1.72	ray::ClientAppActor
619619	1.70	ray::ClientAppActor
619620	1.68	ray::ClientAppActor.run
Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.

[2024-08-13 23:52:45,997][flwr][ERROR] - Task was killed due to the node running low on memory.
Memory on the node (IP: 115.145.171.128, ID: c687613ff34cdbf5097699e5c896055ef2dc44343c5ab44bd40309d5) where the task (actor ID: 52365bb7997979e5a4c7ddbd01000000, name=ClientAppActor.__init__, pid=619618, memory used=1.84GB) was running was 29.49GB / 31.02GB (0.950707), which exceeds the memory usage threshold of 0.95. Ray killed this worker (ID: 7e48cd46c72821b17be65b05a4e61b5846ba998a57d76db44ca4f8fa) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 115.145.171.128`. To see the logs of the worker, use `ray logs worker-7e48cd46c72821b17be65b05a4e61b5846ba998a57d76db44ca4f8fa*out -ip 115.145.171.128. Top 10 memory users:
PID	MEM(GB)	COMMAND
4942	3.48	/snap/pycharm-professional/408/jbr/bin/java -classpath /snap/pycharm-professional/408/lib/platform-l...
616443	2.14	/home/abdenour/miniconda3/envs/fl_with_flower/bin/python /home/abdenour/PycharmProjects/fl_with_flow...
619615	1.90	ray::ClientAppActor
619616	1.88	ray::ClientAppActor
619611	1.87	ray::ClientAppActor
619617	1.86	ray::ClientAppActor
619620	1.85	ray::ClientAppActor
619618	1.84	ray::ClientAppActor
619613	1.82	ray::ClientAppActor
619619	1.80	ray::ClientAppActor
Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.
[2024-08-13 23:52:45,997][flwr][ERROR] - Task was killed due to the node running low on memory.
Memory on the node (IP: 115.145.171.128, ID: c687613ff34cdbf5097699e5c896055ef2dc44343c5ab44bd40309d5) where the task (actor ID: 85928d3f1a247800081c0e1101000000, name=ClientAppActor.__init__, pid=619614, memory used=1.73GB) was running was 29.50GB / 31.02GB (0.951049), which exceeds the memory usage threshold of 0.95. Ray killed this worker (ID: c7f644ac3cca20dffbfa1723daac4f87f04c7de03da25e7cad5fe33c) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 115.145.171.128`. To see the logs of the worker, use `ray logs worker-c7f644ac3cca20dffbfa1723daac4f87f04c7de03da25e7cad5fe33c*out -ip 115.145.171.128. Top 10 memory users:
PID	MEM(GB)	COMMAND
4942	3.49	/snap/pycharm-professional/408/jbr/bin/java -classpath /snap/pycharm-professional/408/lib/platform-l...
619616	1.80	ray::ClientAppActor.run
619615	1.78	ray::ClientAppActor
619617	1.77	ray::ClientAppActor
619611	1.73	ray::ClientAppActor
619618	1.73	ray::ClientAppActor
619614	1.73	ray::ClientAppActor
619612	1.72	ray::ClientAppActor
619619	1.70	ray::ClientAppActor
619620	1.68	ray::ClientAppActor.run
Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.
[2024-08-13 23:52:49,295][flwr][INFO] - aggregate_fit: received 8 results and 2 failures
[2024-08-13 23:52:52,654][flwr][INFO] - fit progress: (222, 15.165506720542908, {'accuracy': 0.7548076923076923}, 1293.9143277599942)
[2024-08-13 23:52:52,654][flwr][INFO] - configure_evaluate: strategy sampled 2 clients (out of 20)
[2024-08-13 23:52:53,192][flwr][INFO] - aggregate_evaluate: received 2 results and 0 failures
[2024-08-13 23:52:53,192][flwr][INFO] - 
[2024-08-13 23:52:53,192][flwr][INFO] - [ROUND 223]
[2024-08-13 23:52:53,192][flwr][INFO] - configure_fit: strategy sampled 10 clients (out of 20)
[2024-08-13 23:52:53,350][flwr][ERROR] - Traceback (most recent call last):
  File "/home/abdenour/miniconda3/envs/fl_with_flower/lib/python3.11/site-packages/flwr/simulation/ray_transport/ray_client_proxy.py", line 94, in _submit_job
    out_mssg, updated_context = self.actor_pool.get_client_result(
                                ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/abdenour/miniconda3/envs/fl_with_flower/lib/python3.11/site-packages/flwr/simulation/ray_transport/ray_actor.py", line 398, in get_client_result
    return self._fetch_future_result(cid)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/abdenour/miniconda3/envs/fl_with_flower/lib/python3.11/site-packages/flwr/simulation/ray_transport/ray_actor.py", line 279, in _fetch_future_result
    res_cid, out_mssg, updated_context = ray.get(
                                         ^^^^^^^^
  File "/home/abdenour/miniconda3/envs/fl_with_flower/lib/python3.11/site-packages/ray/_private/auto_init_hook.py", line 21, in auto_init_wrapper
    return fn(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^
  File "/home/abdenour/miniconda3/envs/fl_with_flower/lib/python3.11/site-packages/ray/_private/client_mode_hook.py", line 103, in wrapper
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "/home/abdenour/miniconda3/envs/fl_with_flower/lib/python3.11/site-packages/ray/_private/worker.py", line 2667, in get
    values, debugger_breakpoint = worker.get_objects(object_refs, timeout=timeout)
                                  ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/abdenour/miniconda3/envs/fl_with_flower/lib/python3.11/site-packages/ray/_private/worker.py", line 866, in get_objects
    raise value
ray.exceptions.OutOfMemoryError: Task was killed due to the node running low on memory.
Memory on the node (IP: 115.145.171.128, ID: c687613ff34cdbf5097699e5c896055ef2dc44343c5ab44bd40309d5) where the task (actor ID: 85928d3f1a247800081c0e1101000000, name=ClientAppActor.__init__, pid=619614, memory used=1.73GB) was running was 29.50GB / 31.02GB (0.951049), which exceeds the memory usage threshold of 0.95. Ray killed this worker (ID: c7f644ac3cca20dffbfa1723daac4f87f04c7de03da25e7cad5fe33c) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 115.145.171.128`. To see the logs of the worker, use `ray logs worker-c7f644ac3cca20dffbfa1723daac4f87f04c7de03da25e7cad5fe33c*out -ip 115.145.171.128. Top 10 memory users:
PID	MEM(GB)	COMMAND
4942	3.49	/snap/pycharm-professional/408/jbr/bin/java -classpath /snap/pycharm-professional/408/lib/platform-l...
619616	1.80	ray::ClientAppActor.run
619615	1.78	ray::ClientAppActor
619617	1.77	ray::ClientAppActor
619611	1.73	ray::ClientAppActor
619618	1.73	ray::ClientAppActor
619614	1.73	ray::ClientAppActor
619612	1.72	ray::ClientAppActor
619619	1.70	ray::ClientAppActor
619620	1.68	ray::ClientAppActor.run
Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.

[2024-08-13 23:52:53,351][flwr][ERROR] - Task was killed due to the node running low on memory.
Memory on the node (IP: 115.145.171.128, ID: c687613ff34cdbf5097699e5c896055ef2dc44343c5ab44bd40309d5) where the task (actor ID: 85928d3f1a247800081c0e1101000000, name=ClientAppActor.__init__, pid=619614, memory used=1.73GB) was running was 29.50GB / 31.02GB (0.951049), which exceeds the memory usage threshold of 0.95. Ray killed this worker (ID: c7f644ac3cca20dffbfa1723daac4f87f04c7de03da25e7cad5fe33c) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 115.145.171.128`. To see the logs of the worker, use `ray logs worker-c7f644ac3cca20dffbfa1723daac4f87f04c7de03da25e7cad5fe33c*out -ip 115.145.171.128. Top 10 memory users:
PID	MEM(GB)	COMMAND
4942	3.49	/snap/pycharm-professional/408/jbr/bin/java -classpath /snap/pycharm-professional/408/lib/platform-l...
619616	1.80	ray::ClientAppActor.run
619615	1.78	ray::ClientAppActor
619617	1.77	ray::ClientAppActor
619611	1.73	ray::ClientAppActor
619618	1.73	ray::ClientAppActor
619614	1.73	ray::ClientAppActor
619612	1.72	ray::ClientAppActor
619619	1.70	ray::ClientAppActor
619620	1.68	ray::ClientAppActor.run
Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.
[2024-08-13 23:52:53,351][flwr][ERROR] - Traceback (most recent call last):
  File "/home/abdenour/miniconda3/envs/fl_with_flower/lib/python3.11/site-packages/flwr/simulation/ray_transport/ray_client_proxy.py", line 94, in _submit_job
    out_mssg, updated_context = self.actor_pool.get_client_result(
                                ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/abdenour/miniconda3/envs/fl_with_flower/lib/python3.11/site-packages/flwr/simulation/ray_transport/ray_actor.py", line 398, in get_client_result
    return self._fetch_future_result(cid)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/abdenour/miniconda3/envs/fl_with_flower/lib/python3.11/site-packages/flwr/simulation/ray_transport/ray_actor.py", line 279, in _fetch_future_result
    res_cid, out_mssg, updated_context = ray.get(
                                         ^^^^^^^^
  File "/home/abdenour/miniconda3/envs/fl_with_flower/lib/python3.11/site-packages/ray/_private/auto_init_hook.py", line 21, in auto_init_wrapper
    return fn(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^
  File "/home/abdenour/miniconda3/envs/fl_with_flower/lib/python3.11/site-packages/ray/_private/client_mode_hook.py", line 103, in wrapper
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "/home/abdenour/miniconda3/envs/fl_with_flower/lib/python3.11/site-packages/ray/_private/worker.py", line 2667, in get
    values, debugger_breakpoint = worker.get_objects(object_refs, timeout=timeout)
                                  ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/abdenour/miniconda3/envs/fl_with_flower/lib/python3.11/site-packages/ray/_private/worker.py", line 866, in get_objects
    raise value
ray.exceptions.OutOfMemoryError: Task was killed due to the node running low on memory.
Memory on the node (IP: 115.145.171.128, ID: c687613ff34cdbf5097699e5c896055ef2dc44343c5ab44bd40309d5) where the task (actor ID: 52365bb7997979e5a4c7ddbd01000000, name=ClientAppActor.__init__, pid=619618, memory used=1.84GB) was running was 29.49GB / 31.02GB (0.950707), which exceeds the memory usage threshold of 0.95. Ray killed this worker (ID: 7e48cd46c72821b17be65b05a4e61b5846ba998a57d76db44ca4f8fa) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 115.145.171.128`. To see the logs of the worker, use `ray logs worker-7e48cd46c72821b17be65b05a4e61b5846ba998a57d76db44ca4f8fa*out -ip 115.145.171.128. Top 10 memory users:
PID	MEM(GB)	COMMAND
4942	3.48	/snap/pycharm-professional/408/jbr/bin/java -classpath /snap/pycharm-professional/408/lib/platform-l...
616443	2.14	/home/abdenour/miniconda3/envs/fl_with_flower/bin/python /home/abdenour/PycharmProjects/fl_with_flow...
619615	1.90	ray::ClientAppActor
619616	1.88	ray::ClientAppActor
619611	1.87	ray::ClientAppActor
619617	1.86	ray::ClientAppActor
619620	1.85	ray::ClientAppActor
619618	1.84	ray::ClientAppActor
619613	1.82	ray::ClientAppActor
619619	1.80	ray::ClientAppActor
Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.

[2024-08-13 23:52:53,351][flwr][ERROR] - Task was killed due to the node running low on memory.
Memory on the node (IP: 115.145.171.128, ID: c687613ff34cdbf5097699e5c896055ef2dc44343c5ab44bd40309d5) where the task (actor ID: 52365bb7997979e5a4c7ddbd01000000, name=ClientAppActor.__init__, pid=619618, memory used=1.84GB) was running was 29.49GB / 31.02GB (0.950707), which exceeds the memory usage threshold of 0.95. Ray killed this worker (ID: 7e48cd46c72821b17be65b05a4e61b5846ba998a57d76db44ca4f8fa) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 115.145.171.128`. To see the logs of the worker, use `ray logs worker-7e48cd46c72821b17be65b05a4e61b5846ba998a57d76db44ca4f8fa*out -ip 115.145.171.128. Top 10 memory users:
PID	MEM(GB)	COMMAND
4942	3.48	/snap/pycharm-professional/408/jbr/bin/java -classpath /snap/pycharm-professional/408/lib/platform-l...
616443	2.14	/home/abdenour/miniconda3/envs/fl_with_flower/bin/python /home/abdenour/PycharmProjects/fl_with_flow...
619615	1.90	ray::ClientAppActor
619616	1.88	ray::ClientAppActor
619611	1.87	ray::ClientAppActor
619617	1.86	ray::ClientAppActor
619620	1.85	ray::ClientAppActor
619618	1.84	ray::ClientAppActor
619613	1.82	ray::ClientAppActor
619619	1.80	ray::ClientAppActor
Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.
[2024-08-13 23:52:55,235][flwr][INFO] - aggregate_fit: received 8 results and 2 failures
[2024-08-13 23:52:58,173][flwr][INFO] - fit progress: (223, 16.35506319999695, {'accuracy': 0.7483974358974359}, 1299.4336699419946)
[2024-08-13 23:52:58,173][flwr][INFO] - configure_evaluate: strategy sampled 2 clients (out of 20)
[2024-08-13 23:52:58,647][flwr][INFO] - aggregate_evaluate: received 2 results and 0 failures
[2024-08-13 23:52:58,647][flwr][INFO] - 
[2024-08-13 23:52:58,647][flwr][INFO] - [ROUND 224]
[2024-08-13 23:52:58,647][flwr][INFO] - configure_fit: strategy sampled 10 clients (out of 20)
[2024-08-13 23:52:58,814][flwr][ERROR] - Traceback (most recent call last):
  File "/home/abdenour/miniconda3/envs/fl_with_flower/lib/python3.11/site-packages/flwr/simulation/ray_transport/ray_client_proxy.py", line 94, in _submit_job
    out_mssg, updated_context = self.actor_pool.get_client_result(
                                ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/abdenour/miniconda3/envs/fl_with_flower/lib/python3.11/site-packages/flwr/simulation/ray_transport/ray_actor.py", line 398, in get_client_result
    return self._fetch_future_result(cid)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/abdenour/miniconda3/envs/fl_with_flower/lib/python3.11/site-packages/flwr/simulation/ray_transport/ray_actor.py", line 279, in _fetch_future_result
    res_cid, out_mssg, updated_context = ray.get(
                                         ^^^^^^^^
  File "/home/abdenour/miniconda3/envs/fl_with_flower/lib/python3.11/site-packages/ray/_private/auto_init_hook.py", line 21, in auto_init_wrapper
    return fn(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^
  File "/home/abdenour/miniconda3/envs/fl_with_flower/lib/python3.11/site-packages/ray/_private/client_mode_hook.py", line 103, in wrapper
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "/home/abdenour/miniconda3/envs/fl_with_flower/lib/python3.11/site-packages/ray/_private/worker.py", line 2667, in get
    values, debugger_breakpoint = worker.get_objects(object_refs, timeout=timeout)
                                  ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/abdenour/miniconda3/envs/fl_with_flower/lib/python3.11/site-packages/ray/_private/worker.py", line 866, in get_objects
    raise value
ray.exceptions.OutOfMemoryError: Task was killed due to the node running low on memory.
Memory on the node (IP: 115.145.171.128, ID: c687613ff34cdbf5097699e5c896055ef2dc44343c5ab44bd40309d5) where the task (actor ID: 52365bb7997979e5a4c7ddbd01000000, name=ClientAppActor.__init__, pid=619618, memory used=1.84GB) was running was 29.49GB / 31.02GB (0.950707), which exceeds the memory usage threshold of 0.95. Ray killed this worker (ID: 7e48cd46c72821b17be65b05a4e61b5846ba998a57d76db44ca4f8fa) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 115.145.171.128`. To see the logs of the worker, use `ray logs worker-7e48cd46c72821b17be65b05a4e61b5846ba998a57d76db44ca4f8fa*out -ip 115.145.171.128. Top 10 memory users:
PID	MEM(GB)	COMMAND
4942	3.48	/snap/pycharm-professional/408/jbr/bin/java -classpath /snap/pycharm-professional/408/lib/platform-l...
616443	2.14	/home/abdenour/miniconda3/envs/fl_with_flower/bin/python /home/abdenour/PycharmProjects/fl_with_flow...
619615	1.90	ray::ClientAppActor
619616	1.88	ray::ClientAppActor
619611	1.87	ray::ClientAppActor
619617	1.86	ray::ClientAppActor
619620	1.85	ray::ClientAppActor
619618	1.84	ray::ClientAppActor
619613	1.82	ray::ClientAppActor
619619	1.80	ray::ClientAppActor
Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.

[2024-08-13 23:52:58,814][flwr][ERROR] - Task was killed due to the node running low on memory.
Memory on the node (IP: 115.145.171.128, ID: c687613ff34cdbf5097699e5c896055ef2dc44343c5ab44bd40309d5) where the task (actor ID: 52365bb7997979e5a4c7ddbd01000000, name=ClientAppActor.__init__, pid=619618, memory used=1.84GB) was running was 29.49GB / 31.02GB (0.950707), which exceeds the memory usage threshold of 0.95. Ray killed this worker (ID: 7e48cd46c72821b17be65b05a4e61b5846ba998a57d76db44ca4f8fa) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 115.145.171.128`. To see the logs of the worker, use `ray logs worker-7e48cd46c72821b17be65b05a4e61b5846ba998a57d76db44ca4f8fa*out -ip 115.145.171.128. Top 10 memory users:
PID	MEM(GB)	COMMAND
4942	3.48	/snap/pycharm-professional/408/jbr/bin/java -classpath /snap/pycharm-professional/408/lib/platform-l...
616443	2.14	/home/abdenour/miniconda3/envs/fl_with_flower/bin/python /home/abdenour/PycharmProjects/fl_with_flow...
619615	1.90	ray::ClientAppActor
619616	1.88	ray::ClientAppActor
619611	1.87	ray::ClientAppActor
619617	1.86	ray::ClientAppActor
619620	1.85	ray::ClientAppActor
619618	1.84	ray::ClientAppActor
619613	1.82	ray::ClientAppActor
619619	1.80	ray::ClientAppActor
Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.
[2024-08-13 23:52:58,815][flwr][ERROR] - Traceback (most recent call last):
  File "/home/abdenour/miniconda3/envs/fl_with_flower/lib/python3.11/site-packages/flwr/simulation/ray_transport/ray_client_proxy.py", line 94, in _submit_job
    out_mssg, updated_context = self.actor_pool.get_client_result(
                                ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/abdenour/miniconda3/envs/fl_with_flower/lib/python3.11/site-packages/flwr/simulation/ray_transport/ray_actor.py", line 398, in get_client_result
    return self._fetch_future_result(cid)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/abdenour/miniconda3/envs/fl_with_flower/lib/python3.11/site-packages/flwr/simulation/ray_transport/ray_actor.py", line 279, in _fetch_future_result
    res_cid, out_mssg, updated_context = ray.get(
                                         ^^^^^^^^
  File "/home/abdenour/miniconda3/envs/fl_with_flower/lib/python3.11/site-packages/ray/_private/auto_init_hook.py", line 21, in auto_init_wrapper
    return fn(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^
  File "/home/abdenour/miniconda3/envs/fl_with_flower/lib/python3.11/site-packages/ray/_private/client_mode_hook.py", line 103, in wrapper
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "/home/abdenour/miniconda3/envs/fl_with_flower/lib/python3.11/site-packages/ray/_private/worker.py", line 2667, in get
    values, debugger_breakpoint = worker.get_objects(object_refs, timeout=timeout)
                                  ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/abdenour/miniconda3/envs/fl_with_flower/lib/python3.11/site-packages/ray/_private/worker.py", line 866, in get_objects
    raise value
ray.exceptions.OutOfMemoryError: Task was killed due to the node running low on memory.
Memory on the node (IP: 115.145.171.128, ID: c687613ff34cdbf5097699e5c896055ef2dc44343c5ab44bd40309d5) where the task (actor ID: 85928d3f1a247800081c0e1101000000, name=ClientAppActor.__init__, pid=619614, memory used=1.73GB) was running was 29.50GB / 31.02GB (0.951049), which exceeds the memory usage threshold of 0.95. Ray killed this worker (ID: c7f644ac3cca20dffbfa1723daac4f87f04c7de03da25e7cad5fe33c) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 115.145.171.128`. To see the logs of the worker, use `ray logs worker-c7f644ac3cca20dffbfa1723daac4f87f04c7de03da25e7cad5fe33c*out -ip 115.145.171.128. Top 10 memory users:
PID	MEM(GB)	COMMAND
4942	3.49	/snap/pycharm-professional/408/jbr/bin/java -classpath /snap/pycharm-professional/408/lib/platform-l...
619616	1.80	ray::ClientAppActor.run
619615	1.78	ray::ClientAppActor
619617	1.77	ray::ClientAppActor
619611	1.73	ray::ClientAppActor
619618	1.73	ray::ClientAppActor
619614	1.73	ray::ClientAppActor
619612	1.72	ray::ClientAppActor
619619	1.70	ray::ClientAppActor
619620	1.68	ray::ClientAppActor.run
Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.

[2024-08-13 23:52:58,815][flwr][ERROR] - Task was killed due to the node running low on memory.
Memory on the node (IP: 115.145.171.128, ID: c687613ff34cdbf5097699e5c896055ef2dc44343c5ab44bd40309d5) where the task (actor ID: 85928d3f1a247800081c0e1101000000, name=ClientAppActor.__init__, pid=619614, memory used=1.73GB) was running was 29.50GB / 31.02GB (0.951049), which exceeds the memory usage threshold of 0.95. Ray killed this worker (ID: c7f644ac3cca20dffbfa1723daac4f87f04c7de03da25e7cad5fe33c) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 115.145.171.128`. To see the logs of the worker, use `ray logs worker-c7f644ac3cca20dffbfa1723daac4f87f04c7de03da25e7cad5fe33c*out -ip 115.145.171.128. Top 10 memory users:
PID	MEM(GB)	COMMAND
4942	3.49	/snap/pycharm-professional/408/jbr/bin/java -classpath /snap/pycharm-professional/408/lib/platform-l...
619616	1.80	ray::ClientAppActor.run
619615	1.78	ray::ClientAppActor
619617	1.77	ray::ClientAppActor
619611	1.73	ray::ClientAppActor
619618	1.73	ray::ClientAppActor
619614	1.73	ray::ClientAppActor
619612	1.72	ray::ClientAppActor
619619	1.70	ray::ClientAppActor
619620	1.68	ray::ClientAppActor.run
Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.
[2024-08-13 23:53:00,673][flwr][INFO] - aggregate_fit: received 8 results and 2 failures
[2024-08-13 23:53:03,543][flwr][INFO] - fit progress: (224, 14.156736731529236, {'accuracy': 0.7532051282051282}, 1304.8029294360022)
[2024-08-13 23:53:03,543][flwr][INFO] - configure_evaluate: strategy sampled 2 clients (out of 20)
[2024-08-13 23:53:03,828][flwr][INFO] - aggregate_evaluate: received 2 results and 0 failures
[2024-08-13 23:53:03,828][flwr][INFO] - 
[2024-08-13 23:53:03,828][flwr][INFO] - [ROUND 225]
[2024-08-13 23:53:03,828][flwr][INFO] - configure_fit: strategy sampled 10 clients (out of 20)
[2024-08-13 23:53:03,979][flwr][ERROR] - Traceback (most recent call last):
  File "/home/abdenour/miniconda3/envs/fl_with_flower/lib/python3.11/site-packages/flwr/simulation/ray_transport/ray_client_proxy.py", line 94, in _submit_job
    out_mssg, updated_context = self.actor_pool.get_client_result(
                                ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/abdenour/miniconda3/envs/fl_with_flower/lib/python3.11/site-packages/flwr/simulation/ray_transport/ray_actor.py", line 398, in get_client_result
    return self._fetch_future_result(cid)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/abdenour/miniconda3/envs/fl_with_flower/lib/python3.11/site-packages/flwr/simulation/ray_transport/ray_actor.py", line 279, in _fetch_future_result
    res_cid, out_mssg, updated_context = ray.get(
                                         ^^^^^^^^
  File "/home/abdenour/miniconda3/envs/fl_with_flower/lib/python3.11/site-packages/ray/_private/auto_init_hook.py", line 21, in auto_init_wrapper
    return fn(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^
  File "/home/abdenour/miniconda3/envs/fl_with_flower/lib/python3.11/site-packages/ray/_private/client_mode_hook.py", line 103, in wrapper
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "/home/abdenour/miniconda3/envs/fl_with_flower/lib/python3.11/site-packages/ray/_private/worker.py", line 2667, in get
    values, debugger_breakpoint = worker.get_objects(object_refs, timeout=timeout)
                                  ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/abdenour/miniconda3/envs/fl_with_flower/lib/python3.11/site-packages/ray/_private/worker.py", line 866, in get_objects
    raise value
ray.exceptions.OutOfMemoryError: Task was killed due to the node running low on memory.
Memory on the node (IP: 115.145.171.128, ID: c687613ff34cdbf5097699e5c896055ef2dc44343c5ab44bd40309d5) where the task (actor ID: 85928d3f1a247800081c0e1101000000, name=ClientAppActor.__init__, pid=619614, memory used=1.73GB) was running was 29.50GB / 31.02GB (0.951049), which exceeds the memory usage threshold of 0.95. Ray killed this worker (ID: c7f644ac3cca20dffbfa1723daac4f87f04c7de03da25e7cad5fe33c) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 115.145.171.128`. To see the logs of the worker, use `ray logs worker-c7f644ac3cca20dffbfa1723daac4f87f04c7de03da25e7cad5fe33c*out -ip 115.145.171.128. Top 10 memory users:
PID	MEM(GB)	COMMAND
4942	3.49	/snap/pycharm-professional/408/jbr/bin/java -classpath /snap/pycharm-professional/408/lib/platform-l...
619616	1.80	ray::ClientAppActor.run
619615	1.78	ray::ClientAppActor
619617	1.77	ray::ClientAppActor
619611	1.73	ray::ClientAppActor
619618	1.73	ray::ClientAppActor
619614	1.73	ray::ClientAppActor
619612	1.72	ray::ClientAppActor
619619	1.70	ray::ClientAppActor
619620	1.68	ray::ClientAppActor.run
Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.

[2024-08-13 23:53:03,979][flwr][ERROR] - Task was killed due to the node running low on memory.
Memory on the node (IP: 115.145.171.128, ID: c687613ff34cdbf5097699e5c896055ef2dc44343c5ab44bd40309d5) where the task (actor ID: 85928d3f1a247800081c0e1101000000, name=ClientAppActor.__init__, pid=619614, memory used=1.73GB) was running was 29.50GB / 31.02GB (0.951049), which exceeds the memory usage threshold of 0.95. Ray killed this worker (ID: c7f644ac3cca20dffbfa1723daac4f87f04c7de03da25e7cad5fe33c) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 115.145.171.128`. To see the logs of the worker, use `ray logs worker-c7f644ac3cca20dffbfa1723daac4f87f04c7de03da25e7cad5fe33c*out -ip 115.145.171.128. Top 10 memory users:
PID	MEM(GB)	COMMAND
4942	3.49	/snap/pycharm-professional/408/jbr/bin/java -classpath /snap/pycharm-professional/408/lib/platform-l...
619616	1.80	ray::ClientAppActor.run
619615	1.78	ray::ClientAppActor
619617	1.77	ray::ClientAppActor
619611	1.73	ray::ClientAppActor
619618	1.73	ray::ClientAppActor
619614	1.73	ray::ClientAppActor
619612	1.72	ray::ClientAppActor
619619	1.70	ray::ClientAppActor
619620	1.68	ray::ClientAppActor.run
Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.
[2024-08-13 23:53:03,986][flwr][ERROR] - Traceback (most recent call last):
  File "/home/abdenour/miniconda3/envs/fl_with_flower/lib/python3.11/site-packages/flwr/simulation/ray_transport/ray_client_proxy.py", line 94, in _submit_job
    out_mssg, updated_context = self.actor_pool.get_client_result(
                                ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/abdenour/miniconda3/envs/fl_with_flower/lib/python3.11/site-packages/flwr/simulation/ray_transport/ray_actor.py", line 398, in get_client_result
    return self._fetch_future_result(cid)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/abdenour/miniconda3/envs/fl_with_flower/lib/python3.11/site-packages/flwr/simulation/ray_transport/ray_actor.py", line 279, in _fetch_future_result
    res_cid, out_mssg, updated_context = ray.get(
                                         ^^^^^^^^
  File "/home/abdenour/miniconda3/envs/fl_with_flower/lib/python3.11/site-packages/ray/_private/auto_init_hook.py", line 21, in auto_init_wrapper
    return fn(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^
  File "/home/abdenour/miniconda3/envs/fl_with_flower/lib/python3.11/site-packages/ray/_private/client_mode_hook.py", line 103, in wrapper
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "/home/abdenour/miniconda3/envs/fl_with_flower/lib/python3.11/site-packages/ray/_private/worker.py", line 2667, in get
    values, debugger_breakpoint = worker.get_objects(object_refs, timeout=timeout)
                                  ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/abdenour/miniconda3/envs/fl_with_flower/lib/python3.11/site-packages/ray/_private/worker.py", line 866, in get_objects
    raise value
ray.exceptions.OutOfMemoryError: Task was killed due to the node running low on memory.
Memory on the node (IP: 115.145.171.128, ID: c687613ff34cdbf5097699e5c896055ef2dc44343c5ab44bd40309d5) where the task (actor ID: 85928d3f1a247800081c0e1101000000, name=ClientAppActor.__init__, pid=619614, memory used=1.73GB) was running was 29.50GB / 31.02GB (0.951049), which exceeds the memory usage threshold of 0.95. Ray killed this worker (ID: c7f644ac3cca20dffbfa1723daac4f87f04c7de03da25e7cad5fe33c) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 115.145.171.128`. To see the logs of the worker, use `ray logs worker-c7f644ac3cca20dffbfa1723daac4f87f04c7de03da25e7cad5fe33c*out -ip 115.145.171.128. Top 10 memory users:
PID	MEM(GB)	COMMAND
4942	3.49	/snap/pycharm-professional/408/jbr/bin/java -classpath /snap/pycharm-professional/408/lib/platform-l...
619616	1.80	ray::ClientAppActor.run
619615	1.78	ray::ClientAppActor
619617	1.77	ray::ClientAppActor
619611	1.73	ray::ClientAppActor
619618	1.73	ray::ClientAppActor
619614	1.73	ray::ClientAppActor
619612	1.72	ray::ClientAppActor
619619	1.70	ray::ClientAppActor
619620	1.68	ray::ClientAppActor.run
Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.

[2024-08-13 23:53:03,986][flwr][ERROR] - Task was killed due to the node running low on memory.
Memory on the node (IP: 115.145.171.128, ID: c687613ff34cdbf5097699e5c896055ef2dc44343c5ab44bd40309d5) where the task (actor ID: 85928d3f1a247800081c0e1101000000, name=ClientAppActor.__init__, pid=619614, memory used=1.73GB) was running was 29.50GB / 31.02GB (0.951049), which exceeds the memory usage threshold of 0.95. Ray killed this worker (ID: c7f644ac3cca20dffbfa1723daac4f87f04c7de03da25e7cad5fe33c) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 115.145.171.128`. To see the logs of the worker, use `ray logs worker-c7f644ac3cca20dffbfa1723daac4f87f04c7de03da25e7cad5fe33c*out -ip 115.145.171.128. Top 10 memory users:
PID	MEM(GB)	COMMAND
4942	3.49	/snap/pycharm-professional/408/jbr/bin/java -classpath /snap/pycharm-professional/408/lib/platform-l...
619616	1.80	ray::ClientAppActor.run
619615	1.78	ray::ClientAppActor
619617	1.77	ray::ClientAppActor
619611	1.73	ray::ClientAppActor
619618	1.73	ray::ClientAppActor
619614	1.73	ray::ClientAppActor
619612	1.72	ray::ClientAppActor
619619	1.70	ray::ClientAppActor
619620	1.68	ray::ClientAppActor.run
Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.
[2024-08-13 23:53:05,582][flwr][INFO] - aggregate_fit: received 8 results and 2 failures
[2024-08-13 23:53:08,627][flwr][INFO] - fit progress: (225, 16.47816115617752, {'accuracy': 0.7435897435897436}, 1309.8875127049978)
[2024-08-13 23:53:08,627][flwr][INFO] - configure_evaluate: strategy sampled 2 clients (out of 20)
[2024-08-13 23:53:08,972][flwr][INFO] - aggregate_evaluate: received 2 results and 0 failures
[2024-08-13 23:53:08,972][flwr][INFO] - 
[2024-08-13 23:53:08,972][flwr][INFO] - [ROUND 226]
[2024-08-13 23:53:08,972][flwr][INFO] - configure_fit: strategy sampled 10 clients (out of 20)
[2024-08-13 23:53:09,149][flwr][ERROR] - Traceback (most recent call last):
  File "/home/abdenour/miniconda3/envs/fl_with_flower/lib/python3.11/site-packages/flwr/simulation/ray_transport/ray_client_proxy.py", line 94, in _submit_job
    out_mssg, updated_context = self.actor_pool.get_client_result(
                                ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/abdenour/miniconda3/envs/fl_with_flower/lib/python3.11/site-packages/flwr/simulation/ray_transport/ray_actor.py", line 398, in get_client_result
    return self._fetch_future_result(cid)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/abdenour/miniconda3/envs/fl_with_flower/lib/python3.11/site-packages/flwr/simulation/ray_transport/ray_actor.py", line 279, in _fetch_future_result
    res_cid, out_mssg, updated_context = ray.get(
                                         ^^^^^^^^
  File "/home/abdenour/miniconda3/envs/fl_with_flower/lib/python3.11/site-packages/ray/_private/auto_init_hook.py", line 21, in auto_init_wrapper
    return fn(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^
  File "/home/abdenour/miniconda3/envs/fl_with_flower/lib/python3.11/site-packages/ray/_private/client_mode_hook.py", line 103, in wrapper
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "/home/abdenour/miniconda3/envs/fl_with_flower/lib/python3.11/site-packages/ray/_private/worker.py", line 2667, in get
    values, debugger_breakpoint = worker.get_objects(object_refs, timeout=timeout)
                                  ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/abdenour/miniconda3/envs/fl_with_flower/lib/python3.11/site-packages/ray/_private/worker.py", line 866, in get_objects
    raise value
ray.exceptions.OutOfMemoryError: Task was killed due to the node running low on memory.
Memory on the node (IP: 115.145.171.128, ID: c687613ff34cdbf5097699e5c896055ef2dc44343c5ab44bd40309d5) where the task (actor ID: 85928d3f1a247800081c0e1101000000, name=ClientAppActor.__init__, pid=619614, memory used=1.73GB) was running was 29.50GB / 31.02GB (0.951049), which exceeds the memory usage threshold of 0.95. Ray killed this worker (ID: c7f644ac3cca20dffbfa1723daac4f87f04c7de03da25e7cad5fe33c) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 115.145.171.128`. To see the logs of the worker, use `ray logs worker-c7f644ac3cca20dffbfa1723daac4f87f04c7de03da25e7cad5fe33c*out -ip 115.145.171.128. Top 10 memory users:
PID	MEM(GB)	COMMAND
4942	3.49	/snap/pycharm-professional/408/jbr/bin/java -classpath /snap/pycharm-professional/408/lib/platform-l...
619616	1.80	ray::ClientAppActor.run
619615	1.78	ray::ClientAppActor
619617	1.77	ray::ClientAppActor
619611	1.73	ray::ClientAppActor
619618	1.73	ray::ClientAppActor
619614	1.73	ray::ClientAppActor
619612	1.72	ray::ClientAppActor
619619	1.70	ray::ClientAppActor
619620	1.68	ray::ClientAppActor.run
Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.

[2024-08-13 23:53:09,150][flwr][ERROR] - Task was killed due to the node running low on memory.
Memory on the node (IP: 115.145.171.128, ID: c687613ff34cdbf5097699e5c896055ef2dc44343c5ab44bd40309d5) where the task (actor ID: 85928d3f1a247800081c0e1101000000, name=ClientAppActor.__init__, pid=619614, memory used=1.73GB) was running was 29.50GB / 31.02GB (0.951049), which exceeds the memory usage threshold of 0.95. Ray killed this worker (ID: c7f644ac3cca20dffbfa1723daac4f87f04c7de03da25e7cad5fe33c) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 115.145.171.128`. To see the logs of the worker, use `ray logs worker-c7f644ac3cca20dffbfa1723daac4f87f04c7de03da25e7cad5fe33c*out -ip 115.145.171.128. Top 10 memory users:
PID	MEM(GB)	COMMAND
4942	3.49	/snap/pycharm-professional/408/jbr/bin/java -classpath /snap/pycharm-professional/408/lib/platform-l...
619616	1.80	ray::ClientAppActor.run
619615	1.78	ray::ClientAppActor
619617	1.77	ray::ClientAppActor
619611	1.73	ray::ClientAppActor
619618	1.73	ray::ClientAppActor
619614	1.73	ray::ClientAppActor
619612	1.72	ray::ClientAppActor
619619	1.70	ray::ClientAppActor
619620	1.68	ray::ClientAppActor.run
Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.
[2024-08-13 23:53:09,150][flwr][ERROR] - Traceback (most recent call last):
  File "/home/abdenour/miniconda3/envs/fl_with_flower/lib/python3.11/site-packages/flwr/simulation/ray_transport/ray_client_proxy.py", line 94, in _submit_job
    out_mssg, updated_context = self.actor_pool.get_client_result(
                                ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/abdenour/miniconda3/envs/fl_with_flower/lib/python3.11/site-packages/flwr/simulation/ray_transport/ray_actor.py", line 398, in get_client_result
    return self._fetch_future_result(cid)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/abdenour/miniconda3/envs/fl_with_flower/lib/python3.11/site-packages/flwr/simulation/ray_transport/ray_actor.py", line 279, in _fetch_future_result
    res_cid, out_mssg, updated_context = ray.get(
                                         ^^^^^^^^
  File "/home/abdenour/miniconda3/envs/fl_with_flower/lib/python3.11/site-packages/ray/_private/auto_init_hook.py", line 21, in auto_init_wrapper
    return fn(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^
  File "/home/abdenour/miniconda3/envs/fl_with_flower/lib/python3.11/site-packages/ray/_private/client_mode_hook.py", line 103, in wrapper
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "/home/abdenour/miniconda3/envs/fl_with_flower/lib/python3.11/site-packages/ray/_private/worker.py", line 2667, in get
    values, debugger_breakpoint = worker.get_objects(object_refs, timeout=timeout)
                                  ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/abdenour/miniconda3/envs/fl_with_flower/lib/python3.11/site-packages/ray/_private/worker.py", line 866, in get_objects
    raise value
ray.exceptions.OutOfMemoryError: Task was killed due to the node running low on memory.
Memory on the node (IP: 115.145.171.128, ID: c687613ff34cdbf5097699e5c896055ef2dc44343c5ab44bd40309d5) where the task (actor ID: 52365bb7997979e5a4c7ddbd01000000, name=ClientAppActor.__init__, pid=619618, memory used=1.84GB) was running was 29.49GB / 31.02GB (0.950707), which exceeds the memory usage threshold of 0.95. Ray killed this worker (ID: 7e48cd46c72821b17be65b05a4e61b5846ba998a57d76db44ca4f8fa) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 115.145.171.128`. To see the logs of the worker, use `ray logs worker-7e48cd46c72821b17be65b05a4e61b5846ba998a57d76db44ca4f8fa*out -ip 115.145.171.128. Top 10 memory users:
PID	MEM(GB)	COMMAND
4942	3.48	/snap/pycharm-professional/408/jbr/bin/java -classpath /snap/pycharm-professional/408/lib/platform-l...
616443	2.14	/home/abdenour/miniconda3/envs/fl_with_flower/bin/python /home/abdenour/PycharmProjects/fl_with_flow...
619615	1.90	ray::ClientAppActor
619616	1.88	ray::ClientAppActor
619611	1.87	ray::ClientAppActor
619617	1.86	ray::ClientAppActor
619620	1.85	ray::ClientAppActor
619618	1.84	ray::ClientAppActor
619613	1.82	ray::ClientAppActor
619619	1.80	ray::ClientAppActor
Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.

[2024-08-13 23:53:09,150][flwr][ERROR] - Task was killed due to the node running low on memory.
Memory on the node (IP: 115.145.171.128, ID: c687613ff34cdbf5097699e5c896055ef2dc44343c5ab44bd40309d5) where the task (actor ID: 52365bb7997979e5a4c7ddbd01000000, name=ClientAppActor.__init__, pid=619618, memory used=1.84GB) was running was 29.49GB / 31.02GB (0.950707), which exceeds the memory usage threshold of 0.95. Ray killed this worker (ID: 7e48cd46c72821b17be65b05a4e61b5846ba998a57d76db44ca4f8fa) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 115.145.171.128`. To see the logs of the worker, use `ray logs worker-7e48cd46c72821b17be65b05a4e61b5846ba998a57d76db44ca4f8fa*out -ip 115.145.171.128. Top 10 memory users:
PID	MEM(GB)	COMMAND
4942	3.48	/snap/pycharm-professional/408/jbr/bin/java -classpath /snap/pycharm-professional/408/lib/platform-l...
616443	2.14	/home/abdenour/miniconda3/envs/fl_with_flower/bin/python /home/abdenour/PycharmProjects/fl_with_flow...
619615	1.90	ray::ClientAppActor
619616	1.88	ray::ClientAppActor
619611	1.87	ray::ClientAppActor
619617	1.86	ray::ClientAppActor
619620	1.85	ray::ClientAppActor
619618	1.84	ray::ClientAppActor
619613	1.82	ray::ClientAppActor
619619	1.80	ray::ClientAppActor
Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.
[2024-08-13 23:53:11,643][flwr][INFO] - aggregate_fit: received 8 results and 2 failures
[2024-08-13 23:53:14,813][flwr][INFO] - fit progress: (226, 16.70440137386322, {'accuracy': 0.7467948717948718}, 1316.073277724994)
[2024-08-13 23:53:14,813][flwr][INFO] - configure_evaluate: strategy sampled 2 clients (out of 20)
[2024-08-13 23:53:15,228][flwr][INFO] - aggregate_evaluate: received 2 results and 0 failures
[2024-08-13 23:53:15,228][flwr][INFO] - 
[2024-08-13 23:53:15,228][flwr][INFO] - [ROUND 227]
[2024-08-13 23:53:15,229][flwr][INFO] - configure_fit: strategy sampled 10 clients (out of 20)
[2024-08-13 23:53:15,418][flwr][ERROR] - Traceback (most recent call last):
  File "/home/abdenour/miniconda3/envs/fl_with_flower/lib/python3.11/site-packages/flwr/simulation/ray_transport/ray_client_proxy.py", line 94, in _submit_job
    out_mssg, updated_context = self.actor_pool.get_client_result(
                                ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/abdenour/miniconda3/envs/fl_with_flower/lib/python3.11/site-packages/flwr/simulation/ray_transport/ray_actor.py", line 398, in get_client_result
    return self._fetch_future_result(cid)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/abdenour/miniconda3/envs/fl_with_flower/lib/python3.11/site-packages/flwr/simulation/ray_transport/ray_actor.py", line 279, in _fetch_future_result
    res_cid, out_mssg, updated_context = ray.get(
                                         ^^^^^^^^
  File "/home/abdenour/miniconda3/envs/fl_with_flower/lib/python3.11/site-packages/ray/_private/auto_init_hook.py", line 21, in auto_init_wrapper
    return fn(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^
  File "/home/abdenour/miniconda3/envs/fl_with_flower/lib/python3.11/site-packages/ray/_private/client_mode_hook.py", line 103, in wrapper
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "/home/abdenour/miniconda3/envs/fl_with_flower/lib/python3.11/site-packages/ray/_private/worker.py", line 2667, in get
    values, debugger_breakpoint = worker.get_objects(object_refs, timeout=timeout)
                                  ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/abdenour/miniconda3/envs/fl_with_flower/lib/python3.11/site-packages/ray/_private/worker.py", line 866, in get_objects
    raise value
ray.exceptions.OutOfMemoryError: Task was killed due to the node running low on memory.
Memory on the node (IP: 115.145.171.128, ID: c687613ff34cdbf5097699e5c896055ef2dc44343c5ab44bd40309d5) where the task (actor ID: 52365bb7997979e5a4c7ddbd01000000, name=ClientAppActor.__init__, pid=619618, memory used=1.84GB) was running was 29.49GB / 31.02GB (0.950707), which exceeds the memory usage threshold of 0.95. Ray killed this worker (ID: 7e48cd46c72821b17be65b05a4e61b5846ba998a57d76db44ca4f8fa) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 115.145.171.128`. To see the logs of the worker, use `ray logs worker-7e48cd46c72821b17be65b05a4e61b5846ba998a57d76db44ca4f8fa*out -ip 115.145.171.128. Top 10 memory users:
PID	MEM(GB)	COMMAND
4942	3.48	/snap/pycharm-professional/408/jbr/bin/java -classpath /snap/pycharm-professional/408/lib/platform-l...
616443	2.14	/home/abdenour/miniconda3/envs/fl_with_flower/bin/python /home/abdenour/PycharmProjects/fl_with_flow...
619615	1.90	ray::ClientAppActor
619616	1.88	ray::ClientAppActor
619611	1.87	ray::ClientAppActor
619617	1.86	ray::ClientAppActor
619620	1.85	ray::ClientAppActor
619618	1.84	ray::ClientAppActor
619613	1.82	ray::ClientAppActor
619619	1.80	ray::ClientAppActor
Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.

[2024-08-13 23:53:15,418][flwr][ERROR] - Task was killed due to the node running low on memory.
Memory on the node (IP: 115.145.171.128, ID: c687613ff34cdbf5097699e5c896055ef2dc44343c5ab44bd40309d5) where the task (actor ID: 52365bb7997979e5a4c7ddbd01000000, name=ClientAppActor.__init__, pid=619618, memory used=1.84GB) was running was 29.49GB / 31.02GB (0.950707), which exceeds the memory usage threshold of 0.95. Ray killed this worker (ID: 7e48cd46c72821b17be65b05a4e61b5846ba998a57d76db44ca4f8fa) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 115.145.171.128`. To see the logs of the worker, use `ray logs worker-7e48cd46c72821b17be65b05a4e61b5846ba998a57d76db44ca4f8fa*out -ip 115.145.171.128. Top 10 memory users:
PID	MEM(GB)	COMMAND
4942	3.48	/snap/pycharm-professional/408/jbr/bin/java -classpath /snap/pycharm-professional/408/lib/platform-l...
616443	2.14	/home/abdenour/miniconda3/envs/fl_with_flower/bin/python /home/abdenour/PycharmProjects/fl_with_flow...
619615	1.90	ray::ClientAppActor
619616	1.88	ray::ClientAppActor
619611	1.87	ray::ClientAppActor
619617	1.86	ray::ClientAppActor
619620	1.85	ray::ClientAppActor
619618	1.84	ray::ClientAppActor
619613	1.82	ray::ClientAppActor
619619	1.80	ray::ClientAppActor
Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.
[2024-08-13 23:53:15,419][flwr][ERROR] - Traceback (most recent call last):
  File "/home/abdenour/miniconda3/envs/fl_with_flower/lib/python3.11/site-packages/flwr/simulation/ray_transport/ray_client_proxy.py", line 94, in _submit_job
    out_mssg, updated_context = self.actor_pool.get_client_result(
                                ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/abdenour/miniconda3/envs/fl_with_flower/lib/python3.11/site-packages/flwr/simulation/ray_transport/ray_actor.py", line 398, in get_client_result
    return self._fetch_future_result(cid)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/abdenour/miniconda3/envs/fl_with_flower/lib/python3.11/site-packages/flwr/simulation/ray_transport/ray_actor.py", line 279, in _fetch_future_result
    res_cid, out_mssg, updated_context = ray.get(
                                         ^^^^^^^^
  File "/home/abdenour/miniconda3/envs/fl_with_flower/lib/python3.11/site-packages/ray/_private/auto_init_hook.py", line 21, in auto_init_wrapper
    return fn(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^
  File "/home/abdenour/miniconda3/envs/fl_with_flower/lib/python3.11/site-packages/ray/_private/client_mode_hook.py", line 103, in wrapper
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "/home/abdenour/miniconda3/envs/fl_with_flower/lib/python3.11/site-packages/ray/_private/worker.py", line 2667, in get
    values, debugger_breakpoint = worker.get_objects(object_refs, timeout=timeout)
                                  ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/abdenour/miniconda3/envs/fl_with_flower/lib/python3.11/site-packages/ray/_private/worker.py", line 866, in get_objects
    raise value
ray.exceptions.OutOfMemoryError: Task was killed due to the node running low on memory.
Memory on the node (IP: 115.145.171.128, ID: c687613ff34cdbf5097699e5c896055ef2dc44343c5ab44bd40309d5) where the task (actor ID: 85928d3f1a247800081c0e1101000000, name=ClientAppActor.__init__, pid=619614, memory used=1.73GB) was running was 29.50GB / 31.02GB (0.951049), which exceeds the memory usage threshold of 0.95. Ray killed this worker (ID: c7f644ac3cca20dffbfa1723daac4f87f04c7de03da25e7cad5fe33c) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 115.145.171.128`. To see the logs of the worker, use `ray logs worker-c7f644ac3cca20dffbfa1723daac4f87f04c7de03da25e7cad5fe33c*out -ip 115.145.171.128. Top 10 memory users:
PID	MEM(GB)	COMMAND
4942	3.49	/snap/pycharm-professional/408/jbr/bin/java -classpath /snap/pycharm-professional/408/lib/platform-l...
619616	1.80	ray::ClientAppActor.run
619615	1.78	ray::ClientAppActor
619617	1.77	ray::ClientAppActor
619611	1.73	ray::ClientAppActor
619618	1.73	ray::ClientAppActor
619614	1.73	ray::ClientAppActor
619612	1.72	ray::ClientAppActor
619619	1.70	ray::ClientAppActor
619620	1.68	ray::ClientAppActor.run
Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.

[2024-08-13 23:53:15,419][flwr][ERROR] - Task was killed due to the node running low on memory.
Memory on the node (IP: 115.145.171.128, ID: c687613ff34cdbf5097699e5c896055ef2dc44343c5ab44bd40309d5) where the task (actor ID: 85928d3f1a247800081c0e1101000000, name=ClientAppActor.__init__, pid=619614, memory used=1.73GB) was running was 29.50GB / 31.02GB (0.951049), which exceeds the memory usage threshold of 0.95. Ray killed this worker (ID: c7f644ac3cca20dffbfa1723daac4f87f04c7de03da25e7cad5fe33c) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 115.145.171.128`. To see the logs of the worker, use `ray logs worker-c7f644ac3cca20dffbfa1723daac4f87f04c7de03da25e7cad5fe33c*out -ip 115.145.171.128. Top 10 memory users:
PID	MEM(GB)	COMMAND
4942	3.49	/snap/pycharm-professional/408/jbr/bin/java -classpath /snap/pycharm-professional/408/lib/platform-l...
619616	1.80	ray::ClientAppActor.run
619615	1.78	ray::ClientAppActor
619617	1.77	ray::ClientAppActor
619611	1.73	ray::ClientAppActor
619618	1.73	ray::ClientAppActor
619614	1.73	ray::ClientAppActor
619612	1.72	ray::ClientAppActor
619619	1.70	ray::ClientAppActor
619620	1.68	ray::ClientAppActor.run
Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.
[2024-08-13 23:53:17,117][flwr][INFO] - aggregate_fit: received 8 results and 2 failures
[2024-08-13 23:53:20,320][flwr][INFO] - fit progress: (227, 15.836360573768616, {'accuracy': 0.7516025641025641}, 1321.5798433159944)
[2024-08-13 23:53:20,320][flwr][INFO] - configure_evaluate: strategy sampled 2 clients (out of 20)
[2024-08-13 23:53:20,621][flwr][INFO] - aggregate_evaluate: received 2 results and 0 failures
[2024-08-13 23:53:20,621][flwr][INFO] - 
[2024-08-13 23:53:20,621][flwr][INFO] - [ROUND 228]
[2024-08-13 23:53:20,621][flwr][INFO] - configure_fit: strategy sampled 10 clients (out of 20)
[2024-08-13 23:53:20,797][flwr][ERROR] - Traceback (most recent call last):
  File "/home/abdenour/miniconda3/envs/fl_with_flower/lib/python3.11/site-packages/flwr/simulation/ray_transport/ray_client_proxy.py", line 94, in _submit_job
    out_mssg, updated_context = self.actor_pool.get_client_result(
                                ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/abdenour/miniconda3/envs/fl_with_flower/lib/python3.11/site-packages/flwr/simulation/ray_transport/ray_actor.py", line 398, in get_client_result
    return self._fetch_future_result(cid)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/abdenour/miniconda3/envs/fl_with_flower/lib/python3.11/site-packages/flwr/simulation/ray_transport/ray_actor.py", line 279, in _fetch_future_result
    res_cid, out_mssg, updated_context = ray.get(
                                         ^^^^^^^^
  File "/home/abdenour/miniconda3/envs/fl_with_flower/lib/python3.11/site-packages/ray/_private/auto_init_hook.py", line 21, in auto_init_wrapper
    return fn(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^
  File "/home/abdenour/miniconda3/envs/fl_with_flower/lib/python3.11/site-packages/ray/_private/client_mode_hook.py", line 103, in wrapper
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "/home/abdenour/miniconda3/envs/fl_with_flower/lib/python3.11/site-packages/ray/_private/worker.py", line 2667, in get
    values, debugger_breakpoint = worker.get_objects(object_refs, timeout=timeout)
                                  ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/abdenour/miniconda3/envs/fl_with_flower/lib/python3.11/site-packages/ray/_private/worker.py", line 866, in get_objects
    raise value
ray.exceptions.OutOfMemoryError: Task was killed due to the node running low on memory.
Memory on the node (IP: 115.145.171.128, ID: c687613ff34cdbf5097699e5c896055ef2dc44343c5ab44bd40309d5) where the task (actor ID: 52365bb7997979e5a4c7ddbd01000000, name=ClientAppActor.__init__, pid=619618, memory used=1.84GB) was running was 29.49GB / 31.02GB (0.950707), which exceeds the memory usage threshold of 0.95. Ray killed this worker (ID: 7e48cd46c72821b17be65b05a4e61b5846ba998a57d76db44ca4f8fa) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 115.145.171.128`. To see the logs of the worker, use `ray logs worker-7e48cd46c72821b17be65b05a4e61b5846ba998a57d76db44ca4f8fa*out -ip 115.145.171.128. Top 10 memory users:
PID	MEM(GB)	COMMAND
4942	3.48	/snap/pycharm-professional/408/jbr/bin/java -classpath /snap/pycharm-professional/408/lib/platform-l...
616443	2.14	/home/abdenour/miniconda3/envs/fl_with_flower/bin/python /home/abdenour/PycharmProjects/fl_with_flow...
619615	1.90	ray::ClientAppActor
619616	1.88	ray::ClientAppActor
619611	1.87	ray::ClientAppActor
619617	1.86	ray::ClientAppActor
619620	1.85	ray::ClientAppActor
619618	1.84	ray::ClientAppActor
619613	1.82	ray::ClientAppActor
619619	1.80	ray::ClientAppActor
Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.

[2024-08-13 23:53:20,798][flwr][ERROR] - Task was killed due to the node running low on memory.
Memory on the node (IP: 115.145.171.128, ID: c687613ff34cdbf5097699e5c896055ef2dc44343c5ab44bd40309d5) where the task (actor ID: 52365bb7997979e5a4c7ddbd01000000, name=ClientAppActor.__init__, pid=619618, memory used=1.84GB) was running was 29.49GB / 31.02GB (0.950707), which exceeds the memory usage threshold of 0.95. Ray killed this worker (ID: 7e48cd46c72821b17be65b05a4e61b5846ba998a57d76db44ca4f8fa) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 115.145.171.128`. To see the logs of the worker, use `ray logs worker-7e48cd46c72821b17be65b05a4e61b5846ba998a57d76db44ca4f8fa*out -ip 115.145.171.128. Top 10 memory users:
PID	MEM(GB)	COMMAND
4942	3.48	/snap/pycharm-professional/408/jbr/bin/java -classpath /snap/pycharm-professional/408/lib/platform-l...
616443	2.14	/home/abdenour/miniconda3/envs/fl_with_flower/bin/python /home/abdenour/PycharmProjects/fl_with_flow...
619615	1.90	ray::ClientAppActor
619616	1.88	ray::ClientAppActor
619611	1.87	ray::ClientAppActor
619617	1.86	ray::ClientAppActor
619620	1.85	ray::ClientAppActor
619618	1.84	ray::ClientAppActor
619613	1.82	ray::ClientAppActor
619619	1.80	ray::ClientAppActor
Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.
[2024-08-13 23:53:20,798][flwr][ERROR] - Traceback (most recent call last):
  File "/home/abdenour/miniconda3/envs/fl_with_flower/lib/python3.11/site-packages/flwr/simulation/ray_transport/ray_client_proxy.py", line 94, in _submit_job
    out_mssg, updated_context = self.actor_pool.get_client_result(
                                ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/abdenour/miniconda3/envs/fl_with_flower/lib/python3.11/site-packages/flwr/simulation/ray_transport/ray_actor.py", line 398, in get_client_result
    return self._fetch_future_result(cid)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/abdenour/miniconda3/envs/fl_with_flower/lib/python3.11/site-packages/flwr/simulation/ray_transport/ray_actor.py", line 279, in _fetch_future_result
    res_cid, out_mssg, updated_context = ray.get(
                                         ^^^^^^^^
  File "/home/abdenour/miniconda3/envs/fl_with_flower/lib/python3.11/site-packages/ray/_private/auto_init_hook.py", line 21, in auto_init_wrapper
    return fn(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^
  File "/home/abdenour/miniconda3/envs/fl_with_flower/lib/python3.11/site-packages/ray/_private/client_mode_hook.py", line 103, in wrapper
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "/home/abdenour/miniconda3/envs/fl_with_flower/lib/python3.11/site-packages/ray/_private/worker.py", line 2667, in get
    values, debugger_breakpoint = worker.get_objects(object_refs, timeout=timeout)
                                  ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/abdenour/miniconda3/envs/fl_with_flower/lib/python3.11/site-packages/ray/_private/worker.py", line 866, in get_objects
    raise value
ray.exceptions.OutOfMemoryError: Task was killed due to the node running low on memory.
Memory on the node (IP: 115.145.171.128, ID: c687613ff34cdbf5097699e5c896055ef2dc44343c5ab44bd40309d5) where the task (actor ID: 85928d3f1a247800081c0e1101000000, name=ClientAppActor.__init__, pid=619614, memory used=1.73GB) was running was 29.50GB / 31.02GB (0.951049), which exceeds the memory usage threshold of 0.95. Ray killed this worker (ID: c7f644ac3cca20dffbfa1723daac4f87f04c7de03da25e7cad5fe33c) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 115.145.171.128`. To see the logs of the worker, use `ray logs worker-c7f644ac3cca20dffbfa1723daac4f87f04c7de03da25e7cad5fe33c*out -ip 115.145.171.128. Top 10 memory users:
PID	MEM(GB)	COMMAND
4942	3.49	/snap/pycharm-professional/408/jbr/bin/java -classpath /snap/pycharm-professional/408/lib/platform-l...
619616	1.80	ray::ClientAppActor.run
619615	1.78	ray::ClientAppActor
619617	1.77	ray::ClientAppActor
619611	1.73	ray::ClientAppActor
619618	1.73	ray::ClientAppActor
619614	1.73	ray::ClientAppActor
619612	1.72	ray::ClientAppActor
619619	1.70	ray::ClientAppActor
619620	1.68	ray::ClientAppActor.run
Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.

[2024-08-13 23:53:20,798][flwr][ERROR] - Task was killed due to the node running low on memory.
Memory on the node (IP: 115.145.171.128, ID: c687613ff34cdbf5097699e5c896055ef2dc44343c5ab44bd40309d5) where the task (actor ID: 85928d3f1a247800081c0e1101000000, name=ClientAppActor.__init__, pid=619614, memory used=1.73GB) was running was 29.50GB / 31.02GB (0.951049), which exceeds the memory usage threshold of 0.95. Ray killed this worker (ID: c7f644ac3cca20dffbfa1723daac4f87f04c7de03da25e7cad5fe33c) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 115.145.171.128`. To see the logs of the worker, use `ray logs worker-c7f644ac3cca20dffbfa1723daac4f87f04c7de03da25e7cad5fe33c*out -ip 115.145.171.128. Top 10 memory users:
PID	MEM(GB)	COMMAND
4942	3.49	/snap/pycharm-professional/408/jbr/bin/java -classpath /snap/pycharm-professional/408/lib/platform-l...
619616	1.80	ray::ClientAppActor.run
619615	1.78	ray::ClientAppActor
619617	1.77	ray::ClientAppActor
619611	1.73	ray::ClientAppActor
619618	1.73	ray::ClientAppActor
619614	1.73	ray::ClientAppActor
619612	1.72	ray::ClientAppActor
619619	1.70	ray::ClientAppActor
619620	1.68	ray::ClientAppActor.run
Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.
[2024-08-13 23:53:22,477][flwr][INFO] - aggregate_fit: received 8 results and 2 failures
[2024-08-13 23:53:26,082][flwr][INFO] - fit progress: (228, 13.85795146226883, {'accuracy': 0.7532051282051282}, 1327.3420038180193)
[2024-08-13 23:53:26,082][flwr][INFO] - configure_evaluate: strategy sampled 2 clients (out of 20)
[2024-08-13 23:53:26,293][flwr][INFO] - aggregate_evaluate: received 2 results and 0 failures
[2024-08-13 23:53:26,293][flwr][INFO] - 
[2024-08-13 23:53:26,293][flwr][INFO] - [ROUND 229]
[2024-08-13 23:53:26,293][flwr][INFO] - configure_fit: strategy sampled 10 clients (out of 20)
[2024-08-13 23:53:26,467][flwr][ERROR] - Traceback (most recent call last):
  File "/home/abdenour/miniconda3/envs/fl_with_flower/lib/python3.11/site-packages/flwr/simulation/ray_transport/ray_client_proxy.py", line 94, in _submit_job
    out_mssg, updated_context = self.actor_pool.get_client_result(
                                ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/abdenour/miniconda3/envs/fl_with_flower/lib/python3.11/site-packages/flwr/simulation/ray_transport/ray_actor.py", line 398, in get_client_result
    return self._fetch_future_result(cid)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/abdenour/miniconda3/envs/fl_with_flower/lib/python3.11/site-packages/flwr/simulation/ray_transport/ray_actor.py", line 279, in _fetch_future_result
    res_cid, out_mssg, updated_context = ray.get(
                                         ^^^^^^^^
  File "/home/abdenour/miniconda3/envs/fl_with_flower/lib/python3.11/site-packages/ray/_private/auto_init_hook.py", line 21, in auto_init_wrapper
    return fn(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^
  File "/home/abdenour/miniconda3/envs/fl_with_flower/lib/python3.11/site-packages/ray/_private/client_mode_hook.py", line 103, in wrapper
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "/home/abdenour/miniconda3/envs/fl_with_flower/lib/python3.11/site-packages/ray/_private/worker.py", line 2667, in get
    values, debugger_breakpoint = worker.get_objects(object_refs, timeout=timeout)
                                  ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/abdenour/miniconda3/envs/fl_with_flower/lib/python3.11/site-packages/ray/_private/worker.py", line 866, in get_objects
    raise value
ray.exceptions.OutOfMemoryError: Task was killed due to the node running low on memory.
Memory on the node (IP: 115.145.171.128, ID: c687613ff34cdbf5097699e5c896055ef2dc44343c5ab44bd40309d5) where the task (actor ID: 52365bb7997979e5a4c7ddbd01000000, name=ClientAppActor.__init__, pid=619618, memory used=1.84GB) was running was 29.49GB / 31.02GB (0.950707), which exceeds the memory usage threshold of 0.95. Ray killed this worker (ID: 7e48cd46c72821b17be65b05a4e61b5846ba998a57d76db44ca4f8fa) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 115.145.171.128`. To see the logs of the worker, use `ray logs worker-7e48cd46c72821b17be65b05a4e61b5846ba998a57d76db44ca4f8fa*out -ip 115.145.171.128. Top 10 memory users:
PID	MEM(GB)	COMMAND
4942	3.48	/snap/pycharm-professional/408/jbr/bin/java -classpath /snap/pycharm-professional/408/lib/platform-l...
616443	2.14	/home/abdenour/miniconda3/envs/fl_with_flower/bin/python /home/abdenour/PycharmProjects/fl_with_flow...
619615	1.90	ray::ClientAppActor
619616	1.88	ray::ClientAppActor
619611	1.87	ray::ClientAppActor
619617	1.86	ray::ClientAppActor
619620	1.85	ray::ClientAppActor
619618	1.84	ray::ClientAppActor
619613	1.82	ray::ClientAppActor
619619	1.80	ray::ClientAppActor
Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.

[2024-08-13 23:53:26,467][flwr][ERROR] - Task was killed due to the node running low on memory.
Memory on the node (IP: 115.145.171.128, ID: c687613ff34cdbf5097699e5c896055ef2dc44343c5ab44bd40309d5) where the task (actor ID: 52365bb7997979e5a4c7ddbd01000000, name=ClientAppActor.__init__, pid=619618, memory used=1.84GB) was running was 29.49GB / 31.02GB (0.950707), which exceeds the memory usage threshold of 0.95. Ray killed this worker (ID: 7e48cd46c72821b17be65b05a4e61b5846ba998a57d76db44ca4f8fa) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 115.145.171.128`. To see the logs of the worker, use `ray logs worker-7e48cd46c72821b17be65b05a4e61b5846ba998a57d76db44ca4f8fa*out -ip 115.145.171.128. Top 10 memory users:
PID	MEM(GB)	COMMAND
4942	3.48	/snap/pycharm-professional/408/jbr/bin/java -classpath /snap/pycharm-professional/408/lib/platform-l...
616443	2.14	/home/abdenour/miniconda3/envs/fl_with_flower/bin/python /home/abdenour/PycharmProjects/fl_with_flow...
619615	1.90	ray::ClientAppActor
619616	1.88	ray::ClientAppActor
619611	1.87	ray::ClientAppActor
619617	1.86	ray::ClientAppActor
619620	1.85	ray::ClientAppActor
619618	1.84	ray::ClientAppActor
619613	1.82	ray::ClientAppActor
619619	1.80	ray::ClientAppActor
Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.
[2024-08-13 23:53:26,468][flwr][ERROR] - Traceback (most recent call last):
  File "/home/abdenour/miniconda3/envs/fl_with_flower/lib/python3.11/site-packages/flwr/simulation/ray_transport/ray_client_proxy.py", line 94, in _submit_job
    out_mssg, updated_context = self.actor_pool.get_client_result(
                                ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/abdenour/miniconda3/envs/fl_with_flower/lib/python3.11/site-packages/flwr/simulation/ray_transport/ray_actor.py", line 398, in get_client_result
    return self._fetch_future_result(cid)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/abdenour/miniconda3/envs/fl_with_flower/lib/python3.11/site-packages/flwr/simulation/ray_transport/ray_actor.py", line 279, in _fetch_future_result
    res_cid, out_mssg, updated_context = ray.get(
                                         ^^^^^^^^
  File "/home/abdenour/miniconda3/envs/fl_with_flower/lib/python3.11/site-packages/ray/_private/auto_init_hook.py", line 21, in auto_init_wrapper
    return fn(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^
  File "/home/abdenour/miniconda3/envs/fl_with_flower/lib/python3.11/site-packages/ray/_private/client_mode_hook.py", line 103, in wrapper
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "/home/abdenour/miniconda3/envs/fl_with_flower/lib/python3.11/site-packages/ray/_private/worker.py", line 2667, in get
    values, debugger_breakpoint = worker.get_objects(object_refs, timeout=timeout)
                                  ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/abdenour/miniconda3/envs/fl_with_flower/lib/python3.11/site-packages/ray/_private/worker.py", line 866, in get_objects
    raise value
ray.exceptions.OutOfMemoryError: Task was killed due to the node running low on memory.
Memory on the node (IP: 115.145.171.128, ID: c687613ff34cdbf5097699e5c896055ef2dc44343c5ab44bd40309d5) where the task (actor ID: 85928d3f1a247800081c0e1101000000, name=ClientAppActor.__init__, pid=619614, memory used=1.73GB) was running was 29.50GB / 31.02GB (0.951049), which exceeds the memory usage threshold of 0.95. Ray killed this worker (ID: c7f644ac3cca20dffbfa1723daac4f87f04c7de03da25e7cad5fe33c) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 115.145.171.128`. To see the logs of the worker, use `ray logs worker-c7f644ac3cca20dffbfa1723daac4f87f04c7de03da25e7cad5fe33c*out -ip 115.145.171.128. Top 10 memory users:
PID	MEM(GB)	COMMAND
4942	3.49	/snap/pycharm-professional/408/jbr/bin/java -classpath /snap/pycharm-professional/408/lib/platform-l...
619616	1.80	ray::ClientAppActor.run
619615	1.78	ray::ClientAppActor
619617	1.77	ray::ClientAppActor
619611	1.73	ray::ClientAppActor
619618	1.73	ray::ClientAppActor
619614	1.73	ray::ClientAppActor
619612	1.72	ray::ClientAppActor
619619	1.70	ray::ClientAppActor
619620	1.68	ray::ClientAppActor.run
Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.

[2024-08-13 23:53:26,468][flwr][ERROR] - Task was killed due to the node running low on memory.
Memory on the node (IP: 115.145.171.128, ID: c687613ff34cdbf5097699e5c896055ef2dc44343c5ab44bd40309d5) where the task (actor ID: 85928d3f1a247800081c0e1101000000, name=ClientAppActor.__init__, pid=619614, memory used=1.73GB) was running was 29.50GB / 31.02GB (0.951049), which exceeds the memory usage threshold of 0.95. Ray killed this worker (ID: c7f644ac3cca20dffbfa1723daac4f87f04c7de03da25e7cad5fe33c) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 115.145.171.128`. To see the logs of the worker, use `ray logs worker-c7f644ac3cca20dffbfa1723daac4f87f04c7de03da25e7cad5fe33c*out -ip 115.145.171.128. Top 10 memory users:
PID	MEM(GB)	COMMAND
4942	3.49	/snap/pycharm-professional/408/jbr/bin/java -classpath /snap/pycharm-professional/408/lib/platform-l...
619616	1.80	ray::ClientAppActor.run
619615	1.78	ray::ClientAppActor
619617	1.77	ray::ClientAppActor
619611	1.73	ray::ClientAppActor
619618	1.73	ray::ClientAppActor
619614	1.73	ray::ClientAppActor
619612	1.72	ray::ClientAppActor
619619	1.70	ray::ClientAppActor
619620	1.68	ray::ClientAppActor.run
Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.
[2024-08-13 23:53:28,198][flwr][INFO] - aggregate_fit: received 8 results and 2 failures
[2024-08-13 23:53:31,195][flwr][INFO] - fit progress: (229, 18.464122414588928, {'accuracy': 0.7323717948717948}, 1332.4556681739923)
[2024-08-13 23:53:31,195][flwr][INFO] - configure_evaluate: strategy sampled 2 clients (out of 20)
[2024-08-13 23:53:31,481][flwr][INFO] - aggregate_evaluate: received 2 results and 0 failures
[2024-08-13 23:53:31,481][flwr][INFO] - 
[2024-08-13 23:53:31,481][flwr][INFO] - [ROUND 230]
[2024-08-13 23:53:31,481][flwr][INFO] - configure_fit: strategy sampled 10 clients (out of 20)
[2024-08-13 23:53:31,655][flwr][ERROR] - Traceback (most recent call last):
  File "/home/abdenour/miniconda3/envs/fl_with_flower/lib/python3.11/site-packages/flwr/simulation/ray_transport/ray_client_proxy.py", line 94, in _submit_job
    out_mssg, updated_context = self.actor_pool.get_client_result(
                                ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/abdenour/miniconda3/envs/fl_with_flower/lib/python3.11/site-packages/flwr/simulation/ray_transport/ray_actor.py", line 398, in get_client_result
    return self._fetch_future_result(cid)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/abdenour/miniconda3/envs/fl_with_flower/lib/python3.11/site-packages/flwr/simulation/ray_transport/ray_actor.py", line 279, in _fetch_future_result
    res_cid, out_mssg, updated_context = ray.get(
                                         ^^^^^^^^
  File "/home/abdenour/miniconda3/envs/fl_with_flower/lib/python3.11/site-packages/ray/_private/auto_init_hook.py", line 21, in auto_init_wrapper
    return fn(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^
  File "/home/abdenour/miniconda3/envs/fl_with_flower/lib/python3.11/site-packages/ray/_private/client_mode_hook.py", line 103, in wrapper
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "/home/abdenour/miniconda3/envs/fl_with_flower/lib/python3.11/site-packages/ray/_private/worker.py", line 2667, in get
    values, debugger_breakpoint = worker.get_objects(object_refs, timeout=timeout)
                                  ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/abdenour/miniconda3/envs/fl_with_flower/lib/python3.11/site-packages/ray/_private/worker.py", line 866, in get_objects
    raise value
ray.exceptions.OutOfMemoryError: Task was killed due to the node running low on memory.
Memory on the node (IP: 115.145.171.128, ID: c687613ff34cdbf5097699e5c896055ef2dc44343c5ab44bd40309d5) where the task (actor ID: 85928d3f1a247800081c0e1101000000, name=ClientAppActor.__init__, pid=619614, memory used=1.73GB) was running was 29.50GB / 31.02GB (0.951049), which exceeds the memory usage threshold of 0.95. Ray killed this worker (ID: c7f644ac3cca20dffbfa1723daac4f87f04c7de03da25e7cad5fe33c) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 115.145.171.128`. To see the logs of the worker, use `ray logs worker-c7f644ac3cca20dffbfa1723daac4f87f04c7de03da25e7cad5fe33c*out -ip 115.145.171.128. Top 10 memory users:
PID	MEM(GB)	COMMAND
4942	3.49	/snap/pycharm-professional/408/jbr/bin/java -classpath /snap/pycharm-professional/408/lib/platform-l...
619616	1.80	ray::ClientAppActor.run
619615	1.78	ray::ClientAppActor
619617	1.77	ray::ClientAppActor
619611	1.73	ray::ClientAppActor
619618	1.73	ray::ClientAppActor
619614	1.73	ray::ClientAppActor
619612	1.72	ray::ClientAppActor
619619	1.70	ray::ClientAppActor
619620	1.68	ray::ClientAppActor.run
Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.

[2024-08-13 23:53:31,655][flwr][ERROR] - Traceback (most recent call last):
  File "/home/abdenour/miniconda3/envs/fl_with_flower/lib/python3.11/site-packages/flwr/simulation/ray_transport/ray_client_proxy.py", line 94, in _submit_job
    out_mssg, updated_context = self.actor_pool.get_client_result(
                                ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/abdenour/miniconda3/envs/fl_with_flower/lib/python3.11/site-packages/flwr/simulation/ray_transport/ray_actor.py", line 398, in get_client_result
    return self._fetch_future_result(cid)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/abdenour/miniconda3/envs/fl_with_flower/lib/python3.11/site-packages/flwr/simulation/ray_transport/ray_actor.py", line 279, in _fetch_future_result
    res_cid, out_mssg, updated_context = ray.get(
                                         ^^^^^^^^
  File "/home/abdenour/miniconda3/envs/fl_with_flower/lib/python3.11/site-packages/ray/_private/auto_init_hook.py", line 21, in auto_init_wrapper
    return fn(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^
  File "/home/abdenour/miniconda3/envs/fl_with_flower/lib/python3.11/site-packages/ray/_private/client_mode_hook.py", line 103, in wrapper
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "/home/abdenour/miniconda3/envs/fl_with_flower/lib/python3.11/site-packages/ray/_private/worker.py", line 2667, in get
    values, debugger_breakpoint = worker.get_objects(object_refs, timeout=timeout)
                                  ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/abdenour/miniconda3/envs/fl_with_flower/lib/python3.11/site-packages/ray/_private/worker.py", line 866, in get_objects
    raise value
ray.exceptions.OutOfMemoryError: Task was killed due to the node running low on memory.
Memory on the node (IP: 115.145.171.128, ID: c687613ff34cdbf5097699e5c896055ef2dc44343c5ab44bd40309d5) where the task (actor ID: 52365bb7997979e5a4c7ddbd01000000, name=ClientAppActor.__init__, pid=619618, memory used=1.84GB) was running was 29.49GB / 31.02GB (0.950707), which exceeds the memory usage threshold of 0.95. Ray killed this worker (ID: 7e48cd46c72821b17be65b05a4e61b5846ba998a57d76db44ca4f8fa) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 115.145.171.128`. To see the logs of the worker, use `ray logs worker-7e48cd46c72821b17be65b05a4e61b5846ba998a57d76db44ca4f8fa*out -ip 115.145.171.128. Top 10 memory users:
PID	MEM(GB)	COMMAND
4942	3.48	/snap/pycharm-professional/408/jbr/bin/java -classpath /snap/pycharm-professional/408/lib/platform-l...
616443	2.14	/home/abdenour/miniconda3/envs/fl_with_flower/bin/python /home/abdenour/PycharmProjects/fl_with_flow...
619615	1.90	ray::ClientAppActor
619616	1.88	ray::ClientAppActor
619611	1.87	ray::ClientAppActor
619617	1.86	ray::ClientAppActor
619620	1.85	ray::ClientAppActor
619618	1.84	ray::ClientAppActor
619613	1.82	ray::ClientAppActor
619619	1.80	ray::ClientAppActor
Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.

[2024-08-13 23:53:31,655][flwr][ERROR] - Task was killed due to the node running low on memory.
Memory on the node (IP: 115.145.171.128, ID: c687613ff34cdbf5097699e5c896055ef2dc44343c5ab44bd40309d5) where the task (actor ID: 85928d3f1a247800081c0e1101000000, name=ClientAppActor.__init__, pid=619614, memory used=1.73GB) was running was 29.50GB / 31.02GB (0.951049), which exceeds the memory usage threshold of 0.95. Ray killed this worker (ID: c7f644ac3cca20dffbfa1723daac4f87f04c7de03da25e7cad5fe33c) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 115.145.171.128`. To see the logs of the worker, use `ray logs worker-c7f644ac3cca20dffbfa1723daac4f87f04c7de03da25e7cad5fe33c*out -ip 115.145.171.128. Top 10 memory users:
PID	MEM(GB)	COMMAND
4942	3.49	/snap/pycharm-professional/408/jbr/bin/java -classpath /snap/pycharm-professional/408/lib/platform-l...
619616	1.80	ray::ClientAppActor.run
619615	1.78	ray::ClientAppActor
619617	1.77	ray::ClientAppActor
619611	1.73	ray::ClientAppActor
619618	1.73	ray::ClientAppActor
619614	1.73	ray::ClientAppActor
619612	1.72	ray::ClientAppActor
619619	1.70	ray::ClientAppActor
619620	1.68	ray::ClientAppActor.run
Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.
[2024-08-13 23:53:31,655][flwr][ERROR] - Task was killed due to the node running low on memory.
Memory on the node (IP: 115.145.171.128, ID: c687613ff34cdbf5097699e5c896055ef2dc44343c5ab44bd40309d5) where the task (actor ID: 52365bb7997979e5a4c7ddbd01000000, name=ClientAppActor.__init__, pid=619618, memory used=1.84GB) was running was 29.49GB / 31.02GB (0.950707), which exceeds the memory usage threshold of 0.95. Ray killed this worker (ID: 7e48cd46c72821b17be65b05a4e61b5846ba998a57d76db44ca4f8fa) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 115.145.171.128`. To see the logs of the worker, use `ray logs worker-7e48cd46c72821b17be65b05a4e61b5846ba998a57d76db44ca4f8fa*out -ip 115.145.171.128. Top 10 memory users:
PID	MEM(GB)	COMMAND
4942	3.48	/snap/pycharm-professional/408/jbr/bin/java -classpath /snap/pycharm-professional/408/lib/platform-l...
616443	2.14	/home/abdenour/miniconda3/envs/fl_with_flower/bin/python /home/abdenour/PycharmProjects/fl_with_flow...
619615	1.90	ray::ClientAppActor
619616	1.88	ray::ClientAppActor
619611	1.87	ray::ClientAppActor
619617	1.86	ray::ClientAppActor
619620	1.85	ray::ClientAppActor
619618	1.84	ray::ClientAppActor
619613	1.82	ray::ClientAppActor
619619	1.80	ray::ClientAppActor
Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.
[2024-08-13 23:53:33,255][flwr][INFO] - aggregate_fit: received 8 results and 2 failures
[2024-08-13 23:53:36,091][flwr][INFO] - fit progress: (230, 19.369524598121643, {'accuracy': 0.7243589743589743}, 1337.3512706590118)
[2024-08-13 23:53:36,091][flwr][INFO] - configure_evaluate: strategy sampled 2 clients (out of 20)
[2024-08-13 23:53:36,397][flwr][INFO] - aggregate_evaluate: received 2 results and 0 failures
[2024-08-13 23:53:36,398][flwr][INFO] - 
[2024-08-13 23:53:36,398][flwr][INFO] - [ROUND 231]
[2024-08-13 23:53:36,398][flwr][INFO] - configure_fit: strategy sampled 10 clients (out of 20)
[2024-08-13 23:53:36,538][flwr][ERROR] - Traceback (most recent call last):
  File "/home/abdenour/miniconda3/envs/fl_with_flower/lib/python3.11/site-packages/flwr/simulation/ray_transport/ray_client_proxy.py", line 94, in _submit_job
    out_mssg, updated_context = self.actor_pool.get_client_result(
                                ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/abdenour/miniconda3/envs/fl_with_flower/lib/python3.11/site-packages/flwr/simulation/ray_transport/ray_actor.py", line 398, in get_client_result
    return self._fetch_future_result(cid)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/abdenour/miniconda3/envs/fl_with_flower/lib/python3.11/site-packages/flwr/simulation/ray_transport/ray_actor.py", line 279, in _fetch_future_result
    res_cid, out_mssg, updated_context = ray.get(
                                         ^^^^^^^^
  File "/home/abdenour/miniconda3/envs/fl_with_flower/lib/python3.11/site-packages/ray/_private/auto_init_hook.py", line 21, in auto_init_wrapper
    return fn(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^
  File "/home/abdenour/miniconda3/envs/fl_with_flower/lib/python3.11/site-packages/ray/_private/client_mode_hook.py", line 103, in wrapper
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "/home/abdenour/miniconda3/envs/fl_with_flower/lib/python3.11/site-packages/ray/_private/worker.py", line 2667, in get
    values, debugger_breakpoint = worker.get_objects(object_refs, timeout=timeout)
                                  ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/abdenour/miniconda3/envs/fl_with_flower/lib/python3.11/site-packages/ray/_private/worker.py", line 866, in get_objects
    raise value
ray.exceptions.OutOfMemoryError: Task was killed due to the node running low on memory.
Memory on the node (IP: 115.145.171.128, ID: c687613ff34cdbf5097699e5c896055ef2dc44343c5ab44bd40309d5) where the task (actor ID: 52365bb7997979e5a4c7ddbd01000000, name=ClientAppActor.__init__, pid=619618, memory used=1.84GB) was running was 29.49GB / 31.02GB (0.950707), which exceeds the memory usage threshold of 0.95. Ray killed this worker (ID: 7e48cd46c72821b17be65b05a4e61b5846ba998a57d76db44ca4f8fa) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 115.145.171.128`. To see the logs of the worker, use `ray logs worker-7e48cd46c72821b17be65b05a4e61b5846ba998a57d76db44ca4f8fa*out -ip 115.145.171.128. Top 10 memory users:
PID	MEM(GB)	COMMAND
4942	3.48	/snap/pycharm-professional/408/jbr/bin/java -classpath /snap/pycharm-professional/408/lib/platform-l...
616443	2.14	/home/abdenour/miniconda3/envs/fl_with_flower/bin/python /home/abdenour/PycharmProjects/fl_with_flow...
619615	1.90	ray::ClientAppActor
619616	1.88	ray::ClientAppActor
619611	1.87	ray::ClientAppActor
619617	1.86	ray::ClientAppActor
619620	1.85	ray::ClientAppActor
619618	1.84	ray::ClientAppActor
619613	1.82	ray::ClientAppActor
619619	1.80	ray::ClientAppActor
Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.

[2024-08-13 23:53:36,538][flwr][ERROR] - Traceback (most recent call last):
  File "/home/abdenour/miniconda3/envs/fl_with_flower/lib/python3.11/site-packages/flwr/simulation/ray_transport/ray_client_proxy.py", line 94, in _submit_job
    out_mssg, updated_context = self.actor_pool.get_client_result(
                                ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/abdenour/miniconda3/envs/fl_with_flower/lib/python3.11/site-packages/flwr/simulation/ray_transport/ray_actor.py", line 398, in get_client_result
    return self._fetch_future_result(cid)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/abdenour/miniconda3/envs/fl_with_flower/lib/python3.11/site-packages/flwr/simulation/ray_transport/ray_actor.py", line 279, in _fetch_future_result
    res_cid, out_mssg, updated_context = ray.get(
                                         ^^^^^^^^
  File "/home/abdenour/miniconda3/envs/fl_with_flower/lib/python3.11/site-packages/ray/_private/auto_init_hook.py", line 21, in auto_init_wrapper
    return fn(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^
  File "/home/abdenour/miniconda3/envs/fl_with_flower/lib/python3.11/site-packages/ray/_private/client_mode_hook.py", line 103, in wrapper
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "/home/abdenour/miniconda3/envs/fl_with_flower/lib/python3.11/site-packages/ray/_private/worker.py", line 2667, in get
    values, debugger_breakpoint = worker.get_objects(object_refs, timeout=timeout)
                                  ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/abdenour/miniconda3/envs/fl_with_flower/lib/python3.11/site-packages/ray/_private/worker.py", line 866, in get_objects
    raise value
ray.exceptions.OutOfMemoryError: Task was killed due to the node running low on memory.
Memory on the node (IP: 115.145.171.128, ID: c687613ff34cdbf5097699e5c896055ef2dc44343c5ab44bd40309d5) where the task (actor ID: 85928d3f1a247800081c0e1101000000, name=ClientAppActor.__init__, pid=619614, memory used=1.73GB) was running was 29.50GB / 31.02GB (0.951049), which exceeds the memory usage threshold of 0.95. Ray killed this worker (ID: c7f644ac3cca20dffbfa1723daac4f87f04c7de03da25e7cad5fe33c) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 115.145.171.128`. To see the logs of the worker, use `ray logs worker-c7f644ac3cca20dffbfa1723daac4f87f04c7de03da25e7cad5fe33c*out -ip 115.145.171.128. Top 10 memory users:
PID	MEM(GB)	COMMAND
4942	3.49	/snap/pycharm-professional/408/jbr/bin/java -classpath /snap/pycharm-professional/408/lib/platform-l...
619616	1.80	ray::ClientAppActor.run
619615	1.78	ray::ClientAppActor
619617	1.77	ray::ClientAppActor
619611	1.73	ray::ClientAppActor
619618	1.73	ray::ClientAppActor
619614	1.73	ray::ClientAppActor
619612	1.72	ray::ClientAppActor
619619	1.70	ray::ClientAppActor
619620	1.68	ray::ClientAppActor.run
Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.

[2024-08-13 23:53:36,539][flwr][ERROR] - Task was killed due to the node running low on memory.
Memory on the node (IP: 115.145.171.128, ID: c687613ff34cdbf5097699e5c896055ef2dc44343c5ab44bd40309d5) where the task (actor ID: 52365bb7997979e5a4c7ddbd01000000, name=ClientAppActor.__init__, pid=619618, memory used=1.84GB) was running was 29.49GB / 31.02GB (0.950707), which exceeds the memory usage threshold of 0.95. Ray killed this worker (ID: 7e48cd46c72821b17be65b05a4e61b5846ba998a57d76db44ca4f8fa) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 115.145.171.128`. To see the logs of the worker, use `ray logs worker-7e48cd46c72821b17be65b05a4e61b5846ba998a57d76db44ca4f8fa*out -ip 115.145.171.128. Top 10 memory users:
PID	MEM(GB)	COMMAND
4942	3.48	/snap/pycharm-professional/408/jbr/bin/java -classpath /snap/pycharm-professional/408/lib/platform-l...
616443	2.14	/home/abdenour/miniconda3/envs/fl_with_flower/bin/python /home/abdenour/PycharmProjects/fl_with_flow...
619615	1.90	ray::ClientAppActor
619616	1.88	ray::ClientAppActor
619611	1.87	ray::ClientAppActor
619617	1.86	ray::ClientAppActor
619620	1.85	ray::ClientAppActor
619618	1.84	ray::ClientAppActor
619613	1.82	ray::ClientAppActor
619619	1.80	ray::ClientAppActor
Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.
[2024-08-13 23:53:36,539][flwr][ERROR] - Task was killed due to the node running low on memory.
Memory on the node (IP: 115.145.171.128, ID: c687613ff34cdbf5097699e5c896055ef2dc44343c5ab44bd40309d5) where the task (actor ID: 85928d3f1a247800081c0e1101000000, name=ClientAppActor.__init__, pid=619614, memory used=1.73GB) was running was 29.50GB / 31.02GB (0.951049), which exceeds the memory usage threshold of 0.95. Ray killed this worker (ID: c7f644ac3cca20dffbfa1723daac4f87f04c7de03da25e7cad5fe33c) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 115.145.171.128`. To see the logs of the worker, use `ray logs worker-c7f644ac3cca20dffbfa1723daac4f87f04c7de03da25e7cad5fe33c*out -ip 115.145.171.128. Top 10 memory users:
PID	MEM(GB)	COMMAND
4942	3.49	/snap/pycharm-professional/408/jbr/bin/java -classpath /snap/pycharm-professional/408/lib/platform-l...
619616	1.80	ray::ClientAppActor.run
619615	1.78	ray::ClientAppActor
619617	1.77	ray::ClientAppActor
619611	1.73	ray::ClientAppActor
619618	1.73	ray::ClientAppActor
619614	1.73	ray::ClientAppActor
619612	1.72	ray::ClientAppActor
619619	1.70	ray::ClientAppActor
619620	1.68	ray::ClientAppActor.run
Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.
[2024-08-13 23:53:38,282][flwr][INFO] - aggregate_fit: received 8 results and 2 failures
[2024-08-13 23:53:41,508][flwr][INFO] - fit progress: (231, 16.131164371967316, {'accuracy': 0.75}, 1342.7682306040078)
[2024-08-13 23:53:41,508][flwr][INFO] - configure_evaluate: strategy sampled 2 clients (out of 20)
[2024-08-13 23:53:41,730][flwr][INFO] - aggregate_evaluate: received 2 results and 0 failures
[2024-08-13 23:53:41,730][flwr][INFO] - 
[2024-08-13 23:53:41,730][flwr][INFO] - [ROUND 232]
[2024-08-13 23:53:41,730][flwr][INFO] - configure_fit: strategy sampled 10 clients (out of 20)
[2024-08-13 23:53:41,914][flwr][ERROR] - Traceback (most recent call last):
  File "/home/abdenour/miniconda3/envs/fl_with_flower/lib/python3.11/site-packages/flwr/simulation/ray_transport/ray_client_proxy.py", line 94, in _submit_job
    out_mssg, updated_context = self.actor_pool.get_client_result(
                                ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/abdenour/miniconda3/envs/fl_with_flower/lib/python3.11/site-packages/flwr/simulation/ray_transport/ray_actor.py", line 398, in get_client_result
    return self._fetch_future_result(cid)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/abdenour/miniconda3/envs/fl_with_flower/lib/python3.11/site-packages/flwr/simulation/ray_transport/ray_actor.py", line 279, in _fetch_future_result
    res_cid, out_mssg, updated_context = ray.get(
                                         ^^^^^^^^
  File "/home/abdenour/miniconda3/envs/fl_with_flower/lib/python3.11/site-packages/ray/_private/auto_init_hook.py", line 21, in auto_init_wrapper
    return fn(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^
  File "/home/abdenour/miniconda3/envs/fl_with_flower/lib/python3.11/site-packages/ray/_private/client_mode_hook.py", line 103, in wrapper
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "/home/abdenour/miniconda3/envs/fl_with_flower/lib/python3.11/site-packages/ray/_private/worker.py", line 2667, in get
    values, debugger_breakpoint = worker.get_objects(object_refs, timeout=timeout)
                                  ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/abdenour/miniconda3/envs/fl_with_flower/lib/python3.11/site-packages/ray/_private/worker.py", line 866, in get_objects
    raise value
ray.exceptions.OutOfMemoryError: Task was killed due to the node running low on memory.
Memory on the node (IP: 115.145.171.128, ID: c687613ff34cdbf5097699e5c896055ef2dc44343c5ab44bd40309d5) where the task (actor ID: 85928d3f1a247800081c0e1101000000, name=ClientAppActor.__init__, pid=619614, memory used=1.73GB) was running was 29.50GB / 31.02GB (0.951049), which exceeds the memory usage threshold of 0.95. Ray killed this worker (ID: c7f644ac3cca20dffbfa1723daac4f87f04c7de03da25e7cad5fe33c) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 115.145.171.128`. To see the logs of the worker, use `ray logs worker-c7f644ac3cca20dffbfa1723daac4f87f04c7de03da25e7cad5fe33c*out -ip 115.145.171.128. Top 10 memory users:
PID	MEM(GB)	COMMAND
4942	3.49	/snap/pycharm-professional/408/jbr/bin/java -classpath /snap/pycharm-professional/408/lib/platform-l...
619616	1.80	ray::ClientAppActor.run
619615	1.78	ray::ClientAppActor
619617	1.77	ray::ClientAppActor
619611	1.73	ray::ClientAppActor
619618	1.73	ray::ClientAppActor
619614	1.73	ray::ClientAppActor
619612	1.72	ray::ClientAppActor
619619	1.70	ray::ClientAppActor
619620	1.68	ray::ClientAppActor.run
Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.

[2024-08-13 23:53:41,914][flwr][ERROR] - Task was killed due to the node running low on memory.
Memory on the node (IP: 115.145.171.128, ID: c687613ff34cdbf5097699e5c896055ef2dc44343c5ab44bd40309d5) where the task (actor ID: 85928d3f1a247800081c0e1101000000, name=ClientAppActor.__init__, pid=619614, memory used=1.73GB) was running was 29.50GB / 31.02GB (0.951049), which exceeds the memory usage threshold of 0.95. Ray killed this worker (ID: c7f644ac3cca20dffbfa1723daac4f87f04c7de03da25e7cad5fe33c) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 115.145.171.128`. To see the logs of the worker, use `ray logs worker-c7f644ac3cca20dffbfa1723daac4f87f04c7de03da25e7cad5fe33c*out -ip 115.145.171.128. Top 10 memory users:
PID	MEM(GB)	COMMAND
4942	3.49	/snap/pycharm-professional/408/jbr/bin/java -classpath /snap/pycharm-professional/408/lib/platform-l...
619616	1.80	ray::ClientAppActor.run
619615	1.78	ray::ClientAppActor
619617	1.77	ray::ClientAppActor
619611	1.73	ray::ClientAppActor
619618	1.73	ray::ClientAppActor
619614	1.73	ray::ClientAppActor
619612	1.72	ray::ClientAppActor
619619	1.70	ray::ClientAppActor
619620	1.68	ray::ClientAppActor.run
Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.
[2024-08-13 23:53:41,915][flwr][ERROR] - Traceback (most recent call last):
  File "/home/abdenour/miniconda3/envs/fl_with_flower/lib/python3.11/site-packages/flwr/simulation/ray_transport/ray_client_proxy.py", line 94, in _submit_job
    out_mssg, updated_context = self.actor_pool.get_client_result(
                                ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/abdenour/miniconda3/envs/fl_with_flower/lib/python3.11/site-packages/flwr/simulation/ray_transport/ray_actor.py", line 398, in get_client_result
    return self._fetch_future_result(cid)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/abdenour/miniconda3/envs/fl_with_flower/lib/python3.11/site-packages/flwr/simulation/ray_transport/ray_actor.py", line 279, in _fetch_future_result
    res_cid, out_mssg, updated_context = ray.get(
                                         ^^^^^^^^
  File "/home/abdenour/miniconda3/envs/fl_with_flower/lib/python3.11/site-packages/ray/_private/auto_init_hook.py", line 21, in auto_init_wrapper
    return fn(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^
  File "/home/abdenour/miniconda3/envs/fl_with_flower/lib/python3.11/site-packages/ray/_private/client_mode_hook.py", line 103, in wrapper
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "/home/abdenour/miniconda3/envs/fl_with_flower/lib/python3.11/site-packages/ray/_private/worker.py", line 2667, in get
    values, debugger_breakpoint = worker.get_objects(object_refs, timeout=timeout)
                                  ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/abdenour/miniconda3/envs/fl_with_flower/lib/python3.11/site-packages/ray/_private/worker.py", line 866, in get_objects
    raise value
ray.exceptions.OutOfMemoryError: Task was killed due to the node running low on memory.
Memory on the node (IP: 115.145.171.128, ID: c687613ff34cdbf5097699e5c896055ef2dc44343c5ab44bd40309d5) where the task (actor ID: 52365bb7997979e5a4c7ddbd01000000, name=ClientAppActor.__init__, pid=619618, memory used=1.84GB) was running was 29.49GB / 31.02GB (0.950707), which exceeds the memory usage threshold of 0.95. Ray killed this worker (ID: 7e48cd46c72821b17be65b05a4e61b5846ba998a57d76db44ca4f8fa) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 115.145.171.128`. To see the logs of the worker, use `ray logs worker-7e48cd46c72821b17be65b05a4e61b5846ba998a57d76db44ca4f8fa*out -ip 115.145.171.128. Top 10 memory users:
PID	MEM(GB)	COMMAND
4942	3.48	/snap/pycharm-professional/408/jbr/bin/java -classpath /snap/pycharm-professional/408/lib/platform-l...
616443	2.14	/home/abdenour/miniconda3/envs/fl_with_flower/bin/python /home/abdenour/PycharmProjects/fl_with_flow...
619615	1.90	ray::ClientAppActor
619616	1.88	ray::ClientAppActor
619611	1.87	ray::ClientAppActor
619617	1.86	ray::ClientAppActor
619620	1.85	ray::ClientAppActor
619618	1.84	ray::ClientAppActor
619613	1.82	ray::ClientAppActor
619619	1.80	ray::ClientAppActor
Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.

[2024-08-13 23:53:41,915][flwr][ERROR] - Task was killed due to the node running low on memory.
Memory on the node (IP: 115.145.171.128, ID: c687613ff34cdbf5097699e5c896055ef2dc44343c5ab44bd40309d5) where the task (actor ID: 52365bb7997979e5a4c7ddbd01000000, name=ClientAppActor.__init__, pid=619618, memory used=1.84GB) was running was 29.49GB / 31.02GB (0.950707), which exceeds the memory usage threshold of 0.95. Ray killed this worker (ID: 7e48cd46c72821b17be65b05a4e61b5846ba998a57d76db44ca4f8fa) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 115.145.171.128`. To see the logs of the worker, use `ray logs worker-7e48cd46c72821b17be65b05a4e61b5846ba998a57d76db44ca4f8fa*out -ip 115.145.171.128. Top 10 memory users:
PID	MEM(GB)	COMMAND
4942	3.48	/snap/pycharm-professional/408/jbr/bin/java -classpath /snap/pycharm-professional/408/lib/platform-l...
616443	2.14	/home/abdenour/miniconda3/envs/fl_with_flower/bin/python /home/abdenour/PycharmProjects/fl_with_flow...
619615	1.90	ray::ClientAppActor
619616	1.88	ray::ClientAppActor
619611	1.87	ray::ClientAppActor
619617	1.86	ray::ClientAppActor
619620	1.85	ray::ClientAppActor
619618	1.84	ray::ClientAppActor
619613	1.82	ray::ClientAppActor
619619	1.80	ray::ClientAppActor
Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.
[2024-08-13 23:53:43,620][flwr][INFO] - aggregate_fit: received 8 results and 2 failures
[2024-08-13 23:53:46,630][flwr][INFO] - fit progress: (232, 16.546838223934174, {'accuracy': 0.7483974358974359}, 1347.8906057190034)
[2024-08-13 23:53:46,630][flwr][INFO] - configure_evaluate: strategy sampled 2 clients (out of 20)
[2024-08-13 23:53:46,983][flwr][INFO] - aggregate_evaluate: received 2 results and 0 failures
[2024-08-13 23:53:46,983][flwr][INFO] - 
[2024-08-13 23:53:46,983][flwr][INFO] - [ROUND 233]
[2024-08-13 23:53:46,983][flwr][INFO] - configure_fit: strategy sampled 10 clients (out of 20)
[2024-08-13 23:53:47,126][flwr][ERROR] - Traceback (most recent call last):
  File "/home/abdenour/miniconda3/envs/fl_with_flower/lib/python3.11/site-packages/flwr/simulation/ray_transport/ray_client_proxy.py", line 94, in _submit_job
    out_mssg, updated_context = self.actor_pool.get_client_result(
                                ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/abdenour/miniconda3/envs/fl_with_flower/lib/python3.11/site-packages/flwr/simulation/ray_transport/ray_actor.py", line 398, in get_client_result
    return self._fetch_future_result(cid)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/abdenour/miniconda3/envs/fl_with_flower/lib/python3.11/site-packages/flwr/simulation/ray_transport/ray_actor.py", line 279, in _fetch_future_result
    res_cid, out_mssg, updated_context = ray.get(
                                         ^^^^^^^^
  File "/home/abdenour/miniconda3/envs/fl_with_flower/lib/python3.11/site-packages/ray/_private/auto_init_hook.py", line 21, in auto_init_wrapper
    return fn(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^
  File "/home/abdenour/miniconda3/envs/fl_with_flower/lib/python3.11/site-packages/ray/_private/client_mode_hook.py", line 103, in wrapper
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "/home/abdenour/miniconda3/envs/fl_with_flower/lib/python3.11/site-packages/ray/_private/worker.py", line 2667, in get
    values, debugger_breakpoint = worker.get_objects(object_refs, timeout=timeout)
                                  ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/abdenour/miniconda3/envs/fl_with_flower/lib/python3.11/site-packages/ray/_private/worker.py", line 866, in get_objects
    raise value
ray.exceptions.OutOfMemoryError: Task was killed due to the node running low on memory.
Memory on the node (IP: 115.145.171.128, ID: c687613ff34cdbf5097699e5c896055ef2dc44343c5ab44bd40309d5) where the task (actor ID: 52365bb7997979e5a4c7ddbd01000000, name=ClientAppActor.__init__, pid=619618, memory used=1.84GB) was running was 29.49GB / 31.02GB (0.950707), which exceeds the memory usage threshold of 0.95. Ray killed this worker (ID: 7e48cd46c72821b17be65b05a4e61b5846ba998a57d76db44ca4f8fa) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 115.145.171.128`. To see the logs of the worker, use `ray logs worker-7e48cd46c72821b17be65b05a4e61b5846ba998a57d76db44ca4f8fa*out -ip 115.145.171.128. Top 10 memory users:
PID	MEM(GB)	COMMAND
4942	3.48	/snap/pycharm-professional/408/jbr/bin/java -classpath /snap/pycharm-professional/408/lib/platform-l...
616443	2.14	/home/abdenour/miniconda3/envs/fl_with_flower/bin/python /home/abdenour/PycharmProjects/fl_with_flow...
619615	1.90	ray::ClientAppActor
619616	1.88	ray::ClientAppActor
619611	1.87	ray::ClientAppActor
619617	1.86	ray::ClientAppActor
619620	1.85	ray::ClientAppActor
619618	1.84	ray::ClientAppActor
619613	1.82	ray::ClientAppActor
619619	1.80	ray::ClientAppActor
Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.

[2024-08-13 23:53:47,127][flwr][ERROR] - Task was killed due to the node running low on memory.
Memory on the node (IP: 115.145.171.128, ID: c687613ff34cdbf5097699e5c896055ef2dc44343c5ab44bd40309d5) where the task (actor ID: 52365bb7997979e5a4c7ddbd01000000, name=ClientAppActor.__init__, pid=619618, memory used=1.84GB) was running was 29.49GB / 31.02GB (0.950707), which exceeds the memory usage threshold of 0.95. Ray killed this worker (ID: 7e48cd46c72821b17be65b05a4e61b5846ba998a57d76db44ca4f8fa) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 115.145.171.128`. To see the logs of the worker, use `ray logs worker-7e48cd46c72821b17be65b05a4e61b5846ba998a57d76db44ca4f8fa*out -ip 115.145.171.128. Top 10 memory users:
PID	MEM(GB)	COMMAND
4942	3.48	/snap/pycharm-professional/408/jbr/bin/java -classpath /snap/pycharm-professional/408/lib/platform-l...
616443	2.14	/home/abdenour/miniconda3/envs/fl_with_flower/bin/python /home/abdenour/PycharmProjects/fl_with_flow...
619615	1.90	ray::ClientAppActor
619616	1.88	ray::ClientAppActor
619611	1.87	ray::ClientAppActor
619617	1.86	ray::ClientAppActor
619620	1.85	ray::ClientAppActor
619618	1.84	ray::ClientAppActor
619613	1.82	ray::ClientAppActor
619619	1.80	ray::ClientAppActor
Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.
[2024-08-13 23:53:47,127][flwr][ERROR] - Traceback (most recent call last):
  File "/home/abdenour/miniconda3/envs/fl_with_flower/lib/python3.11/site-packages/flwr/simulation/ray_transport/ray_client_proxy.py", line 94, in _submit_job
    out_mssg, updated_context = self.actor_pool.get_client_result(
                                ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/abdenour/miniconda3/envs/fl_with_flower/lib/python3.11/site-packages/flwr/simulation/ray_transport/ray_actor.py", line 398, in get_client_result
    return self._fetch_future_result(cid)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/abdenour/miniconda3/envs/fl_with_flower/lib/python3.11/site-packages/flwr/simulation/ray_transport/ray_actor.py", line 279, in _fetch_future_result
    res_cid, out_mssg, updated_context = ray.get(
                                         ^^^^^^^^
  File "/home/abdenour/miniconda3/envs/fl_with_flower/lib/python3.11/site-packages/ray/_private/auto_init_hook.py", line 21, in auto_init_wrapper
    return fn(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^
  File "/home/abdenour/miniconda3/envs/fl_with_flower/lib/python3.11/site-packages/ray/_private/client_mode_hook.py", line 103, in wrapper
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "/home/abdenour/miniconda3/envs/fl_with_flower/lib/python3.11/site-packages/ray/_private/worker.py", line 2667, in get
    values, debugger_breakpoint = worker.get_objects(object_refs, timeout=timeout)
                                  ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/abdenour/miniconda3/envs/fl_with_flower/lib/python3.11/site-packages/ray/_private/worker.py", line 866, in get_objects
    raise value
ray.exceptions.OutOfMemoryError: Task was killed due to the node running low on memory.
Memory on the node (IP: 115.145.171.128, ID: c687613ff34cdbf5097699e5c896055ef2dc44343c5ab44bd40309d5) where the task (actor ID: 85928d3f1a247800081c0e1101000000, name=ClientAppActor.__init__, pid=619614, memory used=1.73GB) was running was 29.50GB / 31.02GB (0.951049), which exceeds the memory usage threshold of 0.95. Ray killed this worker (ID: c7f644ac3cca20dffbfa1723daac4f87f04c7de03da25e7cad5fe33c) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 115.145.171.128`. To see the logs of the worker, use `ray logs worker-c7f644ac3cca20dffbfa1723daac4f87f04c7de03da25e7cad5fe33c*out -ip 115.145.171.128. Top 10 memory users:
PID	MEM(GB)	COMMAND
4942	3.49	/snap/pycharm-professional/408/jbr/bin/java -classpath /snap/pycharm-professional/408/lib/platform-l...
619616	1.80	ray::ClientAppActor.run
619615	1.78	ray::ClientAppActor
619617	1.77	ray::ClientAppActor
619611	1.73	ray::ClientAppActor
619618	1.73	ray::ClientAppActor
619614	1.73	ray::ClientAppActor
619612	1.72	ray::ClientAppActor
619619	1.70	ray::ClientAppActor
619620	1.68	ray::ClientAppActor.run
Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.

[2024-08-13 23:53:47,127][flwr][ERROR] - Task was killed due to the node running low on memory.
Memory on the node (IP: 115.145.171.128, ID: c687613ff34cdbf5097699e5c896055ef2dc44343c5ab44bd40309d5) where the task (actor ID: 85928d3f1a247800081c0e1101000000, name=ClientAppActor.__init__, pid=619614, memory used=1.73GB) was running was 29.50GB / 31.02GB (0.951049), which exceeds the memory usage threshold of 0.95. Ray killed this worker (ID: c7f644ac3cca20dffbfa1723daac4f87f04c7de03da25e7cad5fe33c) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 115.145.171.128`. To see the logs of the worker, use `ray logs worker-c7f644ac3cca20dffbfa1723daac4f87f04c7de03da25e7cad5fe33c*out -ip 115.145.171.128. Top 10 memory users:
PID	MEM(GB)	COMMAND
4942	3.49	/snap/pycharm-professional/408/jbr/bin/java -classpath /snap/pycharm-professional/408/lib/platform-l...
619616	1.80	ray::ClientAppActor.run
619615	1.78	ray::ClientAppActor
619617	1.77	ray::ClientAppActor
619611	1.73	ray::ClientAppActor
619618	1.73	ray::ClientAppActor
619614	1.73	ray::ClientAppActor
619612	1.72	ray::ClientAppActor
619619	1.70	ray::ClientAppActor
619620	1.68	ray::ClientAppActor.run
Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.
[2024-08-13 23:53:48,765][flwr][INFO] - aggregate_fit: received 8 results and 2 failures
[2024-08-13 23:53:51,609][flwr][INFO] - fit progress: (233, 17.108411371707916, {'accuracy': 0.7467948717948718}, 1352.8695583860099)
[2024-08-13 23:53:51,609][flwr][INFO] - configure_evaluate: strategy sampled 2 clients (out of 20)
[2024-08-13 23:53:51,831][flwr][INFO] - aggregate_evaluate: received 2 results and 0 failures
[2024-08-13 23:53:51,831][flwr][INFO] - 
[2024-08-13 23:53:51,831][flwr][INFO] - [ROUND 234]
[2024-08-13 23:53:51,831][flwr][INFO] - configure_fit: strategy sampled 10 clients (out of 20)
[2024-08-13 23:53:52,010][flwr][ERROR] - Traceback (most recent call last):
  File "/home/abdenour/miniconda3/envs/fl_with_flower/lib/python3.11/site-packages/flwr/simulation/ray_transport/ray_client_proxy.py", line 94, in _submit_job
    out_mssg, updated_context = self.actor_pool.get_client_result(
                                ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/abdenour/miniconda3/envs/fl_with_flower/lib/python3.11/site-packages/flwr/simulation/ray_transport/ray_actor.py", line 398, in get_client_result
    return self._fetch_future_result(cid)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/abdenour/miniconda3/envs/fl_with_flower/lib/python3.11/site-packages/flwr/simulation/ray_transport/ray_actor.py", line 279, in _fetch_future_result
    res_cid, out_mssg, updated_context = ray.get(
                                         ^^^^^^^^
  File "/home/abdenour/miniconda3/envs/fl_with_flower/lib/python3.11/site-packages/ray/_private/auto_init_hook.py", line 21, in auto_init_wrapper
    return fn(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^
  File "/home/abdenour/miniconda3/envs/fl_with_flower/lib/python3.11/site-packages/ray/_private/client_mode_hook.py", line 103, in wrapper
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "/home/abdenour/miniconda3/envs/fl_with_flower/lib/python3.11/site-packages/ray/_private/worker.py", line 2667, in get
    values, debugger_breakpoint = worker.get_objects(object_refs, timeout=timeout)
                                  ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/abdenour/miniconda3/envs/fl_with_flower/lib/python3.11/site-packages/ray/_private/worker.py", line 866, in get_objects
    raise value
ray.exceptions.OutOfMemoryError: Task was killed due to the node running low on memory.
Memory on the node (IP: 115.145.171.128, ID: c687613ff34cdbf5097699e5c896055ef2dc44343c5ab44bd40309d5) where the task (actor ID: 52365bb7997979e5a4c7ddbd01000000, name=ClientAppActor.__init__, pid=619618, memory used=1.84GB) was running was 29.49GB / 31.02GB (0.950707), which exceeds the memory usage threshold of 0.95. Ray killed this worker (ID: 7e48cd46c72821b17be65b05a4e61b5846ba998a57d76db44ca4f8fa) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 115.145.171.128`. To see the logs of the worker, use `ray logs worker-7e48cd46c72821b17be65b05a4e61b5846ba998a57d76db44ca4f8fa*out -ip 115.145.171.128. Top 10 memory users:
PID	MEM(GB)	COMMAND
4942	3.48	/snap/pycharm-professional/408/jbr/bin/java -classpath /snap/pycharm-professional/408/lib/platform-l...
616443	2.14	/home/abdenour/miniconda3/envs/fl_with_flower/bin/python /home/abdenour/PycharmProjects/fl_with_flow...
619615	1.90	ray::ClientAppActor
619616	1.88	ray::ClientAppActor
619611	1.87	ray::ClientAppActor
619617	1.86	ray::ClientAppActor
619620	1.85	ray::ClientAppActor
619618	1.84	ray::ClientAppActor
619613	1.82	ray::ClientAppActor
619619	1.80	ray::ClientAppActor
Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.

[2024-08-13 23:53:52,010][flwr][ERROR] - Task was killed due to the node running low on memory.
Memory on the node (IP: 115.145.171.128, ID: c687613ff34cdbf5097699e5c896055ef2dc44343c5ab44bd40309d5) where the task (actor ID: 52365bb7997979e5a4c7ddbd01000000, name=ClientAppActor.__init__, pid=619618, memory used=1.84GB) was running was 29.49GB / 31.02GB (0.950707), which exceeds the memory usage threshold of 0.95. Ray killed this worker (ID: 7e48cd46c72821b17be65b05a4e61b5846ba998a57d76db44ca4f8fa) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 115.145.171.128`. To see the logs of the worker, use `ray logs worker-7e48cd46c72821b17be65b05a4e61b5846ba998a57d76db44ca4f8fa*out -ip 115.145.171.128. Top 10 memory users:
PID	MEM(GB)	COMMAND
4942	3.48	/snap/pycharm-professional/408/jbr/bin/java -classpath /snap/pycharm-professional/408/lib/platform-l...
616443	2.14	/home/abdenour/miniconda3/envs/fl_with_flower/bin/python /home/abdenour/PycharmProjects/fl_with_flow...
619615	1.90	ray::ClientAppActor
619616	1.88	ray::ClientAppActor
619611	1.87	ray::ClientAppActor
619617	1.86	ray::ClientAppActor
619620	1.85	ray::ClientAppActor
619618	1.84	ray::ClientAppActor
619613	1.82	ray::ClientAppActor
619619	1.80	ray::ClientAppActor
Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.
[2024-08-13 23:53:52,011][flwr][ERROR] - Traceback (most recent call last):
  File "/home/abdenour/miniconda3/envs/fl_with_flower/lib/python3.11/site-packages/flwr/simulation/ray_transport/ray_client_proxy.py", line 94, in _submit_job
    out_mssg, updated_context = self.actor_pool.get_client_result(
                                ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/abdenour/miniconda3/envs/fl_with_flower/lib/python3.11/site-packages/flwr/simulation/ray_transport/ray_actor.py", line 398, in get_client_result
    return self._fetch_future_result(cid)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/abdenour/miniconda3/envs/fl_with_flower/lib/python3.11/site-packages/flwr/simulation/ray_transport/ray_actor.py", line 279, in _fetch_future_result
    res_cid, out_mssg, updated_context = ray.get(
                                         ^^^^^^^^
  File "/home/abdenour/miniconda3/envs/fl_with_flower/lib/python3.11/site-packages/ray/_private/auto_init_hook.py", line 21, in auto_init_wrapper
    return fn(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^
  File "/home/abdenour/miniconda3/envs/fl_with_flower/lib/python3.11/site-packages/ray/_private/client_mode_hook.py", line 103, in wrapper
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "/home/abdenour/miniconda3/envs/fl_with_flower/lib/python3.11/site-packages/ray/_private/worker.py", line 2667, in get
    values, debugger_breakpoint = worker.get_objects(object_refs, timeout=timeout)
                                  ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/abdenour/miniconda3/envs/fl_with_flower/lib/python3.11/site-packages/ray/_private/worker.py", line 866, in get_objects
    raise value
ray.exceptions.OutOfMemoryError: Task was killed due to the node running low on memory.
Memory on the node (IP: 115.145.171.128, ID: c687613ff34cdbf5097699e5c896055ef2dc44343c5ab44bd40309d5) where the task (actor ID: 85928d3f1a247800081c0e1101000000, name=ClientAppActor.__init__, pid=619614, memory used=1.73GB) was running was 29.50GB / 31.02GB (0.951049), which exceeds the memory usage threshold of 0.95. Ray killed this worker (ID: c7f644ac3cca20dffbfa1723daac4f87f04c7de03da25e7cad5fe33c) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 115.145.171.128`. To see the logs of the worker, use `ray logs worker-c7f644ac3cca20dffbfa1723daac4f87f04c7de03da25e7cad5fe33c*out -ip 115.145.171.128. Top 10 memory users:
PID	MEM(GB)	COMMAND
4942	3.49	/snap/pycharm-professional/408/jbr/bin/java -classpath /snap/pycharm-professional/408/lib/platform-l...
619616	1.80	ray::ClientAppActor.run
619615	1.78	ray::ClientAppActor
619617	1.77	ray::ClientAppActor
619611	1.73	ray::ClientAppActor
619618	1.73	ray::ClientAppActor
619614	1.73	ray::ClientAppActor
619612	1.72	ray::ClientAppActor
619619	1.70	ray::ClientAppActor
619620	1.68	ray::ClientAppActor.run
Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.

[2024-08-13 23:53:52,011][flwr][ERROR] - Task was killed due to the node running low on memory.
Memory on the node (IP: 115.145.171.128, ID: c687613ff34cdbf5097699e5c896055ef2dc44343c5ab44bd40309d5) where the task (actor ID: 85928d3f1a247800081c0e1101000000, name=ClientAppActor.__init__, pid=619614, memory used=1.73GB) was running was 29.50GB / 31.02GB (0.951049), which exceeds the memory usage threshold of 0.95. Ray killed this worker (ID: c7f644ac3cca20dffbfa1723daac4f87f04c7de03da25e7cad5fe33c) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 115.145.171.128`. To see the logs of the worker, use `ray logs worker-c7f644ac3cca20dffbfa1723daac4f87f04c7de03da25e7cad5fe33c*out -ip 115.145.171.128. Top 10 memory users:
PID	MEM(GB)	COMMAND
4942	3.49	/snap/pycharm-professional/408/jbr/bin/java -classpath /snap/pycharm-professional/408/lib/platform-l...
619616	1.80	ray::ClientAppActor.run
619615	1.78	ray::ClientAppActor
619617	1.77	ray::ClientAppActor
619611	1.73	ray::ClientAppActor
619618	1.73	ray::ClientAppActor
619614	1.73	ray::ClientAppActor
619612	1.72	ray::ClientAppActor
619619	1.70	ray::ClientAppActor
619620	1.68	ray::ClientAppActor.run
Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.
[2024-08-13 23:53:53,677][flwr][INFO] - aggregate_fit: received 8 results and 2 failures
[2024-08-13 23:53:56,495][flwr][INFO] - fit progress: (234, 12.686436474323273, {'accuracy': 0.7692307692307693}, 1357.7557385420077)
[2024-08-13 23:53:56,496][flwr][INFO] - configure_evaluate: strategy sampled 2 clients (out of 20)
[2024-08-13 23:53:56,729][flwr][INFO] - aggregate_evaluate: received 2 results and 0 failures
[2024-08-13 23:53:56,729][flwr][INFO] - 
[2024-08-13 23:53:56,729][flwr][INFO] - [ROUND 235]
[2024-08-13 23:53:56,729][flwr][INFO] - configure_fit: strategy sampled 10 clients (out of 20)
[2024-08-13 23:53:56,884][flwr][ERROR] - Traceback (most recent call last):
  File "/home/abdenour/miniconda3/envs/fl_with_flower/lib/python3.11/site-packages/flwr/simulation/ray_transport/ray_client_proxy.py", line 94, in _submit_job
    out_mssg, updated_context = self.actor_pool.get_client_result(
                                ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/abdenour/miniconda3/envs/fl_with_flower/lib/python3.11/site-packages/flwr/simulation/ray_transport/ray_actor.py", line 398, in get_client_result
    return self._fetch_future_result(cid)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/abdenour/miniconda3/envs/fl_with_flower/lib/python3.11/site-packages/flwr/simulation/ray_transport/ray_actor.py", line 279, in _fetch_future_result
    res_cid, out_mssg, updated_context = ray.get(
                                         ^^^^^^^^
  File "/home/abdenour/miniconda3/envs/fl_with_flower/lib/python3.11/site-packages/ray/_private/auto_init_hook.py", line 21, in auto_init_wrapper
    return fn(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^
  File "/home/abdenour/miniconda3/envs/fl_with_flower/lib/python3.11/site-packages/ray/_private/client_mode_hook.py", line 103, in wrapper
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "/home/abdenour/miniconda3/envs/fl_with_flower/lib/python3.11/site-packages/ray/_private/worker.py", line 2667, in get
    values, debugger_breakpoint = worker.get_objects(object_refs, timeout=timeout)
                                  ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/abdenour/miniconda3/envs/fl_with_flower/lib/python3.11/site-packages/ray/_private/worker.py", line 866, in get_objects
    raise value
ray.exceptions.OutOfMemoryError: Task was killed due to the node running low on memory.
Memory on the node (IP: 115.145.171.128, ID: c687613ff34cdbf5097699e5c896055ef2dc44343c5ab44bd40309d5) where the task (actor ID: 52365bb7997979e5a4c7ddbd01000000, name=ClientAppActor.__init__, pid=619618, memory used=1.84GB) was running was 29.49GB / 31.02GB (0.950707), which exceeds the memory usage threshold of 0.95. Ray killed this worker (ID: 7e48cd46c72821b17be65b05a4e61b5846ba998a57d76db44ca4f8fa) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 115.145.171.128`. To see the logs of the worker, use `ray logs worker-7e48cd46c72821b17be65b05a4e61b5846ba998a57d76db44ca4f8fa*out -ip 115.145.171.128. Top 10 memory users:
PID	MEM(GB)	COMMAND
4942	3.48	/snap/pycharm-professional/408/jbr/bin/java -classpath /snap/pycharm-professional/408/lib/platform-l...
616443	2.14	/home/abdenour/miniconda3/envs/fl_with_flower/bin/python /home/abdenour/PycharmProjects/fl_with_flow...
619615	1.90	ray::ClientAppActor
619616	1.88	ray::ClientAppActor
619611	1.87	ray::ClientAppActor
619617	1.86	ray::ClientAppActor
619620	1.85	ray::ClientAppActor
619618	1.84	ray::ClientAppActor
619613	1.82	ray::ClientAppActor
619619	1.80	ray::ClientAppActor
Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.

[2024-08-13 23:53:56,884][flwr][ERROR] - Task was killed due to the node running low on memory.
Memory on the node (IP: 115.145.171.128, ID: c687613ff34cdbf5097699e5c896055ef2dc44343c5ab44bd40309d5) where the task (actor ID: 52365bb7997979e5a4c7ddbd01000000, name=ClientAppActor.__init__, pid=619618, memory used=1.84GB) was running was 29.49GB / 31.02GB (0.950707), which exceeds the memory usage threshold of 0.95. Ray killed this worker (ID: 7e48cd46c72821b17be65b05a4e61b5846ba998a57d76db44ca4f8fa) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 115.145.171.128`. To see the logs of the worker, use `ray logs worker-7e48cd46c72821b17be65b05a4e61b5846ba998a57d76db44ca4f8fa*out -ip 115.145.171.128. Top 10 memory users:
PID	MEM(GB)	COMMAND
4942	3.48	/snap/pycharm-professional/408/jbr/bin/java -classpath /snap/pycharm-professional/408/lib/platform-l...
616443	2.14	/home/abdenour/miniconda3/envs/fl_with_flower/bin/python /home/abdenour/PycharmProjects/fl_with_flow...
619615	1.90	ray::ClientAppActor
619616	1.88	ray::ClientAppActor
619611	1.87	ray::ClientAppActor
619617	1.86	ray::ClientAppActor
619620	1.85	ray::ClientAppActor
619618	1.84	ray::ClientAppActor
619613	1.82	ray::ClientAppActor
619619	1.80	ray::ClientAppActor
Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.
[2024-08-13 23:53:56,885][flwr][ERROR] - Traceback (most recent call last):
  File "/home/abdenour/miniconda3/envs/fl_with_flower/lib/python3.11/site-packages/flwr/simulation/ray_transport/ray_client_proxy.py", line 94, in _submit_job
    out_mssg, updated_context = self.actor_pool.get_client_result(
                                ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/abdenour/miniconda3/envs/fl_with_flower/lib/python3.11/site-packages/flwr/simulation/ray_transport/ray_actor.py", line 398, in get_client_result
    return self._fetch_future_result(cid)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/abdenour/miniconda3/envs/fl_with_flower/lib/python3.11/site-packages/flwr/simulation/ray_transport/ray_actor.py", line 279, in _fetch_future_result
    res_cid, out_mssg, updated_context = ray.get(
                                         ^^^^^^^^
  File "/home/abdenour/miniconda3/envs/fl_with_flower/lib/python3.11/site-packages/ray/_private/auto_init_hook.py", line 21, in auto_init_wrapper
    return fn(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^
  File "/home/abdenour/miniconda3/envs/fl_with_flower/lib/python3.11/site-packages/ray/_private/client_mode_hook.py", line 103, in wrapper
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "/home/abdenour/miniconda3/envs/fl_with_flower/lib/python3.11/site-packages/ray/_private/worker.py", line 2667, in get
    values, debugger_breakpoint = worker.get_objects(object_refs, timeout=timeout)
                                  ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/abdenour/miniconda3/envs/fl_with_flower/lib/python3.11/site-packages/ray/_private/worker.py", line 866, in get_objects
    raise value
ray.exceptions.OutOfMemoryError: Task was killed due to the node running low on memory.
Memory on the node (IP: 115.145.171.128, ID: c687613ff34cdbf5097699e5c896055ef2dc44343c5ab44bd40309d5) where the task (actor ID: 85928d3f1a247800081c0e1101000000, name=ClientAppActor.__init__, pid=619614, memory used=1.73GB) was running was 29.50GB / 31.02GB (0.951049), which exceeds the memory usage threshold of 0.95. Ray killed this worker (ID: c7f644ac3cca20dffbfa1723daac4f87f04c7de03da25e7cad5fe33c) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 115.145.171.128`. To see the logs of the worker, use `ray logs worker-c7f644ac3cca20dffbfa1723daac4f87f04c7de03da25e7cad5fe33c*out -ip 115.145.171.128. Top 10 memory users:
PID	MEM(GB)	COMMAND
4942	3.49	/snap/pycharm-professional/408/jbr/bin/java -classpath /snap/pycharm-professional/408/lib/platform-l...
619616	1.80	ray::ClientAppActor.run
619615	1.78	ray::ClientAppActor
619617	1.77	ray::ClientAppActor
619611	1.73	ray::ClientAppActor
619618	1.73	ray::ClientAppActor
619614	1.73	ray::ClientAppActor
619612	1.72	ray::ClientAppActor
619619	1.70	ray::ClientAppActor
619620	1.68	ray::ClientAppActor.run
Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.

[2024-08-13 23:53:56,885][flwr][ERROR] - Task was killed due to the node running low on memory.
Memory on the node (IP: 115.145.171.128, ID: c687613ff34cdbf5097699e5c896055ef2dc44343c5ab44bd40309d5) where the task (actor ID: 85928d3f1a247800081c0e1101000000, name=ClientAppActor.__init__, pid=619614, memory used=1.73GB) was running was 29.50GB / 31.02GB (0.951049), which exceeds the memory usage threshold of 0.95. Ray killed this worker (ID: c7f644ac3cca20dffbfa1723daac4f87f04c7de03da25e7cad5fe33c) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 115.145.171.128`. To see the logs of the worker, use `ray logs worker-c7f644ac3cca20dffbfa1723daac4f87f04c7de03da25e7cad5fe33c*out -ip 115.145.171.128. Top 10 memory users:
PID	MEM(GB)	COMMAND
4942	3.49	/snap/pycharm-professional/408/jbr/bin/java -classpath /snap/pycharm-professional/408/lib/platform-l...
619616	1.80	ray::ClientAppActor.run
619615	1.78	ray::ClientAppActor
619617	1.77	ray::ClientAppActor
619611	1.73	ray::ClientAppActor
619618	1.73	ray::ClientAppActor
619614	1.73	ray::ClientAppActor
619612	1.72	ray::ClientAppActor
619619	1.70	ray::ClientAppActor
619620	1.68	ray::ClientAppActor.run
Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.
[2024-08-13 23:53:58,614][flwr][INFO] - aggregate_fit: received 8 results and 2 failures
[2024-08-13 23:54:01,700][flwr][INFO] - fit progress: (235, 15.146178305149078, {'accuracy': 0.75}, 1362.9607461660053)
[2024-08-13 23:54:01,701][flwr][INFO] - configure_evaluate: strategy sampled 2 clients (out of 20)
[2024-08-13 23:54:01,882][flwr][INFO] - aggregate_evaluate: received 2 results and 0 failures
[2024-08-13 23:54:01,882][flwr][INFO] - 
[2024-08-13 23:54:01,882][flwr][INFO] - [ROUND 236]
[2024-08-13 23:54:01,882][flwr][INFO] - configure_fit: strategy sampled 10 clients (out of 20)
[2024-08-13 23:54:02,051][flwr][ERROR] - Traceback (most recent call last):
  File "/home/abdenour/miniconda3/envs/fl_with_flower/lib/python3.11/site-packages/flwr/simulation/ray_transport/ray_client_proxy.py", line 94, in _submit_job
    out_mssg, updated_context = self.actor_pool.get_client_result(
                                ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/abdenour/miniconda3/envs/fl_with_flower/lib/python3.11/site-packages/flwr/simulation/ray_transport/ray_actor.py", line 398, in get_client_result
    return self._fetch_future_result(cid)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/abdenour/miniconda3/envs/fl_with_flower/lib/python3.11/site-packages/flwr/simulation/ray_transport/ray_actor.py", line 279, in _fetch_future_result
    res_cid, out_mssg, updated_context = ray.get(
                                         ^^^^^^^^
  File "/home/abdenour/miniconda3/envs/fl_with_flower/lib/python3.11/site-packages/ray/_private/auto_init_hook.py", line 21, in auto_init_wrapper
    return fn(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^
  File "/home/abdenour/miniconda3/envs/fl_with_flower/lib/python3.11/site-packages/ray/_private/client_mode_hook.py", line 103, in wrapper
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "/home/abdenour/miniconda3/envs/fl_with_flower/lib/python3.11/site-packages/ray/_private/worker.py", line 2667, in get
    values, debugger_breakpoint = worker.get_objects(object_refs, timeout=timeout)
                                  ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/abdenour/miniconda3/envs/fl_with_flower/lib/python3.11/site-packages/ray/_private/worker.py", line 866, in get_objects
    raise value
ray.exceptions.OutOfMemoryError: Task was killed due to the node running low on memory.
Memory on the node (IP: 115.145.171.128, ID: c687613ff34cdbf5097699e5c896055ef2dc44343c5ab44bd40309d5) where the task (actor ID: 52365bb7997979e5a4c7ddbd01000000, name=ClientAppActor.__init__, pid=619618, memory used=1.84GB) was running was 29.49GB / 31.02GB (0.950707), which exceeds the memory usage threshold of 0.95. Ray killed this worker (ID: 7e48cd46c72821b17be65b05a4e61b5846ba998a57d76db44ca4f8fa) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 115.145.171.128`. To see the logs of the worker, use `ray logs worker-7e48cd46c72821b17be65b05a4e61b5846ba998a57d76db44ca4f8fa*out -ip 115.145.171.128. Top 10 memory users:
PID	MEM(GB)	COMMAND
4942	3.48	/snap/pycharm-professional/408/jbr/bin/java -classpath /snap/pycharm-professional/408/lib/platform-l...
616443	2.14	/home/abdenour/miniconda3/envs/fl_with_flower/bin/python /home/abdenour/PycharmProjects/fl_with_flow...
619615	1.90	ray::ClientAppActor
619616	1.88	ray::ClientAppActor
619611	1.87	ray::ClientAppActor
619617	1.86	ray::ClientAppActor
619620	1.85	ray::ClientAppActor
619618	1.84	ray::ClientAppActor
619613	1.82	ray::ClientAppActor
619619	1.80	ray::ClientAppActor
Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.

[2024-08-13 23:54:02,052][flwr][ERROR] - Traceback (most recent call last):
  File "/home/abdenour/miniconda3/envs/fl_with_flower/lib/python3.11/site-packages/flwr/simulation/ray_transport/ray_client_proxy.py", line 94, in _submit_job
    out_mssg, updated_context = self.actor_pool.get_client_result(
                                ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/abdenour/miniconda3/envs/fl_with_flower/lib/python3.11/site-packages/flwr/simulation/ray_transport/ray_actor.py", line 398, in get_client_result
    return self._fetch_future_result(cid)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/abdenour/miniconda3/envs/fl_with_flower/lib/python3.11/site-packages/flwr/simulation/ray_transport/ray_actor.py", line 279, in _fetch_future_result
    res_cid, out_mssg, updated_context = ray.get(
                                         ^^^^^^^^
  File "/home/abdenour/miniconda3/envs/fl_with_flower/lib/python3.11/site-packages/ray/_private/auto_init_hook.py", line 21, in auto_init_wrapper
    return fn(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^
  File "/home/abdenour/miniconda3/envs/fl_with_flower/lib/python3.11/site-packages/ray/_private/client_mode_hook.py", line 103, in wrapper
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "/home/abdenour/miniconda3/envs/fl_with_flower/lib/python3.11/site-packages/ray/_private/worker.py", line 2667, in get
    values, debugger_breakpoint = worker.get_objects(object_refs, timeout=timeout)
                                  ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/abdenour/miniconda3/envs/fl_with_flower/lib/python3.11/site-packages/ray/_private/worker.py", line 866, in get_objects
    raise value
ray.exceptions.OutOfMemoryError: Task was killed due to the node running low on memory.
Memory on the node (IP: 115.145.171.128, ID: c687613ff34cdbf5097699e5c896055ef2dc44343c5ab44bd40309d5) where the task (actor ID: 85928d3f1a247800081c0e1101000000, name=ClientAppActor.__init__, pid=619614, memory used=1.73GB) was running was 29.50GB / 31.02GB (0.951049), which exceeds the memory usage threshold of 0.95. Ray killed this worker (ID: c7f644ac3cca20dffbfa1723daac4f87f04c7de03da25e7cad5fe33c) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 115.145.171.128`. To see the logs of the worker, use `ray logs worker-c7f644ac3cca20dffbfa1723daac4f87f04c7de03da25e7cad5fe33c*out -ip 115.145.171.128. Top 10 memory users:
PID	MEM(GB)	COMMAND
4942	3.49	/snap/pycharm-professional/408/jbr/bin/java -classpath /snap/pycharm-professional/408/lib/platform-l...
619616	1.80	ray::ClientAppActor.run
619615	1.78	ray::ClientAppActor
619617	1.77	ray::ClientAppActor
619611	1.73	ray::ClientAppActor
619618	1.73	ray::ClientAppActor
619614	1.73	ray::ClientAppActor
619612	1.72	ray::ClientAppActor
619619	1.70	ray::ClientAppActor
619620	1.68	ray::ClientAppActor.run
Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.

[2024-08-13 23:54:02,052][flwr][ERROR] - Task was killed due to the node running low on memory.
Memory on the node (IP: 115.145.171.128, ID: c687613ff34cdbf5097699e5c896055ef2dc44343c5ab44bd40309d5) where the task (actor ID: 52365bb7997979e5a4c7ddbd01000000, name=ClientAppActor.__init__, pid=619618, memory used=1.84GB) was running was 29.49GB / 31.02GB (0.950707), which exceeds the memory usage threshold of 0.95. Ray killed this worker (ID: 7e48cd46c72821b17be65b05a4e61b5846ba998a57d76db44ca4f8fa) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 115.145.171.128`. To see the logs of the worker, use `ray logs worker-7e48cd46c72821b17be65b05a4e61b5846ba998a57d76db44ca4f8fa*out -ip 115.145.171.128. Top 10 memory users:
PID	MEM(GB)	COMMAND
4942	3.48	/snap/pycharm-professional/408/jbr/bin/java -classpath /snap/pycharm-professional/408/lib/platform-l...
616443	2.14	/home/abdenour/miniconda3/envs/fl_with_flower/bin/python /home/abdenour/PycharmProjects/fl_with_flow...
619615	1.90	ray::ClientAppActor
619616	1.88	ray::ClientAppActor
619611	1.87	ray::ClientAppActor
619617	1.86	ray::ClientAppActor
619620	1.85	ray::ClientAppActor
619618	1.84	ray::ClientAppActor
619613	1.82	ray::ClientAppActor
619619	1.80	ray::ClientAppActor
Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.
[2024-08-13 23:54:02,052][flwr][ERROR] - Task was killed due to the node running low on memory.
Memory on the node (IP: 115.145.171.128, ID: c687613ff34cdbf5097699e5c896055ef2dc44343c5ab44bd40309d5) where the task (actor ID: 85928d3f1a247800081c0e1101000000, name=ClientAppActor.__init__, pid=619614, memory used=1.73GB) was running was 29.50GB / 31.02GB (0.951049), which exceeds the memory usage threshold of 0.95. Ray killed this worker (ID: c7f644ac3cca20dffbfa1723daac4f87f04c7de03da25e7cad5fe33c) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 115.145.171.128`. To see the logs of the worker, use `ray logs worker-c7f644ac3cca20dffbfa1723daac4f87f04c7de03da25e7cad5fe33c*out -ip 115.145.171.128. Top 10 memory users:
PID	MEM(GB)	COMMAND
4942	3.49	/snap/pycharm-professional/408/jbr/bin/java -classpath /snap/pycharm-professional/408/lib/platform-l...
619616	1.80	ray::ClientAppActor.run
619615	1.78	ray::ClientAppActor
619617	1.77	ray::ClientAppActor
619611	1.73	ray::ClientAppActor
619618	1.73	ray::ClientAppActor
619614	1.73	ray::ClientAppActor
619612	1.72	ray::ClientAppActor
619619	1.70	ray::ClientAppActor
619620	1.68	ray::ClientAppActor.run
Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.
[2024-08-13 23:54:03,633][flwr][INFO] - aggregate_fit: received 8 results and 2 failures
[2024-08-13 23:54:07,004][flwr][INFO] - fit progress: (236, 20.82251363992691, {'accuracy': 0.7243589743589743}, 1368.2643793470052)
[2024-08-13 23:54:07,004][flwr][INFO] - configure_evaluate: strategy sampled 2 clients (out of 20)
[2024-08-13 23:54:07,200][flwr][INFO] - aggregate_evaluate: received 2 results and 0 failures
[2024-08-13 23:54:07,200][flwr][INFO] - 
[2024-08-13 23:54:07,200][flwr][INFO] - [ROUND 237]
[2024-08-13 23:54:07,200][flwr][INFO] - configure_fit: strategy sampled 10 clients (out of 20)
[2024-08-13 23:54:07,346][flwr][ERROR] - Traceback (most recent call last):
  File "/home/abdenour/miniconda3/envs/fl_with_flower/lib/python3.11/site-packages/flwr/simulation/ray_transport/ray_client_proxy.py", line 94, in _submit_job
    out_mssg, updated_context = self.actor_pool.get_client_result(
                                ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/abdenour/miniconda3/envs/fl_with_flower/lib/python3.11/site-packages/flwr/simulation/ray_transport/ray_actor.py", line 398, in get_client_result
    return self._fetch_future_result(cid)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/abdenour/miniconda3/envs/fl_with_flower/lib/python3.11/site-packages/flwr/simulation/ray_transport/ray_actor.py", line 279, in _fetch_future_result
    res_cid, out_mssg, updated_context = ray.get(
                                         ^^^^^^^^
  File "/home/abdenour/miniconda3/envs/fl_with_flower/lib/python3.11/site-packages/ray/_private/auto_init_hook.py", line 21, in auto_init_wrapper
    return fn(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^
  File "/home/abdenour/miniconda3/envs/fl_with_flower/lib/python3.11/site-packages/ray/_private/client_mode_hook.py", line 103, in wrapper
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "/home/abdenour/miniconda3/envs/fl_with_flower/lib/python3.11/site-packages/ray/_private/worker.py", line 2667, in get
    values, debugger_breakpoint = worker.get_objects(object_refs, timeout=timeout)
                                  ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/abdenour/miniconda3/envs/fl_with_flower/lib/python3.11/site-packages/ray/_private/worker.py", line 866, in get_objects
    raise value
ray.exceptions.OutOfMemoryError: Task was killed due to the node running low on memory.
Memory on the node (IP: 115.145.171.128, ID: c687613ff34cdbf5097699e5c896055ef2dc44343c5ab44bd40309d5) where the task (actor ID: 52365bb7997979e5a4c7ddbd01000000, name=ClientAppActor.__init__, pid=619618, memory used=1.84GB) was running was 29.49GB / 31.02GB (0.950707), which exceeds the memory usage threshold of 0.95. Ray killed this worker (ID: 7e48cd46c72821b17be65b05a4e61b5846ba998a57d76db44ca4f8fa) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 115.145.171.128`. To see the logs of the worker, use `ray logs worker-7e48cd46c72821b17be65b05a4e61b5846ba998a57d76db44ca4f8fa*out -ip 115.145.171.128. Top 10 memory users:
PID	MEM(GB)	COMMAND
4942	3.48	/snap/pycharm-professional/408/jbr/bin/java -classpath /snap/pycharm-professional/408/lib/platform-l...
616443	2.14	/home/abdenour/miniconda3/envs/fl_with_flower/bin/python /home/abdenour/PycharmProjects/fl_with_flow...
619615	1.90	ray::ClientAppActor
619616	1.88	ray::ClientAppActor
619611	1.87	ray::ClientAppActor
619617	1.86	ray::ClientAppActor
619620	1.85	ray::ClientAppActor
619618	1.84	ray::ClientAppActor
619613	1.82	ray::ClientAppActor
619619	1.80	ray::ClientAppActor
Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.

[2024-08-13 23:54:07,346][flwr][ERROR] - Task was killed due to the node running low on memory.
Memory on the node (IP: 115.145.171.128, ID: c687613ff34cdbf5097699e5c896055ef2dc44343c5ab44bd40309d5) where the task (actor ID: 52365bb7997979e5a4c7ddbd01000000, name=ClientAppActor.__init__, pid=619618, memory used=1.84GB) was running was 29.49GB / 31.02GB (0.950707), which exceeds the memory usage threshold of 0.95. Ray killed this worker (ID: 7e48cd46c72821b17be65b05a4e61b5846ba998a57d76db44ca4f8fa) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 115.145.171.128`. To see the logs of the worker, use `ray logs worker-7e48cd46c72821b17be65b05a4e61b5846ba998a57d76db44ca4f8fa*out -ip 115.145.171.128. Top 10 memory users:
PID	MEM(GB)	COMMAND
4942	3.48	/snap/pycharm-professional/408/jbr/bin/java -classpath /snap/pycharm-professional/408/lib/platform-l...
616443	2.14	/home/abdenour/miniconda3/envs/fl_with_flower/bin/python /home/abdenour/PycharmProjects/fl_with_flow...
619615	1.90	ray::ClientAppActor
619616	1.88	ray::ClientAppActor
619611	1.87	ray::ClientAppActor
619617	1.86	ray::ClientAppActor
619620	1.85	ray::ClientAppActor
619618	1.84	ray::ClientAppActor
619613	1.82	ray::ClientAppActor
619619	1.80	ray::ClientAppActor
Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.
[2024-08-13 23:54:07,347][flwr][ERROR] - Traceback (most recent call last):
  File "/home/abdenour/miniconda3/envs/fl_with_flower/lib/python3.11/site-packages/flwr/simulation/ray_transport/ray_client_proxy.py", line 94, in _submit_job
    out_mssg, updated_context = self.actor_pool.get_client_result(
                                ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/abdenour/miniconda3/envs/fl_with_flower/lib/python3.11/site-packages/flwr/simulation/ray_transport/ray_actor.py", line 398, in get_client_result
    return self._fetch_future_result(cid)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/abdenour/miniconda3/envs/fl_with_flower/lib/python3.11/site-packages/flwr/simulation/ray_transport/ray_actor.py", line 279, in _fetch_future_result
    res_cid, out_mssg, updated_context = ray.get(
                                         ^^^^^^^^
  File "/home/abdenour/miniconda3/envs/fl_with_flower/lib/python3.11/site-packages/ray/_private/auto_init_hook.py", line 21, in auto_init_wrapper
    return fn(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^
  File "/home/abdenour/miniconda3/envs/fl_with_flower/lib/python3.11/site-packages/ray/_private/client_mode_hook.py", line 103, in wrapper
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "/home/abdenour/miniconda3/envs/fl_with_flower/lib/python3.11/site-packages/ray/_private/worker.py", line 2667, in get
    values, debugger_breakpoint = worker.get_objects(object_refs, timeout=timeout)
                                  ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/abdenour/miniconda3/envs/fl_with_flower/lib/python3.11/site-packages/ray/_private/worker.py", line 866, in get_objects
    raise value
ray.exceptions.OutOfMemoryError: Task was killed due to the node running low on memory.
Memory on the node (IP: 115.145.171.128, ID: c687613ff34cdbf5097699e5c896055ef2dc44343c5ab44bd40309d5) where the task (actor ID: 85928d3f1a247800081c0e1101000000, name=ClientAppActor.__init__, pid=619614, memory used=1.73GB) was running was 29.50GB / 31.02GB (0.951049), which exceeds the memory usage threshold of 0.95. Ray killed this worker (ID: c7f644ac3cca20dffbfa1723daac4f87f04c7de03da25e7cad5fe33c) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 115.145.171.128`. To see the logs of the worker, use `ray logs worker-c7f644ac3cca20dffbfa1723daac4f87f04c7de03da25e7cad5fe33c*out -ip 115.145.171.128. Top 10 memory users:
PID	MEM(GB)	COMMAND
4942	3.49	/snap/pycharm-professional/408/jbr/bin/java -classpath /snap/pycharm-professional/408/lib/platform-l...
619616	1.80	ray::ClientAppActor.run
619615	1.78	ray::ClientAppActor
619617	1.77	ray::ClientAppActor
619611	1.73	ray::ClientAppActor
619618	1.73	ray::ClientAppActor
619614	1.73	ray::ClientAppActor
619612	1.72	ray::ClientAppActor
619619	1.70	ray::ClientAppActor
619620	1.68	ray::ClientAppActor.run
Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.

[2024-08-13 23:54:07,347][flwr][ERROR] - Task was killed due to the node running low on memory.
Memory on the node (IP: 115.145.171.128, ID: c687613ff34cdbf5097699e5c896055ef2dc44343c5ab44bd40309d5) where the task (actor ID: 85928d3f1a247800081c0e1101000000, name=ClientAppActor.__init__, pid=619614, memory used=1.73GB) was running was 29.50GB / 31.02GB (0.951049), which exceeds the memory usage threshold of 0.95. Ray killed this worker (ID: c7f644ac3cca20dffbfa1723daac4f87f04c7de03da25e7cad5fe33c) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 115.145.171.128`. To see the logs of the worker, use `ray logs worker-c7f644ac3cca20dffbfa1723daac4f87f04c7de03da25e7cad5fe33c*out -ip 115.145.171.128. Top 10 memory users:
PID	MEM(GB)	COMMAND
4942	3.49	/snap/pycharm-professional/408/jbr/bin/java -classpath /snap/pycharm-professional/408/lib/platform-l...
619616	1.80	ray::ClientAppActor.run
619615	1.78	ray::ClientAppActor
619617	1.77	ray::ClientAppActor
619611	1.73	ray::ClientAppActor
619618	1.73	ray::ClientAppActor
619614	1.73	ray::ClientAppActor
619612	1.72	ray::ClientAppActor
619619	1.70	ray::ClientAppActor
619620	1.68	ray::ClientAppActor.run
Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.
[2024-08-13 23:54:09,007][flwr][INFO] - aggregate_fit: received 8 results and 2 failures
[2024-08-13 23:54:11,941][flwr][INFO] - fit progress: (237, 22.517114996910095, {'accuracy': 0.7115384615384616}, 1373.201019835018)
[2024-08-13 23:54:11,941][flwr][INFO] - configure_evaluate: strategy sampled 2 clients (out of 20)
[2024-08-13 23:54:12,156][flwr][INFO] - aggregate_evaluate: received 2 results and 0 failures
[2024-08-13 23:54:12,156][flwr][INFO] - 
[2024-08-13 23:54:12,156][flwr][INFO] - [ROUND 238]
[2024-08-13 23:54:12,156][flwr][INFO] - configure_fit: strategy sampled 10 clients (out of 20)
[2024-08-13 23:54:12,361][flwr][ERROR] - Traceback (most recent call last):
  File "/home/abdenour/miniconda3/envs/fl_with_flower/lib/python3.11/site-packages/flwr/simulation/ray_transport/ray_client_proxy.py", line 94, in _submit_job
    out_mssg, updated_context = self.actor_pool.get_client_result(
                                ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/abdenour/miniconda3/envs/fl_with_flower/lib/python3.11/site-packages/flwr/simulation/ray_transport/ray_actor.py", line 398, in get_client_result
    return self._fetch_future_result(cid)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/abdenour/miniconda3/envs/fl_with_flower/lib/python3.11/site-packages/flwr/simulation/ray_transport/ray_actor.py", line 279, in _fetch_future_result
    res_cid, out_mssg, updated_context = ray.get(
                                         ^^^^^^^^
  File "/home/abdenour/miniconda3/envs/fl_with_flower/lib/python3.11/site-packages/ray/_private/auto_init_hook.py", line 21, in auto_init_wrapper
    return fn(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^
  File "/home/abdenour/miniconda3/envs/fl_with_flower/lib/python3.11/site-packages/ray/_private/client_mode_hook.py", line 103, in wrapper
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "/home/abdenour/miniconda3/envs/fl_with_flower/lib/python3.11/site-packages/ray/_private/worker.py", line 2667, in get
    values, debugger_breakpoint = worker.get_objects(object_refs, timeout=timeout)
                                  ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/abdenour/miniconda3/envs/fl_with_flower/lib/python3.11/site-packages/ray/_private/worker.py", line 866, in get_objects
    raise value
ray.exceptions.OutOfMemoryError: Task was killed due to the node running low on memory.
Memory on the node (IP: 115.145.171.128, ID: c687613ff34cdbf5097699e5c896055ef2dc44343c5ab44bd40309d5) where the task (actor ID: 85928d3f1a247800081c0e1101000000, name=ClientAppActor.__init__, pid=619614, memory used=1.73GB) was running was 29.50GB / 31.02GB (0.951049), which exceeds the memory usage threshold of 0.95. Ray killed this worker (ID: c7f644ac3cca20dffbfa1723daac4f87f04c7de03da25e7cad5fe33c) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 115.145.171.128`. To see the logs of the worker, use `ray logs worker-c7f644ac3cca20dffbfa1723daac4f87f04c7de03da25e7cad5fe33c*out -ip 115.145.171.128. Top 10 memory users:
PID	MEM(GB)	COMMAND
4942	3.49	/snap/pycharm-professional/408/jbr/bin/java -classpath /snap/pycharm-professional/408/lib/platform-l...
619616	1.80	ray::ClientAppActor.run
619615	1.78	ray::ClientAppActor
619617	1.77	ray::ClientAppActor
619611	1.73	ray::ClientAppActor
619618	1.73	ray::ClientAppActor
619614	1.73	ray::ClientAppActor
619612	1.72	ray::ClientAppActor
619619	1.70	ray::ClientAppActor
619620	1.68	ray::ClientAppActor.run
Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.

[2024-08-13 23:54:12,362][flwr][ERROR] - Task was killed due to the node running low on memory.
Memory on the node (IP: 115.145.171.128, ID: c687613ff34cdbf5097699e5c896055ef2dc44343c5ab44bd40309d5) where the task (actor ID: 85928d3f1a247800081c0e1101000000, name=ClientAppActor.__init__, pid=619614, memory used=1.73GB) was running was 29.50GB / 31.02GB (0.951049), which exceeds the memory usage threshold of 0.95. Ray killed this worker (ID: c7f644ac3cca20dffbfa1723daac4f87f04c7de03da25e7cad5fe33c) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 115.145.171.128`. To see the logs of the worker, use `ray logs worker-c7f644ac3cca20dffbfa1723daac4f87f04c7de03da25e7cad5fe33c*out -ip 115.145.171.128. Top 10 memory users:
PID	MEM(GB)	COMMAND
4942	3.49	/snap/pycharm-professional/408/jbr/bin/java -classpath /snap/pycharm-professional/408/lib/platform-l...
619616	1.80	ray::ClientAppActor.run
619615	1.78	ray::ClientAppActor
619617	1.77	ray::ClientAppActor
619611	1.73	ray::ClientAppActor
619618	1.73	ray::ClientAppActor
619614	1.73	ray::ClientAppActor
619612	1.72	ray::ClientAppActor
619619	1.70	ray::ClientAppActor
619620	1.68	ray::ClientAppActor.run
Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.
[2024-08-13 23:54:12,367][flwr][ERROR] - Traceback (most recent call last):
  File "/home/abdenour/miniconda3/envs/fl_with_flower/lib/python3.11/site-packages/flwr/simulation/ray_transport/ray_client_proxy.py", line 94, in _submit_job
    out_mssg, updated_context = self.actor_pool.get_client_result(
                                ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/abdenour/miniconda3/envs/fl_with_flower/lib/python3.11/site-packages/flwr/simulation/ray_transport/ray_actor.py", line 398, in get_client_result
    return self._fetch_future_result(cid)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/abdenour/miniconda3/envs/fl_with_flower/lib/python3.11/site-packages/flwr/simulation/ray_transport/ray_actor.py", line 279, in _fetch_future_result
    res_cid, out_mssg, updated_context = ray.get(
                                         ^^^^^^^^
  File "/home/abdenour/miniconda3/envs/fl_with_flower/lib/python3.11/site-packages/ray/_private/auto_init_hook.py", line 21, in auto_init_wrapper
    return fn(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^
  File "/home/abdenour/miniconda3/envs/fl_with_flower/lib/python3.11/site-packages/ray/_private/client_mode_hook.py", line 103, in wrapper
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "/home/abdenour/miniconda3/envs/fl_with_flower/lib/python3.11/site-packages/ray/_private/worker.py", line 2667, in get
    values, debugger_breakpoint = worker.get_objects(object_refs, timeout=timeout)
                                  ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/abdenour/miniconda3/envs/fl_with_flower/lib/python3.11/site-packages/ray/_private/worker.py", line 866, in get_objects
    raise value
ray.exceptions.OutOfMemoryError: Task was killed due to the node running low on memory.
Memory on the node (IP: 115.145.171.128, ID: c687613ff34cdbf5097699e5c896055ef2dc44343c5ab44bd40309d5) where the task (actor ID: 85928d3f1a247800081c0e1101000000, name=ClientAppActor.__init__, pid=619614, memory used=1.73GB) was running was 29.50GB / 31.02GB (0.951049), which exceeds the memory usage threshold of 0.95. Ray killed this worker (ID: c7f644ac3cca20dffbfa1723daac4f87f04c7de03da25e7cad5fe33c) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 115.145.171.128`. To see the logs of the worker, use `ray logs worker-c7f644ac3cca20dffbfa1723daac4f87f04c7de03da25e7cad5fe33c*out -ip 115.145.171.128. Top 10 memory users:
PID	MEM(GB)	COMMAND
4942	3.49	/snap/pycharm-professional/408/jbr/bin/java -classpath /snap/pycharm-professional/408/lib/platform-l...
619616	1.80	ray::ClientAppActor.run
619615	1.78	ray::ClientAppActor
619617	1.77	ray::ClientAppActor
619611	1.73	ray::ClientAppActor
619618	1.73	ray::ClientAppActor
619614	1.73	ray::ClientAppActor
619612	1.72	ray::ClientAppActor
619619	1.70	ray::ClientAppActor
619620	1.68	ray::ClientAppActor.run
Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.

[2024-08-13 23:54:12,367][flwr][ERROR] - Task was killed due to the node running low on memory.
Memory on the node (IP: 115.145.171.128, ID: c687613ff34cdbf5097699e5c896055ef2dc44343c5ab44bd40309d5) where the task (actor ID: 85928d3f1a247800081c0e1101000000, name=ClientAppActor.__init__, pid=619614, memory used=1.73GB) was running was 29.50GB / 31.02GB (0.951049), which exceeds the memory usage threshold of 0.95. Ray killed this worker (ID: c7f644ac3cca20dffbfa1723daac4f87f04c7de03da25e7cad5fe33c) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 115.145.171.128`. To see the logs of the worker, use `ray logs worker-c7f644ac3cca20dffbfa1723daac4f87f04c7de03da25e7cad5fe33c*out -ip 115.145.171.128. Top 10 memory users:
PID	MEM(GB)	COMMAND
4942	3.49	/snap/pycharm-professional/408/jbr/bin/java -classpath /snap/pycharm-professional/408/lib/platform-l...
619616	1.80	ray::ClientAppActor.run
619615	1.78	ray::ClientAppActor
619617	1.77	ray::ClientAppActor
619611	1.73	ray::ClientAppActor
619618	1.73	ray::ClientAppActor
619614	1.73	ray::ClientAppActor
619612	1.72	ray::ClientAppActor
619619	1.70	ray::ClientAppActor
619620	1.68	ray::ClientAppActor.run
Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.
[2024-08-13 23:54:14,092][flwr][INFO] - aggregate_fit: received 8 results and 2 failures
[2024-08-13 23:54:17,768][flwr][INFO] - fit progress: (238, 15.624370217323303, {'accuracy': 0.7516025641025641}, 1379.028512624005)
[2024-08-13 23:54:17,768][flwr][INFO] - configure_evaluate: strategy sampled 2 clients (out of 20)
[2024-08-13 23:54:17,988][flwr][INFO] - aggregate_evaluate: received 2 results and 0 failures
[2024-08-13 23:54:17,988][flwr][INFO] - 
[2024-08-13 23:54:17,988][flwr][INFO] - [ROUND 239]
[2024-08-13 23:54:17,989][flwr][INFO] - configure_fit: strategy sampled 10 clients (out of 20)
[2024-08-13 23:54:18,137][flwr][ERROR] - Traceback (most recent call last):
  File "/home/abdenour/miniconda3/envs/fl_with_flower/lib/python3.11/site-packages/flwr/simulation/ray_transport/ray_client_proxy.py", line 94, in _submit_job
    out_mssg, updated_context = self.actor_pool.get_client_result(
                                ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/abdenour/miniconda3/envs/fl_with_flower/lib/python3.11/site-packages/flwr/simulation/ray_transport/ray_actor.py", line 398, in get_client_result
    return self._fetch_future_result(cid)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/abdenour/miniconda3/envs/fl_with_flower/lib/python3.11/site-packages/flwr/simulation/ray_transport/ray_actor.py", line 279, in _fetch_future_result
    res_cid, out_mssg, updated_context = ray.get(
                                         ^^^^^^^^
  File "/home/abdenour/miniconda3/envs/fl_with_flower/lib/python3.11/site-packages/ray/_private/auto_init_hook.py", line 21, in auto_init_wrapper
    return fn(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^
  File "/home/abdenour/miniconda3/envs/fl_with_flower/lib/python3.11/site-packages/ray/_private/client_mode_hook.py", line 103, in wrapper
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "/home/abdenour/miniconda3/envs/fl_with_flower/lib/python3.11/site-packages/ray/_private/worker.py", line 2667, in get
    values, debugger_breakpoint = worker.get_objects(object_refs, timeout=timeout)
                                  ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/abdenour/miniconda3/envs/fl_with_flower/lib/python3.11/site-packages/ray/_private/worker.py", line 866, in get_objects
    raise value
ray.exceptions.OutOfMemoryError: Task was killed due to the node running low on memory.
Memory on the node (IP: 115.145.171.128, ID: c687613ff34cdbf5097699e5c896055ef2dc44343c5ab44bd40309d5) where the task (actor ID: 85928d3f1a247800081c0e1101000000, name=ClientAppActor.__init__, pid=619614, memory used=1.73GB) was running was 29.50GB / 31.02GB (0.951049), which exceeds the memory usage threshold of 0.95. Ray killed this worker (ID: c7f644ac3cca20dffbfa1723daac4f87f04c7de03da25e7cad5fe33c) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 115.145.171.128`. To see the logs of the worker, use `ray logs worker-c7f644ac3cca20dffbfa1723daac4f87f04c7de03da25e7cad5fe33c*out -ip 115.145.171.128. Top 10 memory users:
PID	MEM(GB)	COMMAND
4942	3.49	/snap/pycharm-professional/408/jbr/bin/java -classpath /snap/pycharm-professional/408/lib/platform-l...
619616	1.80	ray::ClientAppActor.run
619615	1.78	ray::ClientAppActor
619617	1.77	ray::ClientAppActor
619611	1.73	ray::ClientAppActor
619618	1.73	ray::ClientAppActor
619614	1.73	ray::ClientAppActor
619612	1.72	ray::ClientAppActor
619619	1.70	ray::ClientAppActor
619620	1.68	ray::ClientAppActor.run
Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.

[2024-08-13 23:54:18,138][flwr][ERROR] - Task was killed due to the node running low on memory.
Memory on the node (IP: 115.145.171.128, ID: c687613ff34cdbf5097699e5c896055ef2dc44343c5ab44bd40309d5) where the task (actor ID: 85928d3f1a247800081c0e1101000000, name=ClientAppActor.__init__, pid=619614, memory used=1.73GB) was running was 29.50GB / 31.02GB (0.951049), which exceeds the memory usage threshold of 0.95. Ray killed this worker (ID: c7f644ac3cca20dffbfa1723daac4f87f04c7de03da25e7cad5fe33c) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 115.145.171.128`. To see the logs of the worker, use `ray logs worker-c7f644ac3cca20dffbfa1723daac4f87f04c7de03da25e7cad5fe33c*out -ip 115.145.171.128. Top 10 memory users:
PID	MEM(GB)	COMMAND
4942	3.49	/snap/pycharm-professional/408/jbr/bin/java -classpath /snap/pycharm-professional/408/lib/platform-l...
619616	1.80	ray::ClientAppActor.run
619615	1.78	ray::ClientAppActor
619617	1.77	ray::ClientAppActor
619611	1.73	ray::ClientAppActor
619618	1.73	ray::ClientAppActor
619614	1.73	ray::ClientAppActor
619612	1.72	ray::ClientAppActor
619619	1.70	ray::ClientAppActor
619620	1.68	ray::ClientAppActor.run
Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.
[2024-08-13 23:54:18,138][flwr][ERROR] - Traceback (most recent call last):
  File "/home/abdenour/miniconda3/envs/fl_with_flower/lib/python3.11/site-packages/flwr/simulation/ray_transport/ray_client_proxy.py", line 94, in _submit_job
    out_mssg, updated_context = self.actor_pool.get_client_result(
                                ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/abdenour/miniconda3/envs/fl_with_flower/lib/python3.11/site-packages/flwr/simulation/ray_transport/ray_actor.py", line 398, in get_client_result
    return self._fetch_future_result(cid)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/abdenour/miniconda3/envs/fl_with_flower/lib/python3.11/site-packages/flwr/simulation/ray_transport/ray_actor.py", line 279, in _fetch_future_result
    res_cid, out_mssg, updated_context = ray.get(
                                         ^^^^^^^^
  File "/home/abdenour/miniconda3/envs/fl_with_flower/lib/python3.11/site-packages/ray/_private/auto_init_hook.py", line 21, in auto_init_wrapper
    return fn(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^
  File "/home/abdenour/miniconda3/envs/fl_with_flower/lib/python3.11/site-packages/ray/_private/client_mode_hook.py", line 103, in wrapper
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "/home/abdenour/miniconda3/envs/fl_with_flower/lib/python3.11/site-packages/ray/_private/worker.py", line 2667, in get
    values, debugger_breakpoint = worker.get_objects(object_refs, timeout=timeout)
                                  ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/abdenour/miniconda3/envs/fl_with_flower/lib/python3.11/site-packages/ray/_private/worker.py", line 866, in get_objects
    raise value
ray.exceptions.OutOfMemoryError: Task was killed due to the node running low on memory.
Memory on the node (IP: 115.145.171.128, ID: c687613ff34cdbf5097699e5c896055ef2dc44343c5ab44bd40309d5) where the task (actor ID: 52365bb7997979e5a4c7ddbd01000000, name=ClientAppActor.__init__, pid=619618, memory used=1.84GB) was running was 29.49GB / 31.02GB (0.950707), which exceeds the memory usage threshold of 0.95. Ray killed this worker (ID: 7e48cd46c72821b17be65b05a4e61b5846ba998a57d76db44ca4f8fa) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 115.145.171.128`. To see the logs of the worker, use `ray logs worker-7e48cd46c72821b17be65b05a4e61b5846ba998a57d76db44ca4f8fa*out -ip 115.145.171.128. Top 10 memory users:
PID	MEM(GB)	COMMAND
4942	3.48	/snap/pycharm-professional/408/jbr/bin/java -classpath /snap/pycharm-professional/408/lib/platform-l...
616443	2.14	/home/abdenour/miniconda3/envs/fl_with_flower/bin/python /home/abdenour/PycharmProjects/fl_with_flow...
619615	1.90	ray::ClientAppActor
619616	1.88	ray::ClientAppActor
619611	1.87	ray::ClientAppActor
619617	1.86	ray::ClientAppActor
619620	1.85	ray::ClientAppActor
619618	1.84	ray::ClientAppActor
619613	1.82	ray::ClientAppActor
619619	1.80	ray::ClientAppActor
Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.

[2024-08-13 23:54:18,138][flwr][ERROR] - Task was killed due to the node running low on memory.
Memory on the node (IP: 115.145.171.128, ID: c687613ff34cdbf5097699e5c896055ef2dc44343c5ab44bd40309d5) where the task (actor ID: 52365bb7997979e5a4c7ddbd01000000, name=ClientAppActor.__init__, pid=619618, memory used=1.84GB) was running was 29.49GB / 31.02GB (0.950707), which exceeds the memory usage threshold of 0.95. Ray killed this worker (ID: 7e48cd46c72821b17be65b05a4e61b5846ba998a57d76db44ca4f8fa) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 115.145.171.128`. To see the logs of the worker, use `ray logs worker-7e48cd46c72821b17be65b05a4e61b5846ba998a57d76db44ca4f8fa*out -ip 115.145.171.128. Top 10 memory users:
PID	MEM(GB)	COMMAND
4942	3.48	/snap/pycharm-professional/408/jbr/bin/java -classpath /snap/pycharm-professional/408/lib/platform-l...
616443	2.14	/home/abdenour/miniconda3/envs/fl_with_flower/bin/python /home/abdenour/PycharmProjects/fl_with_flow...
619615	1.90	ray::ClientAppActor
619616	1.88	ray::ClientAppActor
619611	1.87	ray::ClientAppActor
619617	1.86	ray::ClientAppActor
619620	1.85	ray::ClientAppActor
619618	1.84	ray::ClientAppActor
619613	1.82	ray::ClientAppActor
619619	1.80	ray::ClientAppActor
Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.
[2024-08-13 23:54:19,791][flwr][INFO] - aggregate_fit: received 8 results and 2 failures
[2024-08-13 23:54:22,945][flwr][INFO] - fit progress: (239, 16.797933638095856, {'accuracy': 0.7483974358974359}, 1384.2056668900186)
[2024-08-13 23:54:22,945][flwr][INFO] - configure_evaluate: strategy sampled 2 clients (out of 20)
[2024-08-13 23:54:23,127][flwr][INFO] - aggregate_evaluate: received 2 results and 0 failures
[2024-08-13 23:54:23,128][flwr][INFO] - 
[2024-08-13 23:54:23,128][flwr][INFO] - [ROUND 240]
[2024-08-13 23:54:23,128][flwr][INFO] - configure_fit: strategy sampled 10 clients (out of 20)
[2024-08-13 23:54:23,296][flwr][ERROR] - Traceback (most recent call last):
  File "/home/abdenour/miniconda3/envs/fl_with_flower/lib/python3.11/site-packages/flwr/simulation/ray_transport/ray_client_proxy.py", line 94, in _submit_job
    out_mssg, updated_context = self.actor_pool.get_client_result(
                                ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/abdenour/miniconda3/envs/fl_with_flower/lib/python3.11/site-packages/flwr/simulation/ray_transport/ray_actor.py", line 398, in get_client_result
    return self._fetch_future_result(cid)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/abdenour/miniconda3/envs/fl_with_flower/lib/python3.11/site-packages/flwr/simulation/ray_transport/ray_actor.py", line 279, in _fetch_future_result
    res_cid, out_mssg, updated_context = ray.get(
                                         ^^^^^^^^
  File "/home/abdenour/miniconda3/envs/fl_with_flower/lib/python3.11/site-packages/ray/_private/auto_init_hook.py", line 21, in auto_init_wrapper
    return fn(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^
  File "/home/abdenour/miniconda3/envs/fl_with_flower/lib/python3.11/site-packages/ray/_private/client_mode_hook.py", line 103, in wrapper
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "/home/abdenour/miniconda3/envs/fl_with_flower/lib/python3.11/site-packages/ray/_private/worker.py", line 2667, in get
    values, debugger_breakpoint = worker.get_objects(object_refs, timeout=timeout)
                                  ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/abdenour/miniconda3/envs/fl_with_flower/lib/python3.11/site-packages/ray/_private/worker.py", line 866, in get_objects
    raise value
ray.exceptions.OutOfMemoryError: Task was killed due to the node running low on memory.
Memory on the node (IP: 115.145.171.128, ID: c687613ff34cdbf5097699e5c896055ef2dc44343c5ab44bd40309d5) where the task (actor ID: 52365bb7997979e5a4c7ddbd01000000, name=ClientAppActor.__init__, pid=619618, memory used=1.84GB) was running was 29.49GB / 31.02GB (0.950707), which exceeds the memory usage threshold of 0.95. Ray killed this worker (ID: 7e48cd46c72821b17be65b05a4e61b5846ba998a57d76db44ca4f8fa) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 115.145.171.128`. To see the logs of the worker, use `ray logs worker-7e48cd46c72821b17be65b05a4e61b5846ba998a57d76db44ca4f8fa*out -ip 115.145.171.128. Top 10 memory users:
PID	MEM(GB)	COMMAND
4942	3.48	/snap/pycharm-professional/408/jbr/bin/java -classpath /snap/pycharm-professional/408/lib/platform-l...
616443	2.14	/home/abdenour/miniconda3/envs/fl_with_flower/bin/python /home/abdenour/PycharmProjects/fl_with_flow...
619615	1.90	ray::ClientAppActor
619616	1.88	ray::ClientAppActor
619611	1.87	ray::ClientAppActor
619617	1.86	ray::ClientAppActor
619620	1.85	ray::ClientAppActor
619618	1.84	ray::ClientAppActor
619613	1.82	ray::ClientAppActor
619619	1.80	ray::ClientAppActor
Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.

[2024-08-13 23:54:23,296][flwr][ERROR] - Task was killed due to the node running low on memory.
Memory on the node (IP: 115.145.171.128, ID: c687613ff34cdbf5097699e5c896055ef2dc44343c5ab44bd40309d5) where the task (actor ID: 52365bb7997979e5a4c7ddbd01000000, name=ClientAppActor.__init__, pid=619618, memory used=1.84GB) was running was 29.49GB / 31.02GB (0.950707), which exceeds the memory usage threshold of 0.95. Ray killed this worker (ID: 7e48cd46c72821b17be65b05a4e61b5846ba998a57d76db44ca4f8fa) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 115.145.171.128`. To see the logs of the worker, use `ray logs worker-7e48cd46c72821b17be65b05a4e61b5846ba998a57d76db44ca4f8fa*out -ip 115.145.171.128. Top 10 memory users:
PID	MEM(GB)	COMMAND
4942	3.48	/snap/pycharm-professional/408/jbr/bin/java -classpath /snap/pycharm-professional/408/lib/platform-l...
616443	2.14	/home/abdenour/miniconda3/envs/fl_with_flower/bin/python /home/abdenour/PycharmProjects/fl_with_flow...
619615	1.90	ray::ClientAppActor
619616	1.88	ray::ClientAppActor
619611	1.87	ray::ClientAppActor
619617	1.86	ray::ClientAppActor
619620	1.85	ray::ClientAppActor
619618	1.84	ray::ClientAppActor
619613	1.82	ray::ClientAppActor
619619	1.80	ray::ClientAppActor
Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.
[2024-08-13 23:54:23,296][flwr][ERROR] - Traceback (most recent call last):
  File "/home/abdenour/miniconda3/envs/fl_with_flower/lib/python3.11/site-packages/flwr/simulation/ray_transport/ray_client_proxy.py", line 94, in _submit_job
    out_mssg, updated_context = self.actor_pool.get_client_result(
                                ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/abdenour/miniconda3/envs/fl_with_flower/lib/python3.11/site-packages/flwr/simulation/ray_transport/ray_actor.py", line 398, in get_client_result
    return self._fetch_future_result(cid)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/abdenour/miniconda3/envs/fl_with_flower/lib/python3.11/site-packages/flwr/simulation/ray_transport/ray_actor.py", line 279, in _fetch_future_result
    res_cid, out_mssg, updated_context = ray.get(
                                         ^^^^^^^^
  File "/home/abdenour/miniconda3/envs/fl_with_flower/lib/python3.11/site-packages/ray/_private/auto_init_hook.py", line 21, in auto_init_wrapper
    return fn(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^
  File "/home/abdenour/miniconda3/envs/fl_with_flower/lib/python3.11/site-packages/ray/_private/client_mode_hook.py", line 103, in wrapper
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "/home/abdenour/miniconda3/envs/fl_with_flower/lib/python3.11/site-packages/ray/_private/worker.py", line 2667, in get
    values, debugger_breakpoint = worker.get_objects(object_refs, timeout=timeout)
                                  ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/abdenour/miniconda3/envs/fl_with_flower/lib/python3.11/site-packages/ray/_private/worker.py", line 866, in get_objects
    raise value
ray.exceptions.OutOfMemoryError: Task was killed due to the node running low on memory.
Memory on the node (IP: 115.145.171.128, ID: c687613ff34cdbf5097699e5c896055ef2dc44343c5ab44bd40309d5) where the task (actor ID: 85928d3f1a247800081c0e1101000000, name=ClientAppActor.__init__, pid=619614, memory used=1.73GB) was running was 29.50GB / 31.02GB (0.951049), which exceeds the memory usage threshold of 0.95. Ray killed this worker (ID: c7f644ac3cca20dffbfa1723daac4f87f04c7de03da25e7cad5fe33c) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 115.145.171.128`. To see the logs of the worker, use `ray logs worker-c7f644ac3cca20dffbfa1723daac4f87f04c7de03da25e7cad5fe33c*out -ip 115.145.171.128. Top 10 memory users:
PID	MEM(GB)	COMMAND
4942	3.49	/snap/pycharm-professional/408/jbr/bin/java -classpath /snap/pycharm-professional/408/lib/platform-l...
619616	1.80	ray::ClientAppActor.run
619615	1.78	ray::ClientAppActor
619617	1.77	ray::ClientAppActor
619611	1.73	ray::ClientAppActor
619618	1.73	ray::ClientAppActor
619614	1.73	ray::ClientAppActor
619612	1.72	ray::ClientAppActor
619619	1.70	ray::ClientAppActor
619620	1.68	ray::ClientAppActor.run
Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.

[2024-08-13 23:54:23,297][flwr][ERROR] - Task was killed due to the node running low on memory.
Memory on the node (IP: 115.145.171.128, ID: c687613ff34cdbf5097699e5c896055ef2dc44343c5ab44bd40309d5) where the task (actor ID: 85928d3f1a247800081c0e1101000000, name=ClientAppActor.__init__, pid=619614, memory used=1.73GB) was running was 29.50GB / 31.02GB (0.951049), which exceeds the memory usage threshold of 0.95. Ray killed this worker (ID: c7f644ac3cca20dffbfa1723daac4f87f04c7de03da25e7cad5fe33c) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 115.145.171.128`. To see the logs of the worker, use `ray logs worker-c7f644ac3cca20dffbfa1723daac4f87f04c7de03da25e7cad5fe33c*out -ip 115.145.171.128. Top 10 memory users:
PID	MEM(GB)	COMMAND
4942	3.49	/snap/pycharm-professional/408/jbr/bin/java -classpath /snap/pycharm-professional/408/lib/platform-l...
619616	1.80	ray::ClientAppActor.run
619615	1.78	ray::ClientAppActor
619617	1.77	ray::ClientAppActor
619611	1.73	ray::ClientAppActor
619618	1.73	ray::ClientAppActor
619614	1.73	ray::ClientAppActor
619612	1.72	ray::ClientAppActor
619619	1.70	ray::ClientAppActor
619620	1.68	ray::ClientAppActor.run
Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.
[2024-08-13 23:54:24,851][flwr][INFO] - aggregate_fit: received 8 results and 2 failures
[2024-08-13 23:54:27,874][flwr][INFO] - fit progress: (240, 15.246441781520844, {'accuracy': 0.7564102564102564}, 1389.1341660580074)
[2024-08-13 23:54:27,874][flwr][INFO] - configure_evaluate: strategy sampled 2 clients (out of 20)
[2024-08-13 23:54:28,102][flwr][INFO] - aggregate_evaluate: received 2 results and 0 failures
[2024-08-13 23:54:28,102][flwr][INFO] - 
[2024-08-13 23:54:28,102][flwr][INFO] - [ROUND 241]
[2024-08-13 23:54:28,102][flwr][INFO] - configure_fit: strategy sampled 10 clients (out of 20)
[2024-08-13 23:54:28,261][flwr][ERROR] - Traceback (most recent call last):
  File "/home/abdenour/miniconda3/envs/fl_with_flower/lib/python3.11/site-packages/flwr/simulation/ray_transport/ray_client_proxy.py", line 94, in _submit_job
    out_mssg, updated_context = self.actor_pool.get_client_result(
                                ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/abdenour/miniconda3/envs/fl_with_flower/lib/python3.11/site-packages/flwr/simulation/ray_transport/ray_actor.py", line 398, in get_client_result
    return self._fetch_future_result(cid)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/abdenour/miniconda3/envs/fl_with_flower/lib/python3.11/site-packages/flwr/simulation/ray_transport/ray_actor.py", line 279, in _fetch_future_result
    res_cid, out_mssg, updated_context = ray.get(
                                         ^^^^^^^^
  File "/home/abdenour/miniconda3/envs/fl_with_flower/lib/python3.11/site-packages/ray/_private/auto_init_hook.py", line 21, in auto_init_wrapper
    return fn(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^
  File "/home/abdenour/miniconda3/envs/fl_with_flower/lib/python3.11/site-packages/ray/_private/client_mode_hook.py", line 103, in wrapper
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "/home/abdenour/miniconda3/envs/fl_with_flower/lib/python3.11/site-packages/ray/_private/worker.py", line 2667, in get
    values, debugger_breakpoint = worker.get_objects(object_refs, timeout=timeout)
                                  ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/abdenour/miniconda3/envs/fl_with_flower/lib/python3.11/site-packages/ray/_private/worker.py", line 866, in get_objects
    raise value
ray.exceptions.OutOfMemoryError: Task was killed due to the node running low on memory.
Memory on the node (IP: 115.145.171.128, ID: c687613ff34cdbf5097699e5c896055ef2dc44343c5ab44bd40309d5) where the task (actor ID: 85928d3f1a247800081c0e1101000000, name=ClientAppActor.__init__, pid=619614, memory used=1.73GB) was running was 29.50GB / 31.02GB (0.951049), which exceeds the memory usage threshold of 0.95. Ray killed this worker (ID: c7f644ac3cca20dffbfa1723daac4f87f04c7de03da25e7cad5fe33c) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 115.145.171.128`. To see the logs of the worker, use `ray logs worker-c7f644ac3cca20dffbfa1723daac4f87f04c7de03da25e7cad5fe33c*out -ip 115.145.171.128. Top 10 memory users:
PID	MEM(GB)	COMMAND
4942	3.49	/snap/pycharm-professional/408/jbr/bin/java -classpath /snap/pycharm-professional/408/lib/platform-l...
619616	1.80	ray::ClientAppActor.run
619615	1.78	ray::ClientAppActor
619617	1.77	ray::ClientAppActor
619611	1.73	ray::ClientAppActor
619618	1.73	ray::ClientAppActor
619614	1.73	ray::ClientAppActor
619612	1.72	ray::ClientAppActor
619619	1.70	ray::ClientAppActor
619620	1.68	ray::ClientAppActor.run
Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.

[2024-08-13 23:54:28,262][flwr][ERROR] - Task was killed due to the node running low on memory.
Memory on the node (IP: 115.145.171.128, ID: c687613ff34cdbf5097699e5c896055ef2dc44343c5ab44bd40309d5) where the task (actor ID: 85928d3f1a247800081c0e1101000000, name=ClientAppActor.__init__, pid=619614, memory used=1.73GB) was running was 29.50GB / 31.02GB (0.951049), which exceeds the memory usage threshold of 0.95. Ray killed this worker (ID: c7f644ac3cca20dffbfa1723daac4f87f04c7de03da25e7cad5fe33c) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 115.145.171.128`. To see the logs of the worker, use `ray logs worker-c7f644ac3cca20dffbfa1723daac4f87f04c7de03da25e7cad5fe33c*out -ip 115.145.171.128. Top 10 memory users:
PID	MEM(GB)	COMMAND
4942	3.49	/snap/pycharm-professional/408/jbr/bin/java -classpath /snap/pycharm-professional/408/lib/platform-l...
619616	1.80	ray::ClientAppActor.run
619615	1.78	ray::ClientAppActor
619617	1.77	ray::ClientAppActor
619611	1.73	ray::ClientAppActor
619618	1.73	ray::ClientAppActor
619614	1.73	ray::ClientAppActor
619612	1.72	ray::ClientAppActor
619619	1.70	ray::ClientAppActor
619620	1.68	ray::ClientAppActor.run
Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.
[2024-08-13 23:54:28,262][flwr][ERROR] - Traceback (most recent call last):
  File "/home/abdenour/miniconda3/envs/fl_with_flower/lib/python3.11/site-packages/flwr/simulation/ray_transport/ray_client_proxy.py", line 94, in _submit_job
    out_mssg, updated_context = self.actor_pool.get_client_result(
                                ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/abdenour/miniconda3/envs/fl_with_flower/lib/python3.11/site-packages/flwr/simulation/ray_transport/ray_actor.py", line 398, in get_client_result
    return self._fetch_future_result(cid)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/abdenour/miniconda3/envs/fl_with_flower/lib/python3.11/site-packages/flwr/simulation/ray_transport/ray_actor.py", line 279, in _fetch_future_result
    res_cid, out_mssg, updated_context = ray.get(
                                         ^^^^^^^^
  File "/home/abdenour/miniconda3/envs/fl_with_flower/lib/python3.11/site-packages/ray/_private/auto_init_hook.py", line 21, in auto_init_wrapper
    return fn(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^
  File "/home/abdenour/miniconda3/envs/fl_with_flower/lib/python3.11/site-packages/ray/_private/client_mode_hook.py", line 103, in wrapper
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "/home/abdenour/miniconda3/envs/fl_with_flower/lib/python3.11/site-packages/ray/_private/worker.py", line 2667, in get
    values, debugger_breakpoint = worker.get_objects(object_refs, timeout=timeout)
                                  ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/abdenour/miniconda3/envs/fl_with_flower/lib/python3.11/site-packages/ray/_private/worker.py", line 866, in get_objects
    raise value
ray.exceptions.OutOfMemoryError: Task was killed due to the node running low on memory.
Memory on the node (IP: 115.145.171.128, ID: c687613ff34cdbf5097699e5c896055ef2dc44343c5ab44bd40309d5) where the task (actor ID: 52365bb7997979e5a4c7ddbd01000000, name=ClientAppActor.__init__, pid=619618, memory used=1.84GB) was running was 29.49GB / 31.02GB (0.950707), which exceeds the memory usage threshold of 0.95. Ray killed this worker (ID: 7e48cd46c72821b17be65b05a4e61b5846ba998a57d76db44ca4f8fa) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 115.145.171.128`. To see the logs of the worker, use `ray logs worker-7e48cd46c72821b17be65b05a4e61b5846ba998a57d76db44ca4f8fa*out -ip 115.145.171.128. Top 10 memory users:
PID	MEM(GB)	COMMAND
4942	3.48	/snap/pycharm-professional/408/jbr/bin/java -classpath /snap/pycharm-professional/408/lib/platform-l...
616443	2.14	/home/abdenour/miniconda3/envs/fl_with_flower/bin/python /home/abdenour/PycharmProjects/fl_with_flow...
619615	1.90	ray::ClientAppActor
619616	1.88	ray::ClientAppActor
619611	1.87	ray::ClientAppActor
619617	1.86	ray::ClientAppActor
619620	1.85	ray::ClientAppActor
619618	1.84	ray::ClientAppActor
619613	1.82	ray::ClientAppActor
619619	1.80	ray::ClientAppActor
Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.

[2024-08-13 23:54:28,262][flwr][ERROR] - Task was killed due to the node running low on memory.
Memory on the node (IP: 115.145.171.128, ID: c687613ff34cdbf5097699e5c896055ef2dc44343c5ab44bd40309d5) where the task (actor ID: 52365bb7997979e5a4c7ddbd01000000, name=ClientAppActor.__init__, pid=619618, memory used=1.84GB) was running was 29.49GB / 31.02GB (0.950707), which exceeds the memory usage threshold of 0.95. Ray killed this worker (ID: 7e48cd46c72821b17be65b05a4e61b5846ba998a57d76db44ca4f8fa) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 115.145.171.128`. To see the logs of the worker, use `ray logs worker-7e48cd46c72821b17be65b05a4e61b5846ba998a57d76db44ca4f8fa*out -ip 115.145.171.128. Top 10 memory users:
PID	MEM(GB)	COMMAND
4942	3.48	/snap/pycharm-professional/408/jbr/bin/java -classpath /snap/pycharm-professional/408/lib/platform-l...
616443	2.14	/home/abdenour/miniconda3/envs/fl_with_flower/bin/python /home/abdenour/PycharmProjects/fl_with_flow...
619615	1.90	ray::ClientAppActor
619616	1.88	ray::ClientAppActor
619611	1.87	ray::ClientAppActor
619617	1.86	ray::ClientAppActor
619620	1.85	ray::ClientAppActor
619618	1.84	ray::ClientAppActor
619613	1.82	ray::ClientAppActor
619619	1.80	ray::ClientAppActor
Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.
[2024-08-13 23:54:29,971][flwr][INFO] - aggregate_fit: received 8 results and 2 failures
[2024-08-13 23:54:32,805][flwr][INFO] - fit progress: (241, 14.91311126947403, {'accuracy': 0.7564102564102564}, 1394.0654087369912)
[2024-08-13 23:54:32,805][flwr][INFO] - configure_evaluate: strategy sampled 2 clients (out of 20)
[2024-08-13 23:54:33,012][flwr][INFO] - aggregate_evaluate: received 2 results and 0 failures
[2024-08-13 23:54:33,012][flwr][INFO] - 
[2024-08-13 23:54:33,012][flwr][INFO] - [ROUND 242]
[2024-08-13 23:54:33,012][flwr][INFO] - configure_fit: strategy sampled 10 clients (out of 20)
[2024-08-13 23:54:33,201][flwr][ERROR] - Traceback (most recent call last):
  File "/home/abdenour/miniconda3/envs/fl_with_flower/lib/python3.11/site-packages/flwr/simulation/ray_transport/ray_client_proxy.py", line 94, in _submit_job
    out_mssg, updated_context = self.actor_pool.get_client_result(
                                ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/abdenour/miniconda3/envs/fl_with_flower/lib/python3.11/site-packages/flwr/simulation/ray_transport/ray_actor.py", line 398, in get_client_result
    return self._fetch_future_result(cid)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/abdenour/miniconda3/envs/fl_with_flower/lib/python3.11/site-packages/flwr/simulation/ray_transport/ray_actor.py", line 279, in _fetch_future_result
    res_cid, out_mssg, updated_context = ray.get(
                                         ^^^^^^^^
  File "/home/abdenour/miniconda3/envs/fl_with_flower/lib/python3.11/site-packages/ray/_private/auto_init_hook.py", line 21, in auto_init_wrapper
    return fn(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^
  File "/home/abdenour/miniconda3/envs/fl_with_flower/lib/python3.11/site-packages/ray/_private/client_mode_hook.py", line 103, in wrapper
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "/home/abdenour/miniconda3/envs/fl_with_flower/lib/python3.11/site-packages/ray/_private/worker.py", line 2667, in get
    values, debugger_breakpoint = worker.get_objects(object_refs, timeout=timeout)
                                  ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/abdenour/miniconda3/envs/fl_with_flower/lib/python3.11/site-packages/ray/_private/worker.py", line 866, in get_objects
    raise value
ray.exceptions.OutOfMemoryError: Task was killed due to the node running low on memory.
Memory on the node (IP: 115.145.171.128, ID: c687613ff34cdbf5097699e5c896055ef2dc44343c5ab44bd40309d5) where the task (actor ID: 52365bb7997979e5a4c7ddbd01000000, name=ClientAppActor.__init__, pid=619618, memory used=1.84GB) was running was 29.49GB / 31.02GB (0.950707), which exceeds the memory usage threshold of 0.95. Ray killed this worker (ID: 7e48cd46c72821b17be65b05a4e61b5846ba998a57d76db44ca4f8fa) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 115.145.171.128`. To see the logs of the worker, use `ray logs worker-7e48cd46c72821b17be65b05a4e61b5846ba998a57d76db44ca4f8fa*out -ip 115.145.171.128. Top 10 memory users:
PID	MEM(GB)	COMMAND
4942	3.48	/snap/pycharm-professional/408/jbr/bin/java -classpath /snap/pycharm-professional/408/lib/platform-l...
616443	2.14	/home/abdenour/miniconda3/envs/fl_with_flower/bin/python /home/abdenour/PycharmProjects/fl_with_flow...
619615	1.90	ray::ClientAppActor
619616	1.88	ray::ClientAppActor
619611	1.87	ray::ClientAppActor
619617	1.86	ray::ClientAppActor
619620	1.85	ray::ClientAppActor
619618	1.84	ray::ClientAppActor
619613	1.82	ray::ClientAppActor
619619	1.80	ray::ClientAppActor
Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.

[2024-08-13 23:54:33,201][flwr][ERROR] - Task was killed due to the node running low on memory.
Memory on the node (IP: 115.145.171.128, ID: c687613ff34cdbf5097699e5c896055ef2dc44343c5ab44bd40309d5) where the task (actor ID: 52365bb7997979e5a4c7ddbd01000000, name=ClientAppActor.__init__, pid=619618, memory used=1.84GB) was running was 29.49GB / 31.02GB (0.950707), which exceeds the memory usage threshold of 0.95. Ray killed this worker (ID: 7e48cd46c72821b17be65b05a4e61b5846ba998a57d76db44ca4f8fa) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 115.145.171.128`. To see the logs of the worker, use `ray logs worker-7e48cd46c72821b17be65b05a4e61b5846ba998a57d76db44ca4f8fa*out -ip 115.145.171.128. Top 10 memory users:
PID	MEM(GB)	COMMAND
4942	3.48	/snap/pycharm-professional/408/jbr/bin/java -classpath /snap/pycharm-professional/408/lib/platform-l...
616443	2.14	/home/abdenour/miniconda3/envs/fl_with_flower/bin/python /home/abdenour/PycharmProjects/fl_with_flow...
619615	1.90	ray::ClientAppActor
619616	1.88	ray::ClientAppActor
619611	1.87	ray::ClientAppActor
619617	1.86	ray::ClientAppActor
619620	1.85	ray::ClientAppActor
619618	1.84	ray::ClientAppActor
619613	1.82	ray::ClientAppActor
619619	1.80	ray::ClientAppActor
Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.
[2024-08-13 23:54:33,202][flwr][ERROR] - Traceback (most recent call last):
  File "/home/abdenour/miniconda3/envs/fl_with_flower/lib/python3.11/site-packages/flwr/simulation/ray_transport/ray_client_proxy.py", line 94, in _submit_job
    out_mssg, updated_context = self.actor_pool.get_client_result(
                                ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/abdenour/miniconda3/envs/fl_with_flower/lib/python3.11/site-packages/flwr/simulation/ray_transport/ray_actor.py", line 398, in get_client_result
    return self._fetch_future_result(cid)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/abdenour/miniconda3/envs/fl_with_flower/lib/python3.11/site-packages/flwr/simulation/ray_transport/ray_actor.py", line 279, in _fetch_future_result
    res_cid, out_mssg, updated_context = ray.get(
                                         ^^^^^^^^
  File "/home/abdenour/miniconda3/envs/fl_with_flower/lib/python3.11/site-packages/ray/_private/auto_init_hook.py", line 21, in auto_init_wrapper
    return fn(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^
  File "/home/abdenour/miniconda3/envs/fl_with_flower/lib/python3.11/site-packages/ray/_private/client_mode_hook.py", line 103, in wrapper
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "/home/abdenour/miniconda3/envs/fl_with_flower/lib/python3.11/site-packages/ray/_private/worker.py", line 2667, in get
    values, debugger_breakpoint = worker.get_objects(object_refs, timeout=timeout)
                                  ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/abdenour/miniconda3/envs/fl_with_flower/lib/python3.11/site-packages/ray/_private/worker.py", line 866, in get_objects
    raise value
ray.exceptions.OutOfMemoryError: Task was killed due to the node running low on memory.
Memory on the node (IP: 115.145.171.128, ID: c687613ff34cdbf5097699e5c896055ef2dc44343c5ab44bd40309d5) where the task (actor ID: 85928d3f1a247800081c0e1101000000, name=ClientAppActor.__init__, pid=619614, memory used=1.73GB) was running was 29.50GB / 31.02GB (0.951049), which exceeds the memory usage threshold of 0.95. Ray killed this worker (ID: c7f644ac3cca20dffbfa1723daac4f87f04c7de03da25e7cad5fe33c) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 115.145.171.128`. To see the logs of the worker, use `ray logs worker-c7f644ac3cca20dffbfa1723daac4f87f04c7de03da25e7cad5fe33c*out -ip 115.145.171.128. Top 10 memory users:
PID	MEM(GB)	COMMAND
4942	3.49	/snap/pycharm-professional/408/jbr/bin/java -classpath /snap/pycharm-professional/408/lib/platform-l...
619616	1.80	ray::ClientAppActor.run
619615	1.78	ray::ClientAppActor
619617	1.77	ray::ClientAppActor
619611	1.73	ray::ClientAppActor
619618	1.73	ray::ClientAppActor
619614	1.73	ray::ClientAppActor
619612	1.72	ray::ClientAppActor
619619	1.70	ray::ClientAppActor
619620	1.68	ray::ClientAppActor.run
Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.

[2024-08-13 23:54:33,202][flwr][ERROR] - Task was killed due to the node running low on memory.
Memory on the node (IP: 115.145.171.128, ID: c687613ff34cdbf5097699e5c896055ef2dc44343c5ab44bd40309d5) where the task (actor ID: 85928d3f1a247800081c0e1101000000, name=ClientAppActor.__init__, pid=619614, memory used=1.73GB) was running was 29.50GB / 31.02GB (0.951049), which exceeds the memory usage threshold of 0.95. Ray killed this worker (ID: c7f644ac3cca20dffbfa1723daac4f87f04c7de03da25e7cad5fe33c) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 115.145.171.128`. To see the logs of the worker, use `ray logs worker-c7f644ac3cca20dffbfa1723daac4f87f04c7de03da25e7cad5fe33c*out -ip 115.145.171.128. Top 10 memory users:
PID	MEM(GB)	COMMAND
4942	3.49	/snap/pycharm-professional/408/jbr/bin/java -classpath /snap/pycharm-professional/408/lib/platform-l...
619616	1.80	ray::ClientAppActor.run
619615	1.78	ray::ClientAppActor
619617	1.77	ray::ClientAppActor
619611	1.73	ray::ClientAppActor
619618	1.73	ray::ClientAppActor
619614	1.73	ray::ClientAppActor
619612	1.72	ray::ClientAppActor
619619	1.70	ray::ClientAppActor
619620	1.68	ray::ClientAppActor.run
Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.
[2024-08-13 23:54:34,919][flwr][INFO] - aggregate_fit: received 8 results and 2 failures
[2024-08-13 23:54:37,829][flwr][INFO] - fit progress: (242, 16.560327112674713, {'accuracy': 0.75}, 1399.0888384729915)
[2024-08-13 23:54:37,829][flwr][INFO] - configure_evaluate: strategy sampled 2 clients (out of 20)
[2024-08-13 23:54:38,020][flwr][INFO] - aggregate_evaluate: received 2 results and 0 failures
[2024-08-13 23:54:38,020][flwr][INFO] - 
[2024-08-13 23:54:38,020][flwr][INFO] - [ROUND 243]
[2024-08-13 23:54:38,020][flwr][INFO] - configure_fit: strategy sampled 10 clients (out of 20)
[2024-08-13 23:54:38,174][flwr][ERROR] - Traceback (most recent call last):
  File "/home/abdenour/miniconda3/envs/fl_with_flower/lib/python3.11/site-packages/flwr/simulation/ray_transport/ray_client_proxy.py", line 94, in _submit_job
    out_mssg, updated_context = self.actor_pool.get_client_result(
                                ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/abdenour/miniconda3/envs/fl_with_flower/lib/python3.11/site-packages/flwr/simulation/ray_transport/ray_actor.py", line 398, in get_client_result
    return self._fetch_future_result(cid)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/abdenour/miniconda3/envs/fl_with_flower/lib/python3.11/site-packages/flwr/simulation/ray_transport/ray_actor.py", line 279, in _fetch_future_result
    res_cid, out_mssg, updated_context = ray.get(
                                         ^^^^^^^^
  File "/home/abdenour/miniconda3/envs/fl_with_flower/lib/python3.11/site-packages/ray/_private/auto_init_hook.py", line 21, in auto_init_wrapper
    return fn(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^
  File "/home/abdenour/miniconda3/envs/fl_with_flower/lib/python3.11/site-packages/ray/_private/client_mode_hook.py", line 103, in wrapper
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "/home/abdenour/miniconda3/envs/fl_with_flower/lib/python3.11/site-packages/ray/_private/worker.py", line 2667, in get
    values, debugger_breakpoint = worker.get_objects(object_refs, timeout=timeout)
                                  ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/abdenour/miniconda3/envs/fl_with_flower/lib/python3.11/site-packages/ray/_private/worker.py", line 866, in get_objects
    raise value
ray.exceptions.OutOfMemoryError: Task was killed due to the node running low on memory.
Memory on the node (IP: 115.145.171.128, ID: c687613ff34cdbf5097699e5c896055ef2dc44343c5ab44bd40309d5) where the task (actor ID: 85928d3f1a247800081c0e1101000000, name=ClientAppActor.__init__, pid=619614, memory used=1.73GB) was running was 29.50GB / 31.02GB (0.951049), which exceeds the memory usage threshold of 0.95. Ray killed this worker (ID: c7f644ac3cca20dffbfa1723daac4f87f04c7de03da25e7cad5fe33c) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 115.145.171.128`. To see the logs of the worker, use `ray logs worker-c7f644ac3cca20dffbfa1723daac4f87f04c7de03da25e7cad5fe33c*out -ip 115.145.171.128. Top 10 memory users:
PID	MEM(GB)	COMMAND
4942	3.49	/snap/pycharm-professional/408/jbr/bin/java -classpath /snap/pycharm-professional/408/lib/platform-l...
619616	1.80	ray::ClientAppActor.run
619615	1.78	ray::ClientAppActor
619617	1.77	ray::ClientAppActor
619611	1.73	ray::ClientAppActor
619618	1.73	ray::ClientAppActor
619614	1.73	ray::ClientAppActor
619612	1.72	ray::ClientAppActor
619619	1.70	ray::ClientAppActor
619620	1.68	ray::ClientAppActor.run
Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.

[2024-08-13 23:54:38,174][flwr][ERROR] - Traceback (most recent call last):
  File "/home/abdenour/miniconda3/envs/fl_with_flower/lib/python3.11/site-packages/flwr/simulation/ray_transport/ray_client_proxy.py", line 94, in _submit_job
    out_mssg, updated_context = self.actor_pool.get_client_result(
                                ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/abdenour/miniconda3/envs/fl_with_flower/lib/python3.11/site-packages/flwr/simulation/ray_transport/ray_actor.py", line 398, in get_client_result
    return self._fetch_future_result(cid)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/abdenour/miniconda3/envs/fl_with_flower/lib/python3.11/site-packages/flwr/simulation/ray_transport/ray_actor.py", line 279, in _fetch_future_result
    res_cid, out_mssg, updated_context = ray.get(
                                         ^^^^^^^^
  File "/home/abdenour/miniconda3/envs/fl_with_flower/lib/python3.11/site-packages/ray/_private/auto_init_hook.py", line 21, in auto_init_wrapper
    return fn(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^
  File "/home/abdenour/miniconda3/envs/fl_with_flower/lib/python3.11/site-packages/ray/_private/client_mode_hook.py", line 103, in wrapper
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "/home/abdenour/miniconda3/envs/fl_with_flower/lib/python3.11/site-packages/ray/_private/worker.py", line 2667, in get
    values, debugger_breakpoint = worker.get_objects(object_refs, timeout=timeout)
                                  ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/abdenour/miniconda3/envs/fl_with_flower/lib/python3.11/site-packages/ray/_private/worker.py", line 866, in get_objects
    raise value
ray.exceptions.OutOfMemoryError: Task was killed due to the node running low on memory.
Memory on the node (IP: 115.145.171.128, ID: c687613ff34cdbf5097699e5c896055ef2dc44343c5ab44bd40309d5) where the task (actor ID: 52365bb7997979e5a4c7ddbd01000000, name=ClientAppActor.__init__, pid=619618, memory used=1.84GB) was running was 29.49GB / 31.02GB (0.950707), which exceeds the memory usage threshold of 0.95. Ray killed this worker (ID: 7e48cd46c72821b17be65b05a4e61b5846ba998a57d76db44ca4f8fa) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 115.145.171.128`. To see the logs of the worker, use `ray logs worker-7e48cd46c72821b17be65b05a4e61b5846ba998a57d76db44ca4f8fa*out -ip 115.145.171.128. Top 10 memory users:
PID	MEM(GB)	COMMAND
4942	3.48	/snap/pycharm-professional/408/jbr/bin/java -classpath /snap/pycharm-professional/408/lib/platform-l...
616443	2.14	/home/abdenour/miniconda3/envs/fl_with_flower/bin/python /home/abdenour/PycharmProjects/fl_with_flow...
619615	1.90	ray::ClientAppActor
619616	1.88	ray::ClientAppActor
619611	1.87	ray::ClientAppActor
619617	1.86	ray::ClientAppActor
619620	1.85	ray::ClientAppActor
619618	1.84	ray::ClientAppActor
619613	1.82	ray::ClientAppActor
619619	1.80	ray::ClientAppActor
Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.

[2024-08-13 23:54:38,174][flwr][ERROR] - Task was killed due to the node running low on memory.
Memory on the node (IP: 115.145.171.128, ID: c687613ff34cdbf5097699e5c896055ef2dc44343c5ab44bd40309d5) where the task (actor ID: 85928d3f1a247800081c0e1101000000, name=ClientAppActor.__init__, pid=619614, memory used=1.73GB) was running was 29.50GB / 31.02GB (0.951049), which exceeds the memory usage threshold of 0.95. Ray killed this worker (ID: c7f644ac3cca20dffbfa1723daac4f87f04c7de03da25e7cad5fe33c) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 115.145.171.128`. To see the logs of the worker, use `ray logs worker-c7f644ac3cca20dffbfa1723daac4f87f04c7de03da25e7cad5fe33c*out -ip 115.145.171.128. Top 10 memory users:
PID	MEM(GB)	COMMAND
4942	3.49	/snap/pycharm-professional/408/jbr/bin/java -classpath /snap/pycharm-professional/408/lib/platform-l...
619616	1.80	ray::ClientAppActor.run
619615	1.78	ray::ClientAppActor
619617	1.77	ray::ClientAppActor
619611	1.73	ray::ClientAppActor
619618	1.73	ray::ClientAppActor
619614	1.73	ray::ClientAppActor
619612	1.72	ray::ClientAppActor
619619	1.70	ray::ClientAppActor
619620	1.68	ray::ClientAppActor.run
Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.
[2024-08-13 23:54:38,174][flwr][ERROR] - Task was killed due to the node running low on memory.
Memory on the node (IP: 115.145.171.128, ID: c687613ff34cdbf5097699e5c896055ef2dc44343c5ab44bd40309d5) where the task (actor ID: 52365bb7997979e5a4c7ddbd01000000, name=ClientAppActor.__init__, pid=619618, memory used=1.84GB) was running was 29.49GB / 31.02GB (0.950707), which exceeds the memory usage threshold of 0.95. Ray killed this worker (ID: 7e48cd46c72821b17be65b05a4e61b5846ba998a57d76db44ca4f8fa) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 115.145.171.128`. To see the logs of the worker, use `ray logs worker-7e48cd46c72821b17be65b05a4e61b5846ba998a57d76db44ca4f8fa*out -ip 115.145.171.128. Top 10 memory users:
PID	MEM(GB)	COMMAND
4942	3.48	/snap/pycharm-professional/408/jbr/bin/java -classpath /snap/pycharm-professional/408/lib/platform-l...
616443	2.14	/home/abdenour/miniconda3/envs/fl_with_flower/bin/python /home/abdenour/PycharmProjects/fl_with_flow...
619615	1.90	ray::ClientAppActor
619616	1.88	ray::ClientAppActor
619611	1.87	ray::ClientAppActor
619617	1.86	ray::ClientAppActor
619620	1.85	ray::ClientAppActor
619618	1.84	ray::ClientAppActor
619613	1.82	ray::ClientAppActor
619619	1.80	ray::ClientAppActor
Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.
[2024-08-13 23:54:39,757][flwr][INFO] - aggregate_fit: received 8 results and 2 failures
[2024-08-13 23:54:42,638][flwr][INFO] - fit progress: (243, 15.429546236991882, {'accuracy': 0.7548076923076923}, 1403.898233133019)
[2024-08-13 23:54:42,638][flwr][INFO] - configure_evaluate: strategy sampled 2 clients (out of 20)
[2024-08-13 23:54:42,846][flwr][INFO] - aggregate_evaluate: received 2 results and 0 failures
[2024-08-13 23:54:42,846][flwr][INFO] - 
[2024-08-13 23:54:42,846][flwr][INFO] - [ROUND 244]
[2024-08-13 23:54:42,846][flwr][INFO] - configure_fit: strategy sampled 10 clients (out of 20)
[2024-08-13 23:54:43,012][flwr][ERROR] - Traceback (most recent call last):
  File "/home/abdenour/miniconda3/envs/fl_with_flower/lib/python3.11/site-packages/flwr/simulation/ray_transport/ray_client_proxy.py", line 94, in _submit_job
    out_mssg, updated_context = self.actor_pool.get_client_result(
                                ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/abdenour/miniconda3/envs/fl_with_flower/lib/python3.11/site-packages/flwr/simulation/ray_transport/ray_actor.py", line 398, in get_client_result
    return self._fetch_future_result(cid)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/abdenour/miniconda3/envs/fl_with_flower/lib/python3.11/site-packages/flwr/simulation/ray_transport/ray_actor.py", line 279, in _fetch_future_result
    res_cid, out_mssg, updated_context = ray.get(
                                         ^^^^^^^^
  File "/home/abdenour/miniconda3/envs/fl_with_flower/lib/python3.11/site-packages/ray/_private/auto_init_hook.py", line 21, in auto_init_wrapper
    return fn(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^
  File "/home/abdenour/miniconda3/envs/fl_with_flower/lib/python3.11/site-packages/ray/_private/client_mode_hook.py", line 103, in wrapper
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "/home/abdenour/miniconda3/envs/fl_with_flower/lib/python3.11/site-packages/ray/_private/worker.py", line 2667, in get
    values, debugger_breakpoint = worker.get_objects(object_refs, timeout=timeout)
                                  ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/abdenour/miniconda3/envs/fl_with_flower/lib/python3.11/site-packages/ray/_private/worker.py", line 866, in get_objects
    raise value
ray.exceptions.OutOfMemoryError: Task was killed due to the node running low on memory.
Memory on the node (IP: 115.145.171.128, ID: c687613ff34cdbf5097699e5c896055ef2dc44343c5ab44bd40309d5) where the task (actor ID: 85928d3f1a247800081c0e1101000000, name=ClientAppActor.__init__, pid=619614, memory used=1.73GB) was running was 29.50GB / 31.02GB (0.951049), which exceeds the memory usage threshold of 0.95. Ray killed this worker (ID: c7f644ac3cca20dffbfa1723daac4f87f04c7de03da25e7cad5fe33c) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 115.145.171.128`. To see the logs of the worker, use `ray logs worker-c7f644ac3cca20dffbfa1723daac4f87f04c7de03da25e7cad5fe33c*out -ip 115.145.171.128. Top 10 memory users:
PID	MEM(GB)	COMMAND
4942	3.49	/snap/pycharm-professional/408/jbr/bin/java -classpath /snap/pycharm-professional/408/lib/platform-l...
619616	1.80	ray::ClientAppActor.run
619615	1.78	ray::ClientAppActor
619617	1.77	ray::ClientAppActor
619611	1.73	ray::ClientAppActor
619618	1.73	ray::ClientAppActor
619614	1.73	ray::ClientAppActor
619612	1.72	ray::ClientAppActor
619619	1.70	ray::ClientAppActor
619620	1.68	ray::ClientAppActor.run
Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.

[2024-08-13 23:54:43,012][flwr][ERROR] - Task was killed due to the node running low on memory.
Memory on the node (IP: 115.145.171.128, ID: c687613ff34cdbf5097699e5c896055ef2dc44343c5ab44bd40309d5) where the task (actor ID: 85928d3f1a247800081c0e1101000000, name=ClientAppActor.__init__, pid=619614, memory used=1.73GB) was running was 29.50GB / 31.02GB (0.951049), which exceeds the memory usage threshold of 0.95. Ray killed this worker (ID: c7f644ac3cca20dffbfa1723daac4f87f04c7de03da25e7cad5fe33c) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 115.145.171.128`. To see the logs of the worker, use `ray logs worker-c7f644ac3cca20dffbfa1723daac4f87f04c7de03da25e7cad5fe33c*out -ip 115.145.171.128. Top 10 memory users:
PID	MEM(GB)	COMMAND
4942	3.49	/snap/pycharm-professional/408/jbr/bin/java -classpath /snap/pycharm-professional/408/lib/platform-l...
619616	1.80	ray::ClientAppActor.run
619615	1.78	ray::ClientAppActor
619617	1.77	ray::ClientAppActor
619611	1.73	ray::ClientAppActor
619618	1.73	ray::ClientAppActor
619614	1.73	ray::ClientAppActor
619612	1.72	ray::ClientAppActor
619619	1.70	ray::ClientAppActor
619620	1.68	ray::ClientAppActor.run
Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.
[2024-08-13 23:54:43,012][flwr][ERROR] - Traceback (most recent call last):
  File "/home/abdenour/miniconda3/envs/fl_with_flower/lib/python3.11/site-packages/flwr/simulation/ray_transport/ray_client_proxy.py", line 94, in _submit_job
    out_mssg, updated_context = self.actor_pool.get_client_result(
                                ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/abdenour/miniconda3/envs/fl_with_flower/lib/python3.11/site-packages/flwr/simulation/ray_transport/ray_actor.py", line 398, in get_client_result
    return self._fetch_future_result(cid)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/abdenour/miniconda3/envs/fl_with_flower/lib/python3.11/site-packages/flwr/simulation/ray_transport/ray_actor.py", line 279, in _fetch_future_result
    res_cid, out_mssg, updated_context = ray.get(
                                         ^^^^^^^^
  File "/home/abdenour/miniconda3/envs/fl_with_flower/lib/python3.11/site-packages/ray/_private/auto_init_hook.py", line 21, in auto_init_wrapper
    return fn(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^
  File "/home/abdenour/miniconda3/envs/fl_with_flower/lib/python3.11/site-packages/ray/_private/client_mode_hook.py", line 103, in wrapper
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "/home/abdenour/miniconda3/envs/fl_with_flower/lib/python3.11/site-packages/ray/_private/worker.py", line 2667, in get
    values, debugger_breakpoint = worker.get_objects(object_refs, timeout=timeout)
                                  ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/abdenour/miniconda3/envs/fl_with_flower/lib/python3.11/site-packages/ray/_private/worker.py", line 866, in get_objects
    raise value
ray.exceptions.OutOfMemoryError: Task was killed due to the node running low on memory.
Memory on the node (IP: 115.145.171.128, ID: c687613ff34cdbf5097699e5c896055ef2dc44343c5ab44bd40309d5) where the task (actor ID: 52365bb7997979e5a4c7ddbd01000000, name=ClientAppActor.__init__, pid=619618, memory used=1.84GB) was running was 29.49GB / 31.02GB (0.950707), which exceeds the memory usage threshold of 0.95. Ray killed this worker (ID: 7e48cd46c72821b17be65b05a4e61b5846ba998a57d76db44ca4f8fa) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 115.145.171.128`. To see the logs of the worker, use `ray logs worker-7e48cd46c72821b17be65b05a4e61b5846ba998a57d76db44ca4f8fa*out -ip 115.145.171.128. Top 10 memory users:
PID	MEM(GB)	COMMAND
4942	3.48	/snap/pycharm-professional/408/jbr/bin/java -classpath /snap/pycharm-professional/408/lib/platform-l...
616443	2.14	/home/abdenour/miniconda3/envs/fl_with_flower/bin/python /home/abdenour/PycharmProjects/fl_with_flow...
619615	1.90	ray::ClientAppActor
619616	1.88	ray::ClientAppActor
619611	1.87	ray::ClientAppActor
619617	1.86	ray::ClientAppActor
619620	1.85	ray::ClientAppActor
619618	1.84	ray::ClientAppActor
619613	1.82	ray::ClientAppActor
619619	1.80	ray::ClientAppActor
Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.

[2024-08-13 23:54:43,013][flwr][ERROR] - Task was killed due to the node running low on memory.
Memory on the node (IP: 115.145.171.128, ID: c687613ff34cdbf5097699e5c896055ef2dc44343c5ab44bd40309d5) where the task (actor ID: 52365bb7997979e5a4c7ddbd01000000, name=ClientAppActor.__init__, pid=619618, memory used=1.84GB) was running was 29.49GB / 31.02GB (0.950707), which exceeds the memory usage threshold of 0.95. Ray killed this worker (ID: 7e48cd46c72821b17be65b05a4e61b5846ba998a57d76db44ca4f8fa) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 115.145.171.128`. To see the logs of the worker, use `ray logs worker-7e48cd46c72821b17be65b05a4e61b5846ba998a57d76db44ca4f8fa*out -ip 115.145.171.128. Top 10 memory users:
PID	MEM(GB)	COMMAND
4942	3.48	/snap/pycharm-professional/408/jbr/bin/java -classpath /snap/pycharm-professional/408/lib/platform-l...
616443	2.14	/home/abdenour/miniconda3/envs/fl_with_flower/bin/python /home/abdenour/PycharmProjects/fl_with_flow...
619615	1.90	ray::ClientAppActor
619616	1.88	ray::ClientAppActor
619611	1.87	ray::ClientAppActor
619617	1.86	ray::ClientAppActor
619620	1.85	ray::ClientAppActor
619618	1.84	ray::ClientAppActor
619613	1.82	ray::ClientAppActor
619619	1.80	ray::ClientAppActor
Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.
[2024-08-13 23:54:44,715][flwr][INFO] - aggregate_fit: received 8 results and 2 failures
[2024-08-13 23:54:47,657][flwr][INFO] - fit progress: (244, 15.998169422149658, {'accuracy': 0.7516025641025641}, 1408.917609608994)
[2024-08-13 23:54:47,688][flwr][INFO] - configure_evaluate: strategy sampled 2 clients (out of 20)
[2024-08-13 23:54:47,989][flwr][INFO] - aggregate_evaluate: received 2 results and 0 failures
[2024-08-13 23:54:47,989][flwr][INFO] - 
[2024-08-13 23:54:47,989][flwr][INFO] - [ROUND 245]
[2024-08-13 23:54:47,989][flwr][INFO] - configure_fit: strategy sampled 10 clients (out of 20)
[2024-08-13 23:54:48,133][flwr][ERROR] - Traceback (most recent call last):
  File "/home/abdenour/miniconda3/envs/fl_with_flower/lib/python3.11/site-packages/flwr/simulation/ray_transport/ray_client_proxy.py", line 94, in _submit_job
    out_mssg, updated_context = self.actor_pool.get_client_result(
                                ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/abdenour/miniconda3/envs/fl_with_flower/lib/python3.11/site-packages/flwr/simulation/ray_transport/ray_actor.py", line 398, in get_client_result
    return self._fetch_future_result(cid)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/abdenour/miniconda3/envs/fl_with_flower/lib/python3.11/site-packages/flwr/simulation/ray_transport/ray_actor.py", line 279, in _fetch_future_result
    res_cid, out_mssg, updated_context = ray.get(
                                         ^^^^^^^^
  File "/home/abdenour/miniconda3/envs/fl_with_flower/lib/python3.11/site-packages/ray/_private/auto_init_hook.py", line 21, in auto_init_wrapper
    return fn(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^
  File "/home/abdenour/miniconda3/envs/fl_with_flower/lib/python3.11/site-packages/ray/_private/client_mode_hook.py", line 103, in wrapper
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "/home/abdenour/miniconda3/envs/fl_with_flower/lib/python3.11/site-packages/ray/_private/worker.py", line 2667, in get
    values, debugger_breakpoint = worker.get_objects(object_refs, timeout=timeout)
                                  ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/abdenour/miniconda3/envs/fl_with_flower/lib/python3.11/site-packages/ray/_private/worker.py", line 866, in get_objects
    raise value
ray.exceptions.OutOfMemoryError: Task was killed due to the node running low on memory.
Memory on the node (IP: 115.145.171.128, ID: c687613ff34cdbf5097699e5c896055ef2dc44343c5ab44bd40309d5) where the task (actor ID: 85928d3f1a247800081c0e1101000000, name=ClientAppActor.__init__, pid=619614, memory used=1.73GB) was running was 29.50GB / 31.02GB (0.951049), which exceeds the memory usage threshold of 0.95. Ray killed this worker (ID: c7f644ac3cca20dffbfa1723daac4f87f04c7de03da25e7cad5fe33c) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 115.145.171.128`. To see the logs of the worker, use `ray logs worker-c7f644ac3cca20dffbfa1723daac4f87f04c7de03da25e7cad5fe33c*out -ip 115.145.171.128. Top 10 memory users:
PID	MEM(GB)	COMMAND
4942	3.49	/snap/pycharm-professional/408/jbr/bin/java -classpath /snap/pycharm-professional/408/lib/platform-l...
619616	1.80	ray::ClientAppActor.run
619615	1.78	ray::ClientAppActor
619617	1.77	ray::ClientAppActor
619611	1.73	ray::ClientAppActor
619618	1.73	ray::ClientAppActor
619614	1.73	ray::ClientAppActor
619612	1.72	ray::ClientAppActor
619619	1.70	ray::ClientAppActor
619620	1.68	ray::ClientAppActor.run
Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.

[2024-08-13 23:54:48,133][flwr][ERROR] - Task was killed due to the node running low on memory.
Memory on the node (IP: 115.145.171.128, ID: c687613ff34cdbf5097699e5c896055ef2dc44343c5ab44bd40309d5) where the task (actor ID: 85928d3f1a247800081c0e1101000000, name=ClientAppActor.__init__, pid=619614, memory used=1.73GB) was running was 29.50GB / 31.02GB (0.951049), which exceeds the memory usage threshold of 0.95. Ray killed this worker (ID: c7f644ac3cca20dffbfa1723daac4f87f04c7de03da25e7cad5fe33c) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 115.145.171.128`. To see the logs of the worker, use `ray logs worker-c7f644ac3cca20dffbfa1723daac4f87f04c7de03da25e7cad5fe33c*out -ip 115.145.171.128. Top 10 memory users:
PID	MEM(GB)	COMMAND
4942	3.49	/snap/pycharm-professional/408/jbr/bin/java -classpath /snap/pycharm-professional/408/lib/platform-l...
619616	1.80	ray::ClientAppActor.run
619615	1.78	ray::ClientAppActor
619617	1.77	ray::ClientAppActor
619611	1.73	ray::ClientAppActor
619618	1.73	ray::ClientAppActor
619614	1.73	ray::ClientAppActor
619612	1.72	ray::ClientAppActor
619619	1.70	ray::ClientAppActor
619620	1.68	ray::ClientAppActor.run
Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.
[2024-08-13 23:54:48,134][flwr][ERROR] - Traceback (most recent call last):
  File "/home/abdenour/miniconda3/envs/fl_with_flower/lib/python3.11/site-packages/flwr/simulation/ray_transport/ray_client_proxy.py", line 94, in _submit_job
    out_mssg, updated_context = self.actor_pool.get_client_result(
                                ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/abdenour/miniconda3/envs/fl_with_flower/lib/python3.11/site-packages/flwr/simulation/ray_transport/ray_actor.py", line 398, in get_client_result
    return self._fetch_future_result(cid)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/abdenour/miniconda3/envs/fl_with_flower/lib/python3.11/site-packages/flwr/simulation/ray_transport/ray_actor.py", line 279, in _fetch_future_result
    res_cid, out_mssg, updated_context = ray.get(
                                         ^^^^^^^^
  File "/home/abdenour/miniconda3/envs/fl_with_flower/lib/python3.11/site-packages/ray/_private/auto_init_hook.py", line 21, in auto_init_wrapper
    return fn(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^
  File "/home/abdenour/miniconda3/envs/fl_with_flower/lib/python3.11/site-packages/ray/_private/client_mode_hook.py", line 103, in wrapper
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "/home/abdenour/miniconda3/envs/fl_with_flower/lib/python3.11/site-packages/ray/_private/worker.py", line 2667, in get
    values, debugger_breakpoint = worker.get_objects(object_refs, timeout=timeout)
                                  ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/abdenour/miniconda3/envs/fl_with_flower/lib/python3.11/site-packages/ray/_private/worker.py", line 866, in get_objects
    raise value
ray.exceptions.OutOfMemoryError: Task was killed due to the node running low on memory.
Memory on the node (IP: 115.145.171.128, ID: c687613ff34cdbf5097699e5c896055ef2dc44343c5ab44bd40309d5) where the task (actor ID: 52365bb7997979e5a4c7ddbd01000000, name=ClientAppActor.__init__, pid=619618, memory used=1.84GB) was running was 29.49GB / 31.02GB (0.950707), which exceeds the memory usage threshold of 0.95. Ray killed this worker (ID: 7e48cd46c72821b17be65b05a4e61b5846ba998a57d76db44ca4f8fa) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 115.145.171.128`. To see the logs of the worker, use `ray logs worker-7e48cd46c72821b17be65b05a4e61b5846ba998a57d76db44ca4f8fa*out -ip 115.145.171.128. Top 10 memory users:
PID	MEM(GB)	COMMAND
4942	3.48	/snap/pycharm-professional/408/jbr/bin/java -classpath /snap/pycharm-professional/408/lib/platform-l...
616443	2.14	/home/abdenour/miniconda3/envs/fl_with_flower/bin/python /home/abdenour/PycharmProjects/fl_with_flow...
619615	1.90	ray::ClientAppActor
619616	1.88	ray::ClientAppActor
619611	1.87	ray::ClientAppActor
619617	1.86	ray::ClientAppActor
619620	1.85	ray::ClientAppActor
619618	1.84	ray::ClientAppActor
619613	1.82	ray::ClientAppActor
619619	1.80	ray::ClientAppActor
Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.

[2024-08-13 23:54:48,134][flwr][ERROR] - Task was killed due to the node running low on memory.
Memory on the node (IP: 115.145.171.128, ID: c687613ff34cdbf5097699e5c896055ef2dc44343c5ab44bd40309d5) where the task (actor ID: 52365bb7997979e5a4c7ddbd01000000, name=ClientAppActor.__init__, pid=619618, memory used=1.84GB) was running was 29.49GB / 31.02GB (0.950707), which exceeds the memory usage threshold of 0.95. Ray killed this worker (ID: 7e48cd46c72821b17be65b05a4e61b5846ba998a57d76db44ca4f8fa) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 115.145.171.128`. To see the logs of the worker, use `ray logs worker-7e48cd46c72821b17be65b05a4e61b5846ba998a57d76db44ca4f8fa*out -ip 115.145.171.128. Top 10 memory users:
PID	MEM(GB)	COMMAND
4942	3.48	/snap/pycharm-professional/408/jbr/bin/java -classpath /snap/pycharm-professional/408/lib/platform-l...
616443	2.14	/home/abdenour/miniconda3/envs/fl_with_flower/bin/python /home/abdenour/PycharmProjects/fl_with_flow...
619615	1.90	ray::ClientAppActor
619616	1.88	ray::ClientAppActor
619611	1.87	ray::ClientAppActor
619617	1.86	ray::ClientAppActor
619620	1.85	ray::ClientAppActor
619618	1.84	ray::ClientAppActor
619613	1.82	ray::ClientAppActor
619619	1.80	ray::ClientAppActor
Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.
[2024-08-13 23:54:49,727][flwr][INFO] - aggregate_fit: received 8 results and 2 failures
[2024-08-13 23:54:52,547][flwr][INFO] - fit progress: (245, 15.780640542507172, {'accuracy': 0.7532051282051282}, 1413.8070108320098)
[2024-08-13 23:54:52,547][flwr][INFO] - configure_evaluate: strategy sampled 2 clients (out of 20)
[2024-08-13 23:54:52,755][flwr][INFO] - aggregate_evaluate: received 2 results and 0 failures
[2024-08-13 23:54:52,755][flwr][INFO] - 
[2024-08-13 23:54:52,755][flwr][INFO] - [ROUND 246]
[2024-08-13 23:54:52,755][flwr][INFO] - configure_fit: strategy sampled 10 clients (out of 20)
[2024-08-13 23:54:52,927][flwr][ERROR] - Traceback (most recent call last):
  File "/home/abdenour/miniconda3/envs/fl_with_flower/lib/python3.11/site-packages/flwr/simulation/ray_transport/ray_client_proxy.py", line 94, in _submit_job
    out_mssg, updated_context = self.actor_pool.get_client_result(
                                ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/abdenour/miniconda3/envs/fl_with_flower/lib/python3.11/site-packages/flwr/simulation/ray_transport/ray_actor.py", line 398, in get_client_result
    return self._fetch_future_result(cid)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/abdenour/miniconda3/envs/fl_with_flower/lib/python3.11/site-packages/flwr/simulation/ray_transport/ray_actor.py", line 279, in _fetch_future_result
    res_cid, out_mssg, updated_context = ray.get(
                                         ^^^^^^^^
  File "/home/abdenour/miniconda3/envs/fl_with_flower/lib/python3.11/site-packages/ray/_private/auto_init_hook.py", line 21, in auto_init_wrapper
    return fn(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^
  File "/home/abdenour/miniconda3/envs/fl_with_flower/lib/python3.11/site-packages/ray/_private/client_mode_hook.py", line 103, in wrapper
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "/home/abdenour/miniconda3/envs/fl_with_flower/lib/python3.11/site-packages/ray/_private/worker.py", line 2667, in get
    values, debugger_breakpoint = worker.get_objects(object_refs, timeout=timeout)
                                  ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/abdenour/miniconda3/envs/fl_with_flower/lib/python3.11/site-packages/ray/_private/worker.py", line 866, in get_objects
    raise value
ray.exceptions.OutOfMemoryError: Task was killed due to the node running low on memory.
Memory on the node (IP: 115.145.171.128, ID: c687613ff34cdbf5097699e5c896055ef2dc44343c5ab44bd40309d5) where the task (actor ID: 52365bb7997979e5a4c7ddbd01000000, name=ClientAppActor.__init__, pid=619618, memory used=1.84GB) was running was 29.49GB / 31.02GB (0.950707), which exceeds the memory usage threshold of 0.95. Ray killed this worker (ID: 7e48cd46c72821b17be65b05a4e61b5846ba998a57d76db44ca4f8fa) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 115.145.171.128`. To see the logs of the worker, use `ray logs worker-7e48cd46c72821b17be65b05a4e61b5846ba998a57d76db44ca4f8fa*out -ip 115.145.171.128. Top 10 memory users:
PID	MEM(GB)	COMMAND
4942	3.48	/snap/pycharm-professional/408/jbr/bin/java -classpath /snap/pycharm-professional/408/lib/platform-l...
616443	2.14	/home/abdenour/miniconda3/envs/fl_with_flower/bin/python /home/abdenour/PycharmProjects/fl_with_flow...
619615	1.90	ray::ClientAppActor
619616	1.88	ray::ClientAppActor
619611	1.87	ray::ClientAppActor
619617	1.86	ray::ClientAppActor
619620	1.85	ray::ClientAppActor
619618	1.84	ray::ClientAppActor
619613	1.82	ray::ClientAppActor
619619	1.80	ray::ClientAppActor
Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.

[2024-08-13 23:54:52,927][flwr][ERROR] - Task was killed due to the node running low on memory.
Memory on the node (IP: 115.145.171.128, ID: c687613ff34cdbf5097699e5c896055ef2dc44343c5ab44bd40309d5) where the task (actor ID: 52365bb7997979e5a4c7ddbd01000000, name=ClientAppActor.__init__, pid=619618, memory used=1.84GB) was running was 29.49GB / 31.02GB (0.950707), which exceeds the memory usage threshold of 0.95. Ray killed this worker (ID: 7e48cd46c72821b17be65b05a4e61b5846ba998a57d76db44ca4f8fa) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 115.145.171.128`. To see the logs of the worker, use `ray logs worker-7e48cd46c72821b17be65b05a4e61b5846ba998a57d76db44ca4f8fa*out -ip 115.145.171.128. Top 10 memory users:
PID	MEM(GB)	COMMAND
4942	3.48	/snap/pycharm-professional/408/jbr/bin/java -classpath /snap/pycharm-professional/408/lib/platform-l...
616443	2.14	/home/abdenour/miniconda3/envs/fl_with_flower/bin/python /home/abdenour/PycharmProjects/fl_with_flow...
619615	1.90	ray::ClientAppActor
619616	1.88	ray::ClientAppActor
619611	1.87	ray::ClientAppActor
619617	1.86	ray::ClientAppActor
619620	1.85	ray::ClientAppActor
619618	1.84	ray::ClientAppActor
619613	1.82	ray::ClientAppActor
619619	1.80	ray::ClientAppActor
Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.
[2024-08-13 23:54:52,928][flwr][ERROR] - Traceback (most recent call last):
  File "/home/abdenour/miniconda3/envs/fl_with_flower/lib/python3.11/site-packages/flwr/simulation/ray_transport/ray_client_proxy.py", line 94, in _submit_job
    out_mssg, updated_context = self.actor_pool.get_client_result(
                                ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/abdenour/miniconda3/envs/fl_with_flower/lib/python3.11/site-packages/flwr/simulation/ray_transport/ray_actor.py", line 398, in get_client_result
    return self._fetch_future_result(cid)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/abdenour/miniconda3/envs/fl_with_flower/lib/python3.11/site-packages/flwr/simulation/ray_transport/ray_actor.py", line 279, in _fetch_future_result
    res_cid, out_mssg, updated_context = ray.get(
                                         ^^^^^^^^
  File "/home/abdenour/miniconda3/envs/fl_with_flower/lib/python3.11/site-packages/ray/_private/auto_init_hook.py", line 21, in auto_init_wrapper
    return fn(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^
  File "/home/abdenour/miniconda3/envs/fl_with_flower/lib/python3.11/site-packages/ray/_private/client_mode_hook.py", line 103, in wrapper
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "/home/abdenour/miniconda3/envs/fl_with_flower/lib/python3.11/site-packages/ray/_private/worker.py", line 2667, in get
    values, debugger_breakpoint = worker.get_objects(object_refs, timeout=timeout)
                                  ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/abdenour/miniconda3/envs/fl_with_flower/lib/python3.11/site-packages/ray/_private/worker.py", line 866, in get_objects
    raise value
ray.exceptions.OutOfMemoryError: Task was killed due to the node running low on memory.
Memory on the node (IP: 115.145.171.128, ID: c687613ff34cdbf5097699e5c896055ef2dc44343c5ab44bd40309d5) where the task (actor ID: 85928d3f1a247800081c0e1101000000, name=ClientAppActor.__init__, pid=619614, memory used=1.73GB) was running was 29.50GB / 31.02GB (0.951049), which exceeds the memory usage threshold of 0.95. Ray killed this worker (ID: c7f644ac3cca20dffbfa1723daac4f87f04c7de03da25e7cad5fe33c) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 115.145.171.128`. To see the logs of the worker, use `ray logs worker-c7f644ac3cca20dffbfa1723daac4f87f04c7de03da25e7cad5fe33c*out -ip 115.145.171.128. Top 10 memory users:
PID	MEM(GB)	COMMAND
4942	3.49	/snap/pycharm-professional/408/jbr/bin/java -classpath /snap/pycharm-professional/408/lib/platform-l...
619616	1.80	ray::ClientAppActor.run
619615	1.78	ray::ClientAppActor
619617	1.77	ray::ClientAppActor
619611	1.73	ray::ClientAppActor
619618	1.73	ray::ClientAppActor
619614	1.73	ray::ClientAppActor
619612	1.72	ray::ClientAppActor
619619	1.70	ray::ClientAppActor
619620	1.68	ray::ClientAppActor.run
Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.

[2024-08-13 23:54:52,928][flwr][ERROR] - Task was killed due to the node running low on memory.
Memory on the node (IP: 115.145.171.128, ID: c687613ff34cdbf5097699e5c896055ef2dc44343c5ab44bd40309d5) where the task (actor ID: 85928d3f1a247800081c0e1101000000, name=ClientAppActor.__init__, pid=619614, memory used=1.73GB) was running was 29.50GB / 31.02GB (0.951049), which exceeds the memory usage threshold of 0.95. Ray killed this worker (ID: c7f644ac3cca20dffbfa1723daac4f87f04c7de03da25e7cad5fe33c) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 115.145.171.128`. To see the logs of the worker, use `ray logs worker-c7f644ac3cca20dffbfa1723daac4f87f04c7de03da25e7cad5fe33c*out -ip 115.145.171.128. Top 10 memory users:
PID	MEM(GB)	COMMAND
4942	3.49	/snap/pycharm-professional/408/jbr/bin/java -classpath /snap/pycharm-professional/408/lib/platform-l...
619616	1.80	ray::ClientAppActor.run
619615	1.78	ray::ClientAppActor
619617	1.77	ray::ClientAppActor
619611	1.73	ray::ClientAppActor
619618	1.73	ray::ClientAppActor
619614	1.73	ray::ClientAppActor
619612	1.72	ray::ClientAppActor
619619	1.70	ray::ClientAppActor
619620	1.68	ray::ClientAppActor.run
Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.
[2024-08-13 23:54:54,552][flwr][INFO] - aggregate_fit: received 8 results and 2 failures
[2024-08-13 23:54:57,729][flwr][INFO] - fit progress: (246, 17.335886895656586, {'accuracy': 0.7467948717948718}, 1418.989196348004)
[2024-08-13 23:54:57,729][flwr][INFO] - configure_evaluate: strategy sampled 2 clients (out of 20)
[2024-08-13 23:54:57,918][flwr][INFO] - aggregate_evaluate: received 2 results and 0 failures
[2024-08-13 23:54:57,918][flwr][INFO] - 
[2024-08-13 23:54:57,918][flwr][INFO] - [ROUND 247]
[2024-08-13 23:54:57,918][flwr][INFO] - configure_fit: strategy sampled 10 clients (out of 20)
[2024-08-13 23:54:58,070][flwr][ERROR] - Traceback (most recent call last):
  File "/home/abdenour/miniconda3/envs/fl_with_flower/lib/python3.11/site-packages/flwr/simulation/ray_transport/ray_client_proxy.py", line 94, in _submit_job
    out_mssg, updated_context = self.actor_pool.get_client_result(
                                ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/abdenour/miniconda3/envs/fl_with_flower/lib/python3.11/site-packages/flwr/simulation/ray_transport/ray_actor.py", line 398, in get_client_result
    return self._fetch_future_result(cid)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/abdenour/miniconda3/envs/fl_with_flower/lib/python3.11/site-packages/flwr/simulation/ray_transport/ray_actor.py", line 279, in _fetch_future_result
    res_cid, out_mssg, updated_context = ray.get(
                                         ^^^^^^^^
  File "/home/abdenour/miniconda3/envs/fl_with_flower/lib/python3.11/site-packages/ray/_private/auto_init_hook.py", line 21, in auto_init_wrapper
    return fn(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^
  File "/home/abdenour/miniconda3/envs/fl_with_flower/lib/python3.11/site-packages/ray/_private/client_mode_hook.py", line 103, in wrapper
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "/home/abdenour/miniconda3/envs/fl_with_flower/lib/python3.11/site-packages/ray/_private/worker.py", line 2667, in get
    values, debugger_breakpoint = worker.get_objects(object_refs, timeout=timeout)
                                  ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/abdenour/miniconda3/envs/fl_with_flower/lib/python3.11/site-packages/ray/_private/worker.py", line 866, in get_objects
    raise value
ray.exceptions.OutOfMemoryError: Task was killed due to the node running low on memory.
Memory on the node (IP: 115.145.171.128, ID: c687613ff34cdbf5097699e5c896055ef2dc44343c5ab44bd40309d5) where the task (actor ID: 85928d3f1a247800081c0e1101000000, name=ClientAppActor.__init__, pid=619614, memory used=1.73GB) was running was 29.50GB / 31.02GB (0.951049), which exceeds the memory usage threshold of 0.95. Ray killed this worker (ID: c7f644ac3cca20dffbfa1723daac4f87f04c7de03da25e7cad5fe33c) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 115.145.171.128`. To see the logs of the worker, use `ray logs worker-c7f644ac3cca20dffbfa1723daac4f87f04c7de03da25e7cad5fe33c*out -ip 115.145.171.128. Top 10 memory users:
PID	MEM(GB)	COMMAND
4942	3.49	/snap/pycharm-professional/408/jbr/bin/java -classpath /snap/pycharm-professional/408/lib/platform-l...
619616	1.80	ray::ClientAppActor.run
619615	1.78	ray::ClientAppActor
619617	1.77	ray::ClientAppActor
619611	1.73	ray::ClientAppActor
619618	1.73	ray::ClientAppActor
619614	1.73	ray::ClientAppActor
619612	1.72	ray::ClientAppActor
619619	1.70	ray::ClientAppActor
619620	1.68	ray::ClientAppActor.run
Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.

[2024-08-13 23:54:58,070][flwr][ERROR] - Task was killed due to the node running low on memory.
Memory on the node (IP: 115.145.171.128, ID: c687613ff34cdbf5097699e5c896055ef2dc44343c5ab44bd40309d5) where the task (actor ID: 85928d3f1a247800081c0e1101000000, name=ClientAppActor.__init__, pid=619614, memory used=1.73GB) was running was 29.50GB / 31.02GB (0.951049), which exceeds the memory usage threshold of 0.95. Ray killed this worker (ID: c7f644ac3cca20dffbfa1723daac4f87f04c7de03da25e7cad5fe33c) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 115.145.171.128`. To see the logs of the worker, use `ray logs worker-c7f644ac3cca20dffbfa1723daac4f87f04c7de03da25e7cad5fe33c*out -ip 115.145.171.128. Top 10 memory users:
PID	MEM(GB)	COMMAND
4942	3.49	/snap/pycharm-professional/408/jbr/bin/java -classpath /snap/pycharm-professional/408/lib/platform-l...
619616	1.80	ray::ClientAppActor.run
619615	1.78	ray::ClientAppActor
619617	1.77	ray::ClientAppActor
619611	1.73	ray::ClientAppActor
619618	1.73	ray::ClientAppActor
619614	1.73	ray::ClientAppActor
619612	1.72	ray::ClientAppActor
619619	1.70	ray::ClientAppActor
619620	1.68	ray::ClientAppActor.run
Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.
[2024-08-13 23:54:58,070][flwr][ERROR] - Traceback (most recent call last):
  File "/home/abdenour/miniconda3/envs/fl_with_flower/lib/python3.11/site-packages/flwr/simulation/ray_transport/ray_client_proxy.py", line 94, in _submit_job
    out_mssg, updated_context = self.actor_pool.get_client_result(
                                ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/abdenour/miniconda3/envs/fl_with_flower/lib/python3.11/site-packages/flwr/simulation/ray_transport/ray_actor.py", line 398, in get_client_result
    return self._fetch_future_result(cid)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/abdenour/miniconda3/envs/fl_with_flower/lib/python3.11/site-packages/flwr/simulation/ray_transport/ray_actor.py", line 279, in _fetch_future_result
    res_cid, out_mssg, updated_context = ray.get(
                                         ^^^^^^^^
  File "/home/abdenour/miniconda3/envs/fl_with_flower/lib/python3.11/site-packages/ray/_private/auto_init_hook.py", line 21, in auto_init_wrapper
    return fn(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^
  File "/home/abdenour/miniconda3/envs/fl_with_flower/lib/python3.11/site-packages/ray/_private/client_mode_hook.py", line 103, in wrapper
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "/home/abdenour/miniconda3/envs/fl_with_flower/lib/python3.11/site-packages/ray/_private/worker.py", line 2667, in get
    values, debugger_breakpoint = worker.get_objects(object_refs, timeout=timeout)
                                  ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/abdenour/miniconda3/envs/fl_with_flower/lib/python3.11/site-packages/ray/_private/worker.py", line 866, in get_objects
    raise value
ray.exceptions.OutOfMemoryError: Task was killed due to the node running low on memory.
Memory on the node (IP: 115.145.171.128, ID: c687613ff34cdbf5097699e5c896055ef2dc44343c5ab44bd40309d5) where the task (actor ID: 52365bb7997979e5a4c7ddbd01000000, name=ClientAppActor.__init__, pid=619618, memory used=1.84GB) was running was 29.49GB / 31.02GB (0.950707), which exceeds the memory usage threshold of 0.95. Ray killed this worker (ID: 7e48cd46c72821b17be65b05a4e61b5846ba998a57d76db44ca4f8fa) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 115.145.171.128`. To see the logs of the worker, use `ray logs worker-7e48cd46c72821b17be65b05a4e61b5846ba998a57d76db44ca4f8fa*out -ip 115.145.171.128. Top 10 memory users:
PID	MEM(GB)	COMMAND
4942	3.48	/snap/pycharm-professional/408/jbr/bin/java -classpath /snap/pycharm-professional/408/lib/platform-l...
616443	2.14	/home/abdenour/miniconda3/envs/fl_with_flower/bin/python /home/abdenour/PycharmProjects/fl_with_flow...
619615	1.90	ray::ClientAppActor
619616	1.88	ray::ClientAppActor
619611	1.87	ray::ClientAppActor
619617	1.86	ray::ClientAppActor
619620	1.85	ray::ClientAppActor
619618	1.84	ray::ClientAppActor
619613	1.82	ray::ClientAppActor
619619	1.80	ray::ClientAppActor
Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.

[2024-08-13 23:54:58,070][flwr][ERROR] - Task was killed due to the node running low on memory.
Memory on the node (IP: 115.145.171.128, ID: c687613ff34cdbf5097699e5c896055ef2dc44343c5ab44bd40309d5) where the task (actor ID: 52365bb7997979e5a4c7ddbd01000000, name=ClientAppActor.__init__, pid=619618, memory used=1.84GB) was running was 29.49GB / 31.02GB (0.950707), which exceeds the memory usage threshold of 0.95. Ray killed this worker (ID: 7e48cd46c72821b17be65b05a4e61b5846ba998a57d76db44ca4f8fa) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 115.145.171.128`. To see the logs of the worker, use `ray logs worker-7e48cd46c72821b17be65b05a4e61b5846ba998a57d76db44ca4f8fa*out -ip 115.145.171.128. Top 10 memory users:
PID	MEM(GB)	COMMAND
4942	3.48	/snap/pycharm-professional/408/jbr/bin/java -classpath /snap/pycharm-professional/408/lib/platform-l...
616443	2.14	/home/abdenour/miniconda3/envs/fl_with_flower/bin/python /home/abdenour/PycharmProjects/fl_with_flow...
619615	1.90	ray::ClientAppActor
619616	1.88	ray::ClientAppActor
619611	1.87	ray::ClientAppActor
619617	1.86	ray::ClientAppActor
619620	1.85	ray::ClientAppActor
619618	1.84	ray::ClientAppActor
619613	1.82	ray::ClientAppActor
619619	1.80	ray::ClientAppActor
Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.
[2024-08-13 23:54:59,661][flwr][INFO] - aggregate_fit: received 8 results and 2 failures
[2024-08-13 23:55:02,456][flwr][INFO] - fit progress: (247, 16.83199042081833, {'accuracy': 0.75}, 1423.7166641830117)
[2024-08-13 23:55:02,456][flwr][INFO] - configure_evaluate: strategy sampled 2 clients (out of 20)
[2024-08-13 23:55:02,709][flwr][INFO] - aggregate_evaluate: received 2 results and 0 failures
[2024-08-13 23:55:02,709][flwr][INFO] - 
[2024-08-13 23:55:02,709][flwr][INFO] - [ROUND 248]
[2024-08-13 23:55:02,709][flwr][INFO] - configure_fit: strategy sampled 10 clients (out of 20)
[2024-08-13 23:55:02,876][flwr][ERROR] - Traceback (most recent call last):
  File "/home/abdenour/miniconda3/envs/fl_with_flower/lib/python3.11/site-packages/flwr/simulation/ray_transport/ray_client_proxy.py", line 94, in _submit_job
    out_mssg, updated_context = self.actor_pool.get_client_result(
                                ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/abdenour/miniconda3/envs/fl_with_flower/lib/python3.11/site-packages/flwr/simulation/ray_transport/ray_actor.py", line 398, in get_client_result
    return self._fetch_future_result(cid)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/abdenour/miniconda3/envs/fl_with_flower/lib/python3.11/site-packages/flwr/simulation/ray_transport/ray_actor.py", line 279, in _fetch_future_result
    res_cid, out_mssg, updated_context = ray.get(
                                         ^^^^^^^^
  File "/home/abdenour/miniconda3/envs/fl_with_flower/lib/python3.11/site-packages/ray/_private/auto_init_hook.py", line 21, in auto_init_wrapper
    return fn(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^
  File "/home/abdenour/miniconda3/envs/fl_with_flower/lib/python3.11/site-packages/ray/_private/client_mode_hook.py", line 103, in wrapper
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "/home/abdenour/miniconda3/envs/fl_with_flower/lib/python3.11/site-packages/ray/_private/worker.py", line 2667, in get
    values, debugger_breakpoint = worker.get_objects(object_refs, timeout=timeout)
                                  ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/abdenour/miniconda3/envs/fl_with_flower/lib/python3.11/site-packages/ray/_private/worker.py", line 866, in get_objects
    raise value
ray.exceptions.OutOfMemoryError: Task was killed due to the node running low on memory.
Memory on the node (IP: 115.145.171.128, ID: c687613ff34cdbf5097699e5c896055ef2dc44343c5ab44bd40309d5) where the task (actor ID: 52365bb7997979e5a4c7ddbd01000000, name=ClientAppActor.__init__, pid=619618, memory used=1.84GB) was running was 29.49GB / 31.02GB (0.950707), which exceeds the memory usage threshold of 0.95. Ray killed this worker (ID: 7e48cd46c72821b17be65b05a4e61b5846ba998a57d76db44ca4f8fa) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 115.145.171.128`. To see the logs of the worker, use `ray logs worker-7e48cd46c72821b17be65b05a4e61b5846ba998a57d76db44ca4f8fa*out -ip 115.145.171.128. Top 10 memory users:
PID	MEM(GB)	COMMAND
4942	3.48	/snap/pycharm-professional/408/jbr/bin/java -classpath /snap/pycharm-professional/408/lib/platform-l...
616443	2.14	/home/abdenour/miniconda3/envs/fl_with_flower/bin/python /home/abdenour/PycharmProjects/fl_with_flow...
619615	1.90	ray::ClientAppActor
619616	1.88	ray::ClientAppActor
619611	1.87	ray::ClientAppActor
619617	1.86	ray::ClientAppActor
619620	1.85	ray::ClientAppActor
619618	1.84	ray::ClientAppActor
619613	1.82	ray::ClientAppActor
619619	1.80	ray::ClientAppActor
Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.

[2024-08-13 23:55:02,876][flwr][ERROR] - Task was killed due to the node running low on memory.
Memory on the node (IP: 115.145.171.128, ID: c687613ff34cdbf5097699e5c896055ef2dc44343c5ab44bd40309d5) where the task (actor ID: 52365bb7997979e5a4c7ddbd01000000, name=ClientAppActor.__init__, pid=619618, memory used=1.84GB) was running was 29.49GB / 31.02GB (0.950707), which exceeds the memory usage threshold of 0.95. Ray killed this worker (ID: 7e48cd46c72821b17be65b05a4e61b5846ba998a57d76db44ca4f8fa) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 115.145.171.128`. To see the logs of the worker, use `ray logs worker-7e48cd46c72821b17be65b05a4e61b5846ba998a57d76db44ca4f8fa*out -ip 115.145.171.128. Top 10 memory users:
PID	MEM(GB)	COMMAND
4942	3.48	/snap/pycharm-professional/408/jbr/bin/java -classpath /snap/pycharm-professional/408/lib/platform-l...
616443	2.14	/home/abdenour/miniconda3/envs/fl_with_flower/bin/python /home/abdenour/PycharmProjects/fl_with_flow...
619615	1.90	ray::ClientAppActor
619616	1.88	ray::ClientAppActor
619611	1.87	ray::ClientAppActor
619617	1.86	ray::ClientAppActor
619620	1.85	ray::ClientAppActor
619618	1.84	ray::ClientAppActor
619613	1.82	ray::ClientAppActor
619619	1.80	ray::ClientAppActor
Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.
[2024-08-13 23:55:02,881][flwr][ERROR] - Traceback (most recent call last):
  File "/home/abdenour/miniconda3/envs/fl_with_flower/lib/python3.11/site-packages/flwr/simulation/ray_transport/ray_client_proxy.py", line 94, in _submit_job
    out_mssg, updated_context = self.actor_pool.get_client_result(
                                ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/abdenour/miniconda3/envs/fl_with_flower/lib/python3.11/site-packages/flwr/simulation/ray_transport/ray_actor.py", line 398, in get_client_result
    return self._fetch_future_result(cid)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/abdenour/miniconda3/envs/fl_with_flower/lib/python3.11/site-packages/flwr/simulation/ray_transport/ray_actor.py", line 279, in _fetch_future_result
    res_cid, out_mssg, updated_context = ray.get(
                                         ^^^^^^^^
  File "/home/abdenour/miniconda3/envs/fl_with_flower/lib/python3.11/site-packages/ray/_private/auto_init_hook.py", line 21, in auto_init_wrapper
    return fn(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^
  File "/home/abdenour/miniconda3/envs/fl_with_flower/lib/python3.11/site-packages/ray/_private/client_mode_hook.py", line 103, in wrapper
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "/home/abdenour/miniconda3/envs/fl_with_flower/lib/python3.11/site-packages/ray/_private/worker.py", line 2667, in get
    values, debugger_breakpoint = worker.get_objects(object_refs, timeout=timeout)
                                  ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/abdenour/miniconda3/envs/fl_with_flower/lib/python3.11/site-packages/ray/_private/worker.py", line 866, in get_objects
    raise value
ray.exceptions.OutOfMemoryError: Task was killed due to the node running low on memory.
Memory on the node (IP: 115.145.171.128, ID: c687613ff34cdbf5097699e5c896055ef2dc44343c5ab44bd40309d5) where the task (actor ID: 52365bb7997979e5a4c7ddbd01000000, name=ClientAppActor.__init__, pid=619618, memory used=1.84GB) was running was 29.49GB / 31.02GB (0.950707), which exceeds the memory usage threshold of 0.95. Ray killed this worker (ID: 7e48cd46c72821b17be65b05a4e61b5846ba998a57d76db44ca4f8fa) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 115.145.171.128`. To see the logs of the worker, use `ray logs worker-7e48cd46c72821b17be65b05a4e61b5846ba998a57d76db44ca4f8fa*out -ip 115.145.171.128. Top 10 memory users:
PID	MEM(GB)	COMMAND
4942	3.48	/snap/pycharm-professional/408/jbr/bin/java -classpath /snap/pycharm-professional/408/lib/platform-l...
616443	2.14	/home/abdenour/miniconda3/envs/fl_with_flower/bin/python /home/abdenour/PycharmProjects/fl_with_flow...
619615	1.90	ray::ClientAppActor
619616	1.88	ray::ClientAppActor
619611	1.87	ray::ClientAppActor
619617	1.86	ray::ClientAppActor
619620	1.85	ray::ClientAppActor
619618	1.84	ray::ClientAppActor
619613	1.82	ray::ClientAppActor
619619	1.80	ray::ClientAppActor
Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.

[2024-08-13 23:55:02,881][flwr][ERROR] - Task was killed due to the node running low on memory.
Memory on the node (IP: 115.145.171.128, ID: c687613ff34cdbf5097699e5c896055ef2dc44343c5ab44bd40309d5) where the task (actor ID: 52365bb7997979e5a4c7ddbd01000000, name=ClientAppActor.__init__, pid=619618, memory used=1.84GB) was running was 29.49GB / 31.02GB (0.950707), which exceeds the memory usage threshold of 0.95. Ray killed this worker (ID: 7e48cd46c72821b17be65b05a4e61b5846ba998a57d76db44ca4f8fa) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 115.145.171.128`. To see the logs of the worker, use `ray logs worker-7e48cd46c72821b17be65b05a4e61b5846ba998a57d76db44ca4f8fa*out -ip 115.145.171.128. Top 10 memory users:
PID	MEM(GB)	COMMAND
4942	3.48	/snap/pycharm-professional/408/jbr/bin/java -classpath /snap/pycharm-professional/408/lib/platform-l...
616443	2.14	/home/abdenour/miniconda3/envs/fl_with_flower/bin/python /home/abdenour/PycharmProjects/fl_with_flow...
619615	1.90	ray::ClientAppActor
619616	1.88	ray::ClientAppActor
619611	1.87	ray::ClientAppActor
619617	1.86	ray::ClientAppActor
619620	1.85	ray::ClientAppActor
619618	1.84	ray::ClientAppActor
619613	1.82	ray::ClientAppActor
619619	1.80	ray::ClientAppActor
Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.
[2024-08-13 23:55:04,758][flwr][INFO] - aggregate_fit: received 8 results and 2 failures
[2024-08-13 23:55:07,692][flwr][INFO] - fit progress: (248, 17.35552453994751, {'accuracy': 0.7467948717948718}, 1428.9522166220122)
[2024-08-13 23:55:07,692][flwr][INFO] - configure_evaluate: strategy sampled 2 clients (out of 20)
[2024-08-13 23:55:07,931][flwr][INFO] - aggregate_evaluate: received 2 results and 0 failures
[2024-08-13 23:55:07,931][flwr][INFO] - 
[2024-08-13 23:55:07,931][flwr][INFO] - [ROUND 249]
[2024-08-13 23:55:07,931][flwr][INFO] - configure_fit: strategy sampled 10 clients (out of 20)
[2024-08-13 23:55:08,090][flwr][ERROR] - Traceback (most recent call last):
  File "/home/abdenour/miniconda3/envs/fl_with_flower/lib/python3.11/site-packages/flwr/simulation/ray_transport/ray_client_proxy.py", line 94, in _submit_job
    out_mssg, updated_context = self.actor_pool.get_client_result(
                                ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/abdenour/miniconda3/envs/fl_with_flower/lib/python3.11/site-packages/flwr/simulation/ray_transport/ray_actor.py", line 398, in get_client_result
    return self._fetch_future_result(cid)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/abdenour/miniconda3/envs/fl_with_flower/lib/python3.11/site-packages/flwr/simulation/ray_transport/ray_actor.py", line 279, in _fetch_future_result
    res_cid, out_mssg, updated_context = ray.get(
                                         ^^^^^^^^
  File "/home/abdenour/miniconda3/envs/fl_with_flower/lib/python3.11/site-packages/ray/_private/auto_init_hook.py", line 21, in auto_init_wrapper
    return fn(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^
  File "/home/abdenour/miniconda3/envs/fl_with_flower/lib/python3.11/site-packages/ray/_private/client_mode_hook.py", line 103, in wrapper
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "/home/abdenour/miniconda3/envs/fl_with_flower/lib/python3.11/site-packages/ray/_private/worker.py", line 2667, in get
    values, debugger_breakpoint = worker.get_objects(object_refs, timeout=timeout)
                                  ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/abdenour/miniconda3/envs/fl_with_flower/lib/python3.11/site-packages/ray/_private/worker.py", line 866, in get_objects
    raise value
ray.exceptions.OutOfMemoryError: Task was killed due to the node running low on memory.
Memory on the node (IP: 115.145.171.128, ID: c687613ff34cdbf5097699e5c896055ef2dc44343c5ab44bd40309d5) where the task (actor ID: 52365bb7997979e5a4c7ddbd01000000, name=ClientAppActor.__init__, pid=619618, memory used=1.84GB) was running was 29.49GB / 31.02GB (0.950707), which exceeds the memory usage threshold of 0.95. Ray killed this worker (ID: 7e48cd46c72821b17be65b05a4e61b5846ba998a57d76db44ca4f8fa) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 115.145.171.128`. To see the logs of the worker, use `ray logs worker-7e48cd46c72821b17be65b05a4e61b5846ba998a57d76db44ca4f8fa*out -ip 115.145.171.128. Top 10 memory users:
PID	MEM(GB)	COMMAND
4942	3.48	/snap/pycharm-professional/408/jbr/bin/java -classpath /snap/pycharm-professional/408/lib/platform-l...
616443	2.14	/home/abdenour/miniconda3/envs/fl_with_flower/bin/python /home/abdenour/PycharmProjects/fl_with_flow...
619615	1.90	ray::ClientAppActor
619616	1.88	ray::ClientAppActor
619611	1.87	ray::ClientAppActor
619617	1.86	ray::ClientAppActor
619620	1.85	ray::ClientAppActor
619618	1.84	ray::ClientAppActor
619613	1.82	ray::ClientAppActor
619619	1.80	ray::ClientAppActor
Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.

[2024-08-13 23:55:08,090][flwr][ERROR] - Task was killed due to the node running low on memory.
Memory on the node (IP: 115.145.171.128, ID: c687613ff34cdbf5097699e5c896055ef2dc44343c5ab44bd40309d5) where the task (actor ID: 52365bb7997979e5a4c7ddbd01000000, name=ClientAppActor.__init__, pid=619618, memory used=1.84GB) was running was 29.49GB / 31.02GB (0.950707), which exceeds the memory usage threshold of 0.95. Ray killed this worker (ID: 7e48cd46c72821b17be65b05a4e61b5846ba998a57d76db44ca4f8fa) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 115.145.171.128`. To see the logs of the worker, use `ray logs worker-7e48cd46c72821b17be65b05a4e61b5846ba998a57d76db44ca4f8fa*out -ip 115.145.171.128. Top 10 memory users:
PID	MEM(GB)	COMMAND
4942	3.48	/snap/pycharm-professional/408/jbr/bin/java -classpath /snap/pycharm-professional/408/lib/platform-l...
616443	2.14	/home/abdenour/miniconda3/envs/fl_with_flower/bin/python /home/abdenour/PycharmProjects/fl_with_flow...
619615	1.90	ray::ClientAppActor
619616	1.88	ray::ClientAppActor
619611	1.87	ray::ClientAppActor
619617	1.86	ray::ClientAppActor
619620	1.85	ray::ClientAppActor
619618	1.84	ray::ClientAppActor
619613	1.82	ray::ClientAppActor
619619	1.80	ray::ClientAppActor
Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.
[2024-08-13 23:55:08,091][flwr][ERROR] - Traceback (most recent call last):
  File "/home/abdenour/miniconda3/envs/fl_with_flower/lib/python3.11/site-packages/flwr/simulation/ray_transport/ray_client_proxy.py", line 94, in _submit_job
    out_mssg, updated_context = self.actor_pool.get_client_result(
                                ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/abdenour/miniconda3/envs/fl_with_flower/lib/python3.11/site-packages/flwr/simulation/ray_transport/ray_actor.py", line 398, in get_client_result
    return self._fetch_future_result(cid)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/abdenour/miniconda3/envs/fl_with_flower/lib/python3.11/site-packages/flwr/simulation/ray_transport/ray_actor.py", line 279, in _fetch_future_result
    res_cid, out_mssg, updated_context = ray.get(
                                         ^^^^^^^^
  File "/home/abdenour/miniconda3/envs/fl_with_flower/lib/python3.11/site-packages/ray/_private/auto_init_hook.py", line 21, in auto_init_wrapper
    return fn(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^
  File "/home/abdenour/miniconda3/envs/fl_with_flower/lib/python3.11/site-packages/ray/_private/client_mode_hook.py", line 103, in wrapper
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "/home/abdenour/miniconda3/envs/fl_with_flower/lib/python3.11/site-packages/ray/_private/worker.py", line 2667, in get
    values, debugger_breakpoint = worker.get_objects(object_refs, timeout=timeout)
                                  ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/abdenour/miniconda3/envs/fl_with_flower/lib/python3.11/site-packages/ray/_private/worker.py", line 866, in get_objects
    raise value
ray.exceptions.OutOfMemoryError: Task was killed due to the node running low on memory.
Memory on the node (IP: 115.145.171.128, ID: c687613ff34cdbf5097699e5c896055ef2dc44343c5ab44bd40309d5) where the task (actor ID: 85928d3f1a247800081c0e1101000000, name=ClientAppActor.__init__, pid=619614, memory used=1.73GB) was running was 29.50GB / 31.02GB (0.951049), which exceeds the memory usage threshold of 0.95. Ray killed this worker (ID: c7f644ac3cca20dffbfa1723daac4f87f04c7de03da25e7cad5fe33c) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 115.145.171.128`. To see the logs of the worker, use `ray logs worker-c7f644ac3cca20dffbfa1723daac4f87f04c7de03da25e7cad5fe33c*out -ip 115.145.171.128. Top 10 memory users:
PID	MEM(GB)	COMMAND
4942	3.49	/snap/pycharm-professional/408/jbr/bin/java -classpath /snap/pycharm-professional/408/lib/platform-l...
619616	1.80	ray::ClientAppActor.run
619615	1.78	ray::ClientAppActor
619617	1.77	ray::ClientAppActor
619611	1.73	ray::ClientAppActor
619618	1.73	ray::ClientAppActor
619614	1.73	ray::ClientAppActor
619612	1.72	ray::ClientAppActor
619619	1.70	ray::ClientAppActor
619620	1.68	ray::ClientAppActor.run
Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.

[2024-08-13 23:55:08,091][flwr][ERROR] - Task was killed due to the node running low on memory.
Memory on the node (IP: 115.145.171.128, ID: c687613ff34cdbf5097699e5c896055ef2dc44343c5ab44bd40309d5) where the task (actor ID: 85928d3f1a247800081c0e1101000000, name=ClientAppActor.__init__, pid=619614, memory used=1.73GB) was running was 29.50GB / 31.02GB (0.951049), which exceeds the memory usage threshold of 0.95. Ray killed this worker (ID: c7f644ac3cca20dffbfa1723daac4f87f04c7de03da25e7cad5fe33c) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 115.145.171.128`. To see the logs of the worker, use `ray logs worker-c7f644ac3cca20dffbfa1723daac4f87f04c7de03da25e7cad5fe33c*out -ip 115.145.171.128. Top 10 memory users:
PID	MEM(GB)	COMMAND
4942	3.49	/snap/pycharm-professional/408/jbr/bin/java -classpath /snap/pycharm-professional/408/lib/platform-l...
619616	1.80	ray::ClientAppActor.run
619615	1.78	ray::ClientAppActor
619617	1.77	ray::ClientAppActor
619611	1.73	ray::ClientAppActor
619618	1.73	ray::ClientAppActor
619614	1.73	ray::ClientAppActor
619612	1.72	ray::ClientAppActor
619619	1.70	ray::ClientAppActor
619620	1.68	ray::ClientAppActor.run
Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.
[2024-08-13 23:55:10,241][flwr][INFO] - aggregate_fit: received 8 results and 2 failures
[2024-08-13 23:55:13,105][flwr][INFO] - fit progress: (249, 17.920701801776886, {'accuracy': 0.7451923076923077}, 1434.365502279019)
[2024-08-13 23:55:13,105][flwr][INFO] - configure_evaluate: strategy sampled 2 clients (out of 20)
[2024-08-13 23:55:13,523][flwr][INFO] - aggregate_evaluate: received 2 results and 0 failures
[2024-08-13 23:55:13,523][flwr][INFO] - 
[2024-08-13 23:55:13,523][flwr][INFO] - [ROUND 250]
[2024-08-13 23:55:13,524][flwr][INFO] - configure_fit: strategy sampled 10 clients (out of 20)
[2024-08-13 23:55:13,695][flwr][ERROR] - Traceback (most recent call last):
  File "/home/abdenour/miniconda3/envs/fl_with_flower/lib/python3.11/site-packages/flwr/simulation/ray_transport/ray_client_proxy.py", line 94, in _submit_job
    out_mssg, updated_context = self.actor_pool.get_client_result(
                                ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/abdenour/miniconda3/envs/fl_with_flower/lib/python3.11/site-packages/flwr/simulation/ray_transport/ray_actor.py", line 398, in get_client_result
    return self._fetch_future_result(cid)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/abdenour/miniconda3/envs/fl_with_flower/lib/python3.11/site-packages/flwr/simulation/ray_transport/ray_actor.py", line 279, in _fetch_future_result
    res_cid, out_mssg, updated_context = ray.get(
                                         ^^^^^^^^
  File "/home/abdenour/miniconda3/envs/fl_with_flower/lib/python3.11/site-packages/ray/_private/auto_init_hook.py", line 21, in auto_init_wrapper
    return fn(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^
  File "/home/abdenour/miniconda3/envs/fl_with_flower/lib/python3.11/site-packages/ray/_private/client_mode_hook.py", line 103, in wrapper
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "/home/abdenour/miniconda3/envs/fl_with_flower/lib/python3.11/site-packages/ray/_private/worker.py", line 2667, in get
    values, debugger_breakpoint = worker.get_objects(object_refs, timeout=timeout)
                                  ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/abdenour/miniconda3/envs/fl_with_flower/lib/python3.11/site-packages/ray/_private/worker.py", line 866, in get_objects
    raise value
ray.exceptions.OutOfMemoryError: Task was killed due to the node running low on memory.
Memory on the node (IP: 115.145.171.128, ID: c687613ff34cdbf5097699e5c896055ef2dc44343c5ab44bd40309d5) where the task (actor ID: 85928d3f1a247800081c0e1101000000, name=ClientAppActor.__init__, pid=619614, memory used=1.73GB) was running was 29.50GB / 31.02GB (0.951049), which exceeds the memory usage threshold of 0.95. Ray killed this worker (ID: c7f644ac3cca20dffbfa1723daac4f87f04c7de03da25e7cad5fe33c) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 115.145.171.128`. To see the logs of the worker, use `ray logs worker-c7f644ac3cca20dffbfa1723daac4f87f04c7de03da25e7cad5fe33c*out -ip 115.145.171.128. Top 10 memory users:
PID	MEM(GB)	COMMAND
4942	3.49	/snap/pycharm-professional/408/jbr/bin/java -classpath /snap/pycharm-professional/408/lib/platform-l...
619616	1.80	ray::ClientAppActor.run
619615	1.78	ray::ClientAppActor
619617	1.77	ray::ClientAppActor
619611	1.73	ray::ClientAppActor
619618	1.73	ray::ClientAppActor
619614	1.73	ray::ClientAppActor
619612	1.72	ray::ClientAppActor
619619	1.70	ray::ClientAppActor
619620	1.68	ray::ClientAppActor.run
Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.

[2024-08-13 23:55:13,695][flwr][ERROR] - Task was killed due to the node running low on memory.
Memory on the node (IP: 115.145.171.128, ID: c687613ff34cdbf5097699e5c896055ef2dc44343c5ab44bd40309d5) where the task (actor ID: 85928d3f1a247800081c0e1101000000, name=ClientAppActor.__init__, pid=619614, memory used=1.73GB) was running was 29.50GB / 31.02GB (0.951049), which exceeds the memory usage threshold of 0.95. Ray killed this worker (ID: c7f644ac3cca20dffbfa1723daac4f87f04c7de03da25e7cad5fe33c) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 115.145.171.128`. To see the logs of the worker, use `ray logs worker-c7f644ac3cca20dffbfa1723daac4f87f04c7de03da25e7cad5fe33c*out -ip 115.145.171.128. Top 10 memory users:
PID	MEM(GB)	COMMAND
4942	3.49	/snap/pycharm-professional/408/jbr/bin/java -classpath /snap/pycharm-professional/408/lib/platform-l...
619616	1.80	ray::ClientAppActor.run
619615	1.78	ray::ClientAppActor
619617	1.77	ray::ClientAppActor
619611	1.73	ray::ClientAppActor
619618	1.73	ray::ClientAppActor
619614	1.73	ray::ClientAppActor
619612	1.72	ray::ClientAppActor
619619	1.70	ray::ClientAppActor
619620	1.68	ray::ClientAppActor.run
Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.
[2024-08-13 23:55:13,696][flwr][ERROR] - Traceback (most recent call last):
  File "/home/abdenour/miniconda3/envs/fl_with_flower/lib/python3.11/site-packages/flwr/simulation/ray_transport/ray_client_proxy.py", line 94, in _submit_job
    out_mssg, updated_context = self.actor_pool.get_client_result(
                                ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/abdenour/miniconda3/envs/fl_with_flower/lib/python3.11/site-packages/flwr/simulation/ray_transport/ray_actor.py", line 398, in get_client_result
    return self._fetch_future_result(cid)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/abdenour/miniconda3/envs/fl_with_flower/lib/python3.11/site-packages/flwr/simulation/ray_transport/ray_actor.py", line 279, in _fetch_future_result
    res_cid, out_mssg, updated_context = ray.get(
                                         ^^^^^^^^
  File "/home/abdenour/miniconda3/envs/fl_with_flower/lib/python3.11/site-packages/ray/_private/auto_init_hook.py", line 21, in auto_init_wrapper
    return fn(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^
  File "/home/abdenour/miniconda3/envs/fl_with_flower/lib/python3.11/site-packages/ray/_private/client_mode_hook.py", line 103, in wrapper
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "/home/abdenour/miniconda3/envs/fl_with_flower/lib/python3.11/site-packages/ray/_private/worker.py", line 2667, in get
    values, debugger_breakpoint = worker.get_objects(object_refs, timeout=timeout)
                                  ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/abdenour/miniconda3/envs/fl_with_flower/lib/python3.11/site-packages/ray/_private/worker.py", line 866, in get_objects
    raise value
ray.exceptions.OutOfMemoryError: Task was killed due to the node running low on memory.
Memory on the node (IP: 115.145.171.128, ID: c687613ff34cdbf5097699e5c896055ef2dc44343c5ab44bd40309d5) where the task (actor ID: 52365bb7997979e5a4c7ddbd01000000, name=ClientAppActor.__init__, pid=619618, memory used=1.84GB) was running was 29.49GB / 31.02GB (0.950707), which exceeds the memory usage threshold of 0.95. Ray killed this worker (ID: 7e48cd46c72821b17be65b05a4e61b5846ba998a57d76db44ca4f8fa) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 115.145.171.128`. To see the logs of the worker, use `ray logs worker-7e48cd46c72821b17be65b05a4e61b5846ba998a57d76db44ca4f8fa*out -ip 115.145.171.128. Top 10 memory users:
PID	MEM(GB)	COMMAND
4942	3.48	/snap/pycharm-professional/408/jbr/bin/java -classpath /snap/pycharm-professional/408/lib/platform-l...
616443	2.14	/home/abdenour/miniconda3/envs/fl_with_flower/bin/python /home/abdenour/PycharmProjects/fl_with_flow...
619615	1.90	ray::ClientAppActor
619616	1.88	ray::ClientAppActor
619611	1.87	ray::ClientAppActor
619617	1.86	ray::ClientAppActor
619620	1.85	ray::ClientAppActor
619618	1.84	ray::ClientAppActor
619613	1.82	ray::ClientAppActor
619619	1.80	ray::ClientAppActor
Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.

[2024-08-13 23:55:13,696][flwr][ERROR] - Task was killed due to the node running low on memory.
Memory on the node (IP: 115.145.171.128, ID: c687613ff34cdbf5097699e5c896055ef2dc44343c5ab44bd40309d5) where the task (actor ID: 52365bb7997979e5a4c7ddbd01000000, name=ClientAppActor.__init__, pid=619618, memory used=1.84GB) was running was 29.49GB / 31.02GB (0.950707), which exceeds the memory usage threshold of 0.95. Ray killed this worker (ID: 7e48cd46c72821b17be65b05a4e61b5846ba998a57d76db44ca4f8fa) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 115.145.171.128`. To see the logs of the worker, use `ray logs worker-7e48cd46c72821b17be65b05a4e61b5846ba998a57d76db44ca4f8fa*out -ip 115.145.171.128. Top 10 memory users:
PID	MEM(GB)	COMMAND
4942	3.48	/snap/pycharm-professional/408/jbr/bin/java -classpath /snap/pycharm-professional/408/lib/platform-l...
616443	2.14	/home/abdenour/miniconda3/envs/fl_with_flower/bin/python /home/abdenour/PycharmProjects/fl_with_flow...
619615	1.90	ray::ClientAppActor
619616	1.88	ray::ClientAppActor
619611	1.87	ray::ClientAppActor
619617	1.86	ray::ClientAppActor
619620	1.85	ray::ClientAppActor
619618	1.84	ray::ClientAppActor
619613	1.82	ray::ClientAppActor
619619	1.80	ray::ClientAppActor
Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.
[2024-08-13 23:55:16,411][flwr][INFO] - aggregate_fit: received 8 results and 2 failures
[2024-08-13 23:55:19,550][flwr][INFO] - fit progress: (250, 16.43474268913269, {'accuracy': 0.7548076923076923}, 1440.8104034480057)
[2024-08-13 23:55:19,550][flwr][INFO] - configure_evaluate: strategy sampled 2 clients (out of 20)
[2024-08-13 23:55:19,909][flwr][INFO] - aggregate_evaluate: received 2 results and 0 failures
[2024-08-13 23:55:19,909][flwr][INFO] - 
[2024-08-13 23:55:19,909][flwr][INFO] - [ROUND 251]
[2024-08-13 23:55:19,909][flwr][INFO] - configure_fit: strategy sampled 10 clients (out of 20)
[2024-08-13 23:55:20,077][flwr][ERROR] - Traceback (most recent call last):
  File "/home/abdenour/miniconda3/envs/fl_with_flower/lib/python3.11/site-packages/flwr/simulation/ray_transport/ray_client_proxy.py", line 94, in _submit_job
    out_mssg, updated_context = self.actor_pool.get_client_result(
                                ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/abdenour/miniconda3/envs/fl_with_flower/lib/python3.11/site-packages/flwr/simulation/ray_transport/ray_actor.py", line 398, in get_client_result
    return self._fetch_future_result(cid)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/abdenour/miniconda3/envs/fl_with_flower/lib/python3.11/site-packages/flwr/simulation/ray_transport/ray_actor.py", line 279, in _fetch_future_result
    res_cid, out_mssg, updated_context = ray.get(
                                         ^^^^^^^^
  File "/home/abdenour/miniconda3/envs/fl_with_flower/lib/python3.11/site-packages/ray/_private/auto_init_hook.py", line 21, in auto_init_wrapper
    return fn(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^
  File "/home/abdenour/miniconda3/envs/fl_with_flower/lib/python3.11/site-packages/ray/_private/client_mode_hook.py", line 103, in wrapper
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "/home/abdenour/miniconda3/envs/fl_with_flower/lib/python3.11/site-packages/ray/_private/worker.py", line 2667, in get
    values, debugger_breakpoint = worker.get_objects(object_refs, timeout=timeout)
                                  ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/abdenour/miniconda3/envs/fl_with_flower/lib/python3.11/site-packages/ray/_private/worker.py", line 866, in get_objects
    raise value
ray.exceptions.OutOfMemoryError: Task was killed due to the node running low on memory.
Memory on the node (IP: 115.145.171.128, ID: c687613ff34cdbf5097699e5c896055ef2dc44343c5ab44bd40309d5) where the task (actor ID: 52365bb7997979e5a4c7ddbd01000000, name=ClientAppActor.__init__, pid=619618, memory used=1.84GB) was running was 29.49GB / 31.02GB (0.950707), which exceeds the memory usage threshold of 0.95. Ray killed this worker (ID: 7e48cd46c72821b17be65b05a4e61b5846ba998a57d76db44ca4f8fa) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 115.145.171.128`. To see the logs of the worker, use `ray logs worker-7e48cd46c72821b17be65b05a4e61b5846ba998a57d76db44ca4f8fa*out -ip 115.145.171.128. Top 10 memory users:
PID	MEM(GB)	COMMAND
4942	3.48	/snap/pycharm-professional/408/jbr/bin/java -classpath /snap/pycharm-professional/408/lib/platform-l...
616443	2.14	/home/abdenour/miniconda3/envs/fl_with_flower/bin/python /home/abdenour/PycharmProjects/fl_with_flow...
619615	1.90	ray::ClientAppActor
619616	1.88	ray::ClientAppActor
619611	1.87	ray::ClientAppActor
619617	1.86	ray::ClientAppActor
619620	1.85	ray::ClientAppActor
619618	1.84	ray::ClientAppActor
619613	1.82	ray::ClientAppActor
619619	1.80	ray::ClientAppActor
Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.

[2024-08-13 23:55:20,078][flwr][ERROR] - Traceback (most recent call last):
  File "/home/abdenour/miniconda3/envs/fl_with_flower/lib/python3.11/site-packages/flwr/simulation/ray_transport/ray_client_proxy.py", line 94, in _submit_job
    out_mssg, updated_context = self.actor_pool.get_client_result(
                                ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/abdenour/miniconda3/envs/fl_with_flower/lib/python3.11/site-packages/flwr/simulation/ray_transport/ray_actor.py", line 398, in get_client_result
    return self._fetch_future_result(cid)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/abdenour/miniconda3/envs/fl_with_flower/lib/python3.11/site-packages/flwr/simulation/ray_transport/ray_actor.py", line 279, in _fetch_future_result
    res_cid, out_mssg, updated_context = ray.get(
                                         ^^^^^^^^
  File "/home/abdenour/miniconda3/envs/fl_with_flower/lib/python3.11/site-packages/ray/_private/auto_init_hook.py", line 21, in auto_init_wrapper
    return fn(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^
  File "/home/abdenour/miniconda3/envs/fl_with_flower/lib/python3.11/site-packages/ray/_private/client_mode_hook.py", line 103, in wrapper
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "/home/abdenour/miniconda3/envs/fl_with_flower/lib/python3.11/site-packages/ray/_private/worker.py", line 2667, in get
    values, debugger_breakpoint = worker.get_objects(object_refs, timeout=timeout)
                                  ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/abdenour/miniconda3/envs/fl_with_flower/lib/python3.11/site-packages/ray/_private/worker.py", line 866, in get_objects
    raise value
ray.exceptions.OutOfMemoryError: Task was killed due to the node running low on memory.
Memory on the node (IP: 115.145.171.128, ID: c687613ff34cdbf5097699e5c896055ef2dc44343c5ab44bd40309d5) where the task (actor ID: 85928d3f1a247800081c0e1101000000, name=ClientAppActor.__init__, pid=619614, memory used=1.73GB) was running was 29.50GB / 31.02GB (0.951049), which exceeds the memory usage threshold of 0.95. Ray killed this worker (ID: c7f644ac3cca20dffbfa1723daac4f87f04c7de03da25e7cad5fe33c) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 115.145.171.128`. To see the logs of the worker, use `ray logs worker-c7f644ac3cca20dffbfa1723daac4f87f04c7de03da25e7cad5fe33c*out -ip 115.145.171.128. Top 10 memory users:
PID	MEM(GB)	COMMAND
4942	3.49	/snap/pycharm-professional/408/jbr/bin/java -classpath /snap/pycharm-professional/408/lib/platform-l...
619616	1.80	ray::ClientAppActor.run
619615	1.78	ray::ClientAppActor
619617	1.77	ray::ClientAppActor
619611	1.73	ray::ClientAppActor
619618	1.73	ray::ClientAppActor
619614	1.73	ray::ClientAppActor
619612	1.72	ray::ClientAppActor
619619	1.70	ray::ClientAppActor
619620	1.68	ray::ClientAppActor.run
Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.

[2024-08-13 23:55:20,078][flwr][ERROR] - Task was killed due to the node running low on memory.
Memory on the node (IP: 115.145.171.128, ID: c687613ff34cdbf5097699e5c896055ef2dc44343c5ab44bd40309d5) where the task (actor ID: 52365bb7997979e5a4c7ddbd01000000, name=ClientAppActor.__init__, pid=619618, memory used=1.84GB) was running was 29.49GB / 31.02GB (0.950707), which exceeds the memory usage threshold of 0.95. Ray killed this worker (ID: 7e48cd46c72821b17be65b05a4e61b5846ba998a57d76db44ca4f8fa) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 115.145.171.128`. To see the logs of the worker, use `ray logs worker-7e48cd46c72821b17be65b05a4e61b5846ba998a57d76db44ca4f8fa*out -ip 115.145.171.128. Top 10 memory users:
PID	MEM(GB)	COMMAND
4942	3.48	/snap/pycharm-professional/408/jbr/bin/java -classpath /snap/pycharm-professional/408/lib/platform-l...
616443	2.14	/home/abdenour/miniconda3/envs/fl_with_flower/bin/python /home/abdenour/PycharmProjects/fl_with_flow...
619615	1.90	ray::ClientAppActor
619616	1.88	ray::ClientAppActor
619611	1.87	ray::ClientAppActor
619617	1.86	ray::ClientAppActor
619620	1.85	ray::ClientAppActor
619618	1.84	ray::ClientAppActor
619613	1.82	ray::ClientAppActor
619619	1.80	ray::ClientAppActor
Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.
[2024-08-13 23:55:20,078][flwr][ERROR] - Task was killed due to the node running low on memory.
Memory on the node (IP: 115.145.171.128, ID: c687613ff34cdbf5097699e5c896055ef2dc44343c5ab44bd40309d5) where the task (actor ID: 85928d3f1a247800081c0e1101000000, name=ClientAppActor.__init__, pid=619614, memory used=1.73GB) was running was 29.50GB / 31.02GB (0.951049), which exceeds the memory usage threshold of 0.95. Ray killed this worker (ID: c7f644ac3cca20dffbfa1723daac4f87f04c7de03da25e7cad5fe33c) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 115.145.171.128`. To see the logs of the worker, use `ray logs worker-c7f644ac3cca20dffbfa1723daac4f87f04c7de03da25e7cad5fe33c*out -ip 115.145.171.128. Top 10 memory users:
PID	MEM(GB)	COMMAND
4942	3.49	/snap/pycharm-professional/408/jbr/bin/java -classpath /snap/pycharm-professional/408/lib/platform-l...
619616	1.80	ray::ClientAppActor.run
619615	1.78	ray::ClientAppActor
619617	1.77	ray::ClientAppActor
619611	1.73	ray::ClientAppActor
619618	1.73	ray::ClientAppActor
619614	1.73	ray::ClientAppActor
619612	1.72	ray::ClientAppActor
619619	1.70	ray::ClientAppActor
619620	1.68	ray::ClientAppActor.run
Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.
[2024-08-13 23:55:22,664][flwr][INFO] - aggregate_fit: received 8 results and 2 failures
[2024-08-13 23:55:25,590][flwr][INFO] - fit progress: (251, 18.331319391727448, {'accuracy': 0.7403846153846154}, 1446.8502704300045)
[2024-08-13 23:55:25,590][flwr][INFO] - configure_evaluate: strategy sampled 2 clients (out of 20)
[2024-08-13 23:55:25,858][flwr][INFO] - aggregate_evaluate: received 2 results and 0 failures
[2024-08-13 23:55:25,859][flwr][INFO] - 
[2024-08-13 23:55:25,859][flwr][INFO] - [ROUND 252]
[2024-08-13 23:55:25,859][flwr][INFO] - configure_fit: strategy sampled 10 clients (out of 20)
[2024-08-13 23:55:26,007][flwr][ERROR] - Traceback (most recent call last):
  File "/home/abdenour/miniconda3/envs/fl_with_flower/lib/python3.11/site-packages/flwr/simulation/ray_transport/ray_client_proxy.py", line 94, in _submit_job
    out_mssg, updated_context = self.actor_pool.get_client_result(
                                ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/abdenour/miniconda3/envs/fl_with_flower/lib/python3.11/site-packages/flwr/simulation/ray_transport/ray_actor.py", line 398, in get_client_result
    return self._fetch_future_result(cid)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/abdenour/miniconda3/envs/fl_with_flower/lib/python3.11/site-packages/flwr/simulation/ray_transport/ray_actor.py", line 279, in _fetch_future_result
    res_cid, out_mssg, updated_context = ray.get(
                                         ^^^^^^^^
  File "/home/abdenour/miniconda3/envs/fl_with_flower/lib/python3.11/site-packages/ray/_private/auto_init_hook.py", line 21, in auto_init_wrapper
    return fn(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^
  File "/home/abdenour/miniconda3/envs/fl_with_flower/lib/python3.11/site-packages/ray/_private/client_mode_hook.py", line 103, in wrapper
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "/home/abdenour/miniconda3/envs/fl_with_flower/lib/python3.11/site-packages/ray/_private/worker.py", line 2667, in get
    values, debugger_breakpoint = worker.get_objects(object_refs, timeout=timeout)
                                  ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/abdenour/miniconda3/envs/fl_with_flower/lib/python3.11/site-packages/ray/_private/worker.py", line 866, in get_objects
    raise value
ray.exceptions.OutOfMemoryError: Task was killed due to the node running low on memory.
Memory on the node (IP: 115.145.171.128, ID: c687613ff34cdbf5097699e5c896055ef2dc44343c5ab44bd40309d5) where the task (actor ID: 85928d3f1a247800081c0e1101000000, name=ClientAppActor.__init__, pid=619614, memory used=1.73GB) was running was 29.50GB / 31.02GB (0.951049), which exceeds the memory usage threshold of 0.95. Ray killed this worker (ID: c7f644ac3cca20dffbfa1723daac4f87f04c7de03da25e7cad5fe33c) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 115.145.171.128`. To see the logs of the worker, use `ray logs worker-c7f644ac3cca20dffbfa1723daac4f87f04c7de03da25e7cad5fe33c*out -ip 115.145.171.128. Top 10 memory users:
PID	MEM(GB)	COMMAND
4942	3.49	/snap/pycharm-professional/408/jbr/bin/java -classpath /snap/pycharm-professional/408/lib/platform-l...
619616	1.80	ray::ClientAppActor.run
619615	1.78	ray::ClientAppActor
619617	1.77	ray::ClientAppActor
619611	1.73	ray::ClientAppActor
619618	1.73	ray::ClientAppActor
619614	1.73	ray::ClientAppActor
619612	1.72	ray::ClientAppActor
619619	1.70	ray::ClientAppActor
619620	1.68	ray::ClientAppActor.run
Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.

[2024-08-13 23:55:26,007][flwr][ERROR] - Task was killed due to the node running low on memory.
Memory on the node (IP: 115.145.171.128, ID: c687613ff34cdbf5097699e5c896055ef2dc44343c5ab44bd40309d5) where the task (actor ID: 85928d3f1a247800081c0e1101000000, name=ClientAppActor.__init__, pid=619614, memory used=1.73GB) was running was 29.50GB / 31.02GB (0.951049), which exceeds the memory usage threshold of 0.95. Ray killed this worker (ID: c7f644ac3cca20dffbfa1723daac4f87f04c7de03da25e7cad5fe33c) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 115.145.171.128`. To see the logs of the worker, use `ray logs worker-c7f644ac3cca20dffbfa1723daac4f87f04c7de03da25e7cad5fe33c*out -ip 115.145.171.128. Top 10 memory users:
PID	MEM(GB)	COMMAND
4942	3.49	/snap/pycharm-professional/408/jbr/bin/java -classpath /snap/pycharm-professional/408/lib/platform-l...
619616	1.80	ray::ClientAppActor.run
619615	1.78	ray::ClientAppActor
619617	1.77	ray::ClientAppActor
619611	1.73	ray::ClientAppActor
619618	1.73	ray::ClientAppActor
619614	1.73	ray::ClientAppActor
619612	1.72	ray::ClientAppActor
619619	1.70	ray::ClientAppActor
619620	1.68	ray::ClientAppActor.run
Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.
[2024-08-13 23:55:26,007][flwr][ERROR] - Traceback (most recent call last):
  File "/home/abdenour/miniconda3/envs/fl_with_flower/lib/python3.11/site-packages/flwr/simulation/ray_transport/ray_client_proxy.py", line 94, in _submit_job
    out_mssg, updated_context = self.actor_pool.get_client_result(
                                ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/abdenour/miniconda3/envs/fl_with_flower/lib/python3.11/site-packages/flwr/simulation/ray_transport/ray_actor.py", line 398, in get_client_result
    return self._fetch_future_result(cid)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/abdenour/miniconda3/envs/fl_with_flower/lib/python3.11/site-packages/flwr/simulation/ray_transport/ray_actor.py", line 279, in _fetch_future_result
    res_cid, out_mssg, updated_context = ray.get(
                                         ^^^^^^^^
  File "/home/abdenour/miniconda3/envs/fl_with_flower/lib/python3.11/site-packages/ray/_private/auto_init_hook.py", line 21, in auto_init_wrapper
    return fn(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^
  File "/home/abdenour/miniconda3/envs/fl_with_flower/lib/python3.11/site-packages/ray/_private/client_mode_hook.py", line 103, in wrapper
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "/home/abdenour/miniconda3/envs/fl_with_flower/lib/python3.11/site-packages/ray/_private/worker.py", line 2667, in get
    values, debugger_breakpoint = worker.get_objects(object_refs, timeout=timeout)
                                  ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/abdenour/miniconda3/envs/fl_with_flower/lib/python3.11/site-packages/ray/_private/worker.py", line 866, in get_objects
    raise value
ray.exceptions.OutOfMemoryError: Task was killed due to the node running low on memory.
Memory on the node (IP: 115.145.171.128, ID: c687613ff34cdbf5097699e5c896055ef2dc44343c5ab44bd40309d5) where the task (actor ID: 52365bb7997979e5a4c7ddbd01000000, name=ClientAppActor.__init__, pid=619618, memory used=1.84GB) was running was 29.49GB / 31.02GB (0.950707), which exceeds the memory usage threshold of 0.95. Ray killed this worker (ID: 7e48cd46c72821b17be65b05a4e61b5846ba998a57d76db44ca4f8fa) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 115.145.171.128`. To see the logs of the worker, use `ray logs worker-7e48cd46c72821b17be65b05a4e61b5846ba998a57d76db44ca4f8fa*out -ip 115.145.171.128. Top 10 memory users:
PID	MEM(GB)	COMMAND
4942	3.48	/snap/pycharm-professional/408/jbr/bin/java -classpath /snap/pycharm-professional/408/lib/platform-l...
616443	2.14	/home/abdenour/miniconda3/envs/fl_with_flower/bin/python /home/abdenour/PycharmProjects/fl_with_flow...
619615	1.90	ray::ClientAppActor
619616	1.88	ray::ClientAppActor
619611	1.87	ray::ClientAppActor
619617	1.86	ray::ClientAppActor
619620	1.85	ray::ClientAppActor
619618	1.84	ray::ClientAppActor
619613	1.82	ray::ClientAppActor
619619	1.80	ray::ClientAppActor
Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.

[2024-08-13 23:55:26,007][flwr][ERROR] - Task was killed due to the node running low on memory.
Memory on the node (IP: 115.145.171.128, ID: c687613ff34cdbf5097699e5c896055ef2dc44343c5ab44bd40309d5) where the task (actor ID: 52365bb7997979e5a4c7ddbd01000000, name=ClientAppActor.__init__, pid=619618, memory used=1.84GB) was running was 29.49GB / 31.02GB (0.950707), which exceeds the memory usage threshold of 0.95. Ray killed this worker (ID: 7e48cd46c72821b17be65b05a4e61b5846ba998a57d76db44ca4f8fa) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 115.145.171.128`. To see the logs of the worker, use `ray logs worker-7e48cd46c72821b17be65b05a4e61b5846ba998a57d76db44ca4f8fa*out -ip 115.145.171.128. Top 10 memory users:
PID	MEM(GB)	COMMAND
4942	3.48	/snap/pycharm-professional/408/jbr/bin/java -classpath /snap/pycharm-professional/408/lib/platform-l...
616443	2.14	/home/abdenour/miniconda3/envs/fl_with_flower/bin/python /home/abdenour/PycharmProjects/fl_with_flow...
619615	1.90	ray::ClientAppActor
619616	1.88	ray::ClientAppActor
619611	1.87	ray::ClientAppActor
619617	1.86	ray::ClientAppActor
619620	1.85	ray::ClientAppActor
619618	1.84	ray::ClientAppActor
619613	1.82	ray::ClientAppActor
619619	1.80	ray::ClientAppActor
Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.
[2024-08-13 23:55:29,656][flwr][INFO] - aggregate_fit: received 8 results and 2 failures
[2024-08-13 23:55:32,828][flwr][INFO] - fit progress: (252, 17.949856281280518, {'accuracy': 0.7516025641025641}, 1454.0886521420034)
[2024-08-13 23:55:32,828][flwr][INFO] - configure_evaluate: strategy sampled 2 clients (out of 20)
[2024-08-13 23:55:33,182][flwr][INFO] - aggregate_evaluate: received 2 results and 0 failures
[2024-08-13 23:55:33,183][flwr][INFO] - 
[2024-08-13 23:55:33,183][flwr][INFO] - [ROUND 253]
[2024-08-13 23:55:33,183][flwr][INFO] - configure_fit: strategy sampled 10 clients (out of 20)
[2024-08-13 23:55:33,340][flwr][ERROR] - Traceback (most recent call last):
  File "/home/abdenour/miniconda3/envs/fl_with_flower/lib/python3.11/site-packages/flwr/simulation/ray_transport/ray_client_proxy.py", line 94, in _submit_job
    out_mssg, updated_context = self.actor_pool.get_client_result(
                                ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/abdenour/miniconda3/envs/fl_with_flower/lib/python3.11/site-packages/flwr/simulation/ray_transport/ray_actor.py", line 398, in get_client_result
    return self._fetch_future_result(cid)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/abdenour/miniconda3/envs/fl_with_flower/lib/python3.11/site-packages/flwr/simulation/ray_transport/ray_actor.py", line 279, in _fetch_future_result
    res_cid, out_mssg, updated_context = ray.get(
                                         ^^^^^^^^
  File "/home/abdenour/miniconda3/envs/fl_with_flower/lib/python3.11/site-packages/ray/_private/auto_init_hook.py", line 21, in auto_init_wrapper
    return fn(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^
  File "/home/abdenour/miniconda3/envs/fl_with_flower/lib/python3.11/site-packages/ray/_private/client_mode_hook.py", line 103, in wrapper
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "/home/abdenour/miniconda3/envs/fl_with_flower/lib/python3.11/site-packages/ray/_private/worker.py", line 2667, in get
    values, debugger_breakpoint = worker.get_objects(object_refs, timeout=timeout)
                                  ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/abdenour/miniconda3/envs/fl_with_flower/lib/python3.11/site-packages/ray/_private/worker.py", line 866, in get_objects
    raise value
ray.exceptions.OutOfMemoryError: Task was killed due to the node running low on memory.
Memory on the node (IP: 115.145.171.128, ID: c687613ff34cdbf5097699e5c896055ef2dc44343c5ab44bd40309d5) where the task (actor ID: 52365bb7997979e5a4c7ddbd01000000, name=ClientAppActor.__init__, pid=619618, memory used=1.84GB) was running was 29.49GB / 31.02GB (0.950707), which exceeds the memory usage threshold of 0.95. Ray killed this worker (ID: 7e48cd46c72821b17be65b05a4e61b5846ba998a57d76db44ca4f8fa) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 115.145.171.128`. To see the logs of the worker, use `ray logs worker-7e48cd46c72821b17be65b05a4e61b5846ba998a57d76db44ca4f8fa*out -ip 115.145.171.128. Top 10 memory users:
PID	MEM(GB)	COMMAND
4942	3.48	/snap/pycharm-professional/408/jbr/bin/java -classpath /snap/pycharm-professional/408/lib/platform-l...
616443	2.14	/home/abdenour/miniconda3/envs/fl_with_flower/bin/python /home/abdenour/PycharmProjects/fl_with_flow...
619615	1.90	ray::ClientAppActor
619616	1.88	ray::ClientAppActor
619611	1.87	ray::ClientAppActor
619617	1.86	ray::ClientAppActor
619620	1.85	ray::ClientAppActor
619618	1.84	ray::ClientAppActor
619613	1.82	ray::ClientAppActor
619619	1.80	ray::ClientAppActor
Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.

[2024-08-13 23:55:33,340][flwr][ERROR] - Traceback (most recent call last):
  File "/home/abdenour/miniconda3/envs/fl_with_flower/lib/python3.11/site-packages/flwr/simulation/ray_transport/ray_client_proxy.py", line 94, in _submit_job
    out_mssg, updated_context = self.actor_pool.get_client_result(
                                ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/abdenour/miniconda3/envs/fl_with_flower/lib/python3.11/site-packages/flwr/simulation/ray_transport/ray_actor.py", line 398, in get_client_result
    return self._fetch_future_result(cid)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/abdenour/miniconda3/envs/fl_with_flower/lib/python3.11/site-packages/flwr/simulation/ray_transport/ray_actor.py", line 279, in _fetch_future_result
    res_cid, out_mssg, updated_context = ray.get(
                                         ^^^^^^^^
  File "/home/abdenour/miniconda3/envs/fl_with_flower/lib/python3.11/site-packages/ray/_private/auto_init_hook.py", line 21, in auto_init_wrapper
    return fn(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^
  File "/home/abdenour/miniconda3/envs/fl_with_flower/lib/python3.11/site-packages/ray/_private/client_mode_hook.py", line 103, in wrapper
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "/home/abdenour/miniconda3/envs/fl_with_flower/lib/python3.11/site-packages/ray/_private/worker.py", line 2667, in get
    values, debugger_breakpoint = worker.get_objects(object_refs, timeout=timeout)
                                  ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/abdenour/miniconda3/envs/fl_with_flower/lib/python3.11/site-packages/ray/_private/worker.py", line 866, in get_objects
    raise value
ray.exceptions.OutOfMemoryError: Task was killed due to the node running low on memory.
Memory on the node (IP: 115.145.171.128, ID: c687613ff34cdbf5097699e5c896055ef2dc44343c5ab44bd40309d5) where the task (actor ID: 85928d3f1a247800081c0e1101000000, name=ClientAppActor.__init__, pid=619614, memory used=1.73GB) was running was 29.50GB / 31.02GB (0.951049), which exceeds the memory usage threshold of 0.95. Ray killed this worker (ID: c7f644ac3cca20dffbfa1723daac4f87f04c7de03da25e7cad5fe33c) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 115.145.171.128`. To see the logs of the worker, use `ray logs worker-c7f644ac3cca20dffbfa1723daac4f87f04c7de03da25e7cad5fe33c*out -ip 115.145.171.128. Top 10 memory users:
PID	MEM(GB)	COMMAND
4942	3.49	/snap/pycharm-professional/408/jbr/bin/java -classpath /snap/pycharm-professional/408/lib/platform-l...
619616	1.80	ray::ClientAppActor.run
619615	1.78	ray::ClientAppActor
619617	1.77	ray::ClientAppActor
619611	1.73	ray::ClientAppActor
619618	1.73	ray::ClientAppActor
619614	1.73	ray::ClientAppActor
619612	1.72	ray::ClientAppActor
619619	1.70	ray::ClientAppActor
619620	1.68	ray::ClientAppActor.run
Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.

[2024-08-13 23:55:33,341][flwr][ERROR] - Task was killed due to the node running low on memory.
Memory on the node (IP: 115.145.171.128, ID: c687613ff34cdbf5097699e5c896055ef2dc44343c5ab44bd40309d5) where the task (actor ID: 52365bb7997979e5a4c7ddbd01000000, name=ClientAppActor.__init__, pid=619618, memory used=1.84GB) was running was 29.49GB / 31.02GB (0.950707), which exceeds the memory usage threshold of 0.95. Ray killed this worker (ID: 7e48cd46c72821b17be65b05a4e61b5846ba998a57d76db44ca4f8fa) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 115.145.171.128`. To see the logs of the worker, use `ray logs worker-7e48cd46c72821b17be65b05a4e61b5846ba998a57d76db44ca4f8fa*out -ip 115.145.171.128. Top 10 memory users:
PID	MEM(GB)	COMMAND
4942	3.48	/snap/pycharm-professional/408/jbr/bin/java -classpath /snap/pycharm-professional/408/lib/platform-l...
616443	2.14	/home/abdenour/miniconda3/envs/fl_with_flower/bin/python /home/abdenour/PycharmProjects/fl_with_flow...
619615	1.90	ray::ClientAppActor
619616	1.88	ray::ClientAppActor
619611	1.87	ray::ClientAppActor
619617	1.86	ray::ClientAppActor
619620	1.85	ray::ClientAppActor
619618	1.84	ray::ClientAppActor
619613	1.82	ray::ClientAppActor
619619	1.80	ray::ClientAppActor
Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.
[2024-08-13 23:55:33,341][flwr][ERROR] - Task was killed due to the node running low on memory.
Memory on the node (IP: 115.145.171.128, ID: c687613ff34cdbf5097699e5c896055ef2dc44343c5ab44bd40309d5) where the task (actor ID: 85928d3f1a247800081c0e1101000000, name=ClientAppActor.__init__, pid=619614, memory used=1.73GB) was running was 29.50GB / 31.02GB (0.951049), which exceeds the memory usage threshold of 0.95. Ray killed this worker (ID: c7f644ac3cca20dffbfa1723daac4f87f04c7de03da25e7cad5fe33c) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 115.145.171.128`. To see the logs of the worker, use `ray logs worker-c7f644ac3cca20dffbfa1723daac4f87f04c7de03da25e7cad5fe33c*out -ip 115.145.171.128. Top 10 memory users:
PID	MEM(GB)	COMMAND
4942	3.49	/snap/pycharm-professional/408/jbr/bin/java -classpath /snap/pycharm-professional/408/lib/platform-l...
619616	1.80	ray::ClientAppActor.run
619615	1.78	ray::ClientAppActor
619617	1.77	ray::ClientAppActor
619611	1.73	ray::ClientAppActor
619618	1.73	ray::ClientAppActor
619614	1.73	ray::ClientAppActor
619612	1.72	ray::ClientAppActor
619619	1.70	ray::ClientAppActor
619620	1.68	ray::ClientAppActor.run
Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.
[2024-08-13 23:55:37,913][flwr][INFO] - aggregate_fit: received 8 results and 2 failures
[2024-08-13 23:55:40,953][flwr][INFO] - fit progress: (253, 14.673768639564514, {'accuracy': 0.7548076923076923}, 1462.2134569920017)
[2024-08-13 23:55:40,953][flwr][INFO] - configure_evaluate: strategy sampled 2 clients (out of 20)
[2024-08-13 23:55:41,591][flwr][INFO] - aggregate_evaluate: received 2 results and 0 failures
[2024-08-13 23:55:41,591][flwr][INFO] - 
[2024-08-13 23:55:41,591][flwr][INFO] - [ROUND 254]
[2024-08-13 23:55:41,591][flwr][INFO] - configure_fit: strategy sampled 10 clients (out of 20)
[2024-08-13 23:55:41,760][flwr][ERROR] - Traceback (most recent call last):
  File "/home/abdenour/miniconda3/envs/fl_with_flower/lib/python3.11/site-packages/flwr/simulation/ray_transport/ray_client_proxy.py", line 94, in _submit_job
    out_mssg, updated_context = self.actor_pool.get_client_result(
                                ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/abdenour/miniconda3/envs/fl_with_flower/lib/python3.11/site-packages/flwr/simulation/ray_transport/ray_actor.py", line 398, in get_client_result
    return self._fetch_future_result(cid)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/abdenour/miniconda3/envs/fl_with_flower/lib/python3.11/site-packages/flwr/simulation/ray_transport/ray_actor.py", line 279, in _fetch_future_result
    res_cid, out_mssg, updated_context = ray.get(
                                         ^^^^^^^^
  File "/home/abdenour/miniconda3/envs/fl_with_flower/lib/python3.11/site-packages/ray/_private/auto_init_hook.py", line 21, in auto_init_wrapper
    return fn(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^
  File "/home/abdenour/miniconda3/envs/fl_with_flower/lib/python3.11/site-packages/ray/_private/client_mode_hook.py", line 103, in wrapper
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "/home/abdenour/miniconda3/envs/fl_with_flower/lib/python3.11/site-packages/ray/_private/worker.py", line 2667, in get
    values, debugger_breakpoint = worker.get_objects(object_refs, timeout=timeout)
                                  ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/abdenour/miniconda3/envs/fl_with_flower/lib/python3.11/site-packages/ray/_private/worker.py", line 866, in get_objects
    raise value
ray.exceptions.OutOfMemoryError: Task was killed due to the node running low on memory.
Memory on the node (IP: 115.145.171.128, ID: c687613ff34cdbf5097699e5c896055ef2dc44343c5ab44bd40309d5) where the task (actor ID: 52365bb7997979e5a4c7ddbd01000000, name=ClientAppActor.__init__, pid=619618, memory used=1.84GB) was running was 29.49GB / 31.02GB (0.950707), which exceeds the memory usage threshold of 0.95. Ray killed this worker (ID: 7e48cd46c72821b17be65b05a4e61b5846ba998a57d76db44ca4f8fa) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 115.145.171.128`. To see the logs of the worker, use `ray logs worker-7e48cd46c72821b17be65b05a4e61b5846ba998a57d76db44ca4f8fa*out -ip 115.145.171.128. Top 10 memory users:
PID	MEM(GB)	COMMAND
4942	3.48	/snap/pycharm-professional/408/jbr/bin/java -classpath /snap/pycharm-professional/408/lib/platform-l...
616443	2.14	/home/abdenour/miniconda3/envs/fl_with_flower/bin/python /home/abdenour/PycharmProjects/fl_with_flow...
619615	1.90	ray::ClientAppActor
619616	1.88	ray::ClientAppActor
619611	1.87	ray::ClientAppActor
619617	1.86	ray::ClientAppActor
619620	1.85	ray::ClientAppActor
619618	1.84	ray::ClientAppActor
619613	1.82	ray::ClientAppActor
619619	1.80	ray::ClientAppActor
Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.

[2024-08-13 23:55:41,760][flwr][ERROR] - Traceback (most recent call last):
  File "/home/abdenour/miniconda3/envs/fl_with_flower/lib/python3.11/site-packages/flwr/simulation/ray_transport/ray_client_proxy.py", line 94, in _submit_job
    out_mssg, updated_context = self.actor_pool.get_client_result(
                                ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/abdenour/miniconda3/envs/fl_with_flower/lib/python3.11/site-packages/flwr/simulation/ray_transport/ray_actor.py", line 398, in get_client_result
    return self._fetch_future_result(cid)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/abdenour/miniconda3/envs/fl_with_flower/lib/python3.11/site-packages/flwr/simulation/ray_transport/ray_actor.py", line 279, in _fetch_future_result
    res_cid, out_mssg, updated_context = ray.get(
                                         ^^^^^^^^
  File "/home/abdenour/miniconda3/envs/fl_with_flower/lib/python3.11/site-packages/ray/_private/auto_init_hook.py", line 21, in auto_init_wrapper
    return fn(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^
  File "/home/abdenour/miniconda3/envs/fl_with_flower/lib/python3.11/site-packages/ray/_private/client_mode_hook.py", line 103, in wrapper
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "/home/abdenour/miniconda3/envs/fl_with_flower/lib/python3.11/site-packages/ray/_private/worker.py", line 2667, in get
    values, debugger_breakpoint = worker.get_objects(object_refs, timeout=timeout)
                                  ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/abdenour/miniconda3/envs/fl_with_flower/lib/python3.11/site-packages/ray/_private/worker.py", line 866, in get_objects
    raise value
ray.exceptions.OutOfMemoryError: Task was killed due to the node running low on memory.
Memory on the node (IP: 115.145.171.128, ID: c687613ff34cdbf5097699e5c896055ef2dc44343c5ab44bd40309d5) where the task (actor ID: 85928d3f1a247800081c0e1101000000, name=ClientAppActor.__init__, pid=619614, memory used=1.73GB) was running was 29.50GB / 31.02GB (0.951049), which exceeds the memory usage threshold of 0.95. Ray killed this worker (ID: c7f644ac3cca20dffbfa1723daac4f87f04c7de03da25e7cad5fe33c) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 115.145.171.128`. To see the logs of the worker, use `ray logs worker-c7f644ac3cca20dffbfa1723daac4f87f04c7de03da25e7cad5fe33c*out -ip 115.145.171.128. Top 10 memory users:
PID	MEM(GB)	COMMAND
4942	3.49	/snap/pycharm-professional/408/jbr/bin/java -classpath /snap/pycharm-professional/408/lib/platform-l...
619616	1.80	ray::ClientAppActor.run
619615	1.78	ray::ClientAppActor
619617	1.77	ray::ClientAppActor
619611	1.73	ray::ClientAppActor
619618	1.73	ray::ClientAppActor
619614	1.73	ray::ClientAppActor
619612	1.72	ray::ClientAppActor
619619	1.70	ray::ClientAppActor
619620	1.68	ray::ClientAppActor.run
Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.

[2024-08-13 23:55:41,760][flwr][ERROR] - Task was killed due to the node running low on memory.
Memory on the node (IP: 115.145.171.128, ID: c687613ff34cdbf5097699e5c896055ef2dc44343c5ab44bd40309d5) where the task (actor ID: 52365bb7997979e5a4c7ddbd01000000, name=ClientAppActor.__init__, pid=619618, memory used=1.84GB) was running was 29.49GB / 31.02GB (0.950707), which exceeds the memory usage threshold of 0.95. Ray killed this worker (ID: 7e48cd46c72821b17be65b05a4e61b5846ba998a57d76db44ca4f8fa) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 115.145.171.128`. To see the logs of the worker, use `ray logs worker-7e48cd46c72821b17be65b05a4e61b5846ba998a57d76db44ca4f8fa*out -ip 115.145.171.128. Top 10 memory users:
PID	MEM(GB)	COMMAND
4942	3.48	/snap/pycharm-professional/408/jbr/bin/java -classpath /snap/pycharm-professional/408/lib/platform-l...
616443	2.14	/home/abdenour/miniconda3/envs/fl_with_flower/bin/python /home/abdenour/PycharmProjects/fl_with_flow...
619615	1.90	ray::ClientAppActor
619616	1.88	ray::ClientAppActor
619611	1.87	ray::ClientAppActor
619617	1.86	ray::ClientAppActor
619620	1.85	ray::ClientAppActor
619618	1.84	ray::ClientAppActor
619613	1.82	ray::ClientAppActor
619619	1.80	ray::ClientAppActor
Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.
[2024-08-13 23:55:41,760][flwr][ERROR] - Task was killed due to the node running low on memory.
Memory on the node (IP: 115.145.171.128, ID: c687613ff34cdbf5097699e5c896055ef2dc44343c5ab44bd40309d5) where the task (actor ID: 85928d3f1a247800081c0e1101000000, name=ClientAppActor.__init__, pid=619614, memory used=1.73GB) was running was 29.50GB / 31.02GB (0.951049), which exceeds the memory usage threshold of 0.95. Ray killed this worker (ID: c7f644ac3cca20dffbfa1723daac4f87f04c7de03da25e7cad5fe33c) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 115.145.171.128`. To see the logs of the worker, use `ray logs worker-c7f644ac3cca20dffbfa1723daac4f87f04c7de03da25e7cad5fe33c*out -ip 115.145.171.128. Top 10 memory users:
PID	MEM(GB)	COMMAND
4942	3.49	/snap/pycharm-professional/408/jbr/bin/java -classpath /snap/pycharm-professional/408/lib/platform-l...
619616	1.80	ray::ClientAppActor.run
619615	1.78	ray::ClientAppActor
619617	1.77	ray::ClientAppActor
619611	1.73	ray::ClientAppActor
619618	1.73	ray::ClientAppActor
619614	1.73	ray::ClientAppActor
619612	1.72	ray::ClientAppActor
619619	1.70	ray::ClientAppActor
619620	1.68	ray::ClientAppActor.run
Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.
[2024-08-13 23:55:43,752][flwr][INFO] - aggregate_fit: received 8 results and 2 failures
[2024-08-13 23:55:46,689][flwr][INFO] - fit progress: (254, 16.82463312149048, {'accuracy': 0.7548076923076923}, 1467.9493596719985)
[2024-08-13 23:55:46,689][flwr][INFO] - configure_evaluate: strategy sampled 2 clients (out of 20)
[2024-08-13 23:55:47,176][flwr][INFO] - aggregate_evaluate: received 2 results and 0 failures
[2024-08-13 23:55:47,176][flwr][INFO] - 
[2024-08-13 23:55:47,176][flwr][INFO] - [ROUND 255]
[2024-08-13 23:55:47,177][flwr][INFO] - configure_fit: strategy sampled 10 clients (out of 20)
[2024-08-13 23:55:47,337][flwr][ERROR] - Traceback (most recent call last):
  File "/home/abdenour/miniconda3/envs/fl_with_flower/lib/python3.11/site-packages/flwr/simulation/ray_transport/ray_client_proxy.py", line 94, in _submit_job
    out_mssg, updated_context = self.actor_pool.get_client_result(
                                ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/abdenour/miniconda3/envs/fl_with_flower/lib/python3.11/site-packages/flwr/simulation/ray_transport/ray_actor.py", line 398, in get_client_result
    return self._fetch_future_result(cid)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/abdenour/miniconda3/envs/fl_with_flower/lib/python3.11/site-packages/flwr/simulation/ray_transport/ray_actor.py", line 279, in _fetch_future_result
    res_cid, out_mssg, updated_context = ray.get(
                                         ^^^^^^^^
  File "/home/abdenour/miniconda3/envs/fl_with_flower/lib/python3.11/site-packages/ray/_private/auto_init_hook.py", line 21, in auto_init_wrapper
    return fn(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^
  File "/home/abdenour/miniconda3/envs/fl_with_flower/lib/python3.11/site-packages/ray/_private/client_mode_hook.py", line 103, in wrapper
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "/home/abdenour/miniconda3/envs/fl_with_flower/lib/python3.11/site-packages/ray/_private/worker.py", line 2667, in get
    values, debugger_breakpoint = worker.get_objects(object_refs, timeout=timeout)
                                  ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/abdenour/miniconda3/envs/fl_with_flower/lib/python3.11/site-packages/ray/_private/worker.py", line 866, in get_objects
    raise value
ray.exceptions.OutOfMemoryError: Task was killed due to the node running low on memory.
Memory on the node (IP: 115.145.171.128, ID: c687613ff34cdbf5097699e5c896055ef2dc44343c5ab44bd40309d5) where the task (actor ID: 85928d3f1a247800081c0e1101000000, name=ClientAppActor.__init__, pid=619614, memory used=1.73GB) was running was 29.50GB / 31.02GB (0.951049), which exceeds the memory usage threshold of 0.95. Ray killed this worker (ID: c7f644ac3cca20dffbfa1723daac4f87f04c7de03da25e7cad5fe33c) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 115.145.171.128`. To see the logs of the worker, use `ray logs worker-c7f644ac3cca20dffbfa1723daac4f87f04c7de03da25e7cad5fe33c*out -ip 115.145.171.128. Top 10 memory users:
PID	MEM(GB)	COMMAND
4942	3.49	/snap/pycharm-professional/408/jbr/bin/java -classpath /snap/pycharm-professional/408/lib/platform-l...
619616	1.80	ray::ClientAppActor.run
619615	1.78	ray::ClientAppActor
619617	1.77	ray::ClientAppActor
619611	1.73	ray::ClientAppActor
619618	1.73	ray::ClientAppActor
619614	1.73	ray::ClientAppActor
619612	1.72	ray::ClientAppActor
619619	1.70	ray::ClientAppActor
619620	1.68	ray::ClientAppActor.run
Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.

[2024-08-13 23:55:47,337][flwr][ERROR] - Task was killed due to the node running low on memory.
Memory on the node (IP: 115.145.171.128, ID: c687613ff34cdbf5097699e5c896055ef2dc44343c5ab44bd40309d5) where the task (actor ID: 85928d3f1a247800081c0e1101000000, name=ClientAppActor.__init__, pid=619614, memory used=1.73GB) was running was 29.50GB / 31.02GB (0.951049), which exceeds the memory usage threshold of 0.95. Ray killed this worker (ID: c7f644ac3cca20dffbfa1723daac4f87f04c7de03da25e7cad5fe33c) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 115.145.171.128`. To see the logs of the worker, use `ray logs worker-c7f644ac3cca20dffbfa1723daac4f87f04c7de03da25e7cad5fe33c*out -ip 115.145.171.128. Top 10 memory users:
PID	MEM(GB)	COMMAND
4942	3.49	/snap/pycharm-professional/408/jbr/bin/java -classpath /snap/pycharm-professional/408/lib/platform-l...
619616	1.80	ray::ClientAppActor.run
619615	1.78	ray::ClientAppActor
619617	1.77	ray::ClientAppActor
619611	1.73	ray::ClientAppActor
619618	1.73	ray::ClientAppActor
619614	1.73	ray::ClientAppActor
619612	1.72	ray::ClientAppActor
619619	1.70	ray::ClientAppActor
619620	1.68	ray::ClientAppActor.run
Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.
[2024-08-13 23:55:47,337][flwr][ERROR] - Traceback (most recent call last):
  File "/home/abdenour/miniconda3/envs/fl_with_flower/lib/python3.11/site-packages/flwr/simulation/ray_transport/ray_client_proxy.py", line 94, in _submit_job
    out_mssg, updated_context = self.actor_pool.get_client_result(
                                ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/abdenour/miniconda3/envs/fl_with_flower/lib/python3.11/site-packages/flwr/simulation/ray_transport/ray_actor.py", line 398, in get_client_result
    return self._fetch_future_result(cid)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/abdenour/miniconda3/envs/fl_with_flower/lib/python3.11/site-packages/flwr/simulation/ray_transport/ray_actor.py", line 279, in _fetch_future_result
    res_cid, out_mssg, updated_context = ray.get(
                                         ^^^^^^^^
  File "/home/abdenour/miniconda3/envs/fl_with_flower/lib/python3.11/site-packages/ray/_private/auto_init_hook.py", line 21, in auto_init_wrapper
    return fn(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^
  File "/home/abdenour/miniconda3/envs/fl_with_flower/lib/python3.11/site-packages/ray/_private/client_mode_hook.py", line 103, in wrapper
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "/home/abdenour/miniconda3/envs/fl_with_flower/lib/python3.11/site-packages/ray/_private/worker.py", line 2667, in get
    values, debugger_breakpoint = worker.get_objects(object_refs, timeout=timeout)
                                  ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/abdenour/miniconda3/envs/fl_with_flower/lib/python3.11/site-packages/ray/_private/worker.py", line 866, in get_objects
    raise value
ray.exceptions.OutOfMemoryError: Task was killed due to the node running low on memory.
Memory on the node (IP: 115.145.171.128, ID: c687613ff34cdbf5097699e5c896055ef2dc44343c5ab44bd40309d5) where the task (actor ID: 52365bb7997979e5a4c7ddbd01000000, name=ClientAppActor.__init__, pid=619618, memory used=1.84GB) was running was 29.49GB / 31.02GB (0.950707), which exceeds the memory usage threshold of 0.95. Ray killed this worker (ID: 7e48cd46c72821b17be65b05a4e61b5846ba998a57d76db44ca4f8fa) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 115.145.171.128`. To see the logs of the worker, use `ray logs worker-7e48cd46c72821b17be65b05a4e61b5846ba998a57d76db44ca4f8fa*out -ip 115.145.171.128. Top 10 memory users:
PID	MEM(GB)	COMMAND
4942	3.48	/snap/pycharm-professional/408/jbr/bin/java -classpath /snap/pycharm-professional/408/lib/platform-l...
616443	2.14	/home/abdenour/miniconda3/envs/fl_with_flower/bin/python /home/abdenour/PycharmProjects/fl_with_flow...
619615	1.90	ray::ClientAppActor
619616	1.88	ray::ClientAppActor
619611	1.87	ray::ClientAppActor
619617	1.86	ray::ClientAppActor
619620	1.85	ray::ClientAppActor
619618	1.84	ray::ClientAppActor
619613	1.82	ray::ClientAppActor
619619	1.80	ray::ClientAppActor
Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.

[2024-08-13 23:55:47,337][flwr][ERROR] - Task was killed due to the node running low on memory.
Memory on the node (IP: 115.145.171.128, ID: c687613ff34cdbf5097699e5c896055ef2dc44343c5ab44bd40309d5) where the task (actor ID: 52365bb7997979e5a4c7ddbd01000000, name=ClientAppActor.__init__, pid=619618, memory used=1.84GB) was running was 29.49GB / 31.02GB (0.950707), which exceeds the memory usage threshold of 0.95. Ray killed this worker (ID: 7e48cd46c72821b17be65b05a4e61b5846ba998a57d76db44ca4f8fa) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 115.145.171.128`. To see the logs of the worker, use `ray logs worker-7e48cd46c72821b17be65b05a4e61b5846ba998a57d76db44ca4f8fa*out -ip 115.145.171.128. Top 10 memory users:
PID	MEM(GB)	COMMAND
4942	3.48	/snap/pycharm-professional/408/jbr/bin/java -classpath /snap/pycharm-professional/408/lib/platform-l...
616443	2.14	/home/abdenour/miniconda3/envs/fl_with_flower/bin/python /home/abdenour/PycharmProjects/fl_with_flow...
619615	1.90	ray::ClientAppActor
619616	1.88	ray::ClientAppActor
619611	1.87	ray::ClientAppActor
619617	1.86	ray::ClientAppActor
619620	1.85	ray::ClientAppActor
619618	1.84	ray::ClientAppActor
619613	1.82	ray::ClientAppActor
619619	1.80	ray::ClientAppActor
Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.
[2024-08-13 23:55:50,392][flwr][INFO] - aggregate_fit: received 8 results and 2 failures
[2024-08-13 23:55:53,248][flwr][INFO] - fit progress: (255, 17.26326411962509, {'accuracy': 0.7532051282051282}, 1474.5081530610041)
[2024-08-13 23:55:53,248][flwr][INFO] - configure_evaluate: strategy sampled 2 clients (out of 20)
[2024-08-13 23:55:53,598][flwr][INFO] - aggregate_evaluate: received 2 results and 0 failures
[2024-08-13 23:55:53,598][flwr][INFO] - 
[2024-08-13 23:55:53,598][flwr][INFO] - [ROUND 256]
[2024-08-13 23:55:53,598][flwr][INFO] - configure_fit: strategy sampled 10 clients (out of 20)
[2024-08-13 23:55:53,733][flwr][ERROR] - Traceback (most recent call last):
  File "/home/abdenour/miniconda3/envs/fl_with_flower/lib/python3.11/site-packages/flwr/simulation/ray_transport/ray_client_proxy.py", line 94, in _submit_job
    out_mssg, updated_context = self.actor_pool.get_client_result(
                                ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/abdenour/miniconda3/envs/fl_with_flower/lib/python3.11/site-packages/flwr/simulation/ray_transport/ray_actor.py", line 398, in get_client_result
    return self._fetch_future_result(cid)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/abdenour/miniconda3/envs/fl_with_flower/lib/python3.11/site-packages/flwr/simulation/ray_transport/ray_actor.py", line 279, in _fetch_future_result
    res_cid, out_mssg, updated_context = ray.get(
                                         ^^^^^^^^
  File "/home/abdenour/miniconda3/envs/fl_with_flower/lib/python3.11/site-packages/ray/_private/auto_init_hook.py", line 21, in auto_init_wrapper
    return fn(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^
  File "/home/abdenour/miniconda3/envs/fl_with_flower/lib/python3.11/site-packages/ray/_private/client_mode_hook.py", line 103, in wrapper
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "/home/abdenour/miniconda3/envs/fl_with_flower/lib/python3.11/site-packages/ray/_private/worker.py", line 2667, in get
    values, debugger_breakpoint = worker.get_objects(object_refs, timeout=timeout)
                                  ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/abdenour/miniconda3/envs/fl_with_flower/lib/python3.11/site-packages/ray/_private/worker.py", line 866, in get_objects
    raise value
ray.exceptions.OutOfMemoryError: Task was killed due to the node running low on memory.
Memory on the node (IP: 115.145.171.128, ID: c687613ff34cdbf5097699e5c896055ef2dc44343c5ab44bd40309d5) where the task (actor ID: 52365bb7997979e5a4c7ddbd01000000, name=ClientAppActor.__init__, pid=619618, memory used=1.84GB) was running was 29.49GB / 31.02GB (0.950707), which exceeds the memory usage threshold of 0.95. Ray killed this worker (ID: 7e48cd46c72821b17be65b05a4e61b5846ba998a57d76db44ca4f8fa) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 115.145.171.128`. To see the logs of the worker, use `ray logs worker-7e48cd46c72821b17be65b05a4e61b5846ba998a57d76db44ca4f8fa*out -ip 115.145.171.128. Top 10 memory users:
PID	MEM(GB)	COMMAND
4942	3.48	/snap/pycharm-professional/408/jbr/bin/java -classpath /snap/pycharm-professional/408/lib/platform-l...
616443	2.14	/home/abdenour/miniconda3/envs/fl_with_flower/bin/python /home/abdenour/PycharmProjects/fl_with_flow...
619615	1.90	ray::ClientAppActor
619616	1.88	ray::ClientAppActor
619611	1.87	ray::ClientAppActor
619617	1.86	ray::ClientAppActor
619620	1.85	ray::ClientAppActor
619618	1.84	ray::ClientAppActor
619613	1.82	ray::ClientAppActor
619619	1.80	ray::ClientAppActor
Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.

[2024-08-13 23:55:53,734][flwr][ERROR] - Task was killed due to the node running low on memory.
Memory on the node (IP: 115.145.171.128, ID: c687613ff34cdbf5097699e5c896055ef2dc44343c5ab44bd40309d5) where the task (actor ID: 52365bb7997979e5a4c7ddbd01000000, name=ClientAppActor.__init__, pid=619618, memory used=1.84GB) was running was 29.49GB / 31.02GB (0.950707), which exceeds the memory usage threshold of 0.95. Ray killed this worker (ID: 7e48cd46c72821b17be65b05a4e61b5846ba998a57d76db44ca4f8fa) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 115.145.171.128`. To see the logs of the worker, use `ray logs worker-7e48cd46c72821b17be65b05a4e61b5846ba998a57d76db44ca4f8fa*out -ip 115.145.171.128. Top 10 memory users:
PID	MEM(GB)	COMMAND
4942	3.48	/snap/pycharm-professional/408/jbr/bin/java -classpath /snap/pycharm-professional/408/lib/platform-l...
616443	2.14	/home/abdenour/miniconda3/envs/fl_with_flower/bin/python /home/abdenour/PycharmProjects/fl_with_flow...
619615	1.90	ray::ClientAppActor
619616	1.88	ray::ClientAppActor
619611	1.87	ray::ClientAppActor
619617	1.86	ray::ClientAppActor
619620	1.85	ray::ClientAppActor
619618	1.84	ray::ClientAppActor
619613	1.82	ray::ClientAppActor
619619	1.80	ray::ClientAppActor
Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.
[2024-08-13 23:55:53,734][flwr][ERROR] - Traceback (most recent call last):
  File "/home/abdenour/miniconda3/envs/fl_with_flower/lib/python3.11/site-packages/flwr/simulation/ray_transport/ray_client_proxy.py", line 94, in _submit_job
    out_mssg, updated_context = self.actor_pool.get_client_result(
                                ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/abdenour/miniconda3/envs/fl_with_flower/lib/python3.11/site-packages/flwr/simulation/ray_transport/ray_actor.py", line 398, in get_client_result
    return self._fetch_future_result(cid)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/abdenour/miniconda3/envs/fl_with_flower/lib/python3.11/site-packages/flwr/simulation/ray_transport/ray_actor.py", line 279, in _fetch_future_result
    res_cid, out_mssg, updated_context = ray.get(
                                         ^^^^^^^^
  File "/home/abdenour/miniconda3/envs/fl_with_flower/lib/python3.11/site-packages/ray/_private/auto_init_hook.py", line 21, in auto_init_wrapper
    return fn(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^
  File "/home/abdenour/miniconda3/envs/fl_with_flower/lib/python3.11/site-packages/ray/_private/client_mode_hook.py", line 103, in wrapper
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "/home/abdenour/miniconda3/envs/fl_with_flower/lib/python3.11/site-packages/ray/_private/worker.py", line 2667, in get
    values, debugger_breakpoint = worker.get_objects(object_refs, timeout=timeout)
                                  ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/abdenour/miniconda3/envs/fl_with_flower/lib/python3.11/site-packages/ray/_private/worker.py", line 866, in get_objects
    raise value
ray.exceptions.OutOfMemoryError: Task was killed due to the node running low on memory.
Memory on the node (IP: 115.145.171.128, ID: c687613ff34cdbf5097699e5c896055ef2dc44343c5ab44bd40309d5) where the task (actor ID: 85928d3f1a247800081c0e1101000000, name=ClientAppActor.__init__, pid=619614, memory used=1.73GB) was running was 29.50GB / 31.02GB (0.951049), which exceeds the memory usage threshold of 0.95. Ray killed this worker (ID: c7f644ac3cca20dffbfa1723daac4f87f04c7de03da25e7cad5fe33c) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 115.145.171.128`. To see the logs of the worker, use `ray logs worker-c7f644ac3cca20dffbfa1723daac4f87f04c7de03da25e7cad5fe33c*out -ip 115.145.171.128. Top 10 memory users:
PID	MEM(GB)	COMMAND
4942	3.49	/snap/pycharm-professional/408/jbr/bin/java -classpath /snap/pycharm-professional/408/lib/platform-l...
619616	1.80	ray::ClientAppActor.run
619615	1.78	ray::ClientAppActor
619617	1.77	ray::ClientAppActor
619611	1.73	ray::ClientAppActor
619618	1.73	ray::ClientAppActor
619614	1.73	ray::ClientAppActor
619612	1.72	ray::ClientAppActor
619619	1.70	ray::ClientAppActor
619620	1.68	ray::ClientAppActor.run
Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.

[2024-08-13 23:55:53,734][flwr][ERROR] - Traceback (most recent call last):
  File "/home/abdenour/miniconda3/envs/fl_with_flower/lib/python3.11/site-packages/flwr/simulation/ray_transport/ray_client_proxy.py", line 94, in _submit_job
    out_mssg, updated_context = self.actor_pool.get_client_result(
                                ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/abdenour/miniconda3/envs/fl_with_flower/lib/python3.11/site-packages/flwr/simulation/ray_transport/ray_actor.py", line 398, in get_client_result
    return self._fetch_future_result(cid)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/abdenour/miniconda3/envs/fl_with_flower/lib/python3.11/site-packages/flwr/simulation/ray_transport/ray_actor.py", line 279, in _fetch_future_result
    res_cid, out_mssg, updated_context = ray.get(
                                         ^^^^^^^^
  File "/home/abdenour/miniconda3/envs/fl_with_flower/lib/python3.11/site-packages/ray/_private/auto_init_hook.py", line 21, in auto_init_wrapper
    return fn(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^
  File "/home/abdenour/miniconda3/envs/fl_with_flower/lib/python3.11/site-packages/ray/_private/client_mode_hook.py", line 103, in wrapper
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "/home/abdenour/miniconda3/envs/fl_with_flower/lib/python3.11/site-packages/ray/_private/worker.py", line 2667, in get
    values, debugger_breakpoint = worker.get_objects(object_refs, timeout=timeout)
                                  ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/abdenour/miniconda3/envs/fl_with_flower/lib/python3.11/site-packages/ray/_private/worker.py", line 866, in get_objects
    raise value
ray.exceptions.OutOfMemoryError: Task was killed due to the node running low on memory.
Memory on the node (IP: 115.145.171.128, ID: c687613ff34cdbf5097699e5c896055ef2dc44343c5ab44bd40309d5) where the task (actor ID: 2a8bb25e35bf7bb5a0a6c00401000000, name=ClientAppActor.__init__, pid=619620, memory used=1.97GB) was running was 29.48GB / 31.02GB (0.950487), which exceeds the memory usage threshold of 0.95. Ray killed this worker (ID: 38a4d571c3fa7ff405dd9f5d1555b4753efb95dc84bb70ce01b6ecb4) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 115.145.171.128`. To see the logs of the worker, use `ray logs worker-38a4d571c3fa7ff405dd9f5d1555b4753efb95dc84bb70ce01b6ecb4*out -ip 115.145.171.128. Top 10 memory users:
PID	MEM(GB)	COMMAND
4942	3.48	/snap/pycharm-professional/408/jbr/bin/java -classpath /snap/pycharm-professional/408/lib/platform-l...
616443	2.85	/home/abdenour/miniconda3/envs/fl_with_flower/bin/python /home/abdenour/PycharmProjects/fl_with_flow...
619615	2.06	ray::ClientAppActor
619616	2.04	ray::ClientAppActor
619611	2.04	ray::ClientAppActor.run
619619	2.03	ray::ClientAppActor.run
619612	1.98	ray::ClientAppActor.run
619613	1.97	ray::ClientAppActor.run
619620	1.97	ray::ClientAppActor
619617	1.96	ray::ClientAppActor
Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.

[2024-08-13 23:55:53,734][flwr][ERROR] - Task was killed due to the node running low on memory.
Memory on the node (IP: 115.145.171.128, ID: c687613ff34cdbf5097699e5c896055ef2dc44343c5ab44bd40309d5) where the task (actor ID: 85928d3f1a247800081c0e1101000000, name=ClientAppActor.__init__, pid=619614, memory used=1.73GB) was running was 29.50GB / 31.02GB (0.951049), which exceeds the memory usage threshold of 0.95. Ray killed this worker (ID: c7f644ac3cca20dffbfa1723daac4f87f04c7de03da25e7cad5fe33c) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 115.145.171.128`. To see the logs of the worker, use `ray logs worker-c7f644ac3cca20dffbfa1723daac4f87f04c7de03da25e7cad5fe33c*out -ip 115.145.171.128. Top 10 memory users:
PID	MEM(GB)	COMMAND
4942	3.49	/snap/pycharm-professional/408/jbr/bin/java -classpath /snap/pycharm-professional/408/lib/platform-l...
619616	1.80	ray::ClientAppActor.run
619615	1.78	ray::ClientAppActor
619617	1.77	ray::ClientAppActor
619611	1.73	ray::ClientAppActor
619618	1.73	ray::ClientAppActor
619614	1.73	ray::ClientAppActor
619612	1.72	ray::ClientAppActor
619619	1.70	ray::ClientAppActor
619620	1.68	ray::ClientAppActor.run
Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.
[2024-08-13 23:55:53,734][flwr][ERROR] - Task was killed due to the node running low on memory.
Memory on the node (IP: 115.145.171.128, ID: c687613ff34cdbf5097699e5c896055ef2dc44343c5ab44bd40309d5) where the task (actor ID: 2a8bb25e35bf7bb5a0a6c00401000000, name=ClientAppActor.__init__, pid=619620, memory used=1.97GB) was running was 29.48GB / 31.02GB (0.950487), which exceeds the memory usage threshold of 0.95. Ray killed this worker (ID: 38a4d571c3fa7ff405dd9f5d1555b4753efb95dc84bb70ce01b6ecb4) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 115.145.171.128`. To see the logs of the worker, use `ray logs worker-38a4d571c3fa7ff405dd9f5d1555b4753efb95dc84bb70ce01b6ecb4*out -ip 115.145.171.128. Top 10 memory users:
PID	MEM(GB)	COMMAND
4942	3.48	/snap/pycharm-professional/408/jbr/bin/java -classpath /snap/pycharm-professional/408/lib/platform-l...
616443	2.85	/home/abdenour/miniconda3/envs/fl_with_flower/bin/python /home/abdenour/PycharmProjects/fl_with_flow...
619615	2.06	ray::ClientAppActor
619616	2.04	ray::ClientAppActor
619611	2.04	ray::ClientAppActor.run
619619	2.03	ray::ClientAppActor.run
619612	1.98	ray::ClientAppActor.run
619613	1.97	ray::ClientAppActor.run
619620	1.97	ray::ClientAppActor
619617	1.96	ray::ClientAppActor
Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.
[2024-08-13 23:55:56,302][flwr][INFO] - aggregate_fit: received 7 results and 3 failures
[2024-08-13 23:55:59,263][flwr][INFO] - fit progress: (256, 16.56855869293213, {'accuracy': 0.7532051282051282}, 1480.523362070002)
[2024-08-13 23:55:59,263][flwr][INFO] - configure_evaluate: strategy sampled 2 clients (out of 20)
[2024-08-13 23:55:59,554][flwr][INFO] - aggregate_evaluate: received 2 results and 0 failures
[2024-08-13 23:55:59,554][flwr][INFO] - 
[2024-08-13 23:55:59,555][flwr][INFO] - [ROUND 257]
[2024-08-13 23:55:59,555][flwr][INFO] - configure_fit: strategy sampled 10 clients (out of 20)
[2024-08-13 23:55:59,693][flwr][ERROR] - Traceback (most recent call last):
  File "/home/abdenour/miniconda3/envs/fl_with_flower/lib/python3.11/site-packages/flwr/simulation/ray_transport/ray_client_proxy.py", line 94, in _submit_job
    out_mssg, updated_context = self.actor_pool.get_client_result(
                                ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/abdenour/miniconda3/envs/fl_with_flower/lib/python3.11/site-packages/flwr/simulation/ray_transport/ray_actor.py", line 398, in get_client_result
    return self._fetch_future_result(cid)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/abdenour/miniconda3/envs/fl_with_flower/lib/python3.11/site-packages/flwr/simulation/ray_transport/ray_actor.py", line 279, in _fetch_future_result
    res_cid, out_mssg, updated_context = ray.get(
                                         ^^^^^^^^
  File "/home/abdenour/miniconda3/envs/fl_with_flower/lib/python3.11/site-packages/ray/_private/auto_init_hook.py", line 21, in auto_init_wrapper
    return fn(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^
  File "/home/abdenour/miniconda3/envs/fl_with_flower/lib/python3.11/site-packages/ray/_private/client_mode_hook.py", line 103, in wrapper
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "/home/abdenour/miniconda3/envs/fl_with_flower/lib/python3.11/site-packages/ray/_private/worker.py", line 2667, in get
    values, debugger_breakpoint = worker.get_objects(object_refs, timeout=timeout)
                                  ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/abdenour/miniconda3/envs/fl_with_flower/lib/python3.11/site-packages/ray/_private/worker.py", line 866, in get_objects
    raise value
ray.exceptions.OutOfMemoryError: Task was killed due to the node running low on memory.
Memory on the node (IP: 115.145.171.128, ID: c687613ff34cdbf5097699e5c896055ef2dc44343c5ab44bd40309d5) where the task (actor ID: 85928d3f1a247800081c0e1101000000, name=ClientAppActor.__init__, pid=619614, memory used=1.73GB) was running was 29.50GB / 31.02GB (0.951049), which exceeds the memory usage threshold of 0.95. Ray killed this worker (ID: c7f644ac3cca20dffbfa1723daac4f87f04c7de03da25e7cad5fe33c) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 115.145.171.128`. To see the logs of the worker, use `ray logs worker-c7f644ac3cca20dffbfa1723daac4f87f04c7de03da25e7cad5fe33c*out -ip 115.145.171.128. Top 10 memory users:
PID	MEM(GB)	COMMAND
4942	3.49	/snap/pycharm-professional/408/jbr/bin/java -classpath /snap/pycharm-professional/408/lib/platform-l...
619616	1.80	ray::ClientAppActor.run
619615	1.78	ray::ClientAppActor
619617	1.77	ray::ClientAppActor
619611	1.73	ray::ClientAppActor
619618	1.73	ray::ClientAppActor
619614	1.73	ray::ClientAppActor
619612	1.72	ray::ClientAppActor
619619	1.70	ray::ClientAppActor
619620	1.68	ray::ClientAppActor.run
Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.

[2024-08-13 23:55:59,693][flwr][ERROR] - Task was killed due to the node running low on memory.
Memory on the node (IP: 115.145.171.128, ID: c687613ff34cdbf5097699e5c896055ef2dc44343c5ab44bd40309d5) where the task (actor ID: 85928d3f1a247800081c0e1101000000, name=ClientAppActor.__init__, pid=619614, memory used=1.73GB) was running was 29.50GB / 31.02GB (0.951049), which exceeds the memory usage threshold of 0.95. Ray killed this worker (ID: c7f644ac3cca20dffbfa1723daac4f87f04c7de03da25e7cad5fe33c) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 115.145.171.128`. To see the logs of the worker, use `ray logs worker-c7f644ac3cca20dffbfa1723daac4f87f04c7de03da25e7cad5fe33c*out -ip 115.145.171.128. Top 10 memory users:
PID	MEM(GB)	COMMAND
4942	3.49	/snap/pycharm-professional/408/jbr/bin/java -classpath /snap/pycharm-professional/408/lib/platform-l...
619616	1.80	ray::ClientAppActor.run
619615	1.78	ray::ClientAppActor
619617	1.77	ray::ClientAppActor
619611	1.73	ray::ClientAppActor
619618	1.73	ray::ClientAppActor
619614	1.73	ray::ClientAppActor
619612	1.72	ray::ClientAppActor
619619	1.70	ray::ClientAppActor
619620	1.68	ray::ClientAppActor.run
Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.
[2024-08-13 23:55:59,694][flwr][ERROR] - Traceback (most recent call last):
  File "/home/abdenour/miniconda3/envs/fl_with_flower/lib/python3.11/site-packages/flwr/simulation/ray_transport/ray_client_proxy.py", line 94, in _submit_job
    out_mssg, updated_context = self.actor_pool.get_client_result(
                                ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/abdenour/miniconda3/envs/fl_with_flower/lib/python3.11/site-packages/flwr/simulation/ray_transport/ray_actor.py", line 398, in get_client_result
    return self._fetch_future_result(cid)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/abdenour/miniconda3/envs/fl_with_flower/lib/python3.11/site-packages/flwr/simulation/ray_transport/ray_actor.py", line 279, in _fetch_future_result
    res_cid, out_mssg, updated_context = ray.get(
                                         ^^^^^^^^
  File "/home/abdenour/miniconda3/envs/fl_with_flower/lib/python3.11/site-packages/ray/_private/auto_init_hook.py", line 21, in auto_init_wrapper
    return fn(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^
  File "/home/abdenour/miniconda3/envs/fl_with_flower/lib/python3.11/site-packages/ray/_private/client_mode_hook.py", line 103, in wrapper
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "/home/abdenour/miniconda3/envs/fl_with_flower/lib/python3.11/site-packages/ray/_private/worker.py", line 2667, in get
    values, debugger_breakpoint = worker.get_objects(object_refs, timeout=timeout)
                                  ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/abdenour/miniconda3/envs/fl_with_flower/lib/python3.11/site-packages/ray/_private/worker.py", line 866, in get_objects
    raise value
ray.exceptions.OutOfMemoryError: Task was killed due to the node running low on memory.
Memory on the node (IP: 115.145.171.128, ID: c687613ff34cdbf5097699e5c896055ef2dc44343c5ab44bd40309d5) where the task (actor ID: 52365bb7997979e5a4c7ddbd01000000, name=ClientAppActor.__init__, pid=619618, memory used=1.84GB) was running was 29.49GB / 31.02GB (0.950707), which exceeds the memory usage threshold of 0.95. Ray killed this worker (ID: 7e48cd46c72821b17be65b05a4e61b5846ba998a57d76db44ca4f8fa) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 115.145.171.128`. To see the logs of the worker, use `ray logs worker-7e48cd46c72821b17be65b05a4e61b5846ba998a57d76db44ca4f8fa*out -ip 115.145.171.128. Top 10 memory users:
PID	MEM(GB)	COMMAND
4942	3.48	/snap/pycharm-professional/408/jbr/bin/java -classpath /snap/pycharm-professional/408/lib/platform-l...
616443	2.14	/home/abdenour/miniconda3/envs/fl_with_flower/bin/python /home/abdenour/PycharmProjects/fl_with_flow...
619615	1.90	ray::ClientAppActor
619616	1.88	ray::ClientAppActor
619611	1.87	ray::ClientAppActor
619617	1.86	ray::ClientAppActor
619620	1.85	ray::ClientAppActor
619618	1.84	ray::ClientAppActor
619613	1.82	ray::ClientAppActor
619619	1.80	ray::ClientAppActor
Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.

[2024-08-13 23:55:59,694][flwr][ERROR] - Task was killed due to the node running low on memory.
Memory on the node (IP: 115.145.171.128, ID: c687613ff34cdbf5097699e5c896055ef2dc44343c5ab44bd40309d5) where the task (actor ID: 52365bb7997979e5a4c7ddbd01000000, name=ClientAppActor.__init__, pid=619618, memory used=1.84GB) was running was 29.49GB / 31.02GB (0.950707), which exceeds the memory usage threshold of 0.95. Ray killed this worker (ID: 7e48cd46c72821b17be65b05a4e61b5846ba998a57d76db44ca4f8fa) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 115.145.171.128`. To see the logs of the worker, use `ray logs worker-7e48cd46c72821b17be65b05a4e61b5846ba998a57d76db44ca4f8fa*out -ip 115.145.171.128. Top 10 memory users:
PID	MEM(GB)	COMMAND
4942	3.48	/snap/pycharm-professional/408/jbr/bin/java -classpath /snap/pycharm-professional/408/lib/platform-l...
616443	2.14	/home/abdenour/miniconda3/envs/fl_with_flower/bin/python /home/abdenour/PycharmProjects/fl_with_flow...
619615	1.90	ray::ClientAppActor
619616	1.88	ray::ClientAppActor
619611	1.87	ray::ClientAppActor
619617	1.86	ray::ClientAppActor
619620	1.85	ray::ClientAppActor
619618	1.84	ray::ClientAppActor
619613	1.82	ray::ClientAppActor
619619	1.80	ray::ClientAppActor
Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.
[2024-08-13 23:55:59,695][flwr][ERROR] - Traceback (most recent call last):
  File "/home/abdenour/miniconda3/envs/fl_with_flower/lib/python3.11/site-packages/flwr/simulation/ray_transport/ray_client_proxy.py", line 94, in _submit_job
    out_mssg, updated_context = self.actor_pool.get_client_result(
                                ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/abdenour/miniconda3/envs/fl_with_flower/lib/python3.11/site-packages/flwr/simulation/ray_transport/ray_actor.py", line 398, in get_client_result
    return self._fetch_future_result(cid)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/abdenour/miniconda3/envs/fl_with_flower/lib/python3.11/site-packages/flwr/simulation/ray_transport/ray_actor.py", line 279, in _fetch_future_result
    res_cid, out_mssg, updated_context = ray.get(
                                         ^^^^^^^^
  File "/home/abdenour/miniconda3/envs/fl_with_flower/lib/python3.11/site-packages/ray/_private/auto_init_hook.py", line 21, in auto_init_wrapper
    return fn(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^
  File "/home/abdenour/miniconda3/envs/fl_with_flower/lib/python3.11/site-packages/ray/_private/client_mode_hook.py", line 103, in wrapper
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "/home/abdenour/miniconda3/envs/fl_with_flower/lib/python3.11/site-packages/ray/_private/worker.py", line 2667, in get
    values, debugger_breakpoint = worker.get_objects(object_refs, timeout=timeout)
                                  ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/abdenour/miniconda3/envs/fl_with_flower/lib/python3.11/site-packages/ray/_private/worker.py", line 866, in get_objects
    raise value
ray.exceptions.OutOfMemoryError: Task was killed due to the node running low on memory.
Memory on the node (IP: 115.145.171.128, ID: c687613ff34cdbf5097699e5c896055ef2dc44343c5ab44bd40309d5) where the task (actor ID: 2a8bb25e35bf7bb5a0a6c00401000000, name=ClientAppActor.__init__, pid=619620, memory used=1.97GB) was running was 29.48GB / 31.02GB (0.950487), which exceeds the memory usage threshold of 0.95. Ray killed this worker (ID: 38a4d571c3fa7ff405dd9f5d1555b4753efb95dc84bb70ce01b6ecb4) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 115.145.171.128`. To see the logs of the worker, use `ray logs worker-38a4d571c3fa7ff405dd9f5d1555b4753efb95dc84bb70ce01b6ecb4*out -ip 115.145.171.128. Top 10 memory users:
PID	MEM(GB)	COMMAND
4942	3.48	/snap/pycharm-professional/408/jbr/bin/java -classpath /snap/pycharm-professional/408/lib/platform-l...
616443	2.85	/home/abdenour/miniconda3/envs/fl_with_flower/bin/python /home/abdenour/PycharmProjects/fl_with_flow...
619615	2.06	ray::ClientAppActor
619616	2.04	ray::ClientAppActor
619611	2.04	ray::ClientAppActor.run
619619	2.03	ray::ClientAppActor.run
619612	1.98	ray::ClientAppActor.run
619613	1.97	ray::ClientAppActor.run
619620	1.97	ray::ClientAppActor
619617	1.96	ray::ClientAppActor
Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.

[2024-08-13 23:55:59,695][flwr][ERROR] - Task was killed due to the node running low on memory.
Memory on the node (IP: 115.145.171.128, ID: c687613ff34cdbf5097699e5c896055ef2dc44343c5ab44bd40309d5) where the task (actor ID: 2a8bb25e35bf7bb5a0a6c00401000000, name=ClientAppActor.__init__, pid=619620, memory used=1.97GB) was running was 29.48GB / 31.02GB (0.950487), which exceeds the memory usage threshold of 0.95. Ray killed this worker (ID: 38a4d571c3fa7ff405dd9f5d1555b4753efb95dc84bb70ce01b6ecb4) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 115.145.171.128`. To see the logs of the worker, use `ray logs worker-38a4d571c3fa7ff405dd9f5d1555b4753efb95dc84bb70ce01b6ecb4*out -ip 115.145.171.128. Top 10 memory users:
PID	MEM(GB)	COMMAND
4942	3.48	/snap/pycharm-professional/408/jbr/bin/java -classpath /snap/pycharm-professional/408/lib/platform-l...
616443	2.85	/home/abdenour/miniconda3/envs/fl_with_flower/bin/python /home/abdenour/PycharmProjects/fl_with_flow...
619615	2.06	ray::ClientAppActor
619616	2.04	ray::ClientAppActor
619611	2.04	ray::ClientAppActor.run
619619	2.03	ray::ClientAppActor.run
619612	1.98	ray::ClientAppActor.run
619613	1.97	ray::ClientAppActor.run
619620	1.97	ray::ClientAppActor
619617	1.96	ray::ClientAppActor
Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.
[2024-08-13 23:56:02,347][flwr][INFO] - aggregate_fit: received 7 results and 3 failures
[2024-08-13 23:56:05,158][flwr][INFO] - fit progress: (257, 14.467011034488678, {'accuracy': 0.7564102564102564}, 1486.4184701570193)
[2024-08-13 23:56:05,158][flwr][INFO] - configure_evaluate: strategy sampled 2 clients (out of 20)
[2024-08-13 23:56:05,497][flwr][INFO] - aggregate_evaluate: received 2 results and 0 failures
[2024-08-13 23:56:05,497][flwr][INFO] - 
[2024-08-13 23:56:05,498][flwr][INFO] - [ROUND 258]
[2024-08-13 23:56:05,498][flwr][INFO] - configure_fit: strategy sampled 10 clients (out of 20)
[2024-08-13 23:56:05,657][flwr][ERROR] - Traceback (most recent call last):
  File "/home/abdenour/miniconda3/envs/fl_with_flower/lib/python3.11/site-packages/flwr/simulation/ray_transport/ray_client_proxy.py", line 94, in _submit_job
    out_mssg, updated_context = self.actor_pool.get_client_result(
                                ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/abdenour/miniconda3/envs/fl_with_flower/lib/python3.11/site-packages/flwr/simulation/ray_transport/ray_actor.py", line 398, in get_client_result
    return self._fetch_future_result(cid)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/abdenour/miniconda3/envs/fl_with_flower/lib/python3.11/site-packages/flwr/simulation/ray_transport/ray_actor.py", line 279, in _fetch_future_result
    res_cid, out_mssg, updated_context = ray.get(
                                         ^^^^^^^^
  File "/home/abdenour/miniconda3/envs/fl_with_flower/lib/python3.11/site-packages/ray/_private/auto_init_hook.py", line 21, in auto_init_wrapper
    return fn(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^
  File "/home/abdenour/miniconda3/envs/fl_with_flower/lib/python3.11/site-packages/ray/_private/client_mode_hook.py", line 103, in wrapper
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "/home/abdenour/miniconda3/envs/fl_with_flower/lib/python3.11/site-packages/ray/_private/worker.py", line 2667, in get
    values, debugger_breakpoint = worker.get_objects(object_refs, timeout=timeout)
                                  ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/abdenour/miniconda3/envs/fl_with_flower/lib/python3.11/site-packages/ray/_private/worker.py", line 866, in get_objects
    raise value
ray.exceptions.OutOfMemoryError: Task was killed due to the node running low on memory.
Memory on the node (IP: 115.145.171.128, ID: c687613ff34cdbf5097699e5c896055ef2dc44343c5ab44bd40309d5) where the task (actor ID: 2a8bb25e35bf7bb5a0a6c00401000000, name=ClientAppActor.__init__, pid=619620, memory used=1.97GB) was running was 29.48GB / 31.02GB (0.950487), which exceeds the memory usage threshold of 0.95. Ray killed this worker (ID: 38a4d571c3fa7ff405dd9f5d1555b4753efb95dc84bb70ce01b6ecb4) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 115.145.171.128`. To see the logs of the worker, use `ray logs worker-38a4d571c3fa7ff405dd9f5d1555b4753efb95dc84bb70ce01b6ecb4*out -ip 115.145.171.128. Top 10 memory users:
PID	MEM(GB)	COMMAND
4942	3.48	/snap/pycharm-professional/408/jbr/bin/java -classpath /snap/pycharm-professional/408/lib/platform-l...
616443	2.85	/home/abdenour/miniconda3/envs/fl_with_flower/bin/python /home/abdenour/PycharmProjects/fl_with_flow...
619615	2.06	ray::ClientAppActor
619616	2.04	ray::ClientAppActor
619611	2.04	ray::ClientAppActor.run
619619	2.03	ray::ClientAppActor.run
619612	1.98	ray::ClientAppActor.run
619613	1.97	ray::ClientAppActor.run
619620	1.97	ray::ClientAppActor
619617	1.96	ray::ClientAppActor
Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.

[2024-08-13 23:56:05,658][flwr][ERROR] - Task was killed due to the node running low on memory.
Memory on the node (IP: 115.145.171.128, ID: c687613ff34cdbf5097699e5c896055ef2dc44343c5ab44bd40309d5) where the task (actor ID: 2a8bb25e35bf7bb5a0a6c00401000000, name=ClientAppActor.__init__, pid=619620, memory used=1.97GB) was running was 29.48GB / 31.02GB (0.950487), which exceeds the memory usage threshold of 0.95. Ray killed this worker (ID: 38a4d571c3fa7ff405dd9f5d1555b4753efb95dc84bb70ce01b6ecb4) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 115.145.171.128`. To see the logs of the worker, use `ray logs worker-38a4d571c3fa7ff405dd9f5d1555b4753efb95dc84bb70ce01b6ecb4*out -ip 115.145.171.128. Top 10 memory users:
PID	MEM(GB)	COMMAND
4942	3.48	/snap/pycharm-professional/408/jbr/bin/java -classpath /snap/pycharm-professional/408/lib/platform-l...
616443	2.85	/home/abdenour/miniconda3/envs/fl_with_flower/bin/python /home/abdenour/PycharmProjects/fl_with_flow...
619615	2.06	ray::ClientAppActor
619616	2.04	ray::ClientAppActor
619611	2.04	ray::ClientAppActor.run
619619	2.03	ray::ClientAppActor.run
619612	1.98	ray::ClientAppActor.run
619613	1.97	ray::ClientAppActor.run
619620	1.97	ray::ClientAppActor
619617	1.96	ray::ClientAppActor
Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.
[2024-08-13 23:56:05,658][flwr][ERROR] - Traceback (most recent call last):
  File "/home/abdenour/miniconda3/envs/fl_with_flower/lib/python3.11/site-packages/flwr/simulation/ray_transport/ray_client_proxy.py", line 94, in _submit_job
    out_mssg, updated_context = self.actor_pool.get_client_result(
                                ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/abdenour/miniconda3/envs/fl_with_flower/lib/python3.11/site-packages/flwr/simulation/ray_transport/ray_actor.py", line 398, in get_client_result
    return self._fetch_future_result(cid)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/abdenour/miniconda3/envs/fl_with_flower/lib/python3.11/site-packages/flwr/simulation/ray_transport/ray_actor.py", line 279, in _fetch_future_result
    res_cid, out_mssg, updated_context = ray.get(
                                         ^^^^^^^^
  File "/home/abdenour/miniconda3/envs/fl_with_flower/lib/python3.11/site-packages/ray/_private/auto_init_hook.py", line 21, in auto_init_wrapper
    return fn(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^
  File "/home/abdenour/miniconda3/envs/fl_with_flower/lib/python3.11/site-packages/ray/_private/client_mode_hook.py", line 103, in wrapper
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "/home/abdenour/miniconda3/envs/fl_with_flower/lib/python3.11/site-packages/ray/_private/worker.py", line 2667, in get
    values, debugger_breakpoint = worker.get_objects(object_refs, timeout=timeout)
                                  ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/abdenour/miniconda3/envs/fl_with_flower/lib/python3.11/site-packages/ray/_private/worker.py", line 866, in get_objects
    raise value
ray.exceptions.OutOfMemoryError: Task was killed due to the node running low on memory.
Memory on the node (IP: 115.145.171.128, ID: c687613ff34cdbf5097699e5c896055ef2dc44343c5ab44bd40309d5) where the task (actor ID: 52365bb7997979e5a4c7ddbd01000000, name=ClientAppActor.__init__, pid=619618, memory used=1.84GB) was running was 29.49GB / 31.02GB (0.950707), which exceeds the memory usage threshold of 0.95. Ray killed this worker (ID: 7e48cd46c72821b17be65b05a4e61b5846ba998a57d76db44ca4f8fa) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 115.145.171.128`. To see the logs of the worker, use `ray logs worker-7e48cd46c72821b17be65b05a4e61b5846ba998a57d76db44ca4f8fa*out -ip 115.145.171.128. Top 10 memory users:
PID	MEM(GB)	COMMAND
4942	3.48	/snap/pycharm-professional/408/jbr/bin/java -classpath /snap/pycharm-professional/408/lib/platform-l...
616443	2.14	/home/abdenour/miniconda3/envs/fl_with_flower/bin/python /home/abdenour/PycharmProjects/fl_with_flow...
619615	1.90	ray::ClientAppActor
619616	1.88	ray::ClientAppActor
619611	1.87	ray::ClientAppActor
619617	1.86	ray::ClientAppActor
619620	1.85	ray::ClientAppActor
619618	1.84	ray::ClientAppActor
619613	1.82	ray::ClientAppActor
619619	1.80	ray::ClientAppActor
Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.

[2024-08-13 23:56:05,658][flwr][ERROR] - Task was killed due to the node running low on memory.
Memory on the node (IP: 115.145.171.128, ID: c687613ff34cdbf5097699e5c896055ef2dc44343c5ab44bd40309d5) where the task (actor ID: 52365bb7997979e5a4c7ddbd01000000, name=ClientAppActor.__init__, pid=619618, memory used=1.84GB) was running was 29.49GB / 31.02GB (0.950707), which exceeds the memory usage threshold of 0.95. Ray killed this worker (ID: 7e48cd46c72821b17be65b05a4e61b5846ba998a57d76db44ca4f8fa) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 115.145.171.128`. To see the logs of the worker, use `ray logs worker-7e48cd46c72821b17be65b05a4e61b5846ba998a57d76db44ca4f8fa*out -ip 115.145.171.128. Top 10 memory users:
PID	MEM(GB)	COMMAND
4942	3.48	/snap/pycharm-professional/408/jbr/bin/java -classpath /snap/pycharm-professional/408/lib/platform-l...
616443	2.14	/home/abdenour/miniconda3/envs/fl_with_flower/bin/python /home/abdenour/PycharmProjects/fl_with_flow...
619615	1.90	ray::ClientAppActor
619616	1.88	ray::ClientAppActor
619611	1.87	ray::ClientAppActor
619617	1.86	ray::ClientAppActor
619620	1.85	ray::ClientAppActor
619618	1.84	ray::ClientAppActor
619613	1.82	ray::ClientAppActor
619619	1.80	ray::ClientAppActor
Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.
[2024-08-13 23:56:05,659][flwr][ERROR] - Traceback (most recent call last):
  File "/home/abdenour/miniconda3/envs/fl_with_flower/lib/python3.11/site-packages/flwr/simulation/ray_transport/ray_client_proxy.py", line 94, in _submit_job
    out_mssg, updated_context = self.actor_pool.get_client_result(
                                ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/abdenour/miniconda3/envs/fl_with_flower/lib/python3.11/site-packages/flwr/simulation/ray_transport/ray_actor.py", line 398, in get_client_result
    return self._fetch_future_result(cid)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/abdenour/miniconda3/envs/fl_with_flower/lib/python3.11/site-packages/flwr/simulation/ray_transport/ray_actor.py", line 279, in _fetch_future_result
    res_cid, out_mssg, updated_context = ray.get(
                                         ^^^^^^^^
  File "/home/abdenour/miniconda3/envs/fl_with_flower/lib/python3.11/site-packages/ray/_private/auto_init_hook.py", line 21, in auto_init_wrapper
    return fn(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^
  File "/home/abdenour/miniconda3/envs/fl_with_flower/lib/python3.11/site-packages/ray/_private/client_mode_hook.py", line 103, in wrapper
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "/home/abdenour/miniconda3/envs/fl_with_flower/lib/python3.11/site-packages/ray/_private/worker.py", line 2667, in get
    values, debugger_breakpoint = worker.get_objects(object_refs, timeout=timeout)
                                  ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/abdenour/miniconda3/envs/fl_with_flower/lib/python3.11/site-packages/ray/_private/worker.py", line 866, in get_objects
    raise value
ray.exceptions.OutOfMemoryError: Task was killed due to the node running low on memory.
Memory on the node (IP: 115.145.171.128, ID: c687613ff34cdbf5097699e5c896055ef2dc44343c5ab44bd40309d5) where the task (actor ID: 85928d3f1a247800081c0e1101000000, name=ClientAppActor.__init__, pid=619614, memory used=1.73GB) was running was 29.50GB / 31.02GB (0.951049), which exceeds the memory usage threshold of 0.95. Ray killed this worker (ID: c7f644ac3cca20dffbfa1723daac4f87f04c7de03da25e7cad5fe33c) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 115.145.171.128`. To see the logs of the worker, use `ray logs worker-c7f644ac3cca20dffbfa1723daac4f87f04c7de03da25e7cad5fe33c*out -ip 115.145.171.128. Top 10 memory users:
PID	MEM(GB)	COMMAND
4942	3.49	/snap/pycharm-professional/408/jbr/bin/java -classpath /snap/pycharm-professional/408/lib/platform-l...
619616	1.80	ray::ClientAppActor.run
619615	1.78	ray::ClientAppActor
619617	1.77	ray::ClientAppActor
619611	1.73	ray::ClientAppActor
619618	1.73	ray::ClientAppActor
619614	1.73	ray::ClientAppActor
619612	1.72	ray::ClientAppActor
619619	1.70	ray::ClientAppActor
619620	1.68	ray::ClientAppActor.run
Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.

[2024-08-13 23:56:05,659][flwr][ERROR] - Task was killed due to the node running low on memory.
Memory on the node (IP: 115.145.171.128, ID: c687613ff34cdbf5097699e5c896055ef2dc44343c5ab44bd40309d5) where the task (actor ID: 85928d3f1a247800081c0e1101000000, name=ClientAppActor.__init__, pid=619614, memory used=1.73GB) was running was 29.50GB / 31.02GB (0.951049), which exceeds the memory usage threshold of 0.95. Ray killed this worker (ID: c7f644ac3cca20dffbfa1723daac4f87f04c7de03da25e7cad5fe33c) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 115.145.171.128`. To see the logs of the worker, use `ray logs worker-c7f644ac3cca20dffbfa1723daac4f87f04c7de03da25e7cad5fe33c*out -ip 115.145.171.128. Top 10 memory users:
PID	MEM(GB)	COMMAND
4942	3.49	/snap/pycharm-professional/408/jbr/bin/java -classpath /snap/pycharm-professional/408/lib/platform-l...
619616	1.80	ray::ClientAppActor.run
619615	1.78	ray::ClientAppActor
619617	1.77	ray::ClientAppActor
619611	1.73	ray::ClientAppActor
619618	1.73	ray::ClientAppActor
619614	1.73	ray::ClientAppActor
619612	1.72	ray::ClientAppActor
619619	1.70	ray::ClientAppActor
619620	1.68	ray::ClientAppActor.run
Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.
[2024-08-13 23:56:07,180][flwr][INFO] - aggregate_fit: received 7 results and 3 failures
[2024-08-13 23:56:10,033][flwr][INFO] - fit progress: (258, 17.131743848323822, {'accuracy': 0.7516025641025641}, 1491.293556146993)
[2024-08-13 23:56:10,033][flwr][INFO] - configure_evaluate: strategy sampled 2 clients (out of 20)
[2024-08-13 23:56:10,357][flwr][INFO] - aggregate_evaluate: received 2 results and 0 failures
[2024-08-13 23:56:10,357][flwr][INFO] - 
[2024-08-13 23:56:10,357][flwr][INFO] - [ROUND 259]
[2024-08-13 23:56:10,357][flwr][INFO] - configure_fit: strategy sampled 10 clients (out of 20)
[2024-08-13 23:56:10,500][flwr][ERROR] - Traceback (most recent call last):
  File "/home/abdenour/miniconda3/envs/fl_with_flower/lib/python3.11/site-packages/flwr/simulation/ray_transport/ray_client_proxy.py", line 94, in _submit_job
    out_mssg, updated_context = self.actor_pool.get_client_result(
                                ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/abdenour/miniconda3/envs/fl_with_flower/lib/python3.11/site-packages/flwr/simulation/ray_transport/ray_actor.py", line 398, in get_client_result
    return self._fetch_future_result(cid)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/abdenour/miniconda3/envs/fl_with_flower/lib/python3.11/site-packages/flwr/simulation/ray_transport/ray_actor.py", line 279, in _fetch_future_result
    res_cid, out_mssg, updated_context = ray.get(
                                         ^^^^^^^^
  File "/home/abdenour/miniconda3/envs/fl_with_flower/lib/python3.11/site-packages/ray/_private/auto_init_hook.py", line 21, in auto_init_wrapper
    return fn(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^
  File "/home/abdenour/miniconda3/envs/fl_with_flower/lib/python3.11/site-packages/ray/_private/client_mode_hook.py", line 103, in wrapper
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "/home/abdenour/miniconda3/envs/fl_with_flower/lib/python3.11/site-packages/ray/_private/worker.py", line 2667, in get
    values, debugger_breakpoint = worker.get_objects(object_refs, timeout=timeout)
                                  ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/abdenour/miniconda3/envs/fl_with_flower/lib/python3.11/site-packages/ray/_private/worker.py", line 866, in get_objects
    raise value
ray.exceptions.OutOfMemoryError: Task was killed due to the node running low on memory.
Memory on the node (IP: 115.145.171.128, ID: c687613ff34cdbf5097699e5c896055ef2dc44343c5ab44bd40309d5) where the task (actor ID: 85928d3f1a247800081c0e1101000000, name=ClientAppActor.__init__, pid=619614, memory used=1.73GB) was running was 29.50GB / 31.02GB (0.951049), which exceeds the memory usage threshold of 0.95. Ray killed this worker (ID: c7f644ac3cca20dffbfa1723daac4f87f04c7de03da25e7cad5fe33c) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 115.145.171.128`. To see the logs of the worker, use `ray logs worker-c7f644ac3cca20dffbfa1723daac4f87f04c7de03da25e7cad5fe33c*out -ip 115.145.171.128. Top 10 memory users:
PID	MEM(GB)	COMMAND
4942	3.49	/snap/pycharm-professional/408/jbr/bin/java -classpath /snap/pycharm-professional/408/lib/platform-l...
619616	1.80	ray::ClientAppActor.run
619615	1.78	ray::ClientAppActor
619617	1.77	ray::ClientAppActor
619611	1.73	ray::ClientAppActor
619618	1.73	ray::ClientAppActor
619614	1.73	ray::ClientAppActor
619612	1.72	ray::ClientAppActor
619619	1.70	ray::ClientAppActor
619620	1.68	ray::ClientAppActor.run
Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.

[2024-08-13 23:56:10,500][flwr][ERROR] - Traceback (most recent call last):
  File "/home/abdenour/miniconda3/envs/fl_with_flower/lib/python3.11/site-packages/flwr/simulation/ray_transport/ray_client_proxy.py", line 94, in _submit_job
    out_mssg, updated_context = self.actor_pool.get_client_result(
                                ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/abdenour/miniconda3/envs/fl_with_flower/lib/python3.11/site-packages/flwr/simulation/ray_transport/ray_actor.py", line 398, in get_client_result
    return self._fetch_future_result(cid)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/abdenour/miniconda3/envs/fl_with_flower/lib/python3.11/site-packages/flwr/simulation/ray_transport/ray_actor.py", line 279, in _fetch_future_result
    res_cid, out_mssg, updated_context = ray.get(
                                         ^^^^^^^^
  File "/home/abdenour/miniconda3/envs/fl_with_flower/lib/python3.11/site-packages/ray/_private/auto_init_hook.py", line 21, in auto_init_wrapper
    return fn(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^
  File "/home/abdenour/miniconda3/envs/fl_with_flower/lib/python3.11/site-packages/ray/_private/client_mode_hook.py", line 103, in wrapper
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "/home/abdenour/miniconda3/envs/fl_with_flower/lib/python3.11/site-packages/ray/_private/worker.py", line 2667, in get
    values, debugger_breakpoint = worker.get_objects(object_refs, timeout=timeout)
                                  ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/abdenour/miniconda3/envs/fl_with_flower/lib/python3.11/site-packages/ray/_private/worker.py", line 866, in get_objects
    raise value
ray.exceptions.OutOfMemoryError: Task was killed due to the node running low on memory.
Memory on the node (IP: 115.145.171.128, ID: c687613ff34cdbf5097699e5c896055ef2dc44343c5ab44bd40309d5) where the task (actor ID: 52365bb7997979e5a4c7ddbd01000000, name=ClientAppActor.__init__, pid=619618, memory used=1.84GB) was running was 29.49GB / 31.02GB (0.950707), which exceeds the memory usage threshold of 0.95. Ray killed this worker (ID: 7e48cd46c72821b17be65b05a4e61b5846ba998a57d76db44ca4f8fa) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 115.145.171.128`. To see the logs of the worker, use `ray logs worker-7e48cd46c72821b17be65b05a4e61b5846ba998a57d76db44ca4f8fa*out -ip 115.145.171.128. Top 10 memory users:
PID	MEM(GB)	COMMAND
4942	3.48	/snap/pycharm-professional/408/jbr/bin/java -classpath /snap/pycharm-professional/408/lib/platform-l...
616443	2.14	/home/abdenour/miniconda3/envs/fl_with_flower/bin/python /home/abdenour/PycharmProjects/fl_with_flow...
619615	1.90	ray::ClientAppActor
619616	1.88	ray::ClientAppActor
619611	1.87	ray::ClientAppActor
619617	1.86	ray::ClientAppActor
619620	1.85	ray::ClientAppActor
619618	1.84	ray::ClientAppActor
619613	1.82	ray::ClientAppActor
619619	1.80	ray::ClientAppActor
Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.

[2024-08-13 23:56:10,500][flwr][ERROR] - Task was killed due to the node running low on memory.
Memory on the node (IP: 115.145.171.128, ID: c687613ff34cdbf5097699e5c896055ef2dc44343c5ab44bd40309d5) where the task (actor ID: 85928d3f1a247800081c0e1101000000, name=ClientAppActor.__init__, pid=619614, memory used=1.73GB) was running was 29.50GB / 31.02GB (0.951049), which exceeds the memory usage threshold of 0.95. Ray killed this worker (ID: c7f644ac3cca20dffbfa1723daac4f87f04c7de03da25e7cad5fe33c) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 115.145.171.128`. To see the logs of the worker, use `ray logs worker-c7f644ac3cca20dffbfa1723daac4f87f04c7de03da25e7cad5fe33c*out -ip 115.145.171.128. Top 10 memory users:
PID	MEM(GB)	COMMAND
4942	3.49	/snap/pycharm-professional/408/jbr/bin/java -classpath /snap/pycharm-professional/408/lib/platform-l...
619616	1.80	ray::ClientAppActor.run
619615	1.78	ray::ClientAppActor
619617	1.77	ray::ClientAppActor
619611	1.73	ray::ClientAppActor
619618	1.73	ray::ClientAppActor
619614	1.73	ray::ClientAppActor
619612	1.72	ray::ClientAppActor
619619	1.70	ray::ClientAppActor
619620	1.68	ray::ClientAppActor.run
Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.
[2024-08-13 23:56:10,500][flwr][ERROR] - Task was killed due to the node running low on memory.
Memory on the node (IP: 115.145.171.128, ID: c687613ff34cdbf5097699e5c896055ef2dc44343c5ab44bd40309d5) where the task (actor ID: 52365bb7997979e5a4c7ddbd01000000, name=ClientAppActor.__init__, pid=619618, memory used=1.84GB) was running was 29.49GB / 31.02GB (0.950707), which exceeds the memory usage threshold of 0.95. Ray killed this worker (ID: 7e48cd46c72821b17be65b05a4e61b5846ba998a57d76db44ca4f8fa) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 115.145.171.128`. To see the logs of the worker, use `ray logs worker-7e48cd46c72821b17be65b05a4e61b5846ba998a57d76db44ca4f8fa*out -ip 115.145.171.128. Top 10 memory users:
PID	MEM(GB)	COMMAND
4942	3.48	/snap/pycharm-professional/408/jbr/bin/java -classpath /snap/pycharm-professional/408/lib/platform-l...
616443	2.14	/home/abdenour/miniconda3/envs/fl_with_flower/bin/python /home/abdenour/PycharmProjects/fl_with_flow...
619615	1.90	ray::ClientAppActor
619616	1.88	ray::ClientAppActor
619611	1.87	ray::ClientAppActor
619617	1.86	ray::ClientAppActor
619620	1.85	ray::ClientAppActor
619618	1.84	ray::ClientAppActor
619613	1.82	ray::ClientAppActor
619619	1.80	ray::ClientAppActor
Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.
[2024-08-13 23:56:10,501][flwr][ERROR] - Traceback (most recent call last):
  File "/home/abdenour/miniconda3/envs/fl_with_flower/lib/python3.11/site-packages/flwr/simulation/ray_transport/ray_client_proxy.py", line 94, in _submit_job
    out_mssg, updated_context = self.actor_pool.get_client_result(
                                ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/abdenour/miniconda3/envs/fl_with_flower/lib/python3.11/site-packages/flwr/simulation/ray_transport/ray_actor.py", line 398, in get_client_result
    return self._fetch_future_result(cid)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/abdenour/miniconda3/envs/fl_with_flower/lib/python3.11/site-packages/flwr/simulation/ray_transport/ray_actor.py", line 279, in _fetch_future_result
    res_cid, out_mssg, updated_context = ray.get(
                                         ^^^^^^^^
  File "/home/abdenour/miniconda3/envs/fl_with_flower/lib/python3.11/site-packages/ray/_private/auto_init_hook.py", line 21, in auto_init_wrapper
    return fn(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^
  File "/home/abdenour/miniconda3/envs/fl_with_flower/lib/python3.11/site-packages/ray/_private/client_mode_hook.py", line 103, in wrapper
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "/home/abdenour/miniconda3/envs/fl_with_flower/lib/python3.11/site-packages/ray/_private/worker.py", line 2667, in get
    values, debugger_breakpoint = worker.get_objects(object_refs, timeout=timeout)
                                  ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/abdenour/miniconda3/envs/fl_with_flower/lib/python3.11/site-packages/ray/_private/worker.py", line 866, in get_objects
    raise value
ray.exceptions.OutOfMemoryError: Task was killed due to the node running low on memory.
Memory on the node (IP: 115.145.171.128, ID: c687613ff34cdbf5097699e5c896055ef2dc44343c5ab44bd40309d5) where the task (actor ID: 2a8bb25e35bf7bb5a0a6c00401000000, name=ClientAppActor.__init__, pid=619620, memory used=1.97GB) was running was 29.48GB / 31.02GB (0.950487), which exceeds the memory usage threshold of 0.95. Ray killed this worker (ID: 38a4d571c3fa7ff405dd9f5d1555b4753efb95dc84bb70ce01b6ecb4) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 115.145.171.128`. To see the logs of the worker, use `ray logs worker-38a4d571c3fa7ff405dd9f5d1555b4753efb95dc84bb70ce01b6ecb4*out -ip 115.145.171.128. Top 10 memory users:
PID	MEM(GB)	COMMAND
4942	3.48	/snap/pycharm-professional/408/jbr/bin/java -classpath /snap/pycharm-professional/408/lib/platform-l...
616443	2.85	/home/abdenour/miniconda3/envs/fl_with_flower/bin/python /home/abdenour/PycharmProjects/fl_with_flow...
619615	2.06	ray::ClientAppActor
619616	2.04	ray::ClientAppActor
619611	2.04	ray::ClientAppActor.run
619619	2.03	ray::ClientAppActor.run
619612	1.98	ray::ClientAppActor.run
619613	1.97	ray::ClientAppActor.run
619620	1.97	ray::ClientAppActor
619617	1.96	ray::ClientAppActor
Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.

[2024-08-13 23:56:10,501][flwr][ERROR] - Task was killed due to the node running low on memory.
Memory on the node (IP: 115.145.171.128, ID: c687613ff34cdbf5097699e5c896055ef2dc44343c5ab44bd40309d5) where the task (actor ID: 2a8bb25e35bf7bb5a0a6c00401000000, name=ClientAppActor.__init__, pid=619620, memory used=1.97GB) was running was 29.48GB / 31.02GB (0.950487), which exceeds the memory usage threshold of 0.95. Ray killed this worker (ID: 38a4d571c3fa7ff405dd9f5d1555b4753efb95dc84bb70ce01b6ecb4) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 115.145.171.128`. To see the logs of the worker, use `ray logs worker-38a4d571c3fa7ff405dd9f5d1555b4753efb95dc84bb70ce01b6ecb4*out -ip 115.145.171.128. Top 10 memory users:
PID	MEM(GB)	COMMAND
4942	3.48	/snap/pycharm-professional/408/jbr/bin/java -classpath /snap/pycharm-professional/408/lib/platform-l...
616443	2.85	/home/abdenour/miniconda3/envs/fl_with_flower/bin/python /home/abdenour/PycharmProjects/fl_with_flow...
619615	2.06	ray::ClientAppActor
619616	2.04	ray::ClientAppActor
619611	2.04	ray::ClientAppActor.run
619619	2.03	ray::ClientAppActor.run
619612	1.98	ray::ClientAppActor.run
619613	1.97	ray::ClientAppActor.run
619620	1.97	ray::ClientAppActor
619617	1.96	ray::ClientAppActor
Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.
[2024-08-13 23:56:12,019][flwr][INFO] - aggregate_fit: received 7 results and 3 failures
[2024-08-13 23:56:14,971][flwr][INFO] - fit progress: (259, 16.556449115276337, {'accuracy': 0.7516025641025641}, 1496.2312419319933)
[2024-08-13 23:56:14,984][flwr][INFO] - configure_evaluate: strategy sampled 2 clients (out of 20)
[2024-08-13 23:56:15,195][flwr][INFO] - aggregate_evaluate: received 2 results and 0 failures
[2024-08-13 23:56:15,195][flwr][INFO] - 
[2024-08-13 23:56:15,195][flwr][INFO] - [ROUND 260]
[2024-08-13 23:56:15,195][flwr][INFO] - configure_fit: strategy sampled 10 clients (out of 20)
[2024-08-13 23:56:15,349][flwr][ERROR] - Traceback (most recent call last):
  File "/home/abdenour/miniconda3/envs/fl_with_flower/lib/python3.11/site-packages/flwr/simulation/ray_transport/ray_client_proxy.py", line 94, in _submit_job
    out_mssg, updated_context = self.actor_pool.get_client_result(
                                ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/abdenour/miniconda3/envs/fl_with_flower/lib/python3.11/site-packages/flwr/simulation/ray_transport/ray_actor.py", line 398, in get_client_result
    return self._fetch_future_result(cid)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/abdenour/miniconda3/envs/fl_with_flower/lib/python3.11/site-packages/flwr/simulation/ray_transport/ray_actor.py", line 279, in _fetch_future_result
    res_cid, out_mssg, updated_context = ray.get(
                                         ^^^^^^^^
  File "/home/abdenour/miniconda3/envs/fl_with_flower/lib/python3.11/site-packages/ray/_private/auto_init_hook.py", line 21, in auto_init_wrapper
    return fn(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^
  File "/home/abdenour/miniconda3/envs/fl_with_flower/lib/python3.11/site-packages/ray/_private/client_mode_hook.py", line 103, in wrapper
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "/home/abdenour/miniconda3/envs/fl_with_flower/lib/python3.11/site-packages/ray/_private/worker.py", line 2667, in get
    values, debugger_breakpoint = worker.get_objects(object_refs, timeout=timeout)
                                  ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/abdenour/miniconda3/envs/fl_with_flower/lib/python3.11/site-packages/ray/_private/worker.py", line 866, in get_objects
    raise value
ray.exceptions.OutOfMemoryError: Task was killed due to the node running low on memory.
Memory on the node (IP: 115.145.171.128, ID: c687613ff34cdbf5097699e5c896055ef2dc44343c5ab44bd40309d5) where the task (actor ID: 2a8bb25e35bf7bb5a0a6c00401000000, name=ClientAppActor.__init__, pid=619620, memory used=1.97GB) was running was 29.48GB / 31.02GB (0.950487), which exceeds the memory usage threshold of 0.95. Ray killed this worker (ID: 38a4d571c3fa7ff405dd9f5d1555b4753efb95dc84bb70ce01b6ecb4) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 115.145.171.128`. To see the logs of the worker, use `ray logs worker-38a4d571c3fa7ff405dd9f5d1555b4753efb95dc84bb70ce01b6ecb4*out -ip 115.145.171.128. Top 10 memory users:
PID	MEM(GB)	COMMAND
4942	3.48	/snap/pycharm-professional/408/jbr/bin/java -classpath /snap/pycharm-professional/408/lib/platform-l...
616443	2.85	/home/abdenour/miniconda3/envs/fl_with_flower/bin/python /home/abdenour/PycharmProjects/fl_with_flow...
619615	2.06	ray::ClientAppActor
619616	2.04	ray::ClientAppActor
619611	2.04	ray::ClientAppActor.run
619619	2.03	ray::ClientAppActor.run
619612	1.98	ray::ClientAppActor.run
619613	1.97	ray::ClientAppActor.run
619620	1.97	ray::ClientAppActor
619617	1.96	ray::ClientAppActor
Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.

[2024-08-13 23:56:15,349][flwr][ERROR] - Task was killed due to the node running low on memory.
Memory on the node (IP: 115.145.171.128, ID: c687613ff34cdbf5097699e5c896055ef2dc44343c5ab44bd40309d5) where the task (actor ID: 2a8bb25e35bf7bb5a0a6c00401000000, name=ClientAppActor.__init__, pid=619620, memory used=1.97GB) was running was 29.48GB / 31.02GB (0.950487), which exceeds the memory usage threshold of 0.95. Ray killed this worker (ID: 38a4d571c3fa7ff405dd9f5d1555b4753efb95dc84bb70ce01b6ecb4) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 115.145.171.128`. To see the logs of the worker, use `ray logs worker-38a4d571c3fa7ff405dd9f5d1555b4753efb95dc84bb70ce01b6ecb4*out -ip 115.145.171.128. Top 10 memory users:
PID	MEM(GB)	COMMAND
4942	3.48	/snap/pycharm-professional/408/jbr/bin/java -classpath /snap/pycharm-professional/408/lib/platform-l...
616443	2.85	/home/abdenour/miniconda3/envs/fl_with_flower/bin/python /home/abdenour/PycharmProjects/fl_with_flow...
619615	2.06	ray::ClientAppActor
619616	2.04	ray::ClientAppActor
619611	2.04	ray::ClientAppActor.run
619619	2.03	ray::ClientAppActor.run
619612	1.98	ray::ClientAppActor.run
619613	1.97	ray::ClientAppActor.run
619620	1.97	ray::ClientAppActor
619617	1.96	ray::ClientAppActor
Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.
[2024-08-13 23:56:15,349][flwr][ERROR] - Traceback (most recent call last):
  File "/home/abdenour/miniconda3/envs/fl_with_flower/lib/python3.11/site-packages/flwr/simulation/ray_transport/ray_client_proxy.py", line 94, in _submit_job
    out_mssg, updated_context = self.actor_pool.get_client_result(
                                ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/abdenour/miniconda3/envs/fl_with_flower/lib/python3.11/site-packages/flwr/simulation/ray_transport/ray_actor.py", line 398, in get_client_result
    return self._fetch_future_result(cid)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/abdenour/miniconda3/envs/fl_with_flower/lib/python3.11/site-packages/flwr/simulation/ray_transport/ray_actor.py", line 279, in _fetch_future_result
    res_cid, out_mssg, updated_context = ray.get(
                                         ^^^^^^^^
  File "/home/abdenour/miniconda3/envs/fl_with_flower/lib/python3.11/site-packages/ray/_private/auto_init_hook.py", line 21, in auto_init_wrapper
    return fn(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^
  File "/home/abdenour/miniconda3/envs/fl_with_flower/lib/python3.11/site-packages/ray/_private/client_mode_hook.py", line 103, in wrapper
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "/home/abdenour/miniconda3/envs/fl_with_flower/lib/python3.11/site-packages/ray/_private/worker.py", line 2667, in get
    values, debugger_breakpoint = worker.get_objects(object_refs, timeout=timeout)
                                  ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/abdenour/miniconda3/envs/fl_with_flower/lib/python3.11/site-packages/ray/_private/worker.py", line 866, in get_objects
    raise value
ray.exceptions.OutOfMemoryError: Task was killed due to the node running low on memory.
Memory on the node (IP: 115.145.171.128, ID: c687613ff34cdbf5097699e5c896055ef2dc44343c5ab44bd40309d5) where the task (actor ID: 52365bb7997979e5a4c7ddbd01000000, name=ClientAppActor.__init__, pid=619618, memory used=1.84GB) was running was 29.49GB / 31.02GB (0.950707), which exceeds the memory usage threshold of 0.95. Ray killed this worker (ID: 7e48cd46c72821b17be65b05a4e61b5846ba998a57d76db44ca4f8fa) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 115.145.171.128`. To see the logs of the worker, use `ray logs worker-7e48cd46c72821b17be65b05a4e61b5846ba998a57d76db44ca4f8fa*out -ip 115.145.171.128. Top 10 memory users:
PID	MEM(GB)	COMMAND
4942	3.48	/snap/pycharm-professional/408/jbr/bin/java -classpath /snap/pycharm-professional/408/lib/platform-l...
616443	2.14	/home/abdenour/miniconda3/envs/fl_with_flower/bin/python /home/abdenour/PycharmProjects/fl_with_flow...
619615	1.90	ray::ClientAppActor
619616	1.88	ray::ClientAppActor
619611	1.87	ray::ClientAppActor
619617	1.86	ray::ClientAppActor
619620	1.85	ray::ClientAppActor
619618	1.84	ray::ClientAppActor
619613	1.82	ray::ClientAppActor
619619	1.80	ray::ClientAppActor
Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.

[2024-08-13 23:56:15,350][flwr][ERROR] - Task was killed due to the node running low on memory.
Memory on the node (IP: 115.145.171.128, ID: c687613ff34cdbf5097699e5c896055ef2dc44343c5ab44bd40309d5) where the task (actor ID: 52365bb7997979e5a4c7ddbd01000000, name=ClientAppActor.__init__, pid=619618, memory used=1.84GB) was running was 29.49GB / 31.02GB (0.950707), which exceeds the memory usage threshold of 0.95. Ray killed this worker (ID: 7e48cd46c72821b17be65b05a4e61b5846ba998a57d76db44ca4f8fa) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 115.145.171.128`. To see the logs of the worker, use `ray logs worker-7e48cd46c72821b17be65b05a4e61b5846ba998a57d76db44ca4f8fa*out -ip 115.145.171.128. Top 10 memory users:
PID	MEM(GB)	COMMAND
4942	3.48	/snap/pycharm-professional/408/jbr/bin/java -classpath /snap/pycharm-professional/408/lib/platform-l...
616443	2.14	/home/abdenour/miniconda3/envs/fl_with_flower/bin/python /home/abdenour/PycharmProjects/fl_with_flow...
619615	1.90	ray::ClientAppActor
619616	1.88	ray::ClientAppActor
619611	1.87	ray::ClientAppActor
619617	1.86	ray::ClientAppActor
619620	1.85	ray::ClientAppActor
619618	1.84	ray::ClientAppActor
619613	1.82	ray::ClientAppActor
619619	1.80	ray::ClientAppActor
Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.
[2024-08-13 23:56:15,350][flwr][ERROR] - Traceback (most recent call last):
  File "/home/abdenour/miniconda3/envs/fl_with_flower/lib/python3.11/site-packages/flwr/simulation/ray_transport/ray_client_proxy.py", line 94, in _submit_job
    out_mssg, updated_context = self.actor_pool.get_client_result(
                                ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/abdenour/miniconda3/envs/fl_with_flower/lib/python3.11/site-packages/flwr/simulation/ray_transport/ray_actor.py", line 398, in get_client_result
    return self._fetch_future_result(cid)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/abdenour/miniconda3/envs/fl_with_flower/lib/python3.11/site-packages/flwr/simulation/ray_transport/ray_actor.py", line 279, in _fetch_future_result
    res_cid, out_mssg, updated_context = ray.get(
                                         ^^^^^^^^
  File "/home/abdenour/miniconda3/envs/fl_with_flower/lib/python3.11/site-packages/ray/_private/auto_init_hook.py", line 21, in auto_init_wrapper
    return fn(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^
  File "/home/abdenour/miniconda3/envs/fl_with_flower/lib/python3.11/site-packages/ray/_private/client_mode_hook.py", line 103, in wrapper
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "/home/abdenour/miniconda3/envs/fl_with_flower/lib/python3.11/site-packages/ray/_private/worker.py", line 2667, in get
    values, debugger_breakpoint = worker.get_objects(object_refs, timeout=timeout)
                                  ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/abdenour/miniconda3/envs/fl_with_flower/lib/python3.11/site-packages/ray/_private/worker.py", line 866, in get_objects
    raise value
ray.exceptions.OutOfMemoryError: Task was killed due to the node running low on memory.
Memory on the node (IP: 115.145.171.128, ID: c687613ff34cdbf5097699e5c896055ef2dc44343c5ab44bd40309d5) where the task (actor ID: 85928d3f1a247800081c0e1101000000, name=ClientAppActor.__init__, pid=619614, memory used=1.73GB) was running was 29.50GB / 31.02GB (0.951049), which exceeds the memory usage threshold of 0.95. Ray killed this worker (ID: c7f644ac3cca20dffbfa1723daac4f87f04c7de03da25e7cad5fe33c) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 115.145.171.128`. To see the logs of the worker, use `ray logs worker-c7f644ac3cca20dffbfa1723daac4f87f04c7de03da25e7cad5fe33c*out -ip 115.145.171.128. Top 10 memory users:
PID	MEM(GB)	COMMAND
4942	3.49	/snap/pycharm-professional/408/jbr/bin/java -classpath /snap/pycharm-professional/408/lib/platform-l...
619616	1.80	ray::ClientAppActor.run
619615	1.78	ray::ClientAppActor
619617	1.77	ray::ClientAppActor
619611	1.73	ray::ClientAppActor
619618	1.73	ray::ClientAppActor
619614	1.73	ray::ClientAppActor
619612	1.72	ray::ClientAppActor
619619	1.70	ray::ClientAppActor
619620	1.68	ray::ClientAppActor.run
Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.

[2024-08-13 23:56:15,350][flwr][ERROR] - Task was killed due to the node running low on memory.
Memory on the node (IP: 115.145.171.128, ID: c687613ff34cdbf5097699e5c896055ef2dc44343c5ab44bd40309d5) where the task (actor ID: 85928d3f1a247800081c0e1101000000, name=ClientAppActor.__init__, pid=619614, memory used=1.73GB) was running was 29.50GB / 31.02GB (0.951049), which exceeds the memory usage threshold of 0.95. Ray killed this worker (ID: c7f644ac3cca20dffbfa1723daac4f87f04c7de03da25e7cad5fe33c) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 115.145.171.128`. To see the logs of the worker, use `ray logs worker-c7f644ac3cca20dffbfa1723daac4f87f04c7de03da25e7cad5fe33c*out -ip 115.145.171.128. Top 10 memory users:
PID	MEM(GB)	COMMAND
4942	3.49	/snap/pycharm-professional/408/jbr/bin/java -classpath /snap/pycharm-professional/408/lib/platform-l...
619616	1.80	ray::ClientAppActor.run
619615	1.78	ray::ClientAppActor
619617	1.77	ray::ClientAppActor
619611	1.73	ray::ClientAppActor
619618	1.73	ray::ClientAppActor
619614	1.73	ray::ClientAppActor
619612	1.72	ray::ClientAppActor
619619	1.70	ray::ClientAppActor
619620	1.68	ray::ClientAppActor.run
Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.
[2024-08-13 23:56:17,160][flwr][INFO] - aggregate_fit: received 7 results and 3 failures
[2024-08-13 23:56:20,753][flwr][INFO] - fit progress: (260, 15.202992141246796, {'accuracy': 0.7564102564102564}, 1502.0132144020172)
[2024-08-13 23:56:20,753][flwr][INFO] - configure_evaluate: strategy sampled 2 clients (out of 20)
[2024-08-13 23:56:21,112][flwr][INFO] - aggregate_evaluate: received 2 results and 0 failures
[2024-08-13 23:56:21,113][flwr][INFO] - 
[2024-08-13 23:56:21,113][flwr][INFO] - [ROUND 261]
[2024-08-13 23:56:21,113][flwr][INFO] - configure_fit: strategy sampled 10 clients (out of 20)
[2024-08-13 23:56:21,258][flwr][ERROR] - Traceback (most recent call last):
  File "/home/abdenour/miniconda3/envs/fl_with_flower/lib/python3.11/site-packages/flwr/simulation/ray_transport/ray_client_proxy.py", line 94, in _submit_job
    out_mssg, updated_context = self.actor_pool.get_client_result(
                                ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/abdenour/miniconda3/envs/fl_with_flower/lib/python3.11/site-packages/flwr/simulation/ray_transport/ray_actor.py", line 398, in get_client_result
    return self._fetch_future_result(cid)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/abdenour/miniconda3/envs/fl_with_flower/lib/python3.11/site-packages/flwr/simulation/ray_transport/ray_actor.py", line 279, in _fetch_future_result
    res_cid, out_mssg, updated_context = ray.get(
                                         ^^^^^^^^
  File "/home/abdenour/miniconda3/envs/fl_with_flower/lib/python3.11/site-packages/ray/_private/auto_init_hook.py", line 21, in auto_init_wrapper
    return fn(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^
  File "/home/abdenour/miniconda3/envs/fl_with_flower/lib/python3.11/site-packages/ray/_private/client_mode_hook.py", line 103, in wrapper
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "/home/abdenour/miniconda3/envs/fl_with_flower/lib/python3.11/site-packages/ray/_private/worker.py", line 2667, in get
    values, debugger_breakpoint = worker.get_objects(object_refs, timeout=timeout)
                                  ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/abdenour/miniconda3/envs/fl_with_flower/lib/python3.11/site-packages/ray/_private/worker.py", line 866, in get_objects
    raise value
ray.exceptions.OutOfMemoryError: Task was killed due to the node running low on memory.
Memory on the node (IP: 115.145.171.128, ID: c687613ff34cdbf5097699e5c896055ef2dc44343c5ab44bd40309d5) where the task (actor ID: 85928d3f1a247800081c0e1101000000, name=ClientAppActor.__init__, pid=619614, memory used=1.73GB) was running was 29.50GB / 31.02GB (0.951049), which exceeds the memory usage threshold of 0.95. Ray killed this worker (ID: c7f644ac3cca20dffbfa1723daac4f87f04c7de03da25e7cad5fe33c) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 115.145.171.128`. To see the logs of the worker, use `ray logs worker-c7f644ac3cca20dffbfa1723daac4f87f04c7de03da25e7cad5fe33c*out -ip 115.145.171.128. Top 10 memory users:
PID	MEM(GB)	COMMAND
4942	3.49	/snap/pycharm-professional/408/jbr/bin/java -classpath /snap/pycharm-professional/408/lib/platform-l...
619616	1.80	ray::ClientAppActor.run
619615	1.78	ray::ClientAppActor
619617	1.77	ray::ClientAppActor
619611	1.73	ray::ClientAppActor
619618	1.73	ray::ClientAppActor
619614	1.73	ray::ClientAppActor
619612	1.72	ray::ClientAppActor
619619	1.70	ray::ClientAppActor
619620	1.68	ray::ClientAppActor.run
Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.

[2024-08-13 23:56:21,258][flwr][ERROR] - Traceback (most recent call last):
  File "/home/abdenour/miniconda3/envs/fl_with_flower/lib/python3.11/site-packages/flwr/simulation/ray_transport/ray_client_proxy.py", line 94, in _submit_job
    out_mssg, updated_context = self.actor_pool.get_client_result(
                                ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/abdenour/miniconda3/envs/fl_with_flower/lib/python3.11/site-packages/flwr/simulation/ray_transport/ray_actor.py", line 398, in get_client_result
    return self._fetch_future_result(cid)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/abdenour/miniconda3/envs/fl_with_flower/lib/python3.11/site-packages/flwr/simulation/ray_transport/ray_actor.py", line 279, in _fetch_future_result
    res_cid, out_mssg, updated_context = ray.get(
                                         ^^^^^^^^
  File "/home/abdenour/miniconda3/envs/fl_with_flower/lib/python3.11/site-packages/ray/_private/auto_init_hook.py", line 21, in auto_init_wrapper
    return fn(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^
  File "/home/abdenour/miniconda3/envs/fl_with_flower/lib/python3.11/site-packages/ray/_private/client_mode_hook.py", line 103, in wrapper
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "/home/abdenour/miniconda3/envs/fl_with_flower/lib/python3.11/site-packages/ray/_private/worker.py", line 2667, in get
    values, debugger_breakpoint = worker.get_objects(object_refs, timeout=timeout)
                                  ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/abdenour/miniconda3/envs/fl_with_flower/lib/python3.11/site-packages/ray/_private/worker.py", line 866, in get_objects
    raise value
ray.exceptions.OutOfMemoryError: Task was killed due to the node running low on memory.
Memory on the node (IP: 115.145.171.128, ID: c687613ff34cdbf5097699e5c896055ef2dc44343c5ab44bd40309d5) where the task (actor ID: 2a8bb25e35bf7bb5a0a6c00401000000, name=ClientAppActor.__init__, pid=619620, memory used=1.97GB) was running was 29.48GB / 31.02GB (0.950487), which exceeds the memory usage threshold of 0.95. Ray killed this worker (ID: 38a4d571c3fa7ff405dd9f5d1555b4753efb95dc84bb70ce01b6ecb4) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 115.145.171.128`. To see the logs of the worker, use `ray logs worker-38a4d571c3fa7ff405dd9f5d1555b4753efb95dc84bb70ce01b6ecb4*out -ip 115.145.171.128. Top 10 memory users:
PID	MEM(GB)	COMMAND
4942	3.48	/snap/pycharm-professional/408/jbr/bin/java -classpath /snap/pycharm-professional/408/lib/platform-l...
616443	2.85	/home/abdenour/miniconda3/envs/fl_with_flower/bin/python /home/abdenour/PycharmProjects/fl_with_flow...
619615	2.06	ray::ClientAppActor
619616	2.04	ray::ClientAppActor
619611	2.04	ray::ClientAppActor.run
619619	2.03	ray::ClientAppActor.run
619612	1.98	ray::ClientAppActor.run
619613	1.97	ray::ClientAppActor.run
619620	1.97	ray::ClientAppActor
619617	1.96	ray::ClientAppActor
Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.

[2024-08-13 23:56:21,259][flwr][ERROR] - Task was killed due to the node running low on memory.
Memory on the node (IP: 115.145.171.128, ID: c687613ff34cdbf5097699e5c896055ef2dc44343c5ab44bd40309d5) where the task (actor ID: 85928d3f1a247800081c0e1101000000, name=ClientAppActor.__init__, pid=619614, memory used=1.73GB) was running was 29.50GB / 31.02GB (0.951049), which exceeds the memory usage threshold of 0.95. Ray killed this worker (ID: c7f644ac3cca20dffbfa1723daac4f87f04c7de03da25e7cad5fe33c) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 115.145.171.128`. To see the logs of the worker, use `ray logs worker-c7f644ac3cca20dffbfa1723daac4f87f04c7de03da25e7cad5fe33c*out -ip 115.145.171.128. Top 10 memory users:
PID	MEM(GB)	COMMAND
4942	3.49	/snap/pycharm-professional/408/jbr/bin/java -classpath /snap/pycharm-professional/408/lib/platform-l...
619616	1.80	ray::ClientAppActor.run
619615	1.78	ray::ClientAppActor
619617	1.77	ray::ClientAppActor
619611	1.73	ray::ClientAppActor
619618	1.73	ray::ClientAppActor
619614	1.73	ray::ClientAppActor
619612	1.72	ray::ClientAppActor
619619	1.70	ray::ClientAppActor
619620	1.68	ray::ClientAppActor.run
Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.
[2024-08-13 23:56:21,259][flwr][ERROR] - Task was killed due to the node running low on memory.
Memory on the node (IP: 115.145.171.128, ID: c687613ff34cdbf5097699e5c896055ef2dc44343c5ab44bd40309d5) where the task (actor ID: 2a8bb25e35bf7bb5a0a6c00401000000, name=ClientAppActor.__init__, pid=619620, memory used=1.97GB) was running was 29.48GB / 31.02GB (0.950487), which exceeds the memory usage threshold of 0.95. Ray killed this worker (ID: 38a4d571c3fa7ff405dd9f5d1555b4753efb95dc84bb70ce01b6ecb4) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 115.145.171.128`. To see the logs of the worker, use `ray logs worker-38a4d571c3fa7ff405dd9f5d1555b4753efb95dc84bb70ce01b6ecb4*out -ip 115.145.171.128. Top 10 memory users:
PID	MEM(GB)	COMMAND
4942	3.48	/snap/pycharm-professional/408/jbr/bin/java -classpath /snap/pycharm-professional/408/lib/platform-l...
616443	2.85	/home/abdenour/miniconda3/envs/fl_with_flower/bin/python /home/abdenour/PycharmProjects/fl_with_flow...
619615	2.06	ray::ClientAppActor
619616	2.04	ray::ClientAppActor
619611	2.04	ray::ClientAppActor.run
619619	2.03	ray::ClientAppActor.run
619612	1.98	ray::ClientAppActor.run
619613	1.97	ray::ClientAppActor.run
619620	1.97	ray::ClientAppActor
619617	1.96	ray::ClientAppActor
Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.
[2024-08-13 23:56:21,259][flwr][ERROR] - Traceback (most recent call last):
  File "/home/abdenour/miniconda3/envs/fl_with_flower/lib/python3.11/site-packages/flwr/simulation/ray_transport/ray_client_proxy.py", line 94, in _submit_job
    out_mssg, updated_context = self.actor_pool.get_client_result(
                                ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/abdenour/miniconda3/envs/fl_with_flower/lib/python3.11/site-packages/flwr/simulation/ray_transport/ray_actor.py", line 398, in get_client_result
    return self._fetch_future_result(cid)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/abdenour/miniconda3/envs/fl_with_flower/lib/python3.11/site-packages/flwr/simulation/ray_transport/ray_actor.py", line 279, in _fetch_future_result
    res_cid, out_mssg, updated_context = ray.get(
                                         ^^^^^^^^
  File "/home/abdenour/miniconda3/envs/fl_with_flower/lib/python3.11/site-packages/ray/_private/auto_init_hook.py", line 21, in auto_init_wrapper
    return fn(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^
  File "/home/abdenour/miniconda3/envs/fl_with_flower/lib/python3.11/site-packages/ray/_private/client_mode_hook.py", line 103, in wrapper
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "/home/abdenour/miniconda3/envs/fl_with_flower/lib/python3.11/site-packages/ray/_private/worker.py", line 2667, in get
    values, debugger_breakpoint = worker.get_objects(object_refs, timeout=timeout)
                                  ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/abdenour/miniconda3/envs/fl_with_flower/lib/python3.11/site-packages/ray/_private/worker.py", line 866, in get_objects
    raise value
ray.exceptions.OutOfMemoryError: Task was killed due to the node running low on memory.
Memory on the node (IP: 115.145.171.128, ID: c687613ff34cdbf5097699e5c896055ef2dc44343c5ab44bd40309d5) where the task (actor ID: 52365bb7997979e5a4c7ddbd01000000, name=ClientAppActor.__init__, pid=619618, memory used=1.84GB) was running was 29.49GB / 31.02GB (0.950707), which exceeds the memory usage threshold of 0.95. Ray killed this worker (ID: 7e48cd46c72821b17be65b05a4e61b5846ba998a57d76db44ca4f8fa) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 115.145.171.128`. To see the logs of the worker, use `ray logs worker-7e48cd46c72821b17be65b05a4e61b5846ba998a57d76db44ca4f8fa*out -ip 115.145.171.128. Top 10 memory users:
PID	MEM(GB)	COMMAND
4942	3.48	/snap/pycharm-professional/408/jbr/bin/java -classpath /snap/pycharm-professional/408/lib/platform-l...
616443	2.14	/home/abdenour/miniconda3/envs/fl_with_flower/bin/python /home/abdenour/PycharmProjects/fl_with_flow...
619615	1.90	ray::ClientAppActor
619616	1.88	ray::ClientAppActor
619611	1.87	ray::ClientAppActor
619617	1.86	ray::ClientAppActor
619620	1.85	ray::ClientAppActor
619618	1.84	ray::ClientAppActor
619613	1.82	ray::ClientAppActor
619619	1.80	ray::ClientAppActor
Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.

[2024-08-13 23:56:21,259][flwr][ERROR] - Task was killed due to the node running low on memory.
Memory on the node (IP: 115.145.171.128, ID: c687613ff34cdbf5097699e5c896055ef2dc44343c5ab44bd40309d5) where the task (actor ID: 52365bb7997979e5a4c7ddbd01000000, name=ClientAppActor.__init__, pid=619618, memory used=1.84GB) was running was 29.49GB / 31.02GB (0.950707), which exceeds the memory usage threshold of 0.95. Ray killed this worker (ID: 7e48cd46c72821b17be65b05a4e61b5846ba998a57d76db44ca4f8fa) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 115.145.171.128`. To see the logs of the worker, use `ray logs worker-7e48cd46c72821b17be65b05a4e61b5846ba998a57d76db44ca4f8fa*out -ip 115.145.171.128. Top 10 memory users:
PID	MEM(GB)	COMMAND
4942	3.48	/snap/pycharm-professional/408/jbr/bin/java -classpath /snap/pycharm-professional/408/lib/platform-l...
616443	2.14	/home/abdenour/miniconda3/envs/fl_with_flower/bin/python /home/abdenour/PycharmProjects/fl_with_flow...
619615	1.90	ray::ClientAppActor
619616	1.88	ray::ClientAppActor
619611	1.87	ray::ClientAppActor
619617	1.86	ray::ClientAppActor
619620	1.85	ray::ClientAppActor
619618	1.84	ray::ClientAppActor
619613	1.82	ray::ClientAppActor
619619	1.80	ray::ClientAppActor
Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.
[2024-08-13 23:56:23,263][flwr][INFO] - aggregate_fit: received 7 results and 3 failures
[2024-08-13 23:56:26,723][flwr][INFO] - fit progress: (261, 15.37399572134018, {'accuracy': 0.75}, 1507.9829188250005)
[2024-08-13 23:56:26,723][flwr][INFO] - configure_evaluate: strategy sampled 2 clients (out of 20)
[2024-08-13 23:56:27,209][flwr][INFO] - aggregate_evaluate: received 2 results and 0 failures
[2024-08-13 23:56:27,209][flwr][INFO] - 
[2024-08-13 23:56:27,209][flwr][INFO] - [ROUND 262]
[2024-08-13 23:56:27,209][flwr][INFO] - configure_fit: strategy sampled 10 clients (out of 20)
[2024-08-13 23:56:27,362][flwr][ERROR] - Traceback (most recent call last):
  File "/home/abdenour/miniconda3/envs/fl_with_flower/lib/python3.11/site-packages/flwr/simulation/ray_transport/ray_client_proxy.py", line 94, in _submit_job
    out_mssg, updated_context = self.actor_pool.get_client_result(
                                ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/abdenour/miniconda3/envs/fl_with_flower/lib/python3.11/site-packages/flwr/simulation/ray_transport/ray_actor.py", line 398, in get_client_result
    return self._fetch_future_result(cid)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/abdenour/miniconda3/envs/fl_with_flower/lib/python3.11/site-packages/flwr/simulation/ray_transport/ray_actor.py", line 279, in _fetch_future_result
    res_cid, out_mssg, updated_context = ray.get(
                                         ^^^^^^^^
  File "/home/abdenour/miniconda3/envs/fl_with_flower/lib/python3.11/site-packages/ray/_private/auto_init_hook.py", line 21, in auto_init_wrapper
    return fn(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^
  File "/home/abdenour/miniconda3/envs/fl_with_flower/lib/python3.11/site-packages/ray/_private/client_mode_hook.py", line 103, in wrapper
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "/home/abdenour/miniconda3/envs/fl_with_flower/lib/python3.11/site-packages/ray/_private/worker.py", line 2667, in get
    values, debugger_breakpoint = worker.get_objects(object_refs, timeout=timeout)
                                  ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/abdenour/miniconda3/envs/fl_with_flower/lib/python3.11/site-packages/ray/_private/worker.py", line 866, in get_objects
    raise value
ray.exceptions.OutOfMemoryError: Task was killed due to the node running low on memory.
Memory on the node (IP: 115.145.171.128, ID: c687613ff34cdbf5097699e5c896055ef2dc44343c5ab44bd40309d5) where the task (actor ID: 52365bb7997979e5a4c7ddbd01000000, name=ClientAppActor.__init__, pid=619618, memory used=1.84GB) was running was 29.49GB / 31.02GB (0.950707), which exceeds the memory usage threshold of 0.95. Ray killed this worker (ID: 7e48cd46c72821b17be65b05a4e61b5846ba998a57d76db44ca4f8fa) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 115.145.171.128`. To see the logs of the worker, use `ray logs worker-7e48cd46c72821b17be65b05a4e61b5846ba998a57d76db44ca4f8fa*out -ip 115.145.171.128. Top 10 memory users:
PID	MEM(GB)	COMMAND
4942	3.48	/snap/pycharm-professional/408/jbr/bin/java -classpath /snap/pycharm-professional/408/lib/platform-l...
616443	2.14	/home/abdenour/miniconda3/envs/fl_with_flower/bin/python /home/abdenour/PycharmProjects/fl_with_flow...
619615	1.90	ray::ClientAppActor
619616	1.88	ray::ClientAppActor
619611	1.87	ray::ClientAppActor
619617	1.86	ray::ClientAppActor
619620	1.85	ray::ClientAppActor
619618	1.84	ray::ClientAppActor
619613	1.82	ray::ClientAppActor
619619	1.80	ray::ClientAppActor
Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.

[2024-08-13 23:56:27,363][flwr][ERROR] - Task was killed due to the node running low on memory.
Memory on the node (IP: 115.145.171.128, ID: c687613ff34cdbf5097699e5c896055ef2dc44343c5ab44bd40309d5) where the task (actor ID: 52365bb7997979e5a4c7ddbd01000000, name=ClientAppActor.__init__, pid=619618, memory used=1.84GB) was running was 29.49GB / 31.02GB (0.950707), which exceeds the memory usage threshold of 0.95. Ray killed this worker (ID: 7e48cd46c72821b17be65b05a4e61b5846ba998a57d76db44ca4f8fa) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 115.145.171.128`. To see the logs of the worker, use `ray logs worker-7e48cd46c72821b17be65b05a4e61b5846ba998a57d76db44ca4f8fa*out -ip 115.145.171.128. Top 10 memory users:
PID	MEM(GB)	COMMAND
4942	3.48	/snap/pycharm-professional/408/jbr/bin/java -classpath /snap/pycharm-professional/408/lib/platform-l...
616443	2.14	/home/abdenour/miniconda3/envs/fl_with_flower/bin/python /home/abdenour/PycharmProjects/fl_with_flow...
619615	1.90	ray::ClientAppActor
619616	1.88	ray::ClientAppActor
619611	1.87	ray::ClientAppActor
619617	1.86	ray::ClientAppActor
619620	1.85	ray::ClientAppActor
619618	1.84	ray::ClientAppActor
619613	1.82	ray::ClientAppActor
619619	1.80	ray::ClientAppActor
Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.
[2024-08-13 23:56:27,363][flwr][ERROR] - Traceback (most recent call last):
  File "/home/abdenour/miniconda3/envs/fl_with_flower/lib/python3.11/site-packages/flwr/simulation/ray_transport/ray_client_proxy.py", line 94, in _submit_job
    out_mssg, updated_context = self.actor_pool.get_client_result(
                                ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/abdenour/miniconda3/envs/fl_with_flower/lib/python3.11/site-packages/flwr/simulation/ray_transport/ray_actor.py", line 398, in get_client_result
    return self._fetch_future_result(cid)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/abdenour/miniconda3/envs/fl_with_flower/lib/python3.11/site-packages/flwr/simulation/ray_transport/ray_actor.py", line 279, in _fetch_future_result
    res_cid, out_mssg, updated_context = ray.get(
                                         ^^^^^^^^
  File "/home/abdenour/miniconda3/envs/fl_with_flower/lib/python3.11/site-packages/ray/_private/auto_init_hook.py", line 21, in auto_init_wrapper
    return fn(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^
  File "/home/abdenour/miniconda3/envs/fl_with_flower/lib/python3.11/site-packages/ray/_private/client_mode_hook.py", line 103, in wrapper
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "/home/abdenour/miniconda3/envs/fl_with_flower/lib/python3.11/site-packages/ray/_private/worker.py", line 2667, in get
    values, debugger_breakpoint = worker.get_objects(object_refs, timeout=timeout)
                                  ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/abdenour/miniconda3/envs/fl_with_flower/lib/python3.11/site-packages/ray/_private/worker.py", line 866, in get_objects
    raise value
ray.exceptions.OutOfMemoryError: Task was killed due to the node running low on memory.
Memory on the node (IP: 115.145.171.128, ID: c687613ff34cdbf5097699e5c896055ef2dc44343c5ab44bd40309d5) where the task (actor ID: 2a8bb25e35bf7bb5a0a6c00401000000, name=ClientAppActor.__init__, pid=619620, memory used=1.97GB) was running was 29.48GB / 31.02GB (0.950487), which exceeds the memory usage threshold of 0.95. Ray killed this worker (ID: 38a4d571c3fa7ff405dd9f5d1555b4753efb95dc84bb70ce01b6ecb4) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 115.145.171.128`. To see the logs of the worker, use `ray logs worker-38a4d571c3fa7ff405dd9f5d1555b4753efb95dc84bb70ce01b6ecb4*out -ip 115.145.171.128. Top 10 memory users:
PID	MEM(GB)	COMMAND
4942	3.48	/snap/pycharm-professional/408/jbr/bin/java -classpath /snap/pycharm-professional/408/lib/platform-l...
616443	2.85	/home/abdenour/miniconda3/envs/fl_with_flower/bin/python /home/abdenour/PycharmProjects/fl_with_flow...
619615	2.06	ray::ClientAppActor
619616	2.04	ray::ClientAppActor
619611	2.04	ray::ClientAppActor.run
619619	2.03	ray::ClientAppActor.run
619612	1.98	ray::ClientAppActor.run
619613	1.97	ray::ClientAppActor.run
619620	1.97	ray::ClientAppActor
619617	1.96	ray::ClientAppActor
Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.

[2024-08-13 23:56:27,363][flwr][ERROR] - Task was killed due to the node running low on memory.
Memory on the node (IP: 115.145.171.128, ID: c687613ff34cdbf5097699e5c896055ef2dc44343c5ab44bd40309d5) where the task (actor ID: 2a8bb25e35bf7bb5a0a6c00401000000, name=ClientAppActor.__init__, pid=619620, memory used=1.97GB) was running was 29.48GB / 31.02GB (0.950487), which exceeds the memory usage threshold of 0.95. Ray killed this worker (ID: 38a4d571c3fa7ff405dd9f5d1555b4753efb95dc84bb70ce01b6ecb4) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 115.145.171.128`. To see the logs of the worker, use `ray logs worker-38a4d571c3fa7ff405dd9f5d1555b4753efb95dc84bb70ce01b6ecb4*out -ip 115.145.171.128. Top 10 memory users:
PID	MEM(GB)	COMMAND
4942	3.48	/snap/pycharm-professional/408/jbr/bin/java -classpath /snap/pycharm-professional/408/lib/platform-l...
616443	2.85	/home/abdenour/miniconda3/envs/fl_with_flower/bin/python /home/abdenour/PycharmProjects/fl_with_flow...
619615	2.06	ray::ClientAppActor
619616	2.04	ray::ClientAppActor
619611	2.04	ray::ClientAppActor.run
619619	2.03	ray::ClientAppActor.run
619612	1.98	ray::ClientAppActor.run
619613	1.97	ray::ClientAppActor.run
619620	1.97	ray::ClientAppActor
619617	1.96	ray::ClientAppActor
Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.
[2024-08-13 23:56:27,364][flwr][ERROR] - Traceback (most recent call last):
  File "/home/abdenour/miniconda3/envs/fl_with_flower/lib/python3.11/site-packages/flwr/simulation/ray_transport/ray_client_proxy.py", line 94, in _submit_job
    out_mssg, updated_context = self.actor_pool.get_client_result(
                                ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/abdenour/miniconda3/envs/fl_with_flower/lib/python3.11/site-packages/flwr/simulation/ray_transport/ray_actor.py", line 398, in get_client_result
    return self._fetch_future_result(cid)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/abdenour/miniconda3/envs/fl_with_flower/lib/python3.11/site-packages/flwr/simulation/ray_transport/ray_actor.py", line 279, in _fetch_future_result
    res_cid, out_mssg, updated_context = ray.get(
                                         ^^^^^^^^
  File "/home/abdenour/miniconda3/envs/fl_with_flower/lib/python3.11/site-packages/ray/_private/auto_init_hook.py", line 21, in auto_init_wrapper
    return fn(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^
  File "/home/abdenour/miniconda3/envs/fl_with_flower/lib/python3.11/site-packages/ray/_private/client_mode_hook.py", line 103, in wrapper
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "/home/abdenour/miniconda3/envs/fl_with_flower/lib/python3.11/site-packages/ray/_private/worker.py", line 2667, in get
    values, debugger_breakpoint = worker.get_objects(object_refs, timeout=timeout)
                                  ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/abdenour/miniconda3/envs/fl_with_flower/lib/python3.11/site-packages/ray/_private/worker.py", line 866, in get_objects
    raise value
ray.exceptions.OutOfMemoryError: Task was killed due to the node running low on memory.
Memory on the node (IP: 115.145.171.128, ID: c687613ff34cdbf5097699e5c896055ef2dc44343c5ab44bd40309d5) where the task (actor ID: 85928d3f1a247800081c0e1101000000, name=ClientAppActor.__init__, pid=619614, memory used=1.73GB) was running was 29.50GB / 31.02GB (0.951049), which exceeds the memory usage threshold of 0.95. Ray killed this worker (ID: c7f644ac3cca20dffbfa1723daac4f87f04c7de03da25e7cad5fe33c) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 115.145.171.128`. To see the logs of the worker, use `ray logs worker-c7f644ac3cca20dffbfa1723daac4f87f04c7de03da25e7cad5fe33c*out -ip 115.145.171.128. Top 10 memory users:
PID	MEM(GB)	COMMAND
4942	3.49	/snap/pycharm-professional/408/jbr/bin/java -classpath /snap/pycharm-professional/408/lib/platform-l...
619616	1.80	ray::ClientAppActor.run
619615	1.78	ray::ClientAppActor
619617	1.77	ray::ClientAppActor
619611	1.73	ray::ClientAppActor
619618	1.73	ray::ClientAppActor
619614	1.73	ray::ClientAppActor
619612	1.72	ray::ClientAppActor
619619	1.70	ray::ClientAppActor
619620	1.68	ray::ClientAppActor.run
Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.

[2024-08-13 23:56:27,364][flwr][ERROR] - Task was killed due to the node running low on memory.
Memory on the node (IP: 115.145.171.128, ID: c687613ff34cdbf5097699e5c896055ef2dc44343c5ab44bd40309d5) where the task (actor ID: 85928d3f1a247800081c0e1101000000, name=ClientAppActor.__init__, pid=619614, memory used=1.73GB) was running was 29.50GB / 31.02GB (0.951049), which exceeds the memory usage threshold of 0.95. Ray killed this worker (ID: c7f644ac3cca20dffbfa1723daac4f87f04c7de03da25e7cad5fe33c) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 115.145.171.128`. To see the logs of the worker, use `ray logs worker-c7f644ac3cca20dffbfa1723daac4f87f04c7de03da25e7cad5fe33c*out -ip 115.145.171.128. Top 10 memory users:
PID	MEM(GB)	COMMAND
4942	3.49	/snap/pycharm-professional/408/jbr/bin/java -classpath /snap/pycharm-professional/408/lib/platform-l...
619616	1.80	ray::ClientAppActor.run
619615	1.78	ray::ClientAppActor
619617	1.77	ray::ClientAppActor
619611	1.73	ray::ClientAppActor
619618	1.73	ray::ClientAppActor
619614	1.73	ray::ClientAppActor
619612	1.72	ray::ClientAppActor
619619	1.70	ray::ClientAppActor
619620	1.68	ray::ClientAppActor.run
Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.
[2024-08-13 23:56:28,899][flwr][INFO] - aggregate_fit: received 7 results and 3 failures
[2024-08-13 23:56:32,670][flwr][INFO] - fit progress: (262, 16.872270464897156, {'accuracy': 0.7532051282051282}, 1513.9303672770038)
[2024-08-13 23:56:32,670][flwr][INFO] - configure_evaluate: strategy sampled 2 clients (out of 20)
[2024-08-13 23:56:33,130][flwr][INFO] - aggregate_evaluate: received 2 results and 0 failures
[2024-08-13 23:56:33,130][flwr][INFO] - 
[2024-08-13 23:56:33,130][flwr][INFO] - [ROUND 263]
[2024-08-13 23:56:33,130][flwr][INFO] - configure_fit: strategy sampled 10 clients (out of 20)
[2024-08-13 23:56:33,267][flwr][ERROR] - Traceback (most recent call last):
  File "/home/abdenour/miniconda3/envs/fl_with_flower/lib/python3.11/site-packages/flwr/simulation/ray_transport/ray_client_proxy.py", line 94, in _submit_job
    out_mssg, updated_context = self.actor_pool.get_client_result(
                                ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/abdenour/miniconda3/envs/fl_with_flower/lib/python3.11/site-packages/flwr/simulation/ray_transport/ray_actor.py", line 398, in get_client_result
    return self._fetch_future_result(cid)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/abdenour/miniconda3/envs/fl_with_flower/lib/python3.11/site-packages/flwr/simulation/ray_transport/ray_actor.py", line 279, in _fetch_future_result
    res_cid, out_mssg, updated_context = ray.get(
                                         ^^^^^^^^
  File "/home/abdenour/miniconda3/envs/fl_with_flower/lib/python3.11/site-packages/ray/_private/auto_init_hook.py", line 21, in auto_init_wrapper
    return fn(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^
  File "/home/abdenour/miniconda3/envs/fl_with_flower/lib/python3.11/site-packages/ray/_private/client_mode_hook.py", line 103, in wrapper
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "/home/abdenour/miniconda3/envs/fl_with_flower/lib/python3.11/site-packages/ray/_private/worker.py", line 2667, in get
    values, debugger_breakpoint = worker.get_objects(object_refs, timeout=timeout)
                                  ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/abdenour/miniconda3/envs/fl_with_flower/lib/python3.11/site-packages/ray/_private/worker.py", line 866, in get_objects
    raise value
ray.exceptions.OutOfMemoryError: Task was killed due to the node running low on memory.
Memory on the node (IP: 115.145.171.128, ID: c687613ff34cdbf5097699e5c896055ef2dc44343c5ab44bd40309d5) where the task (actor ID: 2a8bb25e35bf7bb5a0a6c00401000000, name=ClientAppActor.__init__, pid=619620, memory used=1.97GB) was running was 29.48GB / 31.02GB (0.950487), which exceeds the memory usage threshold of 0.95. Ray killed this worker (ID: 38a4d571c3fa7ff405dd9f5d1555b4753efb95dc84bb70ce01b6ecb4) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 115.145.171.128`. To see the logs of the worker, use `ray logs worker-38a4d571c3fa7ff405dd9f5d1555b4753efb95dc84bb70ce01b6ecb4*out -ip 115.145.171.128. Top 10 memory users:
PID	MEM(GB)	COMMAND
4942	3.48	/snap/pycharm-professional/408/jbr/bin/java -classpath /snap/pycharm-professional/408/lib/platform-l...
616443	2.85	/home/abdenour/miniconda3/envs/fl_with_flower/bin/python /home/abdenour/PycharmProjects/fl_with_flow...
619615	2.06	ray::ClientAppActor
619616	2.04	ray::ClientAppActor
619611	2.04	ray::ClientAppActor.run
619619	2.03	ray::ClientAppActor.run
619612	1.98	ray::ClientAppActor.run
619613	1.97	ray::ClientAppActor.run
619620	1.97	ray::ClientAppActor
619617	1.96	ray::ClientAppActor
Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.

[2024-08-13 23:56:33,267][flwr][ERROR] - Task was killed due to the node running low on memory.
Memory on the node (IP: 115.145.171.128, ID: c687613ff34cdbf5097699e5c896055ef2dc44343c5ab44bd40309d5) where the task (actor ID: 2a8bb25e35bf7bb5a0a6c00401000000, name=ClientAppActor.__init__, pid=619620, memory used=1.97GB) was running was 29.48GB / 31.02GB (0.950487), which exceeds the memory usage threshold of 0.95. Ray killed this worker (ID: 38a4d571c3fa7ff405dd9f5d1555b4753efb95dc84bb70ce01b6ecb4) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 115.145.171.128`. To see the logs of the worker, use `ray logs worker-38a4d571c3fa7ff405dd9f5d1555b4753efb95dc84bb70ce01b6ecb4*out -ip 115.145.171.128. Top 10 memory users:
PID	MEM(GB)	COMMAND
4942	3.48	/snap/pycharm-professional/408/jbr/bin/java -classpath /snap/pycharm-professional/408/lib/platform-l...
616443	2.85	/home/abdenour/miniconda3/envs/fl_with_flower/bin/python /home/abdenour/PycharmProjects/fl_with_flow...
619615	2.06	ray::ClientAppActor
619616	2.04	ray::ClientAppActor
619611	2.04	ray::ClientAppActor.run
619619	2.03	ray::ClientAppActor.run
619612	1.98	ray::ClientAppActor.run
619613	1.97	ray::ClientAppActor.run
619620	1.97	ray::ClientAppActor
619617	1.96	ray::ClientAppActor
Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.
[2024-08-13 23:56:33,271][flwr][ERROR] - Traceback (most recent call last):
  File "/home/abdenour/miniconda3/envs/fl_with_flower/lib/python3.11/site-packages/flwr/simulation/ray_transport/ray_client_proxy.py", line 94, in _submit_job
    out_mssg, updated_context = self.actor_pool.get_client_result(
                                ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/abdenour/miniconda3/envs/fl_with_flower/lib/python3.11/site-packages/flwr/simulation/ray_transport/ray_actor.py", line 398, in get_client_result
    return self._fetch_future_result(cid)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/abdenour/miniconda3/envs/fl_with_flower/lib/python3.11/site-packages/flwr/simulation/ray_transport/ray_actor.py", line 279, in _fetch_future_result
    res_cid, out_mssg, updated_context = ray.get(
                                         ^^^^^^^^
  File "/home/abdenour/miniconda3/envs/fl_with_flower/lib/python3.11/site-packages/ray/_private/auto_init_hook.py", line 21, in auto_init_wrapper
    return fn(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^
  File "/home/abdenour/miniconda3/envs/fl_with_flower/lib/python3.11/site-packages/ray/_private/client_mode_hook.py", line 103, in wrapper
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "/home/abdenour/miniconda3/envs/fl_with_flower/lib/python3.11/site-packages/ray/_private/worker.py", line 2667, in get
    values, debugger_breakpoint = worker.get_objects(object_refs, timeout=timeout)
                                  ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/abdenour/miniconda3/envs/fl_with_flower/lib/python3.11/site-packages/ray/_private/worker.py", line 866, in get_objects
    raise value
ray.exceptions.OutOfMemoryError: Task was killed due to the node running low on memory.
Memory on the node (IP: 115.145.171.128, ID: c687613ff34cdbf5097699e5c896055ef2dc44343c5ab44bd40309d5) where the task (actor ID: 85928d3f1a247800081c0e1101000000, name=ClientAppActor.__init__, pid=619614, memory used=1.73GB) was running was 29.50GB / 31.02GB (0.951049), which exceeds the memory usage threshold of 0.95. Ray killed this worker (ID: c7f644ac3cca20dffbfa1723daac4f87f04c7de03da25e7cad5fe33c) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 115.145.171.128`. To see the logs of the worker, use `ray logs worker-c7f644ac3cca20dffbfa1723daac4f87f04c7de03da25e7cad5fe33c*out -ip 115.145.171.128. Top 10 memory users:
PID	MEM(GB)	COMMAND
4942	3.49	/snap/pycharm-professional/408/jbr/bin/java -classpath /snap/pycharm-professional/408/lib/platform-l...
619616	1.80	ray::ClientAppActor.run
619615	1.78	ray::ClientAppActor
619617	1.77	ray::ClientAppActor
619611	1.73	ray::ClientAppActor
619618	1.73	ray::ClientAppActor
619614	1.73	ray::ClientAppActor
619612	1.72	ray::ClientAppActor
619619	1.70	ray::ClientAppActor
619620	1.68	ray::ClientAppActor.run
Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.

[2024-08-13 23:56:33,271][flwr][ERROR] - Task was killed due to the node running low on memory.
Memory on the node (IP: 115.145.171.128, ID: c687613ff34cdbf5097699e5c896055ef2dc44343c5ab44bd40309d5) where the task (actor ID: 85928d3f1a247800081c0e1101000000, name=ClientAppActor.__init__, pid=619614, memory used=1.73GB) was running was 29.50GB / 31.02GB (0.951049), which exceeds the memory usage threshold of 0.95. Ray killed this worker (ID: c7f644ac3cca20dffbfa1723daac4f87f04c7de03da25e7cad5fe33c) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 115.145.171.128`. To see the logs of the worker, use `ray logs worker-c7f644ac3cca20dffbfa1723daac4f87f04c7de03da25e7cad5fe33c*out -ip 115.145.171.128. Top 10 memory users:
PID	MEM(GB)	COMMAND
4942	3.49	/snap/pycharm-professional/408/jbr/bin/java -classpath /snap/pycharm-professional/408/lib/platform-l...
619616	1.80	ray::ClientAppActor.run
619615	1.78	ray::ClientAppActor
619617	1.77	ray::ClientAppActor
619611	1.73	ray::ClientAppActor
619618	1.73	ray::ClientAppActor
619614	1.73	ray::ClientAppActor
619612	1.72	ray::ClientAppActor
619619	1.70	ray::ClientAppActor
619620	1.68	ray::ClientAppActor.run
Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.
[2024-08-13 23:56:33,271][flwr][ERROR] - Traceback (most recent call last):
  File "/home/abdenour/miniconda3/envs/fl_with_flower/lib/python3.11/site-packages/flwr/simulation/ray_transport/ray_client_proxy.py", line 94, in _submit_job
    out_mssg, updated_context = self.actor_pool.get_client_result(
                                ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/abdenour/miniconda3/envs/fl_with_flower/lib/python3.11/site-packages/flwr/simulation/ray_transport/ray_actor.py", line 398, in get_client_result
    return self._fetch_future_result(cid)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/abdenour/miniconda3/envs/fl_with_flower/lib/python3.11/site-packages/flwr/simulation/ray_transport/ray_actor.py", line 279, in _fetch_future_result
    res_cid, out_mssg, updated_context = ray.get(
                                         ^^^^^^^^
  File "/home/abdenour/miniconda3/envs/fl_with_flower/lib/python3.11/site-packages/ray/_private/auto_init_hook.py", line 21, in auto_init_wrapper
    return fn(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^
  File "/home/abdenour/miniconda3/envs/fl_with_flower/lib/python3.11/site-packages/ray/_private/client_mode_hook.py", line 103, in wrapper
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "/home/abdenour/miniconda3/envs/fl_with_flower/lib/python3.11/site-packages/ray/_private/worker.py", line 2667, in get
    values, debugger_breakpoint = worker.get_objects(object_refs, timeout=timeout)
                                  ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/abdenour/miniconda3/envs/fl_with_flower/lib/python3.11/site-packages/ray/_private/worker.py", line 866, in get_objects
    raise value
ray.exceptions.OutOfMemoryError: Task was killed due to the node running low on memory.
Memory on the node (IP: 115.145.171.128, ID: c687613ff34cdbf5097699e5c896055ef2dc44343c5ab44bd40309d5) where the task (actor ID: 2a8bb25e35bf7bb5a0a6c00401000000, name=ClientAppActor.__init__, pid=619620, memory used=1.97GB) was running was 29.48GB / 31.02GB (0.950487), which exceeds the memory usage threshold of 0.95. Ray killed this worker (ID: 38a4d571c3fa7ff405dd9f5d1555b4753efb95dc84bb70ce01b6ecb4) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 115.145.171.128`. To see the logs of the worker, use `ray logs worker-38a4d571c3fa7ff405dd9f5d1555b4753efb95dc84bb70ce01b6ecb4*out -ip 115.145.171.128. Top 10 memory users:
PID	MEM(GB)	COMMAND
4942	3.48	/snap/pycharm-professional/408/jbr/bin/java -classpath /snap/pycharm-professional/408/lib/platform-l...
616443	2.85	/home/abdenour/miniconda3/envs/fl_with_flower/bin/python /home/abdenour/PycharmProjects/fl_with_flow...
619615	2.06	ray::ClientAppActor
619616	2.04	ray::ClientAppActor
619611	2.04	ray::ClientAppActor.run
619619	2.03	ray::ClientAppActor.run
619612	1.98	ray::ClientAppActor.run
619613	1.97	ray::ClientAppActor.run
619620	1.97	ray::ClientAppActor
619617	1.96	ray::ClientAppActor
Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.

[2024-08-13 23:56:33,271][flwr][ERROR] - Task was killed due to the node running low on memory.
Memory on the node (IP: 115.145.171.128, ID: c687613ff34cdbf5097699e5c896055ef2dc44343c5ab44bd40309d5) where the task (actor ID: 2a8bb25e35bf7bb5a0a6c00401000000, name=ClientAppActor.__init__, pid=619620, memory used=1.97GB) was running was 29.48GB / 31.02GB (0.950487), which exceeds the memory usage threshold of 0.95. Ray killed this worker (ID: 38a4d571c3fa7ff405dd9f5d1555b4753efb95dc84bb70ce01b6ecb4) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 115.145.171.128`. To see the logs of the worker, use `ray logs worker-38a4d571c3fa7ff405dd9f5d1555b4753efb95dc84bb70ce01b6ecb4*out -ip 115.145.171.128. Top 10 memory users:
PID	MEM(GB)	COMMAND
4942	3.48	/snap/pycharm-professional/408/jbr/bin/java -classpath /snap/pycharm-professional/408/lib/platform-l...
616443	2.85	/home/abdenour/miniconda3/envs/fl_with_flower/bin/python /home/abdenour/PycharmProjects/fl_with_flow...
619615	2.06	ray::ClientAppActor
619616	2.04	ray::ClientAppActor
619611	2.04	ray::ClientAppActor.run
619619	2.03	ray::ClientAppActor.run
619612	1.98	ray::ClientAppActor.run
619613	1.97	ray::ClientAppActor.run
619620	1.97	ray::ClientAppActor
619617	1.96	ray::ClientAppActor
Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.
[2024-08-13 23:56:34,847][flwr][INFO] - aggregate_fit: received 7 results and 3 failures
[2024-08-13 23:56:37,802][flwr][INFO] - fit progress: (263, 17.83409422636032, {'accuracy': 0.7532051282051282}, 1519.0626898310147)
[2024-08-13 23:56:37,803][flwr][INFO] - configure_evaluate: strategy sampled 2 clients (out of 20)
[2024-08-13 23:56:38,072][flwr][INFO] - aggregate_evaluate: received 2 results and 0 failures
[2024-08-13 23:56:38,072][flwr][INFO] - 
[2024-08-13 23:56:38,072][flwr][INFO] - [ROUND 264]
[2024-08-13 23:56:38,072][flwr][INFO] - configure_fit: strategy sampled 10 clients (out of 20)
[2024-08-13 23:56:38,242][flwr][ERROR] - Traceback (most recent call last):
  File "/home/abdenour/miniconda3/envs/fl_with_flower/lib/python3.11/site-packages/flwr/simulation/ray_transport/ray_client_proxy.py", line 94, in _submit_job
    out_mssg, updated_context = self.actor_pool.get_client_result(
                                ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/abdenour/miniconda3/envs/fl_with_flower/lib/python3.11/site-packages/flwr/simulation/ray_transport/ray_actor.py", line 398, in get_client_result
    return self._fetch_future_result(cid)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/abdenour/miniconda3/envs/fl_with_flower/lib/python3.11/site-packages/flwr/simulation/ray_transport/ray_actor.py", line 279, in _fetch_future_result
    res_cid, out_mssg, updated_context = ray.get(
                                         ^^^^^^^^
  File "/home/abdenour/miniconda3/envs/fl_with_flower/lib/python3.11/site-packages/ray/_private/auto_init_hook.py", line 21, in auto_init_wrapper
    return fn(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^
  File "/home/abdenour/miniconda3/envs/fl_with_flower/lib/python3.11/site-packages/ray/_private/client_mode_hook.py", line 103, in wrapper
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "/home/abdenour/miniconda3/envs/fl_with_flower/lib/python3.11/site-packages/ray/_private/worker.py", line 2667, in get
    values, debugger_breakpoint = worker.get_objects(object_refs, timeout=timeout)
                                  ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/abdenour/miniconda3/envs/fl_with_flower/lib/python3.11/site-packages/ray/_private/worker.py", line 866, in get_objects
    raise value
ray.exceptions.OutOfMemoryError: Task was killed due to the node running low on memory.
Memory on the node (IP: 115.145.171.128, ID: c687613ff34cdbf5097699e5c896055ef2dc44343c5ab44bd40309d5) where the task (actor ID: 2a8bb25e35bf7bb5a0a6c00401000000, name=ClientAppActor.__init__, pid=619620, memory used=1.97GB) was running was 29.48GB / 31.02GB (0.950487), which exceeds the memory usage threshold of 0.95. Ray killed this worker (ID: 38a4d571c3fa7ff405dd9f5d1555b4753efb95dc84bb70ce01b6ecb4) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 115.145.171.128`. To see the logs of the worker, use `ray logs worker-38a4d571c3fa7ff405dd9f5d1555b4753efb95dc84bb70ce01b6ecb4*out -ip 115.145.171.128. Top 10 memory users:
PID	MEM(GB)	COMMAND
4942	3.48	/snap/pycharm-professional/408/jbr/bin/java -classpath /snap/pycharm-professional/408/lib/platform-l...
616443	2.85	/home/abdenour/miniconda3/envs/fl_with_flower/bin/python /home/abdenour/PycharmProjects/fl_with_flow...
619615	2.06	ray::ClientAppActor
619616	2.04	ray::ClientAppActor
619611	2.04	ray::ClientAppActor.run
619619	2.03	ray::ClientAppActor.run
619612	1.98	ray::ClientAppActor.run
619613	1.97	ray::ClientAppActor.run
619620	1.97	ray::ClientAppActor
619617	1.96	ray::ClientAppActor
Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.

[2024-08-13 23:56:38,242][flwr][ERROR] - Task was killed due to the node running low on memory.
Memory on the node (IP: 115.145.171.128, ID: c687613ff34cdbf5097699e5c896055ef2dc44343c5ab44bd40309d5) where the task (actor ID: 2a8bb25e35bf7bb5a0a6c00401000000, name=ClientAppActor.__init__, pid=619620, memory used=1.97GB) was running was 29.48GB / 31.02GB (0.950487), which exceeds the memory usage threshold of 0.95. Ray killed this worker (ID: 38a4d571c3fa7ff405dd9f5d1555b4753efb95dc84bb70ce01b6ecb4) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 115.145.171.128`. To see the logs of the worker, use `ray logs worker-38a4d571c3fa7ff405dd9f5d1555b4753efb95dc84bb70ce01b6ecb4*out -ip 115.145.171.128. Top 10 memory users:
PID	MEM(GB)	COMMAND
4942	3.48	/snap/pycharm-professional/408/jbr/bin/java -classpath /snap/pycharm-professional/408/lib/platform-l...
616443	2.85	/home/abdenour/miniconda3/envs/fl_with_flower/bin/python /home/abdenour/PycharmProjects/fl_with_flow...
619615	2.06	ray::ClientAppActor
619616	2.04	ray::ClientAppActor
619611	2.04	ray::ClientAppActor.run
619619	2.03	ray::ClientAppActor.run
619612	1.98	ray::ClientAppActor.run
619613	1.97	ray::ClientAppActor.run
619620	1.97	ray::ClientAppActor
619617	1.96	ray::ClientAppActor
Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.
[2024-08-13 23:56:38,243][flwr][ERROR] - Traceback (most recent call last):
  File "/home/abdenour/miniconda3/envs/fl_with_flower/lib/python3.11/site-packages/flwr/simulation/ray_transport/ray_client_proxy.py", line 94, in _submit_job
    out_mssg, updated_context = self.actor_pool.get_client_result(
                                ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/abdenour/miniconda3/envs/fl_with_flower/lib/python3.11/site-packages/flwr/simulation/ray_transport/ray_actor.py", line 398, in get_client_result
    return self._fetch_future_result(cid)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/abdenour/miniconda3/envs/fl_with_flower/lib/python3.11/site-packages/flwr/simulation/ray_transport/ray_actor.py", line 279, in _fetch_future_result
    res_cid, out_mssg, updated_context = ray.get(
                                         ^^^^^^^^
  File "/home/abdenour/miniconda3/envs/fl_with_flower/lib/python3.11/site-packages/ray/_private/auto_init_hook.py", line 21, in auto_init_wrapper
    return fn(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^
  File "/home/abdenour/miniconda3/envs/fl_with_flower/lib/python3.11/site-packages/ray/_private/client_mode_hook.py", line 103, in wrapper
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "/home/abdenour/miniconda3/envs/fl_with_flower/lib/python3.11/site-packages/ray/_private/worker.py", line 2667, in get
    values, debugger_breakpoint = worker.get_objects(object_refs, timeout=timeout)
                                  ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/abdenour/miniconda3/envs/fl_with_flower/lib/python3.11/site-packages/ray/_private/worker.py", line 866, in get_objects
    raise value
ray.exceptions.OutOfMemoryError: Task was killed due to the node running low on memory.
Memory on the node (IP: 115.145.171.128, ID: c687613ff34cdbf5097699e5c896055ef2dc44343c5ab44bd40309d5) where the task (actor ID: 52365bb7997979e5a4c7ddbd01000000, name=ClientAppActor.__init__, pid=619618, memory used=1.84GB) was running was 29.49GB / 31.02GB (0.950707), which exceeds the memory usage threshold of 0.95. Ray killed this worker (ID: 7e48cd46c72821b17be65b05a4e61b5846ba998a57d76db44ca4f8fa) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 115.145.171.128`. To see the logs of the worker, use `ray logs worker-7e48cd46c72821b17be65b05a4e61b5846ba998a57d76db44ca4f8fa*out -ip 115.145.171.128. Top 10 memory users:
PID	MEM(GB)	COMMAND
4942	3.48	/snap/pycharm-professional/408/jbr/bin/java -classpath /snap/pycharm-professional/408/lib/platform-l...
616443	2.14	/home/abdenour/miniconda3/envs/fl_with_flower/bin/python /home/abdenour/PycharmProjects/fl_with_flow...
619615	1.90	ray::ClientAppActor
619616	1.88	ray::ClientAppActor
619611	1.87	ray::ClientAppActor
619617	1.86	ray::ClientAppActor
619620	1.85	ray::ClientAppActor
619618	1.84	ray::ClientAppActor
619613	1.82	ray::ClientAppActor
619619	1.80	ray::ClientAppActor
Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.

[2024-08-13 23:56:38,243][flwr][ERROR] - Task was killed due to the node running low on memory.
Memory on the node (IP: 115.145.171.128, ID: c687613ff34cdbf5097699e5c896055ef2dc44343c5ab44bd40309d5) where the task (actor ID: 52365bb7997979e5a4c7ddbd01000000, name=ClientAppActor.__init__, pid=619618, memory used=1.84GB) was running was 29.49GB / 31.02GB (0.950707), which exceeds the memory usage threshold of 0.95. Ray killed this worker (ID: 7e48cd46c72821b17be65b05a4e61b5846ba998a57d76db44ca4f8fa) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 115.145.171.128`. To see the logs of the worker, use `ray logs worker-7e48cd46c72821b17be65b05a4e61b5846ba998a57d76db44ca4f8fa*out -ip 115.145.171.128. Top 10 memory users:
PID	MEM(GB)	COMMAND
4942	3.48	/snap/pycharm-professional/408/jbr/bin/java -classpath /snap/pycharm-professional/408/lib/platform-l...
616443	2.14	/home/abdenour/miniconda3/envs/fl_with_flower/bin/python /home/abdenour/PycharmProjects/fl_with_flow...
619615	1.90	ray::ClientAppActor
619616	1.88	ray::ClientAppActor
619611	1.87	ray::ClientAppActor
619617	1.86	ray::ClientAppActor
619620	1.85	ray::ClientAppActor
619618	1.84	ray::ClientAppActor
619613	1.82	ray::ClientAppActor
619619	1.80	ray::ClientAppActor
Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.
[2024-08-13 23:56:38,243][flwr][ERROR] - Traceback (most recent call last):
  File "/home/abdenour/miniconda3/envs/fl_with_flower/lib/python3.11/site-packages/flwr/simulation/ray_transport/ray_client_proxy.py", line 94, in _submit_job
    out_mssg, updated_context = self.actor_pool.get_client_result(
                                ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/abdenour/miniconda3/envs/fl_with_flower/lib/python3.11/site-packages/flwr/simulation/ray_transport/ray_actor.py", line 398, in get_client_result
    return self._fetch_future_result(cid)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/abdenour/miniconda3/envs/fl_with_flower/lib/python3.11/site-packages/flwr/simulation/ray_transport/ray_actor.py", line 279, in _fetch_future_result
    res_cid, out_mssg, updated_context = ray.get(
                                         ^^^^^^^^
  File "/home/abdenour/miniconda3/envs/fl_with_flower/lib/python3.11/site-packages/ray/_private/auto_init_hook.py", line 21, in auto_init_wrapper
    return fn(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^
  File "/home/abdenour/miniconda3/envs/fl_with_flower/lib/python3.11/site-packages/ray/_private/client_mode_hook.py", line 103, in wrapper
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "/home/abdenour/miniconda3/envs/fl_with_flower/lib/python3.11/site-packages/ray/_private/worker.py", line 2667, in get
    values, debugger_breakpoint = worker.get_objects(object_refs, timeout=timeout)
                                  ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/abdenour/miniconda3/envs/fl_with_flower/lib/python3.11/site-packages/ray/_private/worker.py", line 866, in get_objects
    raise value
ray.exceptions.OutOfMemoryError: Task was killed due to the node running low on memory.
Memory on the node (IP: 115.145.171.128, ID: c687613ff34cdbf5097699e5c896055ef2dc44343c5ab44bd40309d5) where the task (actor ID: 85928d3f1a247800081c0e1101000000, name=ClientAppActor.__init__, pid=619614, memory used=1.73GB) was running was 29.50GB / 31.02GB (0.951049), which exceeds the memory usage threshold of 0.95. Ray killed this worker (ID: c7f644ac3cca20dffbfa1723daac4f87f04c7de03da25e7cad5fe33c) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 115.145.171.128`. To see the logs of the worker, use `ray logs worker-c7f644ac3cca20dffbfa1723daac4f87f04c7de03da25e7cad5fe33c*out -ip 115.145.171.128. Top 10 memory users:
PID	MEM(GB)	COMMAND
4942	3.49	/snap/pycharm-professional/408/jbr/bin/java -classpath /snap/pycharm-professional/408/lib/platform-l...
619616	1.80	ray::ClientAppActor.run
619615	1.78	ray::ClientAppActor
619617	1.77	ray::ClientAppActor
619611	1.73	ray::ClientAppActor
619618	1.73	ray::ClientAppActor
619614	1.73	ray::ClientAppActor
619612	1.72	ray::ClientAppActor
619619	1.70	ray::ClientAppActor
619620	1.68	ray::ClientAppActor.run
Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.

[2024-08-13 23:56:38,244][flwr][ERROR] - Task was killed due to the node running low on memory.
Memory on the node (IP: 115.145.171.128, ID: c687613ff34cdbf5097699e5c896055ef2dc44343c5ab44bd40309d5) where the task (actor ID: 85928d3f1a247800081c0e1101000000, name=ClientAppActor.__init__, pid=619614, memory used=1.73GB) was running was 29.50GB / 31.02GB (0.951049), which exceeds the memory usage threshold of 0.95. Ray killed this worker (ID: c7f644ac3cca20dffbfa1723daac4f87f04c7de03da25e7cad5fe33c) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 115.145.171.128`. To see the logs of the worker, use `ray logs worker-c7f644ac3cca20dffbfa1723daac4f87f04c7de03da25e7cad5fe33c*out -ip 115.145.171.128. Top 10 memory users:
PID	MEM(GB)	COMMAND
4942	3.49	/snap/pycharm-professional/408/jbr/bin/java -classpath /snap/pycharm-professional/408/lib/platform-l...
619616	1.80	ray::ClientAppActor.run
619615	1.78	ray::ClientAppActor
619617	1.77	ray::ClientAppActor
619611	1.73	ray::ClientAppActor
619618	1.73	ray::ClientAppActor
619614	1.73	ray::ClientAppActor
619612	1.72	ray::ClientAppActor
619619	1.70	ray::ClientAppActor
619620	1.68	ray::ClientAppActor.run
Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.
[2024-08-13 23:56:39,809][flwr][INFO] - aggregate_fit: received 7 results and 3 failures
[2024-08-13 23:56:42,664][flwr][INFO] - fit progress: (264, 17.944072008132935, {'accuracy': 0.75}, 1523.9239870320016)
[2024-08-13 23:56:42,664][flwr][INFO] - configure_evaluate: strategy sampled 2 clients (out of 20)
[2024-08-13 23:56:42,872][flwr][INFO] - aggregate_evaluate: received 2 results and 0 failures
[2024-08-13 23:56:42,872][flwr][INFO] - 
[2024-08-13 23:56:42,872][flwr][INFO] - [ROUND 265]
[2024-08-13 23:56:42,872][flwr][INFO] - configure_fit: strategy sampled 10 clients (out of 20)
[2024-08-13 23:56:43,011][flwr][ERROR] - Traceback (most recent call last):
  File "/home/abdenour/miniconda3/envs/fl_with_flower/lib/python3.11/site-packages/flwr/simulation/ray_transport/ray_client_proxy.py", line 94, in _submit_job
    out_mssg, updated_context = self.actor_pool.get_client_result(
                                ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/abdenour/miniconda3/envs/fl_with_flower/lib/python3.11/site-packages/flwr/simulation/ray_transport/ray_actor.py", line 398, in get_client_result
    return self._fetch_future_result(cid)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/abdenour/miniconda3/envs/fl_with_flower/lib/python3.11/site-packages/flwr/simulation/ray_transport/ray_actor.py", line 279, in _fetch_future_result
    res_cid, out_mssg, updated_context = ray.get(
                                         ^^^^^^^^
  File "/home/abdenour/miniconda3/envs/fl_with_flower/lib/python3.11/site-packages/ray/_private/auto_init_hook.py", line 21, in auto_init_wrapper
    return fn(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^
  File "/home/abdenour/miniconda3/envs/fl_with_flower/lib/python3.11/site-packages/ray/_private/client_mode_hook.py", line 103, in wrapper
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "/home/abdenour/miniconda3/envs/fl_with_flower/lib/python3.11/site-packages/ray/_private/worker.py", line 2667, in get
    values, debugger_breakpoint = worker.get_objects(object_refs, timeout=timeout)
                                  ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/abdenour/miniconda3/envs/fl_with_flower/lib/python3.11/site-packages/ray/_private/worker.py", line 866, in get_objects
    raise value
ray.exceptions.OutOfMemoryError: Task was killed due to the node running low on memory.
Memory on the node (IP: 115.145.171.128, ID: c687613ff34cdbf5097699e5c896055ef2dc44343c5ab44bd40309d5) where the task (actor ID: 85928d3f1a247800081c0e1101000000, name=ClientAppActor.__init__, pid=619614, memory used=1.73GB) was running was 29.50GB / 31.02GB (0.951049), which exceeds the memory usage threshold of 0.95. Ray killed this worker (ID: c7f644ac3cca20dffbfa1723daac4f87f04c7de03da25e7cad5fe33c) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 115.145.171.128`. To see the logs of the worker, use `ray logs worker-c7f644ac3cca20dffbfa1723daac4f87f04c7de03da25e7cad5fe33c*out -ip 115.145.171.128. Top 10 memory users:
PID	MEM(GB)	COMMAND
4942	3.49	/snap/pycharm-professional/408/jbr/bin/java -classpath /snap/pycharm-professional/408/lib/platform-l...
619616	1.80	ray::ClientAppActor.run
619615	1.78	ray::ClientAppActor
619617	1.77	ray::ClientAppActor
619611	1.73	ray::ClientAppActor
619618	1.73	ray::ClientAppActor
619614	1.73	ray::ClientAppActor
619612	1.72	ray::ClientAppActor
619619	1.70	ray::ClientAppActor
619620	1.68	ray::ClientAppActor.run
Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.

[2024-08-13 23:56:43,024][flwr][ERROR] - Traceback (most recent call last):
  File "/home/abdenour/miniconda3/envs/fl_with_flower/lib/python3.11/site-packages/flwr/simulation/ray_transport/ray_client_proxy.py", line 94, in _submit_job
    out_mssg, updated_context = self.actor_pool.get_client_result(
                                ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/abdenour/miniconda3/envs/fl_with_flower/lib/python3.11/site-packages/flwr/simulation/ray_transport/ray_actor.py", line 398, in get_client_result
    return self._fetch_future_result(cid)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/abdenour/miniconda3/envs/fl_with_flower/lib/python3.11/site-packages/flwr/simulation/ray_transport/ray_actor.py", line 279, in _fetch_future_result
    res_cid, out_mssg, updated_context = ray.get(
                                         ^^^^^^^^
  File "/home/abdenour/miniconda3/envs/fl_with_flower/lib/python3.11/site-packages/ray/_private/auto_init_hook.py", line 21, in auto_init_wrapper
    return fn(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^
  File "/home/abdenour/miniconda3/envs/fl_with_flower/lib/python3.11/site-packages/ray/_private/client_mode_hook.py", line 103, in wrapper
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "/home/abdenour/miniconda3/envs/fl_with_flower/lib/python3.11/site-packages/ray/_private/worker.py", line 2667, in get
    values, debugger_breakpoint = worker.get_objects(object_refs, timeout=timeout)
                                  ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/abdenour/miniconda3/envs/fl_with_flower/lib/python3.11/site-packages/ray/_private/worker.py", line 866, in get_objects
    raise value
ray.exceptions.OutOfMemoryError: Task was killed due to the node running low on memory.
Memory on the node (IP: 115.145.171.128, ID: c687613ff34cdbf5097699e5c896055ef2dc44343c5ab44bd40309d5) where the task (actor ID: 52365bb7997979e5a4c7ddbd01000000, name=ClientAppActor.__init__, pid=619618, memory used=1.84GB) was running was 29.49GB / 31.02GB (0.950707), which exceeds the memory usage threshold of 0.95. Ray killed this worker (ID: 7e48cd46c72821b17be65b05a4e61b5846ba998a57d76db44ca4f8fa) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 115.145.171.128`. To see the logs of the worker, use `ray logs worker-7e48cd46c72821b17be65b05a4e61b5846ba998a57d76db44ca4f8fa*out -ip 115.145.171.128. Top 10 memory users:
PID	MEM(GB)	COMMAND
4942	3.48	/snap/pycharm-professional/408/jbr/bin/java -classpath /snap/pycharm-professional/408/lib/platform-l...
616443	2.14	/home/abdenour/miniconda3/envs/fl_with_flower/bin/python /home/abdenour/PycharmProjects/fl_with_flow...
619615	1.90	ray::ClientAppActor
619616	1.88	ray::ClientAppActor
619611	1.87	ray::ClientAppActor
619617	1.86	ray::ClientAppActor
619620	1.85	ray::ClientAppActor
619618	1.84	ray::ClientAppActor
619613	1.82	ray::ClientAppActor
619619	1.80	ray::ClientAppActor
Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.

[2024-08-13 23:56:43,024][flwr][ERROR] - Task was killed due to the node running low on memory.
Memory on the node (IP: 115.145.171.128, ID: c687613ff34cdbf5097699e5c896055ef2dc44343c5ab44bd40309d5) where the task (actor ID: 85928d3f1a247800081c0e1101000000, name=ClientAppActor.__init__, pid=619614, memory used=1.73GB) was running was 29.50GB / 31.02GB (0.951049), which exceeds the memory usage threshold of 0.95. Ray killed this worker (ID: c7f644ac3cca20dffbfa1723daac4f87f04c7de03da25e7cad5fe33c) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 115.145.171.128`. To see the logs of the worker, use `ray logs worker-c7f644ac3cca20dffbfa1723daac4f87f04c7de03da25e7cad5fe33c*out -ip 115.145.171.128. Top 10 memory users:
PID	MEM(GB)	COMMAND
4942	3.49	/snap/pycharm-professional/408/jbr/bin/java -classpath /snap/pycharm-professional/408/lib/platform-l...
619616	1.80	ray::ClientAppActor.run
619615	1.78	ray::ClientAppActor
619617	1.77	ray::ClientAppActor
619611	1.73	ray::ClientAppActor
619618	1.73	ray::ClientAppActor
619614	1.73	ray::ClientAppActor
619612	1.72	ray::ClientAppActor
619619	1.70	ray::ClientAppActor
619620	1.68	ray::ClientAppActor.run
Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.
[2024-08-13 23:56:43,024][flwr][ERROR] - Task was killed due to the node running low on memory.
Memory on the node (IP: 115.145.171.128, ID: c687613ff34cdbf5097699e5c896055ef2dc44343c5ab44bd40309d5) where the task (actor ID: 52365bb7997979e5a4c7ddbd01000000, name=ClientAppActor.__init__, pid=619618, memory used=1.84GB) was running was 29.49GB / 31.02GB (0.950707), which exceeds the memory usage threshold of 0.95. Ray killed this worker (ID: 7e48cd46c72821b17be65b05a4e61b5846ba998a57d76db44ca4f8fa) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 115.145.171.128`. To see the logs of the worker, use `ray logs worker-7e48cd46c72821b17be65b05a4e61b5846ba998a57d76db44ca4f8fa*out -ip 115.145.171.128. Top 10 memory users:
PID	MEM(GB)	COMMAND
4942	3.48	/snap/pycharm-professional/408/jbr/bin/java -classpath /snap/pycharm-professional/408/lib/platform-l...
616443	2.14	/home/abdenour/miniconda3/envs/fl_with_flower/bin/python /home/abdenour/PycharmProjects/fl_with_flow...
619615	1.90	ray::ClientAppActor
619616	1.88	ray::ClientAppActor
619611	1.87	ray::ClientAppActor
619617	1.86	ray::ClientAppActor
619620	1.85	ray::ClientAppActor
619618	1.84	ray::ClientAppActor
619613	1.82	ray::ClientAppActor
619619	1.80	ray::ClientAppActor
Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.
[2024-08-13 23:56:43,030][flwr][ERROR] - Traceback (most recent call last):
  File "/home/abdenour/miniconda3/envs/fl_with_flower/lib/python3.11/site-packages/flwr/simulation/ray_transport/ray_client_proxy.py", line 94, in _submit_job
    out_mssg, updated_context = self.actor_pool.get_client_result(
                                ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/abdenour/miniconda3/envs/fl_with_flower/lib/python3.11/site-packages/flwr/simulation/ray_transport/ray_actor.py", line 398, in get_client_result
    return self._fetch_future_result(cid)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/abdenour/miniconda3/envs/fl_with_flower/lib/python3.11/site-packages/flwr/simulation/ray_transport/ray_actor.py", line 279, in _fetch_future_result
    res_cid, out_mssg, updated_context = ray.get(
                                         ^^^^^^^^
  File "/home/abdenour/miniconda3/envs/fl_with_flower/lib/python3.11/site-packages/ray/_private/auto_init_hook.py", line 21, in auto_init_wrapper
    return fn(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^
  File "/home/abdenour/miniconda3/envs/fl_with_flower/lib/python3.11/site-packages/ray/_private/client_mode_hook.py", line 103, in wrapper
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "/home/abdenour/miniconda3/envs/fl_with_flower/lib/python3.11/site-packages/ray/_private/worker.py", line 2667, in get
    values, debugger_breakpoint = worker.get_objects(object_refs, timeout=timeout)
                                  ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/abdenour/miniconda3/envs/fl_with_flower/lib/python3.11/site-packages/ray/_private/worker.py", line 866, in get_objects
    raise value
ray.exceptions.OutOfMemoryError: Task was killed due to the node running low on memory.
Memory on the node (IP: 115.145.171.128, ID: c687613ff34cdbf5097699e5c896055ef2dc44343c5ab44bd40309d5) where the task (actor ID: 52365bb7997979e5a4c7ddbd01000000, name=ClientAppActor.__init__, pid=619618, memory used=1.84GB) was running was 29.49GB / 31.02GB (0.950707), which exceeds the memory usage threshold of 0.95. Ray killed this worker (ID: 7e48cd46c72821b17be65b05a4e61b5846ba998a57d76db44ca4f8fa) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 115.145.171.128`. To see the logs of the worker, use `ray logs worker-7e48cd46c72821b17be65b05a4e61b5846ba998a57d76db44ca4f8fa*out -ip 115.145.171.128. Top 10 memory users:
PID	MEM(GB)	COMMAND
4942	3.48	/snap/pycharm-professional/408/jbr/bin/java -classpath /snap/pycharm-professional/408/lib/platform-l...
616443	2.14	/home/abdenour/miniconda3/envs/fl_with_flower/bin/python /home/abdenour/PycharmProjects/fl_with_flow...
619615	1.90	ray::ClientAppActor
619616	1.88	ray::ClientAppActor
619611	1.87	ray::ClientAppActor
619617	1.86	ray::ClientAppActor
619620	1.85	ray::ClientAppActor
619618	1.84	ray::ClientAppActor
619613	1.82	ray::ClientAppActor
619619	1.80	ray::ClientAppActor
Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.

[2024-08-13 23:56:43,030][flwr][ERROR] - Task was killed due to the node running low on memory.
Memory on the node (IP: 115.145.171.128, ID: c687613ff34cdbf5097699e5c896055ef2dc44343c5ab44bd40309d5) where the task (actor ID: 52365bb7997979e5a4c7ddbd01000000, name=ClientAppActor.__init__, pid=619618, memory used=1.84GB) was running was 29.49GB / 31.02GB (0.950707), which exceeds the memory usage threshold of 0.95. Ray killed this worker (ID: 7e48cd46c72821b17be65b05a4e61b5846ba998a57d76db44ca4f8fa) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 115.145.171.128`. To see the logs of the worker, use `ray logs worker-7e48cd46c72821b17be65b05a4e61b5846ba998a57d76db44ca4f8fa*out -ip 115.145.171.128. Top 10 memory users:
PID	MEM(GB)	COMMAND
4942	3.48	/snap/pycharm-professional/408/jbr/bin/java -classpath /snap/pycharm-professional/408/lib/platform-l...
616443	2.14	/home/abdenour/miniconda3/envs/fl_with_flower/bin/python /home/abdenour/PycharmProjects/fl_with_flow...
619615	1.90	ray::ClientAppActor
619616	1.88	ray::ClientAppActor
619611	1.87	ray::ClientAppActor
619617	1.86	ray::ClientAppActor
619620	1.85	ray::ClientAppActor
619618	1.84	ray::ClientAppActor
619613	1.82	ray::ClientAppActor
619619	1.80	ray::ClientAppActor
Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.
[2024-08-13 23:56:45,052][flwr][INFO] - aggregate_fit: received 7 results and 3 failures
[2024-08-13 23:56:48,219][flwr][INFO] - fit progress: (265, 16.40019792318344, {'accuracy': 0.7516025641025641}, 1529.4790176880197)
[2024-08-13 23:56:48,219][flwr][INFO] - configure_evaluate: strategy sampled 2 clients (out of 20)
[2024-08-13 23:56:48,436][flwr][INFO] - aggregate_evaluate: received 2 results and 0 failures
[2024-08-13 23:56:48,437][flwr][INFO] - 
[2024-08-13 23:56:48,437][flwr][INFO] - [ROUND 266]
[2024-08-13 23:56:48,437][flwr][INFO] - configure_fit: strategy sampled 10 clients (out of 20)
[2024-08-13 23:56:48,606][flwr][ERROR] - Traceback (most recent call last):
  File "/home/abdenour/miniconda3/envs/fl_with_flower/lib/python3.11/site-packages/flwr/simulation/ray_transport/ray_client_proxy.py", line 94, in _submit_job
    out_mssg, updated_context = self.actor_pool.get_client_result(
                                ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/abdenour/miniconda3/envs/fl_with_flower/lib/python3.11/site-packages/flwr/simulation/ray_transport/ray_actor.py", line 398, in get_client_result
    return self._fetch_future_result(cid)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/abdenour/miniconda3/envs/fl_with_flower/lib/python3.11/site-packages/flwr/simulation/ray_transport/ray_actor.py", line 279, in _fetch_future_result
    res_cid, out_mssg, updated_context = ray.get(
                                         ^^^^^^^^
  File "/home/abdenour/miniconda3/envs/fl_with_flower/lib/python3.11/site-packages/ray/_private/auto_init_hook.py", line 21, in auto_init_wrapper
    return fn(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^
  File "/home/abdenour/miniconda3/envs/fl_with_flower/lib/python3.11/site-packages/ray/_private/client_mode_hook.py", line 103, in wrapper
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "/home/abdenour/miniconda3/envs/fl_with_flower/lib/python3.11/site-packages/ray/_private/worker.py", line 2667, in get
    values, debugger_breakpoint = worker.get_objects(object_refs, timeout=timeout)
                                  ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/abdenour/miniconda3/envs/fl_with_flower/lib/python3.11/site-packages/ray/_private/worker.py", line 866, in get_objects
    raise value
ray.exceptions.OutOfMemoryError: Task was killed due to the node running low on memory.
Memory on the node (IP: 115.145.171.128, ID: c687613ff34cdbf5097699e5c896055ef2dc44343c5ab44bd40309d5) where the task (actor ID: 52365bb7997979e5a4c7ddbd01000000, name=ClientAppActor.__init__, pid=619618, memory used=1.84GB) was running was 29.49GB / 31.02GB (0.950707), which exceeds the memory usage threshold of 0.95. Ray killed this worker (ID: 7e48cd46c72821b17be65b05a4e61b5846ba998a57d76db44ca4f8fa) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 115.145.171.128`. To see the logs of the worker, use `ray logs worker-7e48cd46c72821b17be65b05a4e61b5846ba998a57d76db44ca4f8fa*out -ip 115.145.171.128. Top 10 memory users:
PID	MEM(GB)	COMMAND
4942	3.48	/snap/pycharm-professional/408/jbr/bin/java -classpath /snap/pycharm-professional/408/lib/platform-l...
616443	2.14	/home/abdenour/miniconda3/envs/fl_with_flower/bin/python /home/abdenour/PycharmProjects/fl_with_flow...
619615	1.90	ray::ClientAppActor
619616	1.88	ray::ClientAppActor
619611	1.87	ray::ClientAppActor
619617	1.86	ray::ClientAppActor
619620	1.85	ray::ClientAppActor
619618	1.84	ray::ClientAppActor
619613	1.82	ray::ClientAppActor
619619	1.80	ray::ClientAppActor
Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.

[2024-08-13 23:56:48,606][flwr][ERROR] - Task was killed due to the node running low on memory.
Memory on the node (IP: 115.145.171.128, ID: c687613ff34cdbf5097699e5c896055ef2dc44343c5ab44bd40309d5) where the task (actor ID: 52365bb7997979e5a4c7ddbd01000000, name=ClientAppActor.__init__, pid=619618, memory used=1.84GB) was running was 29.49GB / 31.02GB (0.950707), which exceeds the memory usage threshold of 0.95. Ray killed this worker (ID: 7e48cd46c72821b17be65b05a4e61b5846ba998a57d76db44ca4f8fa) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 115.145.171.128`. To see the logs of the worker, use `ray logs worker-7e48cd46c72821b17be65b05a4e61b5846ba998a57d76db44ca4f8fa*out -ip 115.145.171.128. Top 10 memory users:
PID	MEM(GB)	COMMAND
4942	3.48	/snap/pycharm-professional/408/jbr/bin/java -classpath /snap/pycharm-professional/408/lib/platform-l...
616443	2.14	/home/abdenour/miniconda3/envs/fl_with_flower/bin/python /home/abdenour/PycharmProjects/fl_with_flow...
619615	1.90	ray::ClientAppActor
619616	1.88	ray::ClientAppActor
619611	1.87	ray::ClientAppActor
619617	1.86	ray::ClientAppActor
619620	1.85	ray::ClientAppActor
619618	1.84	ray::ClientAppActor
619613	1.82	ray::ClientAppActor
619619	1.80	ray::ClientAppActor
Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.
[2024-08-13 23:56:48,607][flwr][ERROR] - Traceback (most recent call last):
  File "/home/abdenour/miniconda3/envs/fl_with_flower/lib/python3.11/site-packages/flwr/simulation/ray_transport/ray_client_proxy.py", line 94, in _submit_job
    out_mssg, updated_context = self.actor_pool.get_client_result(
                                ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/abdenour/miniconda3/envs/fl_with_flower/lib/python3.11/site-packages/flwr/simulation/ray_transport/ray_actor.py", line 398, in get_client_result
    return self._fetch_future_result(cid)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/abdenour/miniconda3/envs/fl_with_flower/lib/python3.11/site-packages/flwr/simulation/ray_transport/ray_actor.py", line 279, in _fetch_future_result
    res_cid, out_mssg, updated_context = ray.get(
                                         ^^^^^^^^
  File "/home/abdenour/miniconda3/envs/fl_with_flower/lib/python3.11/site-packages/ray/_private/auto_init_hook.py", line 21, in auto_init_wrapper
    return fn(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^
  File "/home/abdenour/miniconda3/envs/fl_with_flower/lib/python3.11/site-packages/ray/_private/client_mode_hook.py", line 103, in wrapper
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "/home/abdenour/miniconda3/envs/fl_with_flower/lib/python3.11/site-packages/ray/_private/worker.py", line 2667, in get
    values, debugger_breakpoint = worker.get_objects(object_refs, timeout=timeout)
                                  ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/abdenour/miniconda3/envs/fl_with_flower/lib/python3.11/site-packages/ray/_private/worker.py", line 866, in get_objects
    raise value
ray.exceptions.OutOfMemoryError: Task was killed due to the node running low on memory.
Memory on the node (IP: 115.145.171.128, ID: c687613ff34cdbf5097699e5c896055ef2dc44343c5ab44bd40309d5) where the task (actor ID: 85928d3f1a247800081c0e1101000000, name=ClientAppActor.__init__, pid=619614, memory used=1.73GB) was running was 29.50GB / 31.02GB (0.951049), which exceeds the memory usage threshold of 0.95. Ray killed this worker (ID: c7f644ac3cca20dffbfa1723daac4f87f04c7de03da25e7cad5fe33c) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 115.145.171.128`. To see the logs of the worker, use `ray logs worker-c7f644ac3cca20dffbfa1723daac4f87f04c7de03da25e7cad5fe33c*out -ip 115.145.171.128. Top 10 memory users:
PID	MEM(GB)	COMMAND
4942	3.49	/snap/pycharm-professional/408/jbr/bin/java -classpath /snap/pycharm-professional/408/lib/platform-l...
619616	1.80	ray::ClientAppActor.run
619615	1.78	ray::ClientAppActor
619617	1.77	ray::ClientAppActor
619611	1.73	ray::ClientAppActor
619618	1.73	ray::ClientAppActor
619614	1.73	ray::ClientAppActor
619612	1.72	ray::ClientAppActor
619619	1.70	ray::ClientAppActor
619620	1.68	ray::ClientAppActor.run
Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.

[2024-08-13 23:56:48,607][flwr][ERROR] - Task was killed due to the node running low on memory.
Memory on the node (IP: 115.145.171.128, ID: c687613ff34cdbf5097699e5c896055ef2dc44343c5ab44bd40309d5) where the task (actor ID: 85928d3f1a247800081c0e1101000000, name=ClientAppActor.__init__, pid=619614, memory used=1.73GB) was running was 29.50GB / 31.02GB (0.951049), which exceeds the memory usage threshold of 0.95. Ray killed this worker (ID: c7f644ac3cca20dffbfa1723daac4f87f04c7de03da25e7cad5fe33c) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 115.145.171.128`. To see the logs of the worker, use `ray logs worker-c7f644ac3cca20dffbfa1723daac4f87f04c7de03da25e7cad5fe33c*out -ip 115.145.171.128. Top 10 memory users:
PID	MEM(GB)	COMMAND
4942	3.49	/snap/pycharm-professional/408/jbr/bin/java -classpath /snap/pycharm-professional/408/lib/platform-l...
619616	1.80	ray::ClientAppActor.run
619615	1.78	ray::ClientAppActor
619617	1.77	ray::ClientAppActor
619611	1.73	ray::ClientAppActor
619618	1.73	ray::ClientAppActor
619614	1.73	ray::ClientAppActor
619612	1.72	ray::ClientAppActor
619619	1.70	ray::ClientAppActor
619620	1.68	ray::ClientAppActor.run
Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.
[2024-08-13 23:56:48,608][flwr][ERROR] - Traceback (most recent call last):
  File "/home/abdenour/miniconda3/envs/fl_with_flower/lib/python3.11/site-packages/flwr/simulation/ray_transport/ray_client_proxy.py", line 94, in _submit_job
    out_mssg, updated_context = self.actor_pool.get_client_result(
                                ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/abdenour/miniconda3/envs/fl_with_flower/lib/python3.11/site-packages/flwr/simulation/ray_transport/ray_actor.py", line 398, in get_client_result
    return self._fetch_future_result(cid)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/abdenour/miniconda3/envs/fl_with_flower/lib/python3.11/site-packages/flwr/simulation/ray_transport/ray_actor.py", line 279, in _fetch_future_result
    res_cid, out_mssg, updated_context = ray.get(
                                         ^^^^^^^^
  File "/home/abdenour/miniconda3/envs/fl_with_flower/lib/python3.11/site-packages/ray/_private/auto_init_hook.py", line 21, in auto_init_wrapper
    return fn(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^
  File "/home/abdenour/miniconda3/envs/fl_with_flower/lib/python3.11/site-packages/ray/_private/client_mode_hook.py", line 103, in wrapper
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "/home/abdenour/miniconda3/envs/fl_with_flower/lib/python3.11/site-packages/ray/_private/worker.py", line 2667, in get
    values, debugger_breakpoint = worker.get_objects(object_refs, timeout=timeout)
                                  ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/abdenour/miniconda3/envs/fl_with_flower/lib/python3.11/site-packages/ray/_private/worker.py", line 866, in get_objects
    raise value
ray.exceptions.OutOfMemoryError: Task was killed due to the node running low on memory.
Memory on the node (IP: 115.145.171.128, ID: c687613ff34cdbf5097699e5c896055ef2dc44343c5ab44bd40309d5) where the task (actor ID: 2a8bb25e35bf7bb5a0a6c00401000000, name=ClientAppActor.__init__, pid=619620, memory used=1.97GB) was running was 29.48GB / 31.02GB (0.950487), which exceeds the memory usage threshold of 0.95. Ray killed this worker (ID: 38a4d571c3fa7ff405dd9f5d1555b4753efb95dc84bb70ce01b6ecb4) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 115.145.171.128`. To see the logs of the worker, use `ray logs worker-38a4d571c3fa7ff405dd9f5d1555b4753efb95dc84bb70ce01b6ecb4*out -ip 115.145.171.128. Top 10 memory users:
PID	MEM(GB)	COMMAND
4942	3.48	/snap/pycharm-professional/408/jbr/bin/java -classpath /snap/pycharm-professional/408/lib/platform-l...
616443	2.85	/home/abdenour/miniconda3/envs/fl_with_flower/bin/python /home/abdenour/PycharmProjects/fl_with_flow...
619615	2.06	ray::ClientAppActor
619616	2.04	ray::ClientAppActor
619611	2.04	ray::ClientAppActor.run
619619	2.03	ray::ClientAppActor.run
619612	1.98	ray::ClientAppActor.run
619613	1.97	ray::ClientAppActor.run
619620	1.97	ray::ClientAppActor
619617	1.96	ray::ClientAppActor
Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.

[2024-08-13 23:56:48,608][flwr][ERROR] - Task was killed due to the node running low on memory.
Memory on the node (IP: 115.145.171.128, ID: c687613ff34cdbf5097699e5c896055ef2dc44343c5ab44bd40309d5) where the task (actor ID: 2a8bb25e35bf7bb5a0a6c00401000000, name=ClientAppActor.__init__, pid=619620, memory used=1.97GB) was running was 29.48GB / 31.02GB (0.950487), which exceeds the memory usage threshold of 0.95. Ray killed this worker (ID: 38a4d571c3fa7ff405dd9f5d1555b4753efb95dc84bb70ce01b6ecb4) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 115.145.171.128`. To see the logs of the worker, use `ray logs worker-38a4d571c3fa7ff405dd9f5d1555b4753efb95dc84bb70ce01b6ecb4*out -ip 115.145.171.128. Top 10 memory users:
PID	MEM(GB)	COMMAND
4942	3.48	/snap/pycharm-professional/408/jbr/bin/java -classpath /snap/pycharm-professional/408/lib/platform-l...
616443	2.85	/home/abdenour/miniconda3/envs/fl_with_flower/bin/python /home/abdenour/PycharmProjects/fl_with_flow...
619615	2.06	ray::ClientAppActor
619616	2.04	ray::ClientAppActor
619611	2.04	ray::ClientAppActor.run
619619	2.03	ray::ClientAppActor.run
619612	1.98	ray::ClientAppActor.run
619613	1.97	ray::ClientAppActor.run
619620	1.97	ray::ClientAppActor
619617	1.96	ray::ClientAppActor
Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.
[2024-08-13 23:56:50,137][flwr][INFO] - aggregate_fit: received 7 results and 3 failures
[2024-08-13 23:56:53,361][flwr][INFO] - fit progress: (266, 20.218060195446014, {'accuracy': 0.7323717948717948}, 1534.6213337189984)
[2024-08-13 23:56:53,361][flwr][INFO] - configure_evaluate: strategy sampled 2 clients (out of 20)
[2024-08-13 23:56:53,629][flwr][INFO] - aggregate_evaluate: received 2 results and 0 failures
[2024-08-13 23:56:53,629][flwr][INFO] - 
[2024-08-13 23:56:53,629][flwr][INFO] - [ROUND 267]
[2024-08-13 23:56:53,629][flwr][INFO] - configure_fit: strategy sampled 10 clients (out of 20)
[2024-08-13 23:56:53,790][flwr][ERROR] - Traceback (most recent call last):
  File "/home/abdenour/miniconda3/envs/fl_with_flower/lib/python3.11/site-packages/flwr/simulation/ray_transport/ray_client_proxy.py", line 94, in _submit_job
    out_mssg, updated_context = self.actor_pool.get_client_result(
                                ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/abdenour/miniconda3/envs/fl_with_flower/lib/python3.11/site-packages/flwr/simulation/ray_transport/ray_actor.py", line 398, in get_client_result
    return self._fetch_future_result(cid)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/abdenour/miniconda3/envs/fl_with_flower/lib/python3.11/site-packages/flwr/simulation/ray_transport/ray_actor.py", line 279, in _fetch_future_result
    res_cid, out_mssg, updated_context = ray.get(
                                         ^^^^^^^^
  File "/home/abdenour/miniconda3/envs/fl_with_flower/lib/python3.11/site-packages/ray/_private/auto_init_hook.py", line 21, in auto_init_wrapper
    return fn(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^
  File "/home/abdenour/miniconda3/envs/fl_with_flower/lib/python3.11/site-packages/ray/_private/client_mode_hook.py", line 103, in wrapper
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "/home/abdenour/miniconda3/envs/fl_with_flower/lib/python3.11/site-packages/ray/_private/worker.py", line 2667, in get
    values, debugger_breakpoint = worker.get_objects(object_refs, timeout=timeout)
                                  ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/abdenour/miniconda3/envs/fl_with_flower/lib/python3.11/site-packages/ray/_private/worker.py", line 866, in get_objects
    raise value
ray.exceptions.OutOfMemoryError: Task was killed due to the node running low on memory.
Memory on the node (IP: 115.145.171.128, ID: c687613ff34cdbf5097699e5c896055ef2dc44343c5ab44bd40309d5) where the task (actor ID: 2a8bb25e35bf7bb5a0a6c00401000000, name=ClientAppActor.__init__, pid=619620, memory used=1.97GB) was running was 29.48GB / 31.02GB (0.950487), which exceeds the memory usage threshold of 0.95. Ray killed this worker (ID: 38a4d571c3fa7ff405dd9f5d1555b4753efb95dc84bb70ce01b6ecb4) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 115.145.171.128`. To see the logs of the worker, use `ray logs worker-38a4d571c3fa7ff405dd9f5d1555b4753efb95dc84bb70ce01b6ecb4*out -ip 115.145.171.128. Top 10 memory users:
PID	MEM(GB)	COMMAND
4942	3.48	/snap/pycharm-professional/408/jbr/bin/java -classpath /snap/pycharm-professional/408/lib/platform-l...
616443	2.85	/home/abdenour/miniconda3/envs/fl_with_flower/bin/python /home/abdenour/PycharmProjects/fl_with_flow...
619615	2.06	ray::ClientAppActor
619616	2.04	ray::ClientAppActor
619611	2.04	ray::ClientAppActor.run
619619	2.03	ray::ClientAppActor.run
619612	1.98	ray::ClientAppActor.run
619613	1.97	ray::ClientAppActor.run
619620	1.97	ray::ClientAppActor
619617	1.96	ray::ClientAppActor
Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.

[2024-08-13 23:56:53,791][flwr][ERROR] - Task was killed due to the node running low on memory.
Memory on the node (IP: 115.145.171.128, ID: c687613ff34cdbf5097699e5c896055ef2dc44343c5ab44bd40309d5) where the task (actor ID: 2a8bb25e35bf7bb5a0a6c00401000000, name=ClientAppActor.__init__, pid=619620, memory used=1.97GB) was running was 29.48GB / 31.02GB (0.950487), which exceeds the memory usage threshold of 0.95. Ray killed this worker (ID: 38a4d571c3fa7ff405dd9f5d1555b4753efb95dc84bb70ce01b6ecb4) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 115.145.171.128`. To see the logs of the worker, use `ray logs worker-38a4d571c3fa7ff405dd9f5d1555b4753efb95dc84bb70ce01b6ecb4*out -ip 115.145.171.128. Top 10 memory users:
PID	MEM(GB)	COMMAND
4942	3.48	/snap/pycharm-professional/408/jbr/bin/java -classpath /snap/pycharm-professional/408/lib/platform-l...
616443	2.85	/home/abdenour/miniconda3/envs/fl_with_flower/bin/python /home/abdenour/PycharmProjects/fl_with_flow...
619615	2.06	ray::ClientAppActor
619616	2.04	ray::ClientAppActor
619611	2.04	ray::ClientAppActor.run
619619	2.03	ray::ClientAppActor.run
619612	1.98	ray::ClientAppActor.run
619613	1.97	ray::ClientAppActor.run
619620	1.97	ray::ClientAppActor
619617	1.96	ray::ClientAppActor
Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.
[2024-08-13 23:56:53,792][flwr][ERROR] - Traceback (most recent call last):
  File "/home/abdenour/miniconda3/envs/fl_with_flower/lib/python3.11/site-packages/flwr/simulation/ray_transport/ray_client_proxy.py", line 94, in _submit_job
    out_mssg, updated_context = self.actor_pool.get_client_result(
                                ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/abdenour/miniconda3/envs/fl_with_flower/lib/python3.11/site-packages/flwr/simulation/ray_transport/ray_actor.py", line 398, in get_client_result
    return self._fetch_future_result(cid)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/abdenour/miniconda3/envs/fl_with_flower/lib/python3.11/site-packages/flwr/simulation/ray_transport/ray_actor.py", line 279, in _fetch_future_result
    res_cid, out_mssg, updated_context = ray.get(
                                         ^^^^^^^^
  File "/home/abdenour/miniconda3/envs/fl_with_flower/lib/python3.11/site-packages/ray/_private/auto_init_hook.py", line 21, in auto_init_wrapper
    return fn(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^
  File "/home/abdenour/miniconda3/envs/fl_with_flower/lib/python3.11/site-packages/ray/_private/client_mode_hook.py", line 103, in wrapper
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "/home/abdenour/miniconda3/envs/fl_with_flower/lib/python3.11/site-packages/ray/_private/worker.py", line 2667, in get
    values, debugger_breakpoint = worker.get_objects(object_refs, timeout=timeout)
                                  ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/abdenour/miniconda3/envs/fl_with_flower/lib/python3.11/site-packages/ray/_private/worker.py", line 866, in get_objects
    raise value
ray.exceptions.OutOfMemoryError: Task was killed due to the node running low on memory.
Memory on the node (IP: 115.145.171.128, ID: c687613ff34cdbf5097699e5c896055ef2dc44343c5ab44bd40309d5) where the task (actor ID: 52365bb7997979e5a4c7ddbd01000000, name=ClientAppActor.__init__, pid=619618, memory used=1.84GB) was running was 29.49GB / 31.02GB (0.950707), which exceeds the memory usage threshold of 0.95. Ray killed this worker (ID: 7e48cd46c72821b17be65b05a4e61b5846ba998a57d76db44ca4f8fa) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 115.145.171.128`. To see the logs of the worker, use `ray logs worker-7e48cd46c72821b17be65b05a4e61b5846ba998a57d76db44ca4f8fa*out -ip 115.145.171.128. Top 10 memory users:
PID	MEM(GB)	COMMAND
4942	3.48	/snap/pycharm-professional/408/jbr/bin/java -classpath /snap/pycharm-professional/408/lib/platform-l...
616443	2.14	/home/abdenour/miniconda3/envs/fl_with_flower/bin/python /home/abdenour/PycharmProjects/fl_with_flow...
619615	1.90	ray::ClientAppActor
619616	1.88	ray::ClientAppActor
619611	1.87	ray::ClientAppActor
619617	1.86	ray::ClientAppActor
619620	1.85	ray::ClientAppActor
619618	1.84	ray::ClientAppActor
619613	1.82	ray::ClientAppActor
619619	1.80	ray::ClientAppActor
Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.

[2024-08-13 23:56:53,792][flwr][ERROR] - Task was killed due to the node running low on memory.
Memory on the node (IP: 115.145.171.128, ID: c687613ff34cdbf5097699e5c896055ef2dc44343c5ab44bd40309d5) where the task (actor ID: 52365bb7997979e5a4c7ddbd01000000, name=ClientAppActor.__init__, pid=619618, memory used=1.84GB) was running was 29.49GB / 31.02GB (0.950707), which exceeds the memory usage threshold of 0.95. Ray killed this worker (ID: 7e48cd46c72821b17be65b05a4e61b5846ba998a57d76db44ca4f8fa) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 115.145.171.128`. To see the logs of the worker, use `ray logs worker-7e48cd46c72821b17be65b05a4e61b5846ba998a57d76db44ca4f8fa*out -ip 115.145.171.128. Top 10 memory users:
PID	MEM(GB)	COMMAND
4942	3.48	/snap/pycharm-professional/408/jbr/bin/java -classpath /snap/pycharm-professional/408/lib/platform-l...
616443	2.14	/home/abdenour/miniconda3/envs/fl_with_flower/bin/python /home/abdenour/PycharmProjects/fl_with_flow...
619615	1.90	ray::ClientAppActor
619616	1.88	ray::ClientAppActor
619611	1.87	ray::ClientAppActor
619617	1.86	ray::ClientAppActor
619620	1.85	ray::ClientAppActor
619618	1.84	ray::ClientAppActor
619613	1.82	ray::ClientAppActor
619619	1.80	ray::ClientAppActor
Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.
[2024-08-13 23:56:53,792][flwr][ERROR] - Traceback (most recent call last):
  File "/home/abdenour/miniconda3/envs/fl_with_flower/lib/python3.11/site-packages/flwr/simulation/ray_transport/ray_client_proxy.py", line 94, in _submit_job
    out_mssg, updated_context = self.actor_pool.get_client_result(
                                ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/abdenour/miniconda3/envs/fl_with_flower/lib/python3.11/site-packages/flwr/simulation/ray_transport/ray_actor.py", line 398, in get_client_result
    return self._fetch_future_result(cid)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/abdenour/miniconda3/envs/fl_with_flower/lib/python3.11/site-packages/flwr/simulation/ray_transport/ray_actor.py", line 279, in _fetch_future_result
    res_cid, out_mssg, updated_context = ray.get(
                                         ^^^^^^^^
  File "/home/abdenour/miniconda3/envs/fl_with_flower/lib/python3.11/site-packages/ray/_private/auto_init_hook.py", line 21, in auto_init_wrapper
    return fn(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^
  File "/home/abdenour/miniconda3/envs/fl_with_flower/lib/python3.11/site-packages/ray/_private/client_mode_hook.py", line 103, in wrapper
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "/home/abdenour/miniconda3/envs/fl_with_flower/lib/python3.11/site-packages/ray/_private/worker.py", line 2667, in get
    values, debugger_breakpoint = worker.get_objects(object_refs, timeout=timeout)
                                  ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/abdenour/miniconda3/envs/fl_with_flower/lib/python3.11/site-packages/ray/_private/worker.py", line 866, in get_objects
    raise value
ray.exceptions.OutOfMemoryError: Task was killed due to the node running low on memory.
Memory on the node (IP: 115.145.171.128, ID: c687613ff34cdbf5097699e5c896055ef2dc44343c5ab44bd40309d5) where the task (actor ID: 85928d3f1a247800081c0e1101000000, name=ClientAppActor.__init__, pid=619614, memory used=1.73GB) was running was 29.50GB / 31.02GB (0.951049), which exceeds the memory usage threshold of 0.95. Ray killed this worker (ID: c7f644ac3cca20dffbfa1723daac4f87f04c7de03da25e7cad5fe33c) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 115.145.171.128`. To see the logs of the worker, use `ray logs worker-c7f644ac3cca20dffbfa1723daac4f87f04c7de03da25e7cad5fe33c*out -ip 115.145.171.128. Top 10 memory users:
PID	MEM(GB)	COMMAND
4942	3.49	/snap/pycharm-professional/408/jbr/bin/java -classpath /snap/pycharm-professional/408/lib/platform-l...
619616	1.80	ray::ClientAppActor.run
619615	1.78	ray::ClientAppActor
619617	1.77	ray::ClientAppActor
619611	1.73	ray::ClientAppActor
619618	1.73	ray::ClientAppActor
619614	1.73	ray::ClientAppActor
619612	1.72	ray::ClientAppActor
619619	1.70	ray::ClientAppActor
619620	1.68	ray::ClientAppActor.run
Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.

[2024-08-13 23:56:53,792][flwr][ERROR] - Task was killed due to the node running low on memory.
Memory on the node (IP: 115.145.171.128, ID: c687613ff34cdbf5097699e5c896055ef2dc44343c5ab44bd40309d5) where the task (actor ID: 85928d3f1a247800081c0e1101000000, name=ClientAppActor.__init__, pid=619614, memory used=1.73GB) was running was 29.50GB / 31.02GB (0.951049), which exceeds the memory usage threshold of 0.95. Ray killed this worker (ID: c7f644ac3cca20dffbfa1723daac4f87f04c7de03da25e7cad5fe33c) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 115.145.171.128`. To see the logs of the worker, use `ray logs worker-c7f644ac3cca20dffbfa1723daac4f87f04c7de03da25e7cad5fe33c*out -ip 115.145.171.128. Top 10 memory users:
PID	MEM(GB)	COMMAND
4942	3.49	/snap/pycharm-professional/408/jbr/bin/java -classpath /snap/pycharm-professional/408/lib/platform-l...
619616	1.80	ray::ClientAppActor.run
619615	1.78	ray::ClientAppActor
619617	1.77	ray::ClientAppActor
619611	1.73	ray::ClientAppActor
619618	1.73	ray::ClientAppActor
619614	1.73	ray::ClientAppActor
619612	1.72	ray::ClientAppActor
619619	1.70	ray::ClientAppActor
619620	1.68	ray::ClientAppActor.run
Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.
[2024-08-13 23:56:55,376][flwr][INFO] - aggregate_fit: received 7 results and 3 failures
[2024-08-13 23:56:58,173][flwr][INFO] - fit progress: (267, 16.92593604326248, {'accuracy': 0.7548076923076923}, 1539.433509562019)
[2024-08-13 23:56:58,173][flwr][INFO] - configure_evaluate: strategy sampled 2 clients (out of 20)
[2024-08-13 23:56:58,576][flwr][INFO] - aggregate_evaluate: received 2 results and 0 failures
[2024-08-13 23:56:58,576][flwr][INFO] - 
[2024-08-13 23:56:58,576][flwr][INFO] - [ROUND 268]
[2024-08-13 23:56:58,576][flwr][INFO] - configure_fit: strategy sampled 10 clients (out of 20)
[2024-08-13 23:56:58,717][flwr][ERROR] - Traceback (most recent call last):
  File "/home/abdenour/miniconda3/envs/fl_with_flower/lib/python3.11/site-packages/flwr/simulation/ray_transport/ray_client_proxy.py", line 94, in _submit_job
    out_mssg, updated_context = self.actor_pool.get_client_result(
                                ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/abdenour/miniconda3/envs/fl_with_flower/lib/python3.11/site-packages/flwr/simulation/ray_transport/ray_actor.py", line 398, in get_client_result
    return self._fetch_future_result(cid)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/abdenour/miniconda3/envs/fl_with_flower/lib/python3.11/site-packages/flwr/simulation/ray_transport/ray_actor.py", line 279, in _fetch_future_result
    res_cid, out_mssg, updated_context = ray.get(
                                         ^^^^^^^^
  File "/home/abdenour/miniconda3/envs/fl_with_flower/lib/python3.11/site-packages/ray/_private/auto_init_hook.py", line 21, in auto_init_wrapper
    return fn(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^
  File "/home/abdenour/miniconda3/envs/fl_with_flower/lib/python3.11/site-packages/ray/_private/client_mode_hook.py", line 103, in wrapper
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "/home/abdenour/miniconda3/envs/fl_with_flower/lib/python3.11/site-packages/ray/_private/worker.py", line 2667, in get
    values, debugger_breakpoint = worker.get_objects(object_refs, timeout=timeout)
                                  ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/abdenour/miniconda3/envs/fl_with_flower/lib/python3.11/site-packages/ray/_private/worker.py", line 866, in get_objects
    raise value
ray.exceptions.OutOfMemoryError: Task was killed due to the node running low on memory.
Memory on the node (IP: 115.145.171.128, ID: c687613ff34cdbf5097699e5c896055ef2dc44343c5ab44bd40309d5) where the task (actor ID: 85928d3f1a247800081c0e1101000000, name=ClientAppActor.__init__, pid=619614, memory used=1.73GB) was running was 29.50GB / 31.02GB (0.951049), which exceeds the memory usage threshold of 0.95. Ray killed this worker (ID: c7f644ac3cca20dffbfa1723daac4f87f04c7de03da25e7cad5fe33c) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 115.145.171.128`. To see the logs of the worker, use `ray logs worker-c7f644ac3cca20dffbfa1723daac4f87f04c7de03da25e7cad5fe33c*out -ip 115.145.171.128. Top 10 memory users:
PID	MEM(GB)	COMMAND
4942	3.49	/snap/pycharm-professional/408/jbr/bin/java -classpath /snap/pycharm-professional/408/lib/platform-l...
619616	1.80	ray::ClientAppActor.run
619615	1.78	ray::ClientAppActor
619617	1.77	ray::ClientAppActor
619611	1.73	ray::ClientAppActor
619618	1.73	ray::ClientAppActor
619614	1.73	ray::ClientAppActor
619612	1.72	ray::ClientAppActor
619619	1.70	ray::ClientAppActor
619620	1.68	ray::ClientAppActor.run
Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.

[2024-08-13 23:56:58,718][flwr][ERROR] - Task was killed due to the node running low on memory.
Memory on the node (IP: 115.145.171.128, ID: c687613ff34cdbf5097699e5c896055ef2dc44343c5ab44bd40309d5) where the task (actor ID: 85928d3f1a247800081c0e1101000000, name=ClientAppActor.__init__, pid=619614, memory used=1.73GB) was running was 29.50GB / 31.02GB (0.951049), which exceeds the memory usage threshold of 0.95. Ray killed this worker (ID: c7f644ac3cca20dffbfa1723daac4f87f04c7de03da25e7cad5fe33c) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 115.145.171.128`. To see the logs of the worker, use `ray logs worker-c7f644ac3cca20dffbfa1723daac4f87f04c7de03da25e7cad5fe33c*out -ip 115.145.171.128. Top 10 memory users:
PID	MEM(GB)	COMMAND
4942	3.49	/snap/pycharm-professional/408/jbr/bin/java -classpath /snap/pycharm-professional/408/lib/platform-l...
619616	1.80	ray::ClientAppActor.run
619615	1.78	ray::ClientAppActor
619617	1.77	ray::ClientAppActor
619611	1.73	ray::ClientAppActor
619618	1.73	ray::ClientAppActor
619614	1.73	ray::ClientAppActor
619612	1.72	ray::ClientAppActor
619619	1.70	ray::ClientAppActor
619620	1.68	ray::ClientAppActor.run
Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.
[2024-08-13 23:56:58,719][flwr][ERROR] - Traceback (most recent call last):
  File "/home/abdenour/miniconda3/envs/fl_with_flower/lib/python3.11/site-packages/flwr/simulation/ray_transport/ray_client_proxy.py", line 94, in _submit_job
    out_mssg, updated_context = self.actor_pool.get_client_result(
                                ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/abdenour/miniconda3/envs/fl_with_flower/lib/python3.11/site-packages/flwr/simulation/ray_transport/ray_actor.py", line 398, in get_client_result
    return self._fetch_future_result(cid)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/abdenour/miniconda3/envs/fl_with_flower/lib/python3.11/site-packages/flwr/simulation/ray_transport/ray_actor.py", line 279, in _fetch_future_result
    res_cid, out_mssg, updated_context = ray.get(
                                         ^^^^^^^^
  File "/home/abdenour/miniconda3/envs/fl_with_flower/lib/python3.11/site-packages/ray/_private/auto_init_hook.py", line 21, in auto_init_wrapper
    return fn(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^
  File "/home/abdenour/miniconda3/envs/fl_with_flower/lib/python3.11/site-packages/ray/_private/client_mode_hook.py", line 103, in wrapper
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "/home/abdenour/miniconda3/envs/fl_with_flower/lib/python3.11/site-packages/ray/_private/worker.py", line 2667, in get
    values, debugger_breakpoint = worker.get_objects(object_refs, timeout=timeout)
                                  ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/abdenour/miniconda3/envs/fl_with_flower/lib/python3.11/site-packages/ray/_private/worker.py", line 866, in get_objects
    raise value
ray.exceptions.OutOfMemoryError: Task was killed due to the node running low on memory.
Memory on the node (IP: 115.145.171.128, ID: c687613ff34cdbf5097699e5c896055ef2dc44343c5ab44bd40309d5) where the task (actor ID: 52365bb7997979e5a4c7ddbd01000000, name=ClientAppActor.__init__, pid=619618, memory used=1.84GB) was running was 29.49GB / 31.02GB (0.950707), which exceeds the memory usage threshold of 0.95. Ray killed this worker (ID: 7e48cd46c72821b17be65b05a4e61b5846ba998a57d76db44ca4f8fa) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 115.145.171.128`. To see the logs of the worker, use `ray logs worker-7e48cd46c72821b17be65b05a4e61b5846ba998a57d76db44ca4f8fa*out -ip 115.145.171.128. Top 10 memory users:
PID	MEM(GB)	COMMAND
4942	3.48	/snap/pycharm-professional/408/jbr/bin/java -classpath /snap/pycharm-professional/408/lib/platform-l...
616443	2.14	/home/abdenour/miniconda3/envs/fl_with_flower/bin/python /home/abdenour/PycharmProjects/fl_with_flow...
619615	1.90	ray::ClientAppActor
619616	1.88	ray::ClientAppActor
619611	1.87	ray::ClientAppActor
619617	1.86	ray::ClientAppActor
619620	1.85	ray::ClientAppActor
619618	1.84	ray::ClientAppActor
619613	1.82	ray::ClientAppActor
619619	1.80	ray::ClientAppActor
Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.

[2024-08-13 23:56:58,725][flwr][ERROR] - Task was killed due to the node running low on memory.
Memory on the node (IP: 115.145.171.128, ID: c687613ff34cdbf5097699e5c896055ef2dc44343c5ab44bd40309d5) where the task (actor ID: 52365bb7997979e5a4c7ddbd01000000, name=ClientAppActor.__init__, pid=619618, memory used=1.84GB) was running was 29.49GB / 31.02GB (0.950707), which exceeds the memory usage threshold of 0.95. Ray killed this worker (ID: 7e48cd46c72821b17be65b05a4e61b5846ba998a57d76db44ca4f8fa) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 115.145.171.128`. To see the logs of the worker, use `ray logs worker-7e48cd46c72821b17be65b05a4e61b5846ba998a57d76db44ca4f8fa*out -ip 115.145.171.128. Top 10 memory users:
PID	MEM(GB)	COMMAND
4942	3.48	/snap/pycharm-professional/408/jbr/bin/java -classpath /snap/pycharm-professional/408/lib/platform-l...
616443	2.14	/home/abdenour/miniconda3/envs/fl_with_flower/bin/python /home/abdenour/PycharmProjects/fl_with_flow...
619615	1.90	ray::ClientAppActor
619616	1.88	ray::ClientAppActor
619611	1.87	ray::ClientAppActor
619617	1.86	ray::ClientAppActor
619620	1.85	ray::ClientAppActor
619618	1.84	ray::ClientAppActor
619613	1.82	ray::ClientAppActor
619619	1.80	ray::ClientAppActor
Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.
[2024-08-13 23:56:58,747][flwr][ERROR] - Traceback (most recent call last):
  File "/home/abdenour/miniconda3/envs/fl_with_flower/lib/python3.11/site-packages/flwr/simulation/ray_transport/ray_client_proxy.py", line 94, in _submit_job
    out_mssg, updated_context = self.actor_pool.get_client_result(
                                ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/abdenour/miniconda3/envs/fl_with_flower/lib/python3.11/site-packages/flwr/simulation/ray_transport/ray_actor.py", line 398, in get_client_result
    return self._fetch_future_result(cid)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/abdenour/miniconda3/envs/fl_with_flower/lib/python3.11/site-packages/flwr/simulation/ray_transport/ray_actor.py", line 279, in _fetch_future_result
    res_cid, out_mssg, updated_context = ray.get(
                                         ^^^^^^^^
  File "/home/abdenour/miniconda3/envs/fl_with_flower/lib/python3.11/site-packages/ray/_private/auto_init_hook.py", line 21, in auto_init_wrapper
    return fn(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^
  File "/home/abdenour/miniconda3/envs/fl_with_flower/lib/python3.11/site-packages/ray/_private/client_mode_hook.py", line 103, in wrapper
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "/home/abdenour/miniconda3/envs/fl_with_flower/lib/python3.11/site-packages/ray/_private/worker.py", line 2667, in get
    values, debugger_breakpoint = worker.get_objects(object_refs, timeout=timeout)
                                  ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/abdenour/miniconda3/envs/fl_with_flower/lib/python3.11/site-packages/ray/_private/worker.py", line 866, in get_objects
    raise value
ray.exceptions.OutOfMemoryError: Task was killed due to the node running low on memory.
Memory on the node (IP: 115.145.171.128, ID: c687613ff34cdbf5097699e5c896055ef2dc44343c5ab44bd40309d5) where the task (actor ID: 52365bb7997979e5a4c7ddbd01000000, name=ClientAppActor.__init__, pid=619618, memory used=1.84GB) was running was 29.49GB / 31.02GB (0.950707), which exceeds the memory usage threshold of 0.95. Ray killed this worker (ID: 7e48cd46c72821b17be65b05a4e61b5846ba998a57d76db44ca4f8fa) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 115.145.171.128`. To see the logs of the worker, use `ray logs worker-7e48cd46c72821b17be65b05a4e61b5846ba998a57d76db44ca4f8fa*out -ip 115.145.171.128. Top 10 memory users:
PID	MEM(GB)	COMMAND
4942	3.48	/snap/pycharm-professional/408/jbr/bin/java -classpath /snap/pycharm-professional/408/lib/platform-l...
616443	2.14	/home/abdenour/miniconda3/envs/fl_with_flower/bin/python /home/abdenour/PycharmProjects/fl_with_flow...
619615	1.90	ray::ClientAppActor
619616	1.88	ray::ClientAppActor
619611	1.87	ray::ClientAppActor
619617	1.86	ray::ClientAppActor
619620	1.85	ray::ClientAppActor
619618	1.84	ray::ClientAppActor
619613	1.82	ray::ClientAppActor
619619	1.80	ray::ClientAppActor
Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.

[2024-08-13 23:56:58,747][flwr][ERROR] - Task was killed due to the node running low on memory.
Memory on the node (IP: 115.145.171.128, ID: c687613ff34cdbf5097699e5c896055ef2dc44343c5ab44bd40309d5) where the task (actor ID: 52365bb7997979e5a4c7ddbd01000000, name=ClientAppActor.__init__, pid=619618, memory used=1.84GB) was running was 29.49GB / 31.02GB (0.950707), which exceeds the memory usage threshold of 0.95. Ray killed this worker (ID: 7e48cd46c72821b17be65b05a4e61b5846ba998a57d76db44ca4f8fa) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 115.145.171.128`. To see the logs of the worker, use `ray logs worker-7e48cd46c72821b17be65b05a4e61b5846ba998a57d76db44ca4f8fa*out -ip 115.145.171.128. Top 10 memory users:
PID	MEM(GB)	COMMAND
4942	3.48	/snap/pycharm-professional/408/jbr/bin/java -classpath /snap/pycharm-professional/408/lib/platform-l...
616443	2.14	/home/abdenour/miniconda3/envs/fl_with_flower/bin/python /home/abdenour/PycharmProjects/fl_with_flow...
619615	1.90	ray::ClientAppActor
619616	1.88	ray::ClientAppActor
619611	1.87	ray::ClientAppActor
619617	1.86	ray::ClientAppActor
619620	1.85	ray::ClientAppActor
619618	1.84	ray::ClientAppActor
619613	1.82	ray::ClientAppActor
619619	1.80	ray::ClientAppActor
Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.
[2024-08-13 23:57:00,248][flwr][INFO] - aggregate_fit: received 7 results and 3 failures
[2024-08-13 23:57:03,041][flwr][INFO] - fit progress: (268, 19.032843947410583, {'accuracy': 0.7435897435897436}, 1544.3017335820186)
[2024-08-13 23:57:03,042][flwr][INFO] - configure_evaluate: strategy sampled 2 clients (out of 20)
[2024-08-13 23:57:03,251][flwr][INFO] - aggregate_evaluate: received 2 results and 0 failures
[2024-08-13 23:57:03,251][flwr][INFO] - 
[2024-08-13 23:57:03,251][flwr][INFO] - [ROUND 269]
[2024-08-13 23:57:03,251][flwr][INFO] - configure_fit: strategy sampled 10 clients (out of 20)
[2024-08-13 23:57:03,403][flwr][ERROR] - Traceback (most recent call last):
  File "/home/abdenour/miniconda3/envs/fl_with_flower/lib/python3.11/site-packages/flwr/simulation/ray_transport/ray_client_proxy.py", line 94, in _submit_job
    out_mssg, updated_context = self.actor_pool.get_client_result(
                                ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/abdenour/miniconda3/envs/fl_with_flower/lib/python3.11/site-packages/flwr/simulation/ray_transport/ray_actor.py", line 398, in get_client_result
    return self._fetch_future_result(cid)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/abdenour/miniconda3/envs/fl_with_flower/lib/python3.11/site-packages/flwr/simulation/ray_transport/ray_actor.py", line 279, in _fetch_future_result
    res_cid, out_mssg, updated_context = ray.get(
                                         ^^^^^^^^
  File "/home/abdenour/miniconda3/envs/fl_with_flower/lib/python3.11/site-packages/ray/_private/auto_init_hook.py", line 21, in auto_init_wrapper
    return fn(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^
  File "/home/abdenour/miniconda3/envs/fl_with_flower/lib/python3.11/site-packages/ray/_private/client_mode_hook.py", line 103, in wrapper
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "/home/abdenour/miniconda3/envs/fl_with_flower/lib/python3.11/site-packages/ray/_private/worker.py", line 2667, in get
    values, debugger_breakpoint = worker.get_objects(object_refs, timeout=timeout)
                                  ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/abdenour/miniconda3/envs/fl_with_flower/lib/python3.11/site-packages/ray/_private/worker.py", line 866, in get_objects
    raise value
ray.exceptions.OutOfMemoryError: Task was killed due to the node running low on memory.
Memory on the node (IP: 115.145.171.128, ID: c687613ff34cdbf5097699e5c896055ef2dc44343c5ab44bd40309d5) where the task (actor ID: 85928d3f1a247800081c0e1101000000, name=ClientAppActor.__init__, pid=619614, memory used=1.73GB) was running was 29.50GB / 31.02GB (0.951049), which exceeds the memory usage threshold of 0.95. Ray killed this worker (ID: c7f644ac3cca20dffbfa1723daac4f87f04c7de03da25e7cad5fe33c) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 115.145.171.128`. To see the logs of the worker, use `ray logs worker-c7f644ac3cca20dffbfa1723daac4f87f04c7de03da25e7cad5fe33c*out -ip 115.145.171.128. Top 10 memory users:
PID	MEM(GB)	COMMAND
4942	3.49	/snap/pycharm-professional/408/jbr/bin/java -classpath /snap/pycharm-professional/408/lib/platform-l...
619616	1.80	ray::ClientAppActor.run
619615	1.78	ray::ClientAppActor
619617	1.77	ray::ClientAppActor
619611	1.73	ray::ClientAppActor
619618	1.73	ray::ClientAppActor
619614	1.73	ray::ClientAppActor
619612	1.72	ray::ClientAppActor
619619	1.70	ray::ClientAppActor
619620	1.68	ray::ClientAppActor.run
Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.

[2024-08-13 23:57:03,403][flwr][ERROR] - Task was killed due to the node running low on memory.
Memory on the node (IP: 115.145.171.128, ID: c687613ff34cdbf5097699e5c896055ef2dc44343c5ab44bd40309d5) where the task (actor ID: 85928d3f1a247800081c0e1101000000, name=ClientAppActor.__init__, pid=619614, memory used=1.73GB) was running was 29.50GB / 31.02GB (0.951049), which exceeds the memory usage threshold of 0.95. Ray killed this worker (ID: c7f644ac3cca20dffbfa1723daac4f87f04c7de03da25e7cad5fe33c) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 115.145.171.128`. To see the logs of the worker, use `ray logs worker-c7f644ac3cca20dffbfa1723daac4f87f04c7de03da25e7cad5fe33c*out -ip 115.145.171.128. Top 10 memory users:
PID	MEM(GB)	COMMAND
4942	3.49	/snap/pycharm-professional/408/jbr/bin/java -classpath /snap/pycharm-professional/408/lib/platform-l...
619616	1.80	ray::ClientAppActor.run
619615	1.78	ray::ClientAppActor
619617	1.77	ray::ClientAppActor
619611	1.73	ray::ClientAppActor
619618	1.73	ray::ClientAppActor
619614	1.73	ray::ClientAppActor
619612	1.72	ray::ClientAppActor
619619	1.70	ray::ClientAppActor
619620	1.68	ray::ClientAppActor.run
Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.
[2024-08-13 23:57:03,403][flwr][ERROR] - Traceback (most recent call last):
  File "/home/abdenour/miniconda3/envs/fl_with_flower/lib/python3.11/site-packages/flwr/simulation/ray_transport/ray_client_proxy.py", line 94, in _submit_job
    out_mssg, updated_context = self.actor_pool.get_client_result(
                                ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/abdenour/miniconda3/envs/fl_with_flower/lib/python3.11/site-packages/flwr/simulation/ray_transport/ray_actor.py", line 398, in get_client_result
    return self._fetch_future_result(cid)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/abdenour/miniconda3/envs/fl_with_flower/lib/python3.11/site-packages/flwr/simulation/ray_transport/ray_actor.py", line 279, in _fetch_future_result
    res_cid, out_mssg, updated_context = ray.get(
                                         ^^^^^^^^
  File "/home/abdenour/miniconda3/envs/fl_with_flower/lib/python3.11/site-packages/ray/_private/auto_init_hook.py", line 21, in auto_init_wrapper
    return fn(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^
  File "/home/abdenour/miniconda3/envs/fl_with_flower/lib/python3.11/site-packages/ray/_private/client_mode_hook.py", line 103, in wrapper
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "/home/abdenour/miniconda3/envs/fl_with_flower/lib/python3.11/site-packages/ray/_private/worker.py", line 2667, in get
    values, debugger_breakpoint = worker.get_objects(object_refs, timeout=timeout)
                                  ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/abdenour/miniconda3/envs/fl_with_flower/lib/python3.11/site-packages/ray/_private/worker.py", line 866, in get_objects
    raise value
ray.exceptions.OutOfMemoryError: Task was killed due to the node running low on memory.
Memory on the node (IP: 115.145.171.128, ID: c687613ff34cdbf5097699e5c896055ef2dc44343c5ab44bd40309d5) where the task (actor ID: 52365bb7997979e5a4c7ddbd01000000, name=ClientAppActor.__init__, pid=619618, memory used=1.84GB) was running was 29.49GB / 31.02GB (0.950707), which exceeds the memory usage threshold of 0.95. Ray killed this worker (ID: 7e48cd46c72821b17be65b05a4e61b5846ba998a57d76db44ca4f8fa) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 115.145.171.128`. To see the logs of the worker, use `ray logs worker-7e48cd46c72821b17be65b05a4e61b5846ba998a57d76db44ca4f8fa*out -ip 115.145.171.128. Top 10 memory users:
PID	MEM(GB)	COMMAND
4942	3.48	/snap/pycharm-professional/408/jbr/bin/java -classpath /snap/pycharm-professional/408/lib/platform-l...
616443	2.14	/home/abdenour/miniconda3/envs/fl_with_flower/bin/python /home/abdenour/PycharmProjects/fl_with_flow...
619615	1.90	ray::ClientAppActor
619616	1.88	ray::ClientAppActor
619611	1.87	ray::ClientAppActor
619617	1.86	ray::ClientAppActor
619620	1.85	ray::ClientAppActor
619618	1.84	ray::ClientAppActor
619613	1.82	ray::ClientAppActor
619619	1.80	ray::ClientAppActor
Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.

[2024-08-13 23:57:03,404][flwr][ERROR] - Task was killed due to the node running low on memory.
Memory on the node (IP: 115.145.171.128, ID: c687613ff34cdbf5097699e5c896055ef2dc44343c5ab44bd40309d5) where the task (actor ID: 52365bb7997979e5a4c7ddbd01000000, name=ClientAppActor.__init__, pid=619618, memory used=1.84GB) was running was 29.49GB / 31.02GB (0.950707), which exceeds the memory usage threshold of 0.95. Ray killed this worker (ID: 7e48cd46c72821b17be65b05a4e61b5846ba998a57d76db44ca4f8fa) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 115.145.171.128`. To see the logs of the worker, use `ray logs worker-7e48cd46c72821b17be65b05a4e61b5846ba998a57d76db44ca4f8fa*out -ip 115.145.171.128. Top 10 memory users:
PID	MEM(GB)	COMMAND
4942	3.48	/snap/pycharm-professional/408/jbr/bin/java -classpath /snap/pycharm-professional/408/lib/platform-l...
616443	2.14	/home/abdenour/miniconda3/envs/fl_with_flower/bin/python /home/abdenour/PycharmProjects/fl_with_flow...
619615	1.90	ray::ClientAppActor
619616	1.88	ray::ClientAppActor
619611	1.87	ray::ClientAppActor
619617	1.86	ray::ClientAppActor
619620	1.85	ray::ClientAppActor
619618	1.84	ray::ClientAppActor
619613	1.82	ray::ClientAppActor
619619	1.80	ray::ClientAppActor
Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.
[2024-08-13 23:57:03,404][flwr][ERROR] - Traceback (most recent call last):
  File "/home/abdenour/miniconda3/envs/fl_with_flower/lib/python3.11/site-packages/flwr/simulation/ray_transport/ray_client_proxy.py", line 94, in _submit_job
    out_mssg, updated_context = self.actor_pool.get_client_result(
                                ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/abdenour/miniconda3/envs/fl_with_flower/lib/python3.11/site-packages/flwr/simulation/ray_transport/ray_actor.py", line 398, in get_client_result
    return self._fetch_future_result(cid)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/abdenour/miniconda3/envs/fl_with_flower/lib/python3.11/site-packages/flwr/simulation/ray_transport/ray_actor.py", line 279, in _fetch_future_result
    res_cid, out_mssg, updated_context = ray.get(
                                         ^^^^^^^^
  File "/home/abdenour/miniconda3/envs/fl_with_flower/lib/python3.11/site-packages/ray/_private/auto_init_hook.py", line 21, in auto_init_wrapper
    return fn(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^
  File "/home/abdenour/miniconda3/envs/fl_with_flower/lib/python3.11/site-packages/ray/_private/client_mode_hook.py", line 103, in wrapper
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "/home/abdenour/miniconda3/envs/fl_with_flower/lib/python3.11/site-packages/ray/_private/worker.py", line 2667, in get
    values, debugger_breakpoint = worker.get_objects(object_refs, timeout=timeout)
                                  ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/abdenour/miniconda3/envs/fl_with_flower/lib/python3.11/site-packages/ray/_private/worker.py", line 866, in get_objects
    raise value
ray.exceptions.OutOfMemoryError: Task was killed due to the node running low on memory.
Memory on the node (IP: 115.145.171.128, ID: c687613ff34cdbf5097699e5c896055ef2dc44343c5ab44bd40309d5) where the task (actor ID: 2a8bb25e35bf7bb5a0a6c00401000000, name=ClientAppActor.__init__, pid=619620, memory used=1.97GB) was running was 29.48GB / 31.02GB (0.950487), which exceeds the memory usage threshold of 0.95. Ray killed this worker (ID: 38a4d571c3fa7ff405dd9f5d1555b4753efb95dc84bb70ce01b6ecb4) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 115.145.171.128`. To see the logs of the worker, use `ray logs worker-38a4d571c3fa7ff405dd9f5d1555b4753efb95dc84bb70ce01b6ecb4*out -ip 115.145.171.128. Top 10 memory users:
PID	MEM(GB)	COMMAND
4942	3.48	/snap/pycharm-professional/408/jbr/bin/java -classpath /snap/pycharm-professional/408/lib/platform-l...
616443	2.85	/home/abdenour/miniconda3/envs/fl_with_flower/bin/python /home/abdenour/PycharmProjects/fl_with_flow...
619615	2.06	ray::ClientAppActor
619616	2.04	ray::ClientAppActor
619611	2.04	ray::ClientAppActor.run
619619	2.03	ray::ClientAppActor.run
619612	1.98	ray::ClientAppActor.run
619613	1.97	ray::ClientAppActor.run
619620	1.97	ray::ClientAppActor
619617	1.96	ray::ClientAppActor
Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.

[2024-08-13 23:57:03,404][flwr][ERROR] - Task was killed due to the node running low on memory.
Memory on the node (IP: 115.145.171.128, ID: c687613ff34cdbf5097699e5c896055ef2dc44343c5ab44bd40309d5) where the task (actor ID: 2a8bb25e35bf7bb5a0a6c00401000000, name=ClientAppActor.__init__, pid=619620, memory used=1.97GB) was running was 29.48GB / 31.02GB (0.950487), which exceeds the memory usage threshold of 0.95. Ray killed this worker (ID: 38a4d571c3fa7ff405dd9f5d1555b4753efb95dc84bb70ce01b6ecb4) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 115.145.171.128`. To see the logs of the worker, use `ray logs worker-38a4d571c3fa7ff405dd9f5d1555b4753efb95dc84bb70ce01b6ecb4*out -ip 115.145.171.128. Top 10 memory users:
PID	MEM(GB)	COMMAND
4942	3.48	/snap/pycharm-professional/408/jbr/bin/java -classpath /snap/pycharm-professional/408/lib/platform-l...
616443	2.85	/home/abdenour/miniconda3/envs/fl_with_flower/bin/python /home/abdenour/PycharmProjects/fl_with_flow...
619615	2.06	ray::ClientAppActor
619616	2.04	ray::ClientAppActor
619611	2.04	ray::ClientAppActor.run
619619	2.03	ray::ClientAppActor.run
619612	1.98	ray::ClientAppActor.run
619613	1.97	ray::ClientAppActor.run
619620	1.97	ray::ClientAppActor
619617	1.96	ray::ClientAppActor
Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.
[2024-08-13 23:57:04,990][flwr][INFO] - aggregate_fit: received 7 results and 3 failures
[2024-08-13 23:57:08,030][flwr][INFO] - fit progress: (269, 17.381625831127167, {'accuracy': 0.7516025641025641}, 1549.2904485340114)
[2024-08-13 23:57:08,030][flwr][INFO] - configure_evaluate: strategy sampled 2 clients (out of 20)
[2024-08-13 23:57:08,229][flwr][INFO] - aggregate_evaluate: received 2 results and 0 failures
[2024-08-13 23:57:08,229][flwr][INFO] - 
[2024-08-13 23:57:08,229][flwr][INFO] - [ROUND 270]
[2024-08-13 23:57:08,229][flwr][INFO] - configure_fit: strategy sampled 10 clients (out of 20)
[2024-08-13 23:57:08,405][flwr][ERROR] - Traceback (most recent call last):
  File "/home/abdenour/miniconda3/envs/fl_with_flower/lib/python3.11/site-packages/flwr/simulation/ray_transport/ray_client_proxy.py", line 94, in _submit_job
    out_mssg, updated_context = self.actor_pool.get_client_result(
                                ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/abdenour/miniconda3/envs/fl_with_flower/lib/python3.11/site-packages/flwr/simulation/ray_transport/ray_actor.py", line 398, in get_client_result
    return self._fetch_future_result(cid)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/abdenour/miniconda3/envs/fl_with_flower/lib/python3.11/site-packages/flwr/simulation/ray_transport/ray_actor.py", line 279, in _fetch_future_result
    res_cid, out_mssg, updated_context = ray.get(
                                         ^^^^^^^^
  File "/home/abdenour/miniconda3/envs/fl_with_flower/lib/python3.11/site-packages/ray/_private/auto_init_hook.py", line 21, in auto_init_wrapper
    return fn(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^
  File "/home/abdenour/miniconda3/envs/fl_with_flower/lib/python3.11/site-packages/ray/_private/client_mode_hook.py", line 103, in wrapper
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "/home/abdenour/miniconda3/envs/fl_with_flower/lib/python3.11/site-packages/ray/_private/worker.py", line 2667, in get
    values, debugger_breakpoint = worker.get_objects(object_refs, timeout=timeout)
                                  ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/abdenour/miniconda3/envs/fl_with_flower/lib/python3.11/site-packages/ray/_private/worker.py", line 866, in get_objects
    raise value
ray.exceptions.OutOfMemoryError: Task was killed due to the node running low on memory.
Memory on the node (IP: 115.145.171.128, ID: c687613ff34cdbf5097699e5c896055ef2dc44343c5ab44bd40309d5) where the task (actor ID: 2a8bb25e35bf7bb5a0a6c00401000000, name=ClientAppActor.__init__, pid=619620, memory used=1.97GB) was running was 29.48GB / 31.02GB (0.950487), which exceeds the memory usage threshold of 0.95. Ray killed this worker (ID: 38a4d571c3fa7ff405dd9f5d1555b4753efb95dc84bb70ce01b6ecb4) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 115.145.171.128`. To see the logs of the worker, use `ray logs worker-38a4d571c3fa7ff405dd9f5d1555b4753efb95dc84bb70ce01b6ecb4*out -ip 115.145.171.128. Top 10 memory users:
PID	MEM(GB)	COMMAND
4942	3.48	/snap/pycharm-professional/408/jbr/bin/java -classpath /snap/pycharm-professional/408/lib/platform-l...
616443	2.85	/home/abdenour/miniconda3/envs/fl_with_flower/bin/python /home/abdenour/PycharmProjects/fl_with_flow...
619615	2.06	ray::ClientAppActor
619616	2.04	ray::ClientAppActor
619611	2.04	ray::ClientAppActor.run
619619	2.03	ray::ClientAppActor.run
619612	1.98	ray::ClientAppActor.run
619613	1.97	ray::ClientAppActor.run
619620	1.97	ray::ClientAppActor
619617	1.96	ray::ClientAppActor
Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.

[2024-08-13 23:57:08,406][flwr][ERROR] - Task was killed due to the node running low on memory.
Memory on the node (IP: 115.145.171.128, ID: c687613ff34cdbf5097699e5c896055ef2dc44343c5ab44bd40309d5) where the task (actor ID: 2a8bb25e35bf7bb5a0a6c00401000000, name=ClientAppActor.__init__, pid=619620, memory used=1.97GB) was running was 29.48GB / 31.02GB (0.950487), which exceeds the memory usage threshold of 0.95. Ray killed this worker (ID: 38a4d571c3fa7ff405dd9f5d1555b4753efb95dc84bb70ce01b6ecb4) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 115.145.171.128`. To see the logs of the worker, use `ray logs worker-38a4d571c3fa7ff405dd9f5d1555b4753efb95dc84bb70ce01b6ecb4*out -ip 115.145.171.128. Top 10 memory users:
PID	MEM(GB)	COMMAND
4942	3.48	/snap/pycharm-professional/408/jbr/bin/java -classpath /snap/pycharm-professional/408/lib/platform-l...
616443	2.85	/home/abdenour/miniconda3/envs/fl_with_flower/bin/python /home/abdenour/PycharmProjects/fl_with_flow...
619615	2.06	ray::ClientAppActor
619616	2.04	ray::ClientAppActor
619611	2.04	ray::ClientAppActor.run
619619	2.03	ray::ClientAppActor.run
619612	1.98	ray::ClientAppActor.run
619613	1.97	ray::ClientAppActor.run
619620	1.97	ray::ClientAppActor
619617	1.96	ray::ClientAppActor
Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.
[2024-08-13 23:57:08,405][flwr][ERROR] - Traceback (most recent call last):
  File "/home/abdenour/miniconda3/envs/fl_with_flower/lib/python3.11/site-packages/flwr/simulation/ray_transport/ray_client_proxy.py", line 94, in _submit_job
    out_mssg, updated_context = self.actor_pool.get_client_result(
                                ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/abdenour/miniconda3/envs/fl_with_flower/lib/python3.11/site-packages/flwr/simulation/ray_transport/ray_actor.py", line 398, in get_client_result
    return self._fetch_future_result(cid)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/abdenour/miniconda3/envs/fl_with_flower/lib/python3.11/site-packages/flwr/simulation/ray_transport/ray_actor.py", line 279, in _fetch_future_result
    res_cid, out_mssg, updated_context = ray.get(
                                         ^^^^^^^^
  File "/home/abdenour/miniconda3/envs/fl_with_flower/lib/python3.11/site-packages/ray/_private/auto_init_hook.py", line 21, in auto_init_wrapper
    return fn(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^
  File "/home/abdenour/miniconda3/envs/fl_with_flower/lib/python3.11/site-packages/ray/_private/client_mode_hook.py", line 103, in wrapper
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "/home/abdenour/miniconda3/envs/fl_with_flower/lib/python3.11/site-packages/ray/_private/worker.py", line 2667, in get
    values, debugger_breakpoint = worker.get_objects(object_refs, timeout=timeout)
                                  ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/abdenour/miniconda3/envs/fl_with_flower/lib/python3.11/site-packages/ray/_private/worker.py", line 866, in get_objects
    raise value
ray.exceptions.OutOfMemoryError: Task was killed due to the node running low on memory.
Memory on the node (IP: 115.145.171.128, ID: c687613ff34cdbf5097699e5c896055ef2dc44343c5ab44bd40309d5) where the task (actor ID: 52365bb7997979e5a4c7ddbd01000000, name=ClientAppActor.__init__, pid=619618, memory used=1.84GB) was running was 29.49GB / 31.02GB (0.950707), which exceeds the memory usage threshold of 0.95. Ray killed this worker (ID: 7e48cd46c72821b17be65b05a4e61b5846ba998a57d76db44ca4f8fa) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 115.145.171.128`. To see the logs of the worker, use `ray logs worker-7e48cd46c72821b17be65b05a4e61b5846ba998a57d76db44ca4f8fa*out -ip 115.145.171.128. Top 10 memory users:
PID	MEM(GB)	COMMAND
4942	3.48	/snap/pycharm-professional/408/jbr/bin/java -classpath /snap/pycharm-professional/408/lib/platform-l...
616443	2.14	/home/abdenour/miniconda3/envs/fl_with_flower/bin/python /home/abdenour/PycharmProjects/fl_with_flow...
619615	1.90	ray::ClientAppActor
619616	1.88	ray::ClientAppActor
619611	1.87	ray::ClientAppActor
619617	1.86	ray::ClientAppActor
619620	1.85	ray::ClientAppActor
619618	1.84	ray::ClientAppActor
619613	1.82	ray::ClientAppActor
619619	1.80	ray::ClientAppActor
Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.

[2024-08-13 23:57:08,406][flwr][ERROR] - Task was killed due to the node running low on memory.
Memory on the node (IP: 115.145.171.128, ID: c687613ff34cdbf5097699e5c896055ef2dc44343c5ab44bd40309d5) where the task (actor ID: 52365bb7997979e5a4c7ddbd01000000, name=ClientAppActor.__init__, pid=619618, memory used=1.84GB) was running was 29.49GB / 31.02GB (0.950707), which exceeds the memory usage threshold of 0.95. Ray killed this worker (ID: 7e48cd46c72821b17be65b05a4e61b5846ba998a57d76db44ca4f8fa) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 115.145.171.128`. To see the logs of the worker, use `ray logs worker-7e48cd46c72821b17be65b05a4e61b5846ba998a57d76db44ca4f8fa*out -ip 115.145.171.128. Top 10 memory users:
PID	MEM(GB)	COMMAND
4942	3.48	/snap/pycharm-professional/408/jbr/bin/java -classpath /snap/pycharm-professional/408/lib/platform-l...
616443	2.14	/home/abdenour/miniconda3/envs/fl_with_flower/bin/python /home/abdenour/PycharmProjects/fl_with_flow...
619615	1.90	ray::ClientAppActor
619616	1.88	ray::ClientAppActor
619611	1.87	ray::ClientAppActor
619617	1.86	ray::ClientAppActor
619620	1.85	ray::ClientAppActor
619618	1.84	ray::ClientAppActor
619613	1.82	ray::ClientAppActor
619619	1.80	ray::ClientAppActor
Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.
[2024-08-13 23:57:08,406][flwr][ERROR] - Traceback (most recent call last):
  File "/home/abdenour/miniconda3/envs/fl_with_flower/lib/python3.11/site-packages/flwr/simulation/ray_transport/ray_client_proxy.py", line 94, in _submit_job
    out_mssg, updated_context = self.actor_pool.get_client_result(
                                ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/abdenour/miniconda3/envs/fl_with_flower/lib/python3.11/site-packages/flwr/simulation/ray_transport/ray_actor.py", line 398, in get_client_result
    return self._fetch_future_result(cid)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/abdenour/miniconda3/envs/fl_with_flower/lib/python3.11/site-packages/flwr/simulation/ray_transport/ray_actor.py", line 279, in _fetch_future_result
    res_cid, out_mssg, updated_context = ray.get(
                                         ^^^^^^^^
  File "/home/abdenour/miniconda3/envs/fl_with_flower/lib/python3.11/site-packages/ray/_private/auto_init_hook.py", line 21, in auto_init_wrapper
    return fn(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^
  File "/home/abdenour/miniconda3/envs/fl_with_flower/lib/python3.11/site-packages/ray/_private/client_mode_hook.py", line 103, in wrapper
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "/home/abdenour/miniconda3/envs/fl_with_flower/lib/python3.11/site-packages/ray/_private/worker.py", line 2667, in get
    values, debugger_breakpoint = worker.get_objects(object_refs, timeout=timeout)
                                  ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/abdenour/miniconda3/envs/fl_with_flower/lib/python3.11/site-packages/ray/_private/worker.py", line 866, in get_objects
    raise value
ray.exceptions.OutOfMemoryError: Task was killed due to the node running low on memory.
Memory on the node (IP: 115.145.171.128, ID: c687613ff34cdbf5097699e5c896055ef2dc44343c5ab44bd40309d5) where the task (actor ID: 85928d3f1a247800081c0e1101000000, name=ClientAppActor.__init__, pid=619614, memory used=1.73GB) was running was 29.50GB / 31.02GB (0.951049), which exceeds the memory usage threshold of 0.95. Ray killed this worker (ID: c7f644ac3cca20dffbfa1723daac4f87f04c7de03da25e7cad5fe33c) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 115.145.171.128`. To see the logs of the worker, use `ray logs worker-c7f644ac3cca20dffbfa1723daac4f87f04c7de03da25e7cad5fe33c*out -ip 115.145.171.128. Top 10 memory users:
PID	MEM(GB)	COMMAND
4942	3.49	/snap/pycharm-professional/408/jbr/bin/java -classpath /snap/pycharm-professional/408/lib/platform-l...
619616	1.80	ray::ClientAppActor.run
619615	1.78	ray::ClientAppActor
619617	1.77	ray::ClientAppActor
619611	1.73	ray::ClientAppActor
619618	1.73	ray::ClientAppActor
619614	1.73	ray::ClientAppActor
619612	1.72	ray::ClientAppActor
619619	1.70	ray::ClientAppActor
619620	1.68	ray::ClientAppActor.run
Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.

[2024-08-13 23:57:08,406][flwr][ERROR] - Task was killed due to the node running low on memory.
Memory on the node (IP: 115.145.171.128, ID: c687613ff34cdbf5097699e5c896055ef2dc44343c5ab44bd40309d5) where the task (actor ID: 85928d3f1a247800081c0e1101000000, name=ClientAppActor.__init__, pid=619614, memory used=1.73GB) was running was 29.50GB / 31.02GB (0.951049), which exceeds the memory usage threshold of 0.95. Ray killed this worker (ID: c7f644ac3cca20dffbfa1723daac4f87f04c7de03da25e7cad5fe33c) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 115.145.171.128`. To see the logs of the worker, use `ray logs worker-c7f644ac3cca20dffbfa1723daac4f87f04c7de03da25e7cad5fe33c*out -ip 115.145.171.128. Top 10 memory users:
PID	MEM(GB)	COMMAND
4942	3.49	/snap/pycharm-professional/408/jbr/bin/java -classpath /snap/pycharm-professional/408/lib/platform-l...
619616	1.80	ray::ClientAppActor.run
619615	1.78	ray::ClientAppActor
619617	1.77	ray::ClientAppActor
619611	1.73	ray::ClientAppActor
619618	1.73	ray::ClientAppActor
619614	1.73	ray::ClientAppActor
619612	1.72	ray::ClientAppActor
619619	1.70	ray::ClientAppActor
619620	1.68	ray::ClientAppActor.run
Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.
[2024-08-13 23:57:09,848][flwr][INFO] - aggregate_fit: received 7 results and 3 failures
[2024-08-13 23:57:12,857][flwr][INFO] - fit progress: (270, 17.285759150981903, {'accuracy': 0.7532051282051282}, 1554.1176364150015)
[2024-08-13 23:57:12,857][flwr][INFO] - configure_evaluate: strategy sampled 2 clients (out of 20)
[2024-08-13 23:57:13,090][flwr][INFO] - aggregate_evaluate: received 2 results and 0 failures
[2024-08-13 23:57:13,090][flwr][INFO] - 
[2024-08-13 23:57:13,090][flwr][INFO] - [ROUND 271]
[2024-08-13 23:57:13,090][flwr][INFO] - configure_fit: strategy sampled 10 clients (out of 20)
[2024-08-13 23:57:13,248][flwr][ERROR] - Traceback (most recent call last):
  File "/home/abdenour/miniconda3/envs/fl_with_flower/lib/python3.11/site-packages/flwr/simulation/ray_transport/ray_client_proxy.py", line 94, in _submit_job
    out_mssg, updated_context = self.actor_pool.get_client_result(
                                ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/abdenour/miniconda3/envs/fl_with_flower/lib/python3.11/site-packages/flwr/simulation/ray_transport/ray_actor.py", line 398, in get_client_result
    return self._fetch_future_result(cid)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/abdenour/miniconda3/envs/fl_with_flower/lib/python3.11/site-packages/flwr/simulation/ray_transport/ray_actor.py", line 279, in _fetch_future_result
    res_cid, out_mssg, updated_context = ray.get(
                                         ^^^^^^^^
  File "/home/abdenour/miniconda3/envs/fl_with_flower/lib/python3.11/site-packages/ray/_private/auto_init_hook.py", line 21, in auto_init_wrapper
    return fn(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^
  File "/home/abdenour/miniconda3/envs/fl_with_flower/lib/python3.11/site-packages/ray/_private/client_mode_hook.py", line 103, in wrapper
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "/home/abdenour/miniconda3/envs/fl_with_flower/lib/python3.11/site-packages/ray/_private/worker.py", line 2667, in get
    values, debugger_breakpoint = worker.get_objects(object_refs, timeout=timeout)
                                  ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/abdenour/miniconda3/envs/fl_with_flower/lib/python3.11/site-packages/ray/_private/worker.py", line 866, in get_objects
    raise value
ray.exceptions.OutOfMemoryError: Task was killed due to the node running low on memory.
Memory on the node (IP: 115.145.171.128, ID: c687613ff34cdbf5097699e5c896055ef2dc44343c5ab44bd40309d5) where the task (actor ID: 85928d3f1a247800081c0e1101000000, name=ClientAppActor.__init__, pid=619614, memory used=1.73GB) was running was 29.50GB / 31.02GB (0.951049), which exceeds the memory usage threshold of 0.95. Ray killed this worker (ID: c7f644ac3cca20dffbfa1723daac4f87f04c7de03da25e7cad5fe33c) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 115.145.171.128`. To see the logs of the worker, use `ray logs worker-c7f644ac3cca20dffbfa1723daac4f87f04c7de03da25e7cad5fe33c*out -ip 115.145.171.128. Top 10 memory users:
PID	MEM(GB)	COMMAND
4942	3.49	/snap/pycharm-professional/408/jbr/bin/java -classpath /snap/pycharm-professional/408/lib/platform-l...
619616	1.80	ray::ClientAppActor.run
619615	1.78	ray::ClientAppActor
619617	1.77	ray::ClientAppActor
619611	1.73	ray::ClientAppActor
619618	1.73	ray::ClientAppActor
619614	1.73	ray::ClientAppActor
619612	1.72	ray::ClientAppActor
619619	1.70	ray::ClientAppActor
619620	1.68	ray::ClientAppActor.run
Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.

[2024-08-13 23:57:13,248][flwr][ERROR] - Task was killed due to the node running low on memory.
Memory on the node (IP: 115.145.171.128, ID: c687613ff34cdbf5097699e5c896055ef2dc44343c5ab44bd40309d5) where the task (actor ID: 85928d3f1a247800081c0e1101000000, name=ClientAppActor.__init__, pid=619614, memory used=1.73GB) was running was 29.50GB / 31.02GB (0.951049), which exceeds the memory usage threshold of 0.95. Ray killed this worker (ID: c7f644ac3cca20dffbfa1723daac4f87f04c7de03da25e7cad5fe33c) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 115.145.171.128`. To see the logs of the worker, use `ray logs worker-c7f644ac3cca20dffbfa1723daac4f87f04c7de03da25e7cad5fe33c*out -ip 115.145.171.128. Top 10 memory users:
PID	MEM(GB)	COMMAND
4942	3.49	/snap/pycharm-professional/408/jbr/bin/java -classpath /snap/pycharm-professional/408/lib/platform-l...
619616	1.80	ray::ClientAppActor.run
619615	1.78	ray::ClientAppActor
619617	1.77	ray::ClientAppActor
619611	1.73	ray::ClientAppActor
619618	1.73	ray::ClientAppActor
619614	1.73	ray::ClientAppActor
619612	1.72	ray::ClientAppActor
619619	1.70	ray::ClientAppActor
619620	1.68	ray::ClientAppActor.run
Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.
[2024-08-13 23:57:13,249][flwr][ERROR] - Traceback (most recent call last):
  File "/home/abdenour/miniconda3/envs/fl_with_flower/lib/python3.11/site-packages/flwr/simulation/ray_transport/ray_client_proxy.py", line 94, in _submit_job
    out_mssg, updated_context = self.actor_pool.get_client_result(
                                ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/abdenour/miniconda3/envs/fl_with_flower/lib/python3.11/site-packages/flwr/simulation/ray_transport/ray_actor.py", line 398, in get_client_result
    return self._fetch_future_result(cid)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/abdenour/miniconda3/envs/fl_with_flower/lib/python3.11/site-packages/flwr/simulation/ray_transport/ray_actor.py", line 279, in _fetch_future_result
    res_cid, out_mssg, updated_context = ray.get(
                                         ^^^^^^^^
  File "/home/abdenour/miniconda3/envs/fl_with_flower/lib/python3.11/site-packages/ray/_private/auto_init_hook.py", line 21, in auto_init_wrapper
    return fn(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^
  File "/home/abdenour/miniconda3/envs/fl_with_flower/lib/python3.11/site-packages/ray/_private/client_mode_hook.py", line 103, in wrapper
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "/home/abdenour/miniconda3/envs/fl_with_flower/lib/python3.11/site-packages/ray/_private/worker.py", line 2667, in get
    values, debugger_breakpoint = worker.get_objects(object_refs, timeout=timeout)
                                  ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/abdenour/miniconda3/envs/fl_with_flower/lib/python3.11/site-packages/ray/_private/worker.py", line 866, in get_objects
    raise value
ray.exceptions.OutOfMemoryError: Task was killed due to the node running low on memory.
Memory on the node (IP: 115.145.171.128, ID: c687613ff34cdbf5097699e5c896055ef2dc44343c5ab44bd40309d5) where the task (actor ID: 2a8bb25e35bf7bb5a0a6c00401000000, name=ClientAppActor.__init__, pid=619620, memory used=1.97GB) was running was 29.48GB / 31.02GB (0.950487), which exceeds the memory usage threshold of 0.95. Ray killed this worker (ID: 38a4d571c3fa7ff405dd9f5d1555b4753efb95dc84bb70ce01b6ecb4) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 115.145.171.128`. To see the logs of the worker, use `ray logs worker-38a4d571c3fa7ff405dd9f5d1555b4753efb95dc84bb70ce01b6ecb4*out -ip 115.145.171.128. Top 10 memory users:
PID	MEM(GB)	COMMAND
4942	3.48	/snap/pycharm-professional/408/jbr/bin/java -classpath /snap/pycharm-professional/408/lib/platform-l...
616443	2.85	/home/abdenour/miniconda3/envs/fl_with_flower/bin/python /home/abdenour/PycharmProjects/fl_with_flow...
619615	2.06	ray::ClientAppActor
619616	2.04	ray::ClientAppActor
619611	2.04	ray::ClientAppActor.run
619619	2.03	ray::ClientAppActor.run
619612	1.98	ray::ClientAppActor.run
619613	1.97	ray::ClientAppActor.run
619620	1.97	ray::ClientAppActor
619617	1.96	ray::ClientAppActor
Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.

[2024-08-13 23:57:13,250][flwr][ERROR] - Traceback (most recent call last):
  File "/home/abdenour/miniconda3/envs/fl_with_flower/lib/python3.11/site-packages/flwr/simulation/ray_transport/ray_client_proxy.py", line 94, in _submit_job
    out_mssg, updated_context = self.actor_pool.get_client_result(
                                ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/abdenour/miniconda3/envs/fl_with_flower/lib/python3.11/site-packages/flwr/simulation/ray_transport/ray_actor.py", line 398, in get_client_result
    return self._fetch_future_result(cid)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/abdenour/miniconda3/envs/fl_with_flower/lib/python3.11/site-packages/flwr/simulation/ray_transport/ray_actor.py", line 279, in _fetch_future_result
    res_cid, out_mssg, updated_context = ray.get(
                                         ^^^^^^^^
  File "/home/abdenour/miniconda3/envs/fl_with_flower/lib/python3.11/site-packages/ray/_private/auto_init_hook.py", line 21, in auto_init_wrapper
    return fn(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^
  File "/home/abdenour/miniconda3/envs/fl_with_flower/lib/python3.11/site-packages/ray/_private/client_mode_hook.py", line 103, in wrapper
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "/home/abdenour/miniconda3/envs/fl_with_flower/lib/python3.11/site-packages/ray/_private/worker.py", line 2667, in get
    values, debugger_breakpoint = worker.get_objects(object_refs, timeout=timeout)
                                  ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/abdenour/miniconda3/envs/fl_with_flower/lib/python3.11/site-packages/ray/_private/worker.py", line 866, in get_objects
    raise value
ray.exceptions.OutOfMemoryError: Task was killed due to the node running low on memory.
Memory on the node (IP: 115.145.171.128, ID: c687613ff34cdbf5097699e5c896055ef2dc44343c5ab44bd40309d5) where the task (actor ID: 52365bb7997979e5a4c7ddbd01000000, name=ClientAppActor.__init__, pid=619618, memory used=1.84GB) was running was 29.49GB / 31.02GB (0.950707), which exceeds the memory usage threshold of 0.95. Ray killed this worker (ID: 7e48cd46c72821b17be65b05a4e61b5846ba998a57d76db44ca4f8fa) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 115.145.171.128`. To see the logs of the worker, use `ray logs worker-7e48cd46c72821b17be65b05a4e61b5846ba998a57d76db44ca4f8fa*out -ip 115.145.171.128. Top 10 memory users:
PID	MEM(GB)	COMMAND
4942	3.48	/snap/pycharm-professional/408/jbr/bin/java -classpath /snap/pycharm-professional/408/lib/platform-l...
616443	2.14	/home/abdenour/miniconda3/envs/fl_with_flower/bin/python /home/abdenour/PycharmProjects/fl_with_flow...
619615	1.90	ray::ClientAppActor
619616	1.88	ray::ClientAppActor
619611	1.87	ray::ClientAppActor
619617	1.86	ray::ClientAppActor
619620	1.85	ray::ClientAppActor
619618	1.84	ray::ClientAppActor
619613	1.82	ray::ClientAppActor
619619	1.80	ray::ClientAppActor
Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.

[2024-08-13 23:57:13,250][flwr][ERROR] - Task was killed due to the node running low on memory.
Memory on the node (IP: 115.145.171.128, ID: c687613ff34cdbf5097699e5c896055ef2dc44343c5ab44bd40309d5) where the task (actor ID: 2a8bb25e35bf7bb5a0a6c00401000000, name=ClientAppActor.__init__, pid=619620, memory used=1.97GB) was running was 29.48GB / 31.02GB (0.950487), which exceeds the memory usage threshold of 0.95. Ray killed this worker (ID: 38a4d571c3fa7ff405dd9f5d1555b4753efb95dc84bb70ce01b6ecb4) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 115.145.171.128`. To see the logs of the worker, use `ray logs worker-38a4d571c3fa7ff405dd9f5d1555b4753efb95dc84bb70ce01b6ecb4*out -ip 115.145.171.128. Top 10 memory users:
PID	MEM(GB)	COMMAND
4942	3.48	/snap/pycharm-professional/408/jbr/bin/java -classpath /snap/pycharm-professional/408/lib/platform-l...
616443	2.85	/home/abdenour/miniconda3/envs/fl_with_flower/bin/python /home/abdenour/PycharmProjects/fl_with_flow...
619615	2.06	ray::ClientAppActor
619616	2.04	ray::ClientAppActor
619611	2.04	ray::ClientAppActor.run
619619	2.03	ray::ClientAppActor.run
619612	1.98	ray::ClientAppActor.run
619613	1.97	ray::ClientAppActor.run
619620	1.97	ray::ClientAppActor
619617	1.96	ray::ClientAppActor
Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.
[2024-08-13 23:57:13,250][flwr][ERROR] - Task was killed due to the node running low on memory.
Memory on the node (IP: 115.145.171.128, ID: c687613ff34cdbf5097699e5c896055ef2dc44343c5ab44bd40309d5) where the task (actor ID: 52365bb7997979e5a4c7ddbd01000000, name=ClientAppActor.__init__, pid=619618, memory used=1.84GB) was running was 29.49GB / 31.02GB (0.950707), which exceeds the memory usage threshold of 0.95. Ray killed this worker (ID: 7e48cd46c72821b17be65b05a4e61b5846ba998a57d76db44ca4f8fa) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 115.145.171.128`. To see the logs of the worker, use `ray logs worker-7e48cd46c72821b17be65b05a4e61b5846ba998a57d76db44ca4f8fa*out -ip 115.145.171.128. Top 10 memory users:
PID	MEM(GB)	COMMAND
4942	3.48	/snap/pycharm-professional/408/jbr/bin/java -classpath /snap/pycharm-professional/408/lib/platform-l...
616443	2.14	/home/abdenour/miniconda3/envs/fl_with_flower/bin/python /home/abdenour/PycharmProjects/fl_with_flow...
619615	1.90	ray::ClientAppActor
619616	1.88	ray::ClientAppActor
619611	1.87	ray::ClientAppActor
619617	1.86	ray::ClientAppActor
619620	1.85	ray::ClientAppActor
619618	1.84	ray::ClientAppActor
619613	1.82	ray::ClientAppActor
619619	1.80	ray::ClientAppActor
Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.
[2024-08-13 23:57:14,718][flwr][INFO] - aggregate_fit: received 7 results and 3 failures
[2024-08-13 23:57:17,679][flwr][INFO] - fit progress: (271, 16.86074721813202, {'accuracy': 0.7516025641025641}, 1558.9397916610178)
[2024-08-13 23:57:17,680][flwr][INFO] - configure_evaluate: strategy sampled 2 clients (out of 20)
[2024-08-13 23:57:17,892][flwr][INFO] - aggregate_evaluate: received 2 results and 0 failures
[2024-08-13 23:57:17,893][flwr][INFO] - 
[2024-08-13 23:57:17,893][flwr][INFO] - [ROUND 272]
[2024-08-13 23:57:17,893][flwr][INFO] - configure_fit: strategy sampled 10 clients (out of 20)
[2024-08-13 23:57:18,063][flwr][ERROR] - Traceback (most recent call last):
  File "/home/abdenour/miniconda3/envs/fl_with_flower/lib/python3.11/site-packages/flwr/simulation/ray_transport/ray_client_proxy.py", line 94, in _submit_job
    out_mssg, updated_context = self.actor_pool.get_client_result(
                                ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/abdenour/miniconda3/envs/fl_with_flower/lib/python3.11/site-packages/flwr/simulation/ray_transport/ray_actor.py", line 398, in get_client_result
    return self._fetch_future_result(cid)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/abdenour/miniconda3/envs/fl_with_flower/lib/python3.11/site-packages/flwr/simulation/ray_transport/ray_actor.py", line 279, in _fetch_future_result
    res_cid, out_mssg, updated_context = ray.get(
                                         ^^^^^^^^
  File "/home/abdenour/miniconda3/envs/fl_with_flower/lib/python3.11/site-packages/ray/_private/auto_init_hook.py", line 21, in auto_init_wrapper
    return fn(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^
  File "/home/abdenour/miniconda3/envs/fl_with_flower/lib/python3.11/site-packages/ray/_private/client_mode_hook.py", line 103, in wrapper
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "/home/abdenour/miniconda3/envs/fl_with_flower/lib/python3.11/site-packages/ray/_private/worker.py", line 2667, in get
    values, debugger_breakpoint = worker.get_objects(object_refs, timeout=timeout)
                                  ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/abdenour/miniconda3/envs/fl_with_flower/lib/python3.11/site-packages/ray/_private/worker.py", line 866, in get_objects
    raise value
ray.exceptions.OutOfMemoryError: Task was killed due to the node running low on memory.
Memory on the node (IP: 115.145.171.128, ID: c687613ff34cdbf5097699e5c896055ef2dc44343c5ab44bd40309d5) where the task (actor ID: 2a8bb25e35bf7bb5a0a6c00401000000, name=ClientAppActor.__init__, pid=619620, memory used=1.97GB) was running was 29.48GB / 31.02GB (0.950487), which exceeds the memory usage threshold of 0.95. Ray killed this worker (ID: 38a4d571c3fa7ff405dd9f5d1555b4753efb95dc84bb70ce01b6ecb4) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 115.145.171.128`. To see the logs of the worker, use `ray logs worker-38a4d571c3fa7ff405dd9f5d1555b4753efb95dc84bb70ce01b6ecb4*out -ip 115.145.171.128. Top 10 memory users:
PID	MEM(GB)	COMMAND
4942	3.48	/snap/pycharm-professional/408/jbr/bin/java -classpath /snap/pycharm-professional/408/lib/platform-l...
616443	2.85	/home/abdenour/miniconda3/envs/fl_with_flower/bin/python /home/abdenour/PycharmProjects/fl_with_flow...
619615	2.06	ray::ClientAppActor
619616	2.04	ray::ClientAppActor
619611	2.04	ray::ClientAppActor.run
619619	2.03	ray::ClientAppActor.run
619612	1.98	ray::ClientAppActor.run
619613	1.97	ray::ClientAppActor.run
619620	1.97	ray::ClientAppActor
619617	1.96	ray::ClientAppActor
Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.

[2024-08-13 23:57:18,064][flwr][ERROR] - Task was killed due to the node running low on memory.
Memory on the node (IP: 115.145.171.128, ID: c687613ff34cdbf5097699e5c896055ef2dc44343c5ab44bd40309d5) where the task (actor ID: 2a8bb25e35bf7bb5a0a6c00401000000, name=ClientAppActor.__init__, pid=619620, memory used=1.97GB) was running was 29.48GB / 31.02GB (0.950487), which exceeds the memory usage threshold of 0.95. Ray killed this worker (ID: 38a4d571c3fa7ff405dd9f5d1555b4753efb95dc84bb70ce01b6ecb4) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 115.145.171.128`. To see the logs of the worker, use `ray logs worker-38a4d571c3fa7ff405dd9f5d1555b4753efb95dc84bb70ce01b6ecb4*out -ip 115.145.171.128. Top 10 memory users:
PID	MEM(GB)	COMMAND
4942	3.48	/snap/pycharm-professional/408/jbr/bin/java -classpath /snap/pycharm-professional/408/lib/platform-l...
616443	2.85	/home/abdenour/miniconda3/envs/fl_with_flower/bin/python /home/abdenour/PycharmProjects/fl_with_flow...
619615	2.06	ray::ClientAppActor
619616	2.04	ray::ClientAppActor
619611	2.04	ray::ClientAppActor.run
619619	2.03	ray::ClientAppActor.run
619612	1.98	ray::ClientAppActor.run
619613	1.97	ray::ClientAppActor.run
619620	1.97	ray::ClientAppActor
619617	1.96	ray::ClientAppActor
Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.
[2024-08-13 23:57:18,064][flwr][ERROR] - Traceback (most recent call last):
  File "/home/abdenour/miniconda3/envs/fl_with_flower/lib/python3.11/site-packages/flwr/simulation/ray_transport/ray_client_proxy.py", line 94, in _submit_job
    out_mssg, updated_context = self.actor_pool.get_client_result(
                                ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/abdenour/miniconda3/envs/fl_with_flower/lib/python3.11/site-packages/flwr/simulation/ray_transport/ray_actor.py", line 398, in get_client_result
    return self._fetch_future_result(cid)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/abdenour/miniconda3/envs/fl_with_flower/lib/python3.11/site-packages/flwr/simulation/ray_transport/ray_actor.py", line 279, in _fetch_future_result
    res_cid, out_mssg, updated_context = ray.get(
                                         ^^^^^^^^
  File "/home/abdenour/miniconda3/envs/fl_with_flower/lib/python3.11/site-packages/ray/_private/auto_init_hook.py", line 21, in auto_init_wrapper
    return fn(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^
  File "/home/abdenour/miniconda3/envs/fl_with_flower/lib/python3.11/site-packages/ray/_private/client_mode_hook.py", line 103, in wrapper
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "/home/abdenour/miniconda3/envs/fl_with_flower/lib/python3.11/site-packages/ray/_private/worker.py", line 2667, in get
    values, debugger_breakpoint = worker.get_objects(object_refs, timeout=timeout)
                                  ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/abdenour/miniconda3/envs/fl_with_flower/lib/python3.11/site-packages/ray/_private/worker.py", line 866, in get_objects
    raise value
ray.exceptions.OutOfMemoryError: Task was killed due to the node running low on memory.
Memory on the node (IP: 115.145.171.128, ID: c687613ff34cdbf5097699e5c896055ef2dc44343c5ab44bd40309d5) where the task (actor ID: 52365bb7997979e5a4c7ddbd01000000, name=ClientAppActor.__init__, pid=619618, memory used=1.84GB) was running was 29.49GB / 31.02GB (0.950707), which exceeds the memory usage threshold of 0.95. Ray killed this worker (ID: 7e48cd46c72821b17be65b05a4e61b5846ba998a57d76db44ca4f8fa) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 115.145.171.128`. To see the logs of the worker, use `ray logs worker-7e48cd46c72821b17be65b05a4e61b5846ba998a57d76db44ca4f8fa*out -ip 115.145.171.128. Top 10 memory users:
PID	MEM(GB)	COMMAND
4942	3.48	/snap/pycharm-professional/408/jbr/bin/java -classpath /snap/pycharm-professional/408/lib/platform-l...
616443	2.14	/home/abdenour/miniconda3/envs/fl_with_flower/bin/python /home/abdenour/PycharmProjects/fl_with_flow...
619615	1.90	ray::ClientAppActor
619616	1.88	ray::ClientAppActor
619611	1.87	ray::ClientAppActor
619617	1.86	ray::ClientAppActor
619620	1.85	ray::ClientAppActor
619618	1.84	ray::ClientAppActor
619613	1.82	ray::ClientAppActor
619619	1.80	ray::ClientAppActor
Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.

[2024-08-13 23:57:18,064][flwr][ERROR] - Task was killed due to the node running low on memory.
Memory on the node (IP: 115.145.171.128, ID: c687613ff34cdbf5097699e5c896055ef2dc44343c5ab44bd40309d5) where the task (actor ID: 52365bb7997979e5a4c7ddbd01000000, name=ClientAppActor.__init__, pid=619618, memory used=1.84GB) was running was 29.49GB / 31.02GB (0.950707), which exceeds the memory usage threshold of 0.95. Ray killed this worker (ID: 7e48cd46c72821b17be65b05a4e61b5846ba998a57d76db44ca4f8fa) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 115.145.171.128`. To see the logs of the worker, use `ray logs worker-7e48cd46c72821b17be65b05a4e61b5846ba998a57d76db44ca4f8fa*out -ip 115.145.171.128. Top 10 memory users:
PID	MEM(GB)	COMMAND
4942	3.48	/snap/pycharm-professional/408/jbr/bin/java -classpath /snap/pycharm-professional/408/lib/platform-l...
616443	2.14	/home/abdenour/miniconda3/envs/fl_with_flower/bin/python /home/abdenour/PycharmProjects/fl_with_flow...
619615	1.90	ray::ClientAppActor
619616	1.88	ray::ClientAppActor
619611	1.87	ray::ClientAppActor
619617	1.86	ray::ClientAppActor
619620	1.85	ray::ClientAppActor
619618	1.84	ray::ClientAppActor
619613	1.82	ray::ClientAppActor
619619	1.80	ray::ClientAppActor
Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.
[2024-08-13 23:57:18,065][flwr][ERROR] - Traceback (most recent call last):
  File "/home/abdenour/miniconda3/envs/fl_with_flower/lib/python3.11/site-packages/flwr/simulation/ray_transport/ray_client_proxy.py", line 94, in _submit_job
    out_mssg, updated_context = self.actor_pool.get_client_result(
                                ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/abdenour/miniconda3/envs/fl_with_flower/lib/python3.11/site-packages/flwr/simulation/ray_transport/ray_actor.py", line 398, in get_client_result
    return self._fetch_future_result(cid)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/abdenour/miniconda3/envs/fl_with_flower/lib/python3.11/site-packages/flwr/simulation/ray_transport/ray_actor.py", line 279, in _fetch_future_result
    res_cid, out_mssg, updated_context = ray.get(
                                         ^^^^^^^^
  File "/home/abdenour/miniconda3/envs/fl_with_flower/lib/python3.11/site-packages/ray/_private/auto_init_hook.py", line 21, in auto_init_wrapper
    return fn(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^
  File "/home/abdenour/miniconda3/envs/fl_with_flower/lib/python3.11/site-packages/ray/_private/client_mode_hook.py", line 103, in wrapper
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "/home/abdenour/miniconda3/envs/fl_with_flower/lib/python3.11/site-packages/ray/_private/worker.py", line 2667, in get
    values, debugger_breakpoint = worker.get_objects(object_refs, timeout=timeout)
                                  ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/abdenour/miniconda3/envs/fl_with_flower/lib/python3.11/site-packages/ray/_private/worker.py", line 866, in get_objects
    raise value
ray.exceptions.OutOfMemoryError: Task was killed due to the node running low on memory.
Memory on the node (IP: 115.145.171.128, ID: c687613ff34cdbf5097699e5c896055ef2dc44343c5ab44bd40309d5) where the task (actor ID: 85928d3f1a247800081c0e1101000000, name=ClientAppActor.__init__, pid=619614, memory used=1.73GB) was running was 29.50GB / 31.02GB (0.951049), which exceeds the memory usage threshold of 0.95. Ray killed this worker (ID: c7f644ac3cca20dffbfa1723daac4f87f04c7de03da25e7cad5fe33c) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 115.145.171.128`. To see the logs of the worker, use `ray logs worker-c7f644ac3cca20dffbfa1723daac4f87f04c7de03da25e7cad5fe33c*out -ip 115.145.171.128. Top 10 memory users:
PID	MEM(GB)	COMMAND
4942	3.49	/snap/pycharm-professional/408/jbr/bin/java -classpath /snap/pycharm-professional/408/lib/platform-l...
619616	1.80	ray::ClientAppActor.run
619615	1.78	ray::ClientAppActor
619617	1.77	ray::ClientAppActor
619611	1.73	ray::ClientAppActor
619618	1.73	ray::ClientAppActor
619614	1.73	ray::ClientAppActor
619612	1.72	ray::ClientAppActor
619619	1.70	ray::ClientAppActor
619620	1.68	ray::ClientAppActor.run
Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.

[2024-08-13 23:57:18,065][flwr][ERROR] - Task was killed due to the node running low on memory.
Memory on the node (IP: 115.145.171.128, ID: c687613ff34cdbf5097699e5c896055ef2dc44343c5ab44bd40309d5) where the task (actor ID: 85928d3f1a247800081c0e1101000000, name=ClientAppActor.__init__, pid=619614, memory used=1.73GB) was running was 29.50GB / 31.02GB (0.951049), which exceeds the memory usage threshold of 0.95. Ray killed this worker (ID: c7f644ac3cca20dffbfa1723daac4f87f04c7de03da25e7cad5fe33c) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 115.145.171.128`. To see the logs of the worker, use `ray logs worker-c7f644ac3cca20dffbfa1723daac4f87f04c7de03da25e7cad5fe33c*out -ip 115.145.171.128. Top 10 memory users:
PID	MEM(GB)	COMMAND
4942	3.49	/snap/pycharm-professional/408/jbr/bin/java -classpath /snap/pycharm-professional/408/lib/platform-l...
619616	1.80	ray::ClientAppActor.run
619615	1.78	ray::ClientAppActor
619617	1.77	ray::ClientAppActor
619611	1.73	ray::ClientAppActor
619618	1.73	ray::ClientAppActor
619614	1.73	ray::ClientAppActor
619612	1.72	ray::ClientAppActor
619619	1.70	ray::ClientAppActor
619620	1.68	ray::ClientAppActor.run
Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.
[2024-08-13 23:57:19,474][flwr][INFO] - aggregate_fit: received 7 results and 3 failures
[2024-08-13 23:57:22,548][flwr][INFO] - fit progress: (272, 16.061472058296204, {'accuracy': 0.7580128205128205}, 1563.8082232990128)
[2024-08-13 23:57:22,548][flwr][INFO] - configure_evaluate: strategy sampled 2 clients (out of 20)
[2024-08-13 23:57:22,718][flwr][INFO] - aggregate_evaluate: received 2 results and 0 failures
[2024-08-13 23:57:22,718][flwr][INFO] - 
[2024-08-13 23:57:22,718][flwr][INFO] - [ROUND 273]
[2024-08-13 23:57:22,718][flwr][INFO] - configure_fit: strategy sampled 10 clients (out of 20)
[2024-08-13 23:57:22,868][flwr][ERROR] - Traceback (most recent call last):
  File "/home/abdenour/miniconda3/envs/fl_with_flower/lib/python3.11/site-packages/flwr/simulation/ray_transport/ray_client_proxy.py", line 94, in _submit_job
    out_mssg, updated_context = self.actor_pool.get_client_result(
                                ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/abdenour/miniconda3/envs/fl_with_flower/lib/python3.11/site-packages/flwr/simulation/ray_transport/ray_actor.py", line 398, in get_client_result
    return self._fetch_future_result(cid)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/abdenour/miniconda3/envs/fl_with_flower/lib/python3.11/site-packages/flwr/simulation/ray_transport/ray_actor.py", line 279, in _fetch_future_result
    res_cid, out_mssg, updated_context = ray.get(
                                         ^^^^^^^^
  File "/home/abdenour/miniconda3/envs/fl_with_flower/lib/python3.11/site-packages/ray/_private/auto_init_hook.py", line 21, in auto_init_wrapper
    return fn(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^
  File "/home/abdenour/miniconda3/envs/fl_with_flower/lib/python3.11/site-packages/ray/_private/client_mode_hook.py", line 103, in wrapper
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "/home/abdenour/miniconda3/envs/fl_with_flower/lib/python3.11/site-packages/ray/_private/worker.py", line 2667, in get
    values, debugger_breakpoint = worker.get_objects(object_refs, timeout=timeout)
                                  ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/abdenour/miniconda3/envs/fl_with_flower/lib/python3.11/site-packages/ray/_private/worker.py", line 866, in get_objects
    raise value
ray.exceptions.OutOfMemoryError: Task was killed due to the node running low on memory.
Memory on the node (IP: 115.145.171.128, ID: c687613ff34cdbf5097699e5c896055ef2dc44343c5ab44bd40309d5) where the task (actor ID: 52365bb7997979e5a4c7ddbd01000000, name=ClientAppActor.__init__, pid=619618, memory used=1.84GB) was running was 29.49GB / 31.02GB (0.950707), which exceeds the memory usage threshold of 0.95. Ray killed this worker (ID: 7e48cd46c72821b17be65b05a4e61b5846ba998a57d76db44ca4f8fa) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 115.145.171.128`. To see the logs of the worker, use `ray logs worker-7e48cd46c72821b17be65b05a4e61b5846ba998a57d76db44ca4f8fa*out -ip 115.145.171.128. Top 10 memory users:
PID	MEM(GB)	COMMAND
4942	3.48	/snap/pycharm-professional/408/jbr/bin/java -classpath /snap/pycharm-professional/408/lib/platform-l...
616443	2.14	/home/abdenour/miniconda3/envs/fl_with_flower/bin/python /home/abdenour/PycharmProjects/fl_with_flow...
619615	1.90	ray::ClientAppActor
619616	1.88	ray::ClientAppActor
619611	1.87	ray::ClientAppActor
619617	1.86	ray::ClientAppActor
619620	1.85	ray::ClientAppActor
619618	1.84	ray::ClientAppActor
619613	1.82	ray::ClientAppActor
619619	1.80	ray::ClientAppActor
Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.

[2024-08-13 23:57:22,871][flwr][ERROR] - Task was killed due to the node running low on memory.
Memory on the node (IP: 115.145.171.128, ID: c687613ff34cdbf5097699e5c896055ef2dc44343c5ab44bd40309d5) where the task (actor ID: 52365bb7997979e5a4c7ddbd01000000, name=ClientAppActor.__init__, pid=619618, memory used=1.84GB) was running was 29.49GB / 31.02GB (0.950707), which exceeds the memory usage threshold of 0.95. Ray killed this worker (ID: 7e48cd46c72821b17be65b05a4e61b5846ba998a57d76db44ca4f8fa) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 115.145.171.128`. To see the logs of the worker, use `ray logs worker-7e48cd46c72821b17be65b05a4e61b5846ba998a57d76db44ca4f8fa*out -ip 115.145.171.128. Top 10 memory users:
PID	MEM(GB)	COMMAND
4942	3.48	/snap/pycharm-professional/408/jbr/bin/java -classpath /snap/pycharm-professional/408/lib/platform-l...
616443	2.14	/home/abdenour/miniconda3/envs/fl_with_flower/bin/python /home/abdenour/PycharmProjects/fl_with_flow...
619615	1.90	ray::ClientAppActor
619616	1.88	ray::ClientAppActor
619611	1.87	ray::ClientAppActor
619617	1.86	ray::ClientAppActor
619620	1.85	ray::ClientAppActor
619618	1.84	ray::ClientAppActor
619613	1.82	ray::ClientAppActor
619619	1.80	ray::ClientAppActor
Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.
[2024-08-13 23:57:22,876][flwr][ERROR] - Traceback (most recent call last):
  File "/home/abdenour/miniconda3/envs/fl_with_flower/lib/python3.11/site-packages/flwr/simulation/ray_transport/ray_client_proxy.py", line 94, in _submit_job
    out_mssg, updated_context = self.actor_pool.get_client_result(
                                ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/abdenour/miniconda3/envs/fl_with_flower/lib/python3.11/site-packages/flwr/simulation/ray_transport/ray_actor.py", line 398, in get_client_result
    return self._fetch_future_result(cid)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/abdenour/miniconda3/envs/fl_with_flower/lib/python3.11/site-packages/flwr/simulation/ray_transport/ray_actor.py", line 279, in _fetch_future_result
    res_cid, out_mssg, updated_context = ray.get(
                                         ^^^^^^^^
  File "/home/abdenour/miniconda3/envs/fl_with_flower/lib/python3.11/site-packages/ray/_private/auto_init_hook.py", line 21, in auto_init_wrapper
    return fn(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^
  File "/home/abdenour/miniconda3/envs/fl_with_flower/lib/python3.11/site-packages/ray/_private/client_mode_hook.py", line 103, in wrapper
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "/home/abdenour/miniconda3/envs/fl_with_flower/lib/python3.11/site-packages/ray/_private/worker.py", line 2667, in get
    values, debugger_breakpoint = worker.get_objects(object_refs, timeout=timeout)
                                  ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/abdenour/miniconda3/envs/fl_with_flower/lib/python3.11/site-packages/ray/_private/worker.py", line 866, in get_objects
    raise value
ray.exceptions.OutOfMemoryError: Task was killed due to the node running low on memory.
Memory on the node (IP: 115.145.171.128, ID: c687613ff34cdbf5097699e5c896055ef2dc44343c5ab44bd40309d5) where the task (actor ID: 85928d3f1a247800081c0e1101000000, name=ClientAppActor.__init__, pid=619614, memory used=1.73GB) was running was 29.50GB / 31.02GB (0.951049), which exceeds the memory usage threshold of 0.95. Ray killed this worker (ID: c7f644ac3cca20dffbfa1723daac4f87f04c7de03da25e7cad5fe33c) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 115.145.171.128`. To see the logs of the worker, use `ray logs worker-c7f644ac3cca20dffbfa1723daac4f87f04c7de03da25e7cad5fe33c*out -ip 115.145.171.128. Top 10 memory users:
PID	MEM(GB)	COMMAND
4942	3.49	/snap/pycharm-professional/408/jbr/bin/java -classpath /snap/pycharm-professional/408/lib/platform-l...
619616	1.80	ray::ClientAppActor.run
619615	1.78	ray::ClientAppActor
619617	1.77	ray::ClientAppActor
619611	1.73	ray::ClientAppActor
619618	1.73	ray::ClientAppActor
619614	1.73	ray::ClientAppActor
619612	1.72	ray::ClientAppActor
619619	1.70	ray::ClientAppActor
619620	1.68	ray::ClientAppActor.run
Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.

[2024-08-13 23:57:22,876][flwr][ERROR] - Task was killed due to the node running low on memory.
Memory on the node (IP: 115.145.171.128, ID: c687613ff34cdbf5097699e5c896055ef2dc44343c5ab44bd40309d5) where the task (actor ID: 85928d3f1a247800081c0e1101000000, name=ClientAppActor.__init__, pid=619614, memory used=1.73GB) was running was 29.50GB / 31.02GB (0.951049), which exceeds the memory usage threshold of 0.95. Ray killed this worker (ID: c7f644ac3cca20dffbfa1723daac4f87f04c7de03da25e7cad5fe33c) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 115.145.171.128`. To see the logs of the worker, use `ray logs worker-c7f644ac3cca20dffbfa1723daac4f87f04c7de03da25e7cad5fe33c*out -ip 115.145.171.128. Top 10 memory users:
PID	MEM(GB)	COMMAND
4942	3.49	/snap/pycharm-professional/408/jbr/bin/java -classpath /snap/pycharm-professional/408/lib/platform-l...
619616	1.80	ray::ClientAppActor.run
619615	1.78	ray::ClientAppActor
619617	1.77	ray::ClientAppActor
619611	1.73	ray::ClientAppActor
619618	1.73	ray::ClientAppActor
619614	1.73	ray::ClientAppActor
619612	1.72	ray::ClientAppActor
619619	1.70	ray::ClientAppActor
619620	1.68	ray::ClientAppActor.run
Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.
[2024-08-13 23:57:22,877][flwr][ERROR] - Traceback (most recent call last):
  File "/home/abdenour/miniconda3/envs/fl_with_flower/lib/python3.11/site-packages/flwr/simulation/ray_transport/ray_client_proxy.py", line 94, in _submit_job
    out_mssg, updated_context = self.actor_pool.get_client_result(
                                ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/abdenour/miniconda3/envs/fl_with_flower/lib/python3.11/site-packages/flwr/simulation/ray_transport/ray_actor.py", line 398, in get_client_result
    return self._fetch_future_result(cid)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/abdenour/miniconda3/envs/fl_with_flower/lib/python3.11/site-packages/flwr/simulation/ray_transport/ray_actor.py", line 279, in _fetch_future_result
    res_cid, out_mssg, updated_context = ray.get(
                                         ^^^^^^^^
  File "/home/abdenour/miniconda3/envs/fl_with_flower/lib/python3.11/site-packages/ray/_private/auto_init_hook.py", line 21, in auto_init_wrapper
    return fn(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^
  File "/home/abdenour/miniconda3/envs/fl_with_flower/lib/python3.11/site-packages/ray/_private/client_mode_hook.py", line 103, in wrapper
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "/home/abdenour/miniconda3/envs/fl_with_flower/lib/python3.11/site-packages/ray/_private/worker.py", line 2667, in get
    values, debugger_breakpoint = worker.get_objects(object_refs, timeout=timeout)
                                  ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/abdenour/miniconda3/envs/fl_with_flower/lib/python3.11/site-packages/ray/_private/worker.py", line 866, in get_objects
    raise value
ray.exceptions.OutOfMemoryError: Task was killed due to the node running low on memory.
Memory on the node (IP: 115.145.171.128, ID: c687613ff34cdbf5097699e5c896055ef2dc44343c5ab44bd40309d5) where the task (actor ID: 52365bb7997979e5a4c7ddbd01000000, name=ClientAppActor.__init__, pid=619618, memory used=1.84GB) was running was 29.49GB / 31.02GB (0.950707), which exceeds the memory usage threshold of 0.95. Ray killed this worker (ID: 7e48cd46c72821b17be65b05a4e61b5846ba998a57d76db44ca4f8fa) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 115.145.171.128`. To see the logs of the worker, use `ray logs worker-7e48cd46c72821b17be65b05a4e61b5846ba998a57d76db44ca4f8fa*out -ip 115.145.171.128. Top 10 memory users:
PID	MEM(GB)	COMMAND
4942	3.48	/snap/pycharm-professional/408/jbr/bin/java -classpath /snap/pycharm-professional/408/lib/platform-l...
616443	2.14	/home/abdenour/miniconda3/envs/fl_with_flower/bin/python /home/abdenour/PycharmProjects/fl_with_flow...
619615	1.90	ray::ClientAppActor
619616	1.88	ray::ClientAppActor
619611	1.87	ray::ClientAppActor
619617	1.86	ray::ClientAppActor
619620	1.85	ray::ClientAppActor
619618	1.84	ray::ClientAppActor
619613	1.82	ray::ClientAppActor
619619	1.80	ray::ClientAppActor
Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.

[2024-08-13 23:57:22,877][flwr][ERROR] - Task was killed due to the node running low on memory.
Memory on the node (IP: 115.145.171.128, ID: c687613ff34cdbf5097699e5c896055ef2dc44343c5ab44bd40309d5) where the task (actor ID: 52365bb7997979e5a4c7ddbd01000000, name=ClientAppActor.__init__, pid=619618, memory used=1.84GB) was running was 29.49GB / 31.02GB (0.950707), which exceeds the memory usage threshold of 0.95. Ray killed this worker (ID: 7e48cd46c72821b17be65b05a4e61b5846ba998a57d76db44ca4f8fa) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 115.145.171.128`. To see the logs of the worker, use `ray logs worker-7e48cd46c72821b17be65b05a4e61b5846ba998a57d76db44ca4f8fa*out -ip 115.145.171.128. Top 10 memory users:
PID	MEM(GB)	COMMAND
4942	3.48	/snap/pycharm-professional/408/jbr/bin/java -classpath /snap/pycharm-professional/408/lib/platform-l...
616443	2.14	/home/abdenour/miniconda3/envs/fl_with_flower/bin/python /home/abdenour/PycharmProjects/fl_with_flow...
619615	1.90	ray::ClientAppActor
619616	1.88	ray::ClientAppActor
619611	1.87	ray::ClientAppActor
619617	1.86	ray::ClientAppActor
619620	1.85	ray::ClientAppActor
619618	1.84	ray::ClientAppActor
619613	1.82	ray::ClientAppActor
619619	1.80	ray::ClientAppActor
Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.
[2024-08-13 23:57:24,285][flwr][INFO] - aggregate_fit: received 7 results and 3 failures
[2024-08-13 23:57:27,480][flwr][INFO] - fit progress: (273, 18.083288967609406, {'accuracy': 0.7467948717948718}, 1568.7402265979908)
[2024-08-13 23:57:27,480][flwr][INFO] - configure_evaluate: strategy sampled 2 clients (out of 20)
[2024-08-13 23:57:27,873][flwr][INFO] - aggregate_evaluate: received 2 results and 0 failures
[2024-08-13 23:57:27,873][flwr][INFO] - 
[2024-08-13 23:57:27,873][flwr][INFO] - [ROUND 274]
[2024-08-13 23:57:27,873][flwr][INFO] - configure_fit: strategy sampled 10 clients (out of 20)
[2024-08-13 23:57:28,026][flwr][ERROR] - Traceback (most recent call last):
  File "/home/abdenour/miniconda3/envs/fl_with_flower/lib/python3.11/site-packages/flwr/simulation/ray_transport/ray_client_proxy.py", line 94, in _submit_job
    out_mssg, updated_context = self.actor_pool.get_client_result(
                                ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/abdenour/miniconda3/envs/fl_with_flower/lib/python3.11/site-packages/flwr/simulation/ray_transport/ray_actor.py", line 398, in get_client_result
    return self._fetch_future_result(cid)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/abdenour/miniconda3/envs/fl_with_flower/lib/python3.11/site-packages/flwr/simulation/ray_transport/ray_actor.py", line 279, in _fetch_future_result
    res_cid, out_mssg, updated_context = ray.get(
                                         ^^^^^^^^
  File "/home/abdenour/miniconda3/envs/fl_with_flower/lib/python3.11/site-packages/ray/_private/auto_init_hook.py", line 21, in auto_init_wrapper
    return fn(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^
  File "/home/abdenour/miniconda3/envs/fl_with_flower/lib/python3.11/site-packages/ray/_private/client_mode_hook.py", line 103, in wrapper
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "/home/abdenour/miniconda3/envs/fl_with_flower/lib/python3.11/site-packages/ray/_private/worker.py", line 2667, in get
    values, debugger_breakpoint = worker.get_objects(object_refs, timeout=timeout)
                                  ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/abdenour/miniconda3/envs/fl_with_flower/lib/python3.11/site-packages/ray/_private/worker.py", line 866, in get_objects
    raise value
ray.exceptions.OutOfMemoryError: Task was killed due to the node running low on memory.
Memory on the node (IP: 115.145.171.128, ID: c687613ff34cdbf5097699e5c896055ef2dc44343c5ab44bd40309d5) where the task (actor ID: 85928d3f1a247800081c0e1101000000, name=ClientAppActor.__init__, pid=619614, memory used=1.73GB) was running was 29.50GB / 31.02GB (0.951049), which exceeds the memory usage threshold of 0.95. Ray killed this worker (ID: c7f644ac3cca20dffbfa1723daac4f87f04c7de03da25e7cad5fe33c) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 115.145.171.128`. To see the logs of the worker, use `ray logs worker-c7f644ac3cca20dffbfa1723daac4f87f04c7de03da25e7cad5fe33c*out -ip 115.145.171.128. Top 10 memory users:
PID	MEM(GB)	COMMAND
4942	3.49	/snap/pycharm-professional/408/jbr/bin/java -classpath /snap/pycharm-professional/408/lib/platform-l...
619616	1.80	ray::ClientAppActor.run
619615	1.78	ray::ClientAppActor
619617	1.77	ray::ClientAppActor
619611	1.73	ray::ClientAppActor
619618	1.73	ray::ClientAppActor
619614	1.73	ray::ClientAppActor
619612	1.72	ray::ClientAppActor
619619	1.70	ray::ClientAppActor
619620	1.68	ray::ClientAppActor.run
Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.

[2024-08-13 23:57:28,026][flwr][ERROR] - Task was killed due to the node running low on memory.
Memory on the node (IP: 115.145.171.128, ID: c687613ff34cdbf5097699e5c896055ef2dc44343c5ab44bd40309d5) where the task (actor ID: 85928d3f1a247800081c0e1101000000, name=ClientAppActor.__init__, pid=619614, memory used=1.73GB) was running was 29.50GB / 31.02GB (0.951049), which exceeds the memory usage threshold of 0.95. Ray killed this worker (ID: c7f644ac3cca20dffbfa1723daac4f87f04c7de03da25e7cad5fe33c) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 115.145.171.128`. To see the logs of the worker, use `ray logs worker-c7f644ac3cca20dffbfa1723daac4f87f04c7de03da25e7cad5fe33c*out -ip 115.145.171.128. Top 10 memory users:
PID	MEM(GB)	COMMAND
4942	3.49	/snap/pycharm-professional/408/jbr/bin/java -classpath /snap/pycharm-professional/408/lib/platform-l...
619616	1.80	ray::ClientAppActor.run
619615	1.78	ray::ClientAppActor
619617	1.77	ray::ClientAppActor
619611	1.73	ray::ClientAppActor
619618	1.73	ray::ClientAppActor
619614	1.73	ray::ClientAppActor
619612	1.72	ray::ClientAppActor
619619	1.70	ray::ClientAppActor
619620	1.68	ray::ClientAppActor.run
Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.
[2024-08-13 23:57:28,027][flwr][ERROR] - Traceback (most recent call last):
  File "/home/abdenour/miniconda3/envs/fl_with_flower/lib/python3.11/site-packages/flwr/simulation/ray_transport/ray_client_proxy.py", line 94, in _submit_job
    out_mssg, updated_context = self.actor_pool.get_client_result(
                                ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/abdenour/miniconda3/envs/fl_with_flower/lib/python3.11/site-packages/flwr/simulation/ray_transport/ray_actor.py", line 398, in get_client_result
    return self._fetch_future_result(cid)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/abdenour/miniconda3/envs/fl_with_flower/lib/python3.11/site-packages/flwr/simulation/ray_transport/ray_actor.py", line 279, in _fetch_future_result
    res_cid, out_mssg, updated_context = ray.get(
                                         ^^^^^^^^
  File "/home/abdenour/miniconda3/envs/fl_with_flower/lib/python3.11/site-packages/ray/_private/auto_init_hook.py", line 21, in auto_init_wrapper
    return fn(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^
  File "/home/abdenour/miniconda3/envs/fl_with_flower/lib/python3.11/site-packages/ray/_private/client_mode_hook.py", line 103, in wrapper
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "/home/abdenour/miniconda3/envs/fl_with_flower/lib/python3.11/site-packages/ray/_private/worker.py", line 2667, in get
    values, debugger_breakpoint = worker.get_objects(object_refs, timeout=timeout)
                                  ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/abdenour/miniconda3/envs/fl_with_flower/lib/python3.11/site-packages/ray/_private/worker.py", line 866, in get_objects
    raise value
ray.exceptions.OutOfMemoryError: Task was killed due to the node running low on memory.
Memory on the node (IP: 115.145.171.128, ID: c687613ff34cdbf5097699e5c896055ef2dc44343c5ab44bd40309d5) where the task (actor ID: 52365bb7997979e5a4c7ddbd01000000, name=ClientAppActor.__init__, pid=619618, memory used=1.84GB) was running was 29.49GB / 31.02GB (0.950707), which exceeds the memory usage threshold of 0.95. Ray killed this worker (ID: 7e48cd46c72821b17be65b05a4e61b5846ba998a57d76db44ca4f8fa) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 115.145.171.128`. To see the logs of the worker, use `ray logs worker-7e48cd46c72821b17be65b05a4e61b5846ba998a57d76db44ca4f8fa*out -ip 115.145.171.128. Top 10 memory users:
PID	MEM(GB)	COMMAND
4942	3.48	/snap/pycharm-professional/408/jbr/bin/java -classpath /snap/pycharm-professional/408/lib/platform-l...
616443	2.14	/home/abdenour/miniconda3/envs/fl_with_flower/bin/python /home/abdenour/PycharmProjects/fl_with_flow...
619615	1.90	ray::ClientAppActor
619616	1.88	ray::ClientAppActor
619611	1.87	ray::ClientAppActor
619617	1.86	ray::ClientAppActor
619620	1.85	ray::ClientAppActor
619618	1.84	ray::ClientAppActor
619613	1.82	ray::ClientAppActor
619619	1.80	ray::ClientAppActor
Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.

[2024-08-13 23:57:28,027][flwr][ERROR] - Task was killed due to the node running low on memory.
Memory on the node (IP: 115.145.171.128, ID: c687613ff34cdbf5097699e5c896055ef2dc44343c5ab44bd40309d5) where the task (actor ID: 52365bb7997979e5a4c7ddbd01000000, name=ClientAppActor.__init__, pid=619618, memory used=1.84GB) was running was 29.49GB / 31.02GB (0.950707), which exceeds the memory usage threshold of 0.95. Ray killed this worker (ID: 7e48cd46c72821b17be65b05a4e61b5846ba998a57d76db44ca4f8fa) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 115.145.171.128`. To see the logs of the worker, use `ray logs worker-7e48cd46c72821b17be65b05a4e61b5846ba998a57d76db44ca4f8fa*out -ip 115.145.171.128. Top 10 memory users:
PID	MEM(GB)	COMMAND
4942	3.48	/snap/pycharm-professional/408/jbr/bin/java -classpath /snap/pycharm-professional/408/lib/platform-l...
616443	2.14	/home/abdenour/miniconda3/envs/fl_with_flower/bin/python /home/abdenour/PycharmProjects/fl_with_flow...
619615	1.90	ray::ClientAppActor
619616	1.88	ray::ClientAppActor
619611	1.87	ray::ClientAppActor
619617	1.86	ray::ClientAppActor
619620	1.85	ray::ClientAppActor
619618	1.84	ray::ClientAppActor
619613	1.82	ray::ClientAppActor
619619	1.80	ray::ClientAppActor
Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.
[2024-08-13 23:57:28,031][flwr][ERROR] - Traceback (most recent call last):
  File "/home/abdenour/miniconda3/envs/fl_with_flower/lib/python3.11/site-packages/flwr/simulation/ray_transport/ray_client_proxy.py", line 94, in _submit_job
    out_mssg, updated_context = self.actor_pool.get_client_result(
                                ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/abdenour/miniconda3/envs/fl_with_flower/lib/python3.11/site-packages/flwr/simulation/ray_transport/ray_actor.py", line 398, in get_client_result
    return self._fetch_future_result(cid)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/abdenour/miniconda3/envs/fl_with_flower/lib/python3.11/site-packages/flwr/simulation/ray_transport/ray_actor.py", line 279, in _fetch_future_result
    res_cid, out_mssg, updated_context = ray.get(
                                         ^^^^^^^^
  File "/home/abdenour/miniconda3/envs/fl_with_flower/lib/python3.11/site-packages/ray/_private/auto_init_hook.py", line 21, in auto_init_wrapper
    return fn(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^
  File "/home/abdenour/miniconda3/envs/fl_with_flower/lib/python3.11/site-packages/ray/_private/client_mode_hook.py", line 103, in wrapper
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "/home/abdenour/miniconda3/envs/fl_with_flower/lib/python3.11/site-packages/ray/_private/worker.py", line 2667, in get
    values, debugger_breakpoint = worker.get_objects(object_refs, timeout=timeout)
                                  ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/abdenour/miniconda3/envs/fl_with_flower/lib/python3.11/site-packages/ray/_private/worker.py", line 866, in get_objects
    raise value
ray.exceptions.OutOfMemoryError: Task was killed due to the node running low on memory.
Memory on the node (IP: 115.145.171.128, ID: c687613ff34cdbf5097699e5c896055ef2dc44343c5ab44bd40309d5) where the task (actor ID: 85928d3f1a247800081c0e1101000000, name=ClientAppActor.__init__, pid=619614, memory used=1.73GB) was running was 29.50GB / 31.02GB (0.951049), which exceeds the memory usage threshold of 0.95. Ray killed this worker (ID: c7f644ac3cca20dffbfa1723daac4f87f04c7de03da25e7cad5fe33c) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 115.145.171.128`. To see the logs of the worker, use `ray logs worker-c7f644ac3cca20dffbfa1723daac4f87f04c7de03da25e7cad5fe33c*out -ip 115.145.171.128. Top 10 memory users:
PID	MEM(GB)	COMMAND
4942	3.49	/snap/pycharm-professional/408/jbr/bin/java -classpath /snap/pycharm-professional/408/lib/platform-l...
619616	1.80	ray::ClientAppActor.run
619615	1.78	ray::ClientAppActor
619617	1.77	ray::ClientAppActor
619611	1.73	ray::ClientAppActor
619618	1.73	ray::ClientAppActor
619614	1.73	ray::ClientAppActor
619612	1.72	ray::ClientAppActor
619619	1.70	ray::ClientAppActor
619620	1.68	ray::ClientAppActor.run
Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.

[2024-08-13 23:57:28,031][flwr][ERROR] - Task was killed due to the node running low on memory.
Memory on the node (IP: 115.145.171.128, ID: c687613ff34cdbf5097699e5c896055ef2dc44343c5ab44bd40309d5) where the task (actor ID: 85928d3f1a247800081c0e1101000000, name=ClientAppActor.__init__, pid=619614, memory used=1.73GB) was running was 29.50GB / 31.02GB (0.951049), which exceeds the memory usage threshold of 0.95. Ray killed this worker (ID: c7f644ac3cca20dffbfa1723daac4f87f04c7de03da25e7cad5fe33c) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 115.145.171.128`. To see the logs of the worker, use `ray logs worker-c7f644ac3cca20dffbfa1723daac4f87f04c7de03da25e7cad5fe33c*out -ip 115.145.171.128. Top 10 memory users:
PID	MEM(GB)	COMMAND
4942	3.49	/snap/pycharm-professional/408/jbr/bin/java -classpath /snap/pycharm-professional/408/lib/platform-l...
619616	1.80	ray::ClientAppActor.run
619615	1.78	ray::ClientAppActor
619617	1.77	ray::ClientAppActor
619611	1.73	ray::ClientAppActor
619618	1.73	ray::ClientAppActor
619614	1.73	ray::ClientAppActor
619612	1.72	ray::ClientAppActor
619619	1.70	ray::ClientAppActor
619620	1.68	ray::ClientAppActor.run
Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.
[2024-08-13 23:57:29,732][flwr][INFO] - aggregate_fit: received 7 results and 3 failures
[2024-08-13 23:57:33,403][flwr][INFO] - fit progress: (274, 17.53726154565811, {'accuracy': 0.7516025641025641}, 1574.663438409014)
[2024-08-13 23:57:33,403][flwr][INFO] - configure_evaluate: strategy sampled 2 clients (out of 20)
[2024-08-13 23:57:33,644][flwr][INFO] - aggregate_evaluate: received 2 results and 0 failures
[2024-08-13 23:57:33,644][flwr][INFO] - 
[2024-08-13 23:57:33,644][flwr][INFO] - [ROUND 275]
[2024-08-13 23:57:33,644][flwr][INFO] - configure_fit: strategy sampled 10 clients (out of 20)
[2024-08-13 23:57:33,809][flwr][ERROR] - Traceback (most recent call last):
  File "/home/abdenour/miniconda3/envs/fl_with_flower/lib/python3.11/site-packages/flwr/simulation/ray_transport/ray_client_proxy.py", line 94, in _submit_job
    out_mssg, updated_context = self.actor_pool.get_client_result(
                                ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/abdenour/miniconda3/envs/fl_with_flower/lib/python3.11/site-packages/flwr/simulation/ray_transport/ray_actor.py", line 398, in get_client_result
    return self._fetch_future_result(cid)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/abdenour/miniconda3/envs/fl_with_flower/lib/python3.11/site-packages/flwr/simulation/ray_transport/ray_actor.py", line 279, in _fetch_future_result
    res_cid, out_mssg, updated_context = ray.get(
                                         ^^^^^^^^
  File "/home/abdenour/miniconda3/envs/fl_with_flower/lib/python3.11/site-packages/ray/_private/auto_init_hook.py", line 21, in auto_init_wrapper
    return fn(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^
  File "/home/abdenour/miniconda3/envs/fl_with_flower/lib/python3.11/site-packages/ray/_private/client_mode_hook.py", line 103, in wrapper
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "/home/abdenour/miniconda3/envs/fl_with_flower/lib/python3.11/site-packages/ray/_private/worker.py", line 2667, in get
    values, debugger_breakpoint = worker.get_objects(object_refs, timeout=timeout)
                                  ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/abdenour/miniconda3/envs/fl_with_flower/lib/python3.11/site-packages/ray/_private/worker.py", line 866, in get_objects
    raise value
ray.exceptions.OutOfMemoryError: Task was killed due to the node running low on memory.
Memory on the node (IP: 115.145.171.128, ID: c687613ff34cdbf5097699e5c896055ef2dc44343c5ab44bd40309d5) where the task (actor ID: 85928d3f1a247800081c0e1101000000, name=ClientAppActor.__init__, pid=619614, memory used=1.73GB) was running was 29.50GB / 31.02GB (0.951049), which exceeds the memory usage threshold of 0.95. Ray killed this worker (ID: c7f644ac3cca20dffbfa1723daac4f87f04c7de03da25e7cad5fe33c) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 115.145.171.128`. To see the logs of the worker, use `ray logs worker-c7f644ac3cca20dffbfa1723daac4f87f04c7de03da25e7cad5fe33c*out -ip 115.145.171.128. Top 10 memory users:
PID	MEM(GB)	COMMAND
4942	3.49	/snap/pycharm-professional/408/jbr/bin/java -classpath /snap/pycharm-professional/408/lib/platform-l...
619616	1.80	ray::ClientAppActor.run
619615	1.78	ray::ClientAppActor
619617	1.77	ray::ClientAppActor
619611	1.73	ray::ClientAppActor
619618	1.73	ray::ClientAppActor
619614	1.73	ray::ClientAppActor
619612	1.72	ray::ClientAppActor
619619	1.70	ray::ClientAppActor
619620	1.68	ray::ClientAppActor.run
Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.

[2024-08-13 23:57:33,809][flwr][ERROR] - Traceback (most recent call last):
  File "/home/abdenour/miniconda3/envs/fl_with_flower/lib/python3.11/site-packages/flwr/simulation/ray_transport/ray_client_proxy.py", line 94, in _submit_job
    out_mssg, updated_context = self.actor_pool.get_client_result(
                                ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/abdenour/miniconda3/envs/fl_with_flower/lib/python3.11/site-packages/flwr/simulation/ray_transport/ray_actor.py", line 398, in get_client_result
    return self._fetch_future_result(cid)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/abdenour/miniconda3/envs/fl_with_flower/lib/python3.11/site-packages/flwr/simulation/ray_transport/ray_actor.py", line 279, in _fetch_future_result
    res_cid, out_mssg, updated_context = ray.get(
                                         ^^^^^^^^
  File "/home/abdenour/miniconda3/envs/fl_with_flower/lib/python3.11/site-packages/ray/_private/auto_init_hook.py", line 21, in auto_init_wrapper
    return fn(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^
  File "/home/abdenour/miniconda3/envs/fl_with_flower/lib/python3.11/site-packages/ray/_private/client_mode_hook.py", line 103, in wrapper
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "/home/abdenour/miniconda3/envs/fl_with_flower/lib/python3.11/site-packages/ray/_private/worker.py", line 2667, in get
    values, debugger_breakpoint = worker.get_objects(object_refs, timeout=timeout)
                                  ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/abdenour/miniconda3/envs/fl_with_flower/lib/python3.11/site-packages/ray/_private/worker.py", line 866, in get_objects
    raise value
ray.exceptions.OutOfMemoryError: Task was killed due to the node running low on memory.
Memory on the node (IP: 115.145.171.128, ID: c687613ff34cdbf5097699e5c896055ef2dc44343c5ab44bd40309d5) where the task (actor ID: 2a8bb25e35bf7bb5a0a6c00401000000, name=ClientAppActor.__init__, pid=619620, memory used=1.97GB) was running was 29.48GB / 31.02GB (0.950487), which exceeds the memory usage threshold of 0.95. Ray killed this worker (ID: 38a4d571c3fa7ff405dd9f5d1555b4753efb95dc84bb70ce01b6ecb4) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 115.145.171.128`. To see the logs of the worker, use `ray logs worker-38a4d571c3fa7ff405dd9f5d1555b4753efb95dc84bb70ce01b6ecb4*out -ip 115.145.171.128. Top 10 memory users:
PID	MEM(GB)	COMMAND
4942	3.48	/snap/pycharm-professional/408/jbr/bin/java -classpath /snap/pycharm-professional/408/lib/platform-l...
616443	2.85	/home/abdenour/miniconda3/envs/fl_with_flower/bin/python /home/abdenour/PycharmProjects/fl_with_flow...
619615	2.06	ray::ClientAppActor
619616	2.04	ray::ClientAppActor
619611	2.04	ray::ClientAppActor.run
619619	2.03	ray::ClientAppActor.run
619612	1.98	ray::ClientAppActor.run
619613	1.97	ray::ClientAppActor.run
619620	1.97	ray::ClientAppActor
619617	1.96	ray::ClientAppActor
Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.

[2024-08-13 23:57:33,809][flwr][ERROR] - Task was killed due to the node running low on memory.
Memory on the node (IP: 115.145.171.128, ID: c687613ff34cdbf5097699e5c896055ef2dc44343c5ab44bd40309d5) where the task (actor ID: 85928d3f1a247800081c0e1101000000, name=ClientAppActor.__init__, pid=619614, memory used=1.73GB) was running was 29.50GB / 31.02GB (0.951049), which exceeds the memory usage threshold of 0.95. Ray killed this worker (ID: c7f644ac3cca20dffbfa1723daac4f87f04c7de03da25e7cad5fe33c) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 115.145.171.128`. To see the logs of the worker, use `ray logs worker-c7f644ac3cca20dffbfa1723daac4f87f04c7de03da25e7cad5fe33c*out -ip 115.145.171.128. Top 10 memory users:
PID	MEM(GB)	COMMAND
4942	3.49	/snap/pycharm-professional/408/jbr/bin/java -classpath /snap/pycharm-professional/408/lib/platform-l...
619616	1.80	ray::ClientAppActor.run
619615	1.78	ray::ClientAppActor
619617	1.77	ray::ClientAppActor
619611	1.73	ray::ClientAppActor
619618	1.73	ray::ClientAppActor
619614	1.73	ray::ClientAppActor
619612	1.72	ray::ClientAppActor
619619	1.70	ray::ClientAppActor
619620	1.68	ray::ClientAppActor.run
Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.
[2024-08-13 23:57:33,810][flwr][ERROR] - Task was killed due to the node running low on memory.
Memory on the node (IP: 115.145.171.128, ID: c687613ff34cdbf5097699e5c896055ef2dc44343c5ab44bd40309d5) where the task (actor ID: 2a8bb25e35bf7bb5a0a6c00401000000, name=ClientAppActor.__init__, pid=619620, memory used=1.97GB) was running was 29.48GB / 31.02GB (0.950487), which exceeds the memory usage threshold of 0.95. Ray killed this worker (ID: 38a4d571c3fa7ff405dd9f5d1555b4753efb95dc84bb70ce01b6ecb4) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 115.145.171.128`. To see the logs of the worker, use `ray logs worker-38a4d571c3fa7ff405dd9f5d1555b4753efb95dc84bb70ce01b6ecb4*out -ip 115.145.171.128. Top 10 memory users:
PID	MEM(GB)	COMMAND
4942	3.48	/snap/pycharm-professional/408/jbr/bin/java -classpath /snap/pycharm-professional/408/lib/platform-l...
616443	2.85	/home/abdenour/miniconda3/envs/fl_with_flower/bin/python /home/abdenour/PycharmProjects/fl_with_flow...
619615	2.06	ray::ClientAppActor
619616	2.04	ray::ClientAppActor
619611	2.04	ray::ClientAppActor.run
619619	2.03	ray::ClientAppActor.run
619612	1.98	ray::ClientAppActor.run
619613	1.97	ray::ClientAppActor.run
619620	1.97	ray::ClientAppActor
619617	1.96	ray::ClientAppActor
Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.
[2024-08-13 23:57:33,810][flwr][ERROR] - Traceback (most recent call last):
  File "/home/abdenour/miniconda3/envs/fl_with_flower/lib/python3.11/site-packages/flwr/simulation/ray_transport/ray_client_proxy.py", line 94, in _submit_job
    out_mssg, updated_context = self.actor_pool.get_client_result(
                                ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/abdenour/miniconda3/envs/fl_with_flower/lib/python3.11/site-packages/flwr/simulation/ray_transport/ray_actor.py", line 398, in get_client_result
    return self._fetch_future_result(cid)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/abdenour/miniconda3/envs/fl_with_flower/lib/python3.11/site-packages/flwr/simulation/ray_transport/ray_actor.py", line 279, in _fetch_future_result
    res_cid, out_mssg, updated_context = ray.get(
                                         ^^^^^^^^
  File "/home/abdenour/miniconda3/envs/fl_with_flower/lib/python3.11/site-packages/ray/_private/auto_init_hook.py", line 21, in auto_init_wrapper
    return fn(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^
  File "/home/abdenour/miniconda3/envs/fl_with_flower/lib/python3.11/site-packages/ray/_private/client_mode_hook.py", line 103, in wrapper
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "/home/abdenour/miniconda3/envs/fl_with_flower/lib/python3.11/site-packages/ray/_private/worker.py", line 2667, in get
    values, debugger_breakpoint = worker.get_objects(object_refs, timeout=timeout)
                                  ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/abdenour/miniconda3/envs/fl_with_flower/lib/python3.11/site-packages/ray/_private/worker.py", line 866, in get_objects
    raise value
ray.exceptions.OutOfMemoryError: Task was killed due to the node running low on memory.
Memory on the node (IP: 115.145.171.128, ID: c687613ff34cdbf5097699e5c896055ef2dc44343c5ab44bd40309d5) where the task (actor ID: 52365bb7997979e5a4c7ddbd01000000, name=ClientAppActor.__init__, pid=619618, memory used=1.84GB) was running was 29.49GB / 31.02GB (0.950707), which exceeds the memory usage threshold of 0.95. Ray killed this worker (ID: 7e48cd46c72821b17be65b05a4e61b5846ba998a57d76db44ca4f8fa) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 115.145.171.128`. To see the logs of the worker, use `ray logs worker-7e48cd46c72821b17be65b05a4e61b5846ba998a57d76db44ca4f8fa*out -ip 115.145.171.128. Top 10 memory users:
PID	MEM(GB)	COMMAND
4942	3.48	/snap/pycharm-professional/408/jbr/bin/java -classpath /snap/pycharm-professional/408/lib/platform-l...
616443	2.14	/home/abdenour/miniconda3/envs/fl_with_flower/bin/python /home/abdenour/PycharmProjects/fl_with_flow...
619615	1.90	ray::ClientAppActor
619616	1.88	ray::ClientAppActor
619611	1.87	ray::ClientAppActor
619617	1.86	ray::ClientAppActor
619620	1.85	ray::ClientAppActor
619618	1.84	ray::ClientAppActor
619613	1.82	ray::ClientAppActor
619619	1.80	ray::ClientAppActor
Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.

[2024-08-13 23:57:33,810][flwr][ERROR] - Task was killed due to the node running low on memory.
Memory on the node (IP: 115.145.171.128, ID: c687613ff34cdbf5097699e5c896055ef2dc44343c5ab44bd40309d5) where the task (actor ID: 52365bb7997979e5a4c7ddbd01000000, name=ClientAppActor.__init__, pid=619618, memory used=1.84GB) was running was 29.49GB / 31.02GB (0.950707), which exceeds the memory usage threshold of 0.95. Ray killed this worker (ID: 7e48cd46c72821b17be65b05a4e61b5846ba998a57d76db44ca4f8fa) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 115.145.171.128`. To see the logs of the worker, use `ray logs worker-7e48cd46c72821b17be65b05a4e61b5846ba998a57d76db44ca4f8fa*out -ip 115.145.171.128. Top 10 memory users:
PID	MEM(GB)	COMMAND
4942	3.48	/snap/pycharm-professional/408/jbr/bin/java -classpath /snap/pycharm-professional/408/lib/platform-l...
616443	2.14	/home/abdenour/miniconda3/envs/fl_with_flower/bin/python /home/abdenour/PycharmProjects/fl_with_flow...
619615	1.90	ray::ClientAppActor
619616	1.88	ray::ClientAppActor
619611	1.87	ray::ClientAppActor
619617	1.86	ray::ClientAppActor
619620	1.85	ray::ClientAppActor
619618	1.84	ray::ClientAppActor
619613	1.82	ray::ClientAppActor
619619	1.80	ray::ClientAppActor
Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.
[2024-08-13 23:57:35,343][flwr][INFO] - aggregate_fit: received 7 results and 3 failures
[2024-08-13 23:57:38,128][flwr][INFO] - fit progress: (275, 18.209382116794586, {'accuracy': 0.7483974358974359}, 1579.3882751179917)
[2024-08-13 23:57:38,128][flwr][INFO] - configure_evaluate: strategy sampled 2 clients (out of 20)
[2024-08-13 23:57:38,301][flwr][INFO] - aggregate_evaluate: received 2 results and 0 failures
[2024-08-13 23:57:38,301][flwr][INFO] - 
[2024-08-13 23:57:38,301][flwr][INFO] - [ROUND 276]
[2024-08-13 23:57:38,301][flwr][INFO] - configure_fit: strategy sampled 10 clients (out of 20)
[2024-08-13 23:57:38,474][flwr][ERROR] - Traceback (most recent call last):
  File "/home/abdenour/miniconda3/envs/fl_with_flower/lib/python3.11/site-packages/flwr/simulation/ray_transport/ray_client_proxy.py", line 94, in _submit_job
    out_mssg, updated_context = self.actor_pool.get_client_result(
                                ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/abdenour/miniconda3/envs/fl_with_flower/lib/python3.11/site-packages/flwr/simulation/ray_transport/ray_actor.py", line 398, in get_client_result
    return self._fetch_future_result(cid)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/abdenour/miniconda3/envs/fl_with_flower/lib/python3.11/site-packages/flwr/simulation/ray_transport/ray_actor.py", line 279, in _fetch_future_result
    res_cid, out_mssg, updated_context = ray.get(
                                         ^^^^^^^^
  File "/home/abdenour/miniconda3/envs/fl_with_flower/lib/python3.11/site-packages/ray/_private/auto_init_hook.py", line 21, in auto_init_wrapper
    return fn(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^
  File "/home/abdenour/miniconda3/envs/fl_with_flower/lib/python3.11/site-packages/ray/_private/client_mode_hook.py", line 103, in wrapper
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "/home/abdenour/miniconda3/envs/fl_with_flower/lib/python3.11/site-packages/ray/_private/worker.py", line 2667, in get
    values, debugger_breakpoint = worker.get_objects(object_refs, timeout=timeout)
                                  ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/abdenour/miniconda3/envs/fl_with_flower/lib/python3.11/site-packages/ray/_private/worker.py", line 866, in get_objects
    raise value
ray.exceptions.OutOfMemoryError: Task was killed due to the node running low on memory.
Memory on the node (IP: 115.145.171.128, ID: c687613ff34cdbf5097699e5c896055ef2dc44343c5ab44bd40309d5) where the task (actor ID: 52365bb7997979e5a4c7ddbd01000000, name=ClientAppActor.__init__, pid=619618, memory used=1.84GB) was running was 29.49GB / 31.02GB (0.950707), which exceeds the memory usage threshold of 0.95. Ray killed this worker (ID: 7e48cd46c72821b17be65b05a4e61b5846ba998a57d76db44ca4f8fa) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 115.145.171.128`. To see the logs of the worker, use `ray logs worker-7e48cd46c72821b17be65b05a4e61b5846ba998a57d76db44ca4f8fa*out -ip 115.145.171.128. Top 10 memory users:
PID	MEM(GB)	COMMAND
4942	3.48	/snap/pycharm-professional/408/jbr/bin/java -classpath /snap/pycharm-professional/408/lib/platform-l...
616443	2.14	/home/abdenour/miniconda3/envs/fl_with_flower/bin/python /home/abdenour/PycharmProjects/fl_with_flow...
619615	1.90	ray::ClientAppActor
619616	1.88	ray::ClientAppActor
619611	1.87	ray::ClientAppActor
619617	1.86	ray::ClientAppActor
619620	1.85	ray::ClientAppActor
619618	1.84	ray::ClientAppActor
619613	1.82	ray::ClientAppActor
619619	1.80	ray::ClientAppActor
Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.

[2024-08-13 23:57:38,475][flwr][ERROR] - Task was killed due to the node running low on memory.
Memory on the node (IP: 115.145.171.128, ID: c687613ff34cdbf5097699e5c896055ef2dc44343c5ab44bd40309d5) where the task (actor ID: 52365bb7997979e5a4c7ddbd01000000, name=ClientAppActor.__init__, pid=619618, memory used=1.84GB) was running was 29.49GB / 31.02GB (0.950707), which exceeds the memory usage threshold of 0.95. Ray killed this worker (ID: 7e48cd46c72821b17be65b05a4e61b5846ba998a57d76db44ca4f8fa) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 115.145.171.128`. To see the logs of the worker, use `ray logs worker-7e48cd46c72821b17be65b05a4e61b5846ba998a57d76db44ca4f8fa*out -ip 115.145.171.128. Top 10 memory users:
PID	MEM(GB)	COMMAND
4942	3.48	/snap/pycharm-professional/408/jbr/bin/java -classpath /snap/pycharm-professional/408/lib/platform-l...
616443	2.14	/home/abdenour/miniconda3/envs/fl_with_flower/bin/python /home/abdenour/PycharmProjects/fl_with_flow...
619615	1.90	ray::ClientAppActor
619616	1.88	ray::ClientAppActor
619611	1.87	ray::ClientAppActor
619617	1.86	ray::ClientAppActor
619620	1.85	ray::ClientAppActor
619618	1.84	ray::ClientAppActor
619613	1.82	ray::ClientAppActor
619619	1.80	ray::ClientAppActor
Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.
[2024-08-13 23:57:38,476][flwr][ERROR] - Traceback (most recent call last):
  File "/home/abdenour/miniconda3/envs/fl_with_flower/lib/python3.11/site-packages/flwr/simulation/ray_transport/ray_client_proxy.py", line 94, in _submit_job
    out_mssg, updated_context = self.actor_pool.get_client_result(
                                ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/abdenour/miniconda3/envs/fl_with_flower/lib/python3.11/site-packages/flwr/simulation/ray_transport/ray_actor.py", line 398, in get_client_result
    return self._fetch_future_result(cid)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/abdenour/miniconda3/envs/fl_with_flower/lib/python3.11/site-packages/flwr/simulation/ray_transport/ray_actor.py", line 279, in _fetch_future_result
    res_cid, out_mssg, updated_context = ray.get(
                                         ^^^^^^^^
  File "/home/abdenour/miniconda3/envs/fl_with_flower/lib/python3.11/site-packages/ray/_private/auto_init_hook.py", line 21, in auto_init_wrapper
    return fn(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^
  File "/home/abdenour/miniconda3/envs/fl_with_flower/lib/python3.11/site-packages/ray/_private/client_mode_hook.py", line 103, in wrapper
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "/home/abdenour/miniconda3/envs/fl_with_flower/lib/python3.11/site-packages/ray/_private/worker.py", line 2667, in get
    values, debugger_breakpoint = worker.get_objects(object_refs, timeout=timeout)
                                  ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/abdenour/miniconda3/envs/fl_with_flower/lib/python3.11/site-packages/ray/_private/worker.py", line 866, in get_objects
    raise value
ray.exceptions.OutOfMemoryError: Task was killed due to the node running low on memory.
Memory on the node (IP: 115.145.171.128, ID: c687613ff34cdbf5097699e5c896055ef2dc44343c5ab44bd40309d5) where the task (actor ID: 2a8bb25e35bf7bb5a0a6c00401000000, name=ClientAppActor.__init__, pid=619620, memory used=1.97GB) was running was 29.48GB / 31.02GB (0.950487), which exceeds the memory usage threshold of 0.95. Ray killed this worker (ID: 38a4d571c3fa7ff405dd9f5d1555b4753efb95dc84bb70ce01b6ecb4) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 115.145.171.128`. To see the logs of the worker, use `ray logs worker-38a4d571c3fa7ff405dd9f5d1555b4753efb95dc84bb70ce01b6ecb4*out -ip 115.145.171.128. Top 10 memory users:
PID	MEM(GB)	COMMAND
4942	3.48	/snap/pycharm-professional/408/jbr/bin/java -classpath /snap/pycharm-professional/408/lib/platform-l...
616443	2.85	/home/abdenour/miniconda3/envs/fl_with_flower/bin/python /home/abdenour/PycharmProjects/fl_with_flow...
619615	2.06	ray::ClientAppActor
619616	2.04	ray::ClientAppActor
619611	2.04	ray::ClientAppActor.run
619619	2.03	ray::ClientAppActor.run
619612	1.98	ray::ClientAppActor.run
619613	1.97	ray::ClientAppActor.run
619620	1.97	ray::ClientAppActor
619617	1.96	ray::ClientAppActor
Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.

[2024-08-13 23:57:38,476][flwr][ERROR] - Task was killed due to the node running low on memory.
Memory on the node (IP: 115.145.171.128, ID: c687613ff34cdbf5097699e5c896055ef2dc44343c5ab44bd40309d5) where the task (actor ID: 2a8bb25e35bf7bb5a0a6c00401000000, name=ClientAppActor.__init__, pid=619620, memory used=1.97GB) was running was 29.48GB / 31.02GB (0.950487), which exceeds the memory usage threshold of 0.95. Ray killed this worker (ID: 38a4d571c3fa7ff405dd9f5d1555b4753efb95dc84bb70ce01b6ecb4) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 115.145.171.128`. To see the logs of the worker, use `ray logs worker-38a4d571c3fa7ff405dd9f5d1555b4753efb95dc84bb70ce01b6ecb4*out -ip 115.145.171.128. Top 10 memory users:
PID	MEM(GB)	COMMAND
4942	3.48	/snap/pycharm-professional/408/jbr/bin/java -classpath /snap/pycharm-professional/408/lib/platform-l...
616443	2.85	/home/abdenour/miniconda3/envs/fl_with_flower/bin/python /home/abdenour/PycharmProjects/fl_with_flow...
619615	2.06	ray::ClientAppActor
619616	2.04	ray::ClientAppActor
619611	2.04	ray::ClientAppActor.run
619619	2.03	ray::ClientAppActor.run
619612	1.98	ray::ClientAppActor.run
619613	1.97	ray::ClientAppActor.run
619620	1.97	ray::ClientAppActor
619617	1.96	ray::ClientAppActor
Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.
[2024-08-13 23:57:38,477][flwr][ERROR] - Traceback (most recent call last):
  File "/home/abdenour/miniconda3/envs/fl_with_flower/lib/python3.11/site-packages/flwr/simulation/ray_transport/ray_client_proxy.py", line 94, in _submit_job
    out_mssg, updated_context = self.actor_pool.get_client_result(
                                ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/abdenour/miniconda3/envs/fl_with_flower/lib/python3.11/site-packages/flwr/simulation/ray_transport/ray_actor.py", line 398, in get_client_result
    return self._fetch_future_result(cid)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/abdenour/miniconda3/envs/fl_with_flower/lib/python3.11/site-packages/flwr/simulation/ray_transport/ray_actor.py", line 279, in _fetch_future_result
    res_cid, out_mssg, updated_context = ray.get(
                                         ^^^^^^^^
  File "/home/abdenour/miniconda3/envs/fl_with_flower/lib/python3.11/site-packages/ray/_private/auto_init_hook.py", line 21, in auto_init_wrapper
    return fn(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^
  File "/home/abdenour/miniconda3/envs/fl_with_flower/lib/python3.11/site-packages/ray/_private/client_mode_hook.py", line 103, in wrapper
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "/home/abdenour/miniconda3/envs/fl_with_flower/lib/python3.11/site-packages/ray/_private/worker.py", line 2667, in get
    values, debugger_breakpoint = worker.get_objects(object_refs, timeout=timeout)
                                  ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/abdenour/miniconda3/envs/fl_with_flower/lib/python3.11/site-packages/ray/_private/worker.py", line 866, in get_objects
    raise value
ray.exceptions.OutOfMemoryError: Task was killed due to the node running low on memory.
Memory on the node (IP: 115.145.171.128, ID: c687613ff34cdbf5097699e5c896055ef2dc44343c5ab44bd40309d5) where the task (actor ID: 85928d3f1a247800081c0e1101000000, name=ClientAppActor.__init__, pid=619614, memory used=1.73GB) was running was 29.50GB / 31.02GB (0.951049), which exceeds the memory usage threshold of 0.95. Ray killed this worker (ID: c7f644ac3cca20dffbfa1723daac4f87f04c7de03da25e7cad5fe33c) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 115.145.171.128`. To see the logs of the worker, use `ray logs worker-c7f644ac3cca20dffbfa1723daac4f87f04c7de03da25e7cad5fe33c*out -ip 115.145.171.128. Top 10 memory users:
PID	MEM(GB)	COMMAND
4942	3.49	/snap/pycharm-professional/408/jbr/bin/java -classpath /snap/pycharm-professional/408/lib/platform-l...
619616	1.80	ray::ClientAppActor.run
619615	1.78	ray::ClientAppActor
619617	1.77	ray::ClientAppActor
619611	1.73	ray::ClientAppActor
619618	1.73	ray::ClientAppActor
619614	1.73	ray::ClientAppActor
619612	1.72	ray::ClientAppActor
619619	1.70	ray::ClientAppActor
619620	1.68	ray::ClientAppActor.run
Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.

[2024-08-13 23:57:38,477][flwr][ERROR] - Task was killed due to the node running low on memory.
Memory on the node (IP: 115.145.171.128, ID: c687613ff34cdbf5097699e5c896055ef2dc44343c5ab44bd40309d5) where the task (actor ID: 85928d3f1a247800081c0e1101000000, name=ClientAppActor.__init__, pid=619614, memory used=1.73GB) was running was 29.50GB / 31.02GB (0.951049), which exceeds the memory usage threshold of 0.95. Ray killed this worker (ID: c7f644ac3cca20dffbfa1723daac4f87f04c7de03da25e7cad5fe33c) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 115.145.171.128`. To see the logs of the worker, use `ray logs worker-c7f644ac3cca20dffbfa1723daac4f87f04c7de03da25e7cad5fe33c*out -ip 115.145.171.128. Top 10 memory users:
PID	MEM(GB)	COMMAND
4942	3.49	/snap/pycharm-professional/408/jbr/bin/java -classpath /snap/pycharm-professional/408/lib/platform-l...
619616	1.80	ray::ClientAppActor.run
619615	1.78	ray::ClientAppActor
619617	1.77	ray::ClientAppActor
619611	1.73	ray::ClientAppActor
619618	1.73	ray::ClientAppActor
619614	1.73	ray::ClientAppActor
619612	1.72	ray::ClientAppActor
619619	1.70	ray::ClientAppActor
619620	1.68	ray::ClientAppActor.run
Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.
[2024-08-13 23:57:40,009][flwr][INFO] - aggregate_fit: received 7 results and 3 failures
[2024-08-13 23:57:43,237][flwr][INFO] - fit progress: (276, 19.71358960866928, {'accuracy': 0.7387820512820513}, 1584.497661713016)
[2024-08-13 23:57:43,238][flwr][INFO] - configure_evaluate: strategy sampled 2 clients (out of 20)
[2024-08-13 23:57:43,452][flwr][INFO] - aggregate_evaluate: received 2 results and 0 failures
[2024-08-13 23:57:43,452][flwr][INFO] - 
[2024-08-13 23:57:43,452][flwr][INFO] - [ROUND 277]
[2024-08-13 23:57:43,452][flwr][INFO] - configure_fit: strategy sampled 10 clients (out of 20)
[2024-08-13 23:57:43,623][flwr][ERROR] - Traceback (most recent call last):
  File "/home/abdenour/miniconda3/envs/fl_with_flower/lib/python3.11/site-packages/flwr/simulation/ray_transport/ray_client_proxy.py", line 94, in _submit_job
    out_mssg, updated_context = self.actor_pool.get_client_result(
                                ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/abdenour/miniconda3/envs/fl_with_flower/lib/python3.11/site-packages/flwr/simulation/ray_transport/ray_actor.py", line 398, in get_client_result
    return self._fetch_future_result(cid)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/abdenour/miniconda3/envs/fl_with_flower/lib/python3.11/site-packages/flwr/simulation/ray_transport/ray_actor.py", line 279, in _fetch_future_result
    res_cid, out_mssg, updated_context = ray.get(
                                         ^^^^^^^^
  File "/home/abdenour/miniconda3/envs/fl_with_flower/lib/python3.11/site-packages/ray/_private/auto_init_hook.py", line 21, in auto_init_wrapper
    return fn(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^
  File "/home/abdenour/miniconda3/envs/fl_with_flower/lib/python3.11/site-packages/ray/_private/client_mode_hook.py", line 103, in wrapper
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "/home/abdenour/miniconda3/envs/fl_with_flower/lib/python3.11/site-packages/ray/_private/worker.py", line 2667, in get
    values, debugger_breakpoint = worker.get_objects(object_refs, timeout=timeout)
                                  ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/abdenour/miniconda3/envs/fl_with_flower/lib/python3.11/site-packages/ray/_private/worker.py", line 866, in get_objects
    raise value
ray.exceptions.OutOfMemoryError: Task was killed due to the node running low on memory.
Memory on the node (IP: 115.145.171.128, ID: c687613ff34cdbf5097699e5c896055ef2dc44343c5ab44bd40309d5) where the task (actor ID: 85928d3f1a247800081c0e1101000000, name=ClientAppActor.__init__, pid=619614, memory used=1.73GB) was running was 29.50GB / 31.02GB (0.951049), which exceeds the memory usage threshold of 0.95. Ray killed this worker (ID: c7f644ac3cca20dffbfa1723daac4f87f04c7de03da25e7cad5fe33c) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 115.145.171.128`. To see the logs of the worker, use `ray logs worker-c7f644ac3cca20dffbfa1723daac4f87f04c7de03da25e7cad5fe33c*out -ip 115.145.171.128. Top 10 memory users:
PID	MEM(GB)	COMMAND
4942	3.49	/snap/pycharm-professional/408/jbr/bin/java -classpath /snap/pycharm-professional/408/lib/platform-l...
619616	1.80	ray::ClientAppActor.run
619615	1.78	ray::ClientAppActor
619617	1.77	ray::ClientAppActor
619611	1.73	ray::ClientAppActor
619618	1.73	ray::ClientAppActor
619614	1.73	ray::ClientAppActor
619612	1.72	ray::ClientAppActor
619619	1.70	ray::ClientAppActor
619620	1.68	ray::ClientAppActor.run
Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.

[2024-08-13 23:57:43,623][flwr][ERROR] - Task was killed due to the node running low on memory.
Memory on the node (IP: 115.145.171.128, ID: c687613ff34cdbf5097699e5c896055ef2dc44343c5ab44bd40309d5) where the task (actor ID: 85928d3f1a247800081c0e1101000000, name=ClientAppActor.__init__, pid=619614, memory used=1.73GB) was running was 29.50GB / 31.02GB (0.951049), which exceeds the memory usage threshold of 0.95. Ray killed this worker (ID: c7f644ac3cca20dffbfa1723daac4f87f04c7de03da25e7cad5fe33c) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 115.145.171.128`. To see the logs of the worker, use `ray logs worker-c7f644ac3cca20dffbfa1723daac4f87f04c7de03da25e7cad5fe33c*out -ip 115.145.171.128. Top 10 memory users:
PID	MEM(GB)	COMMAND
4942	3.49	/snap/pycharm-professional/408/jbr/bin/java -classpath /snap/pycharm-professional/408/lib/platform-l...
619616	1.80	ray::ClientAppActor.run
619615	1.78	ray::ClientAppActor
619617	1.77	ray::ClientAppActor
619611	1.73	ray::ClientAppActor
619618	1.73	ray::ClientAppActor
619614	1.73	ray::ClientAppActor
619612	1.72	ray::ClientAppActor
619619	1.70	ray::ClientAppActor
619620	1.68	ray::ClientAppActor.run
Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.
[2024-08-13 23:57:43,624][flwr][ERROR] - Traceback (most recent call last):
  File "/home/abdenour/miniconda3/envs/fl_with_flower/lib/python3.11/site-packages/flwr/simulation/ray_transport/ray_client_proxy.py", line 94, in _submit_job
    out_mssg, updated_context = self.actor_pool.get_client_result(
                                ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/abdenour/miniconda3/envs/fl_with_flower/lib/python3.11/site-packages/flwr/simulation/ray_transport/ray_actor.py", line 398, in get_client_result
    return self._fetch_future_result(cid)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/abdenour/miniconda3/envs/fl_with_flower/lib/python3.11/site-packages/flwr/simulation/ray_transport/ray_actor.py", line 279, in _fetch_future_result
    res_cid, out_mssg, updated_context = ray.get(
                                         ^^^^^^^^
  File "/home/abdenour/miniconda3/envs/fl_with_flower/lib/python3.11/site-packages/ray/_private/auto_init_hook.py", line 21, in auto_init_wrapper
    return fn(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^
  File "/home/abdenour/miniconda3/envs/fl_with_flower/lib/python3.11/site-packages/ray/_private/client_mode_hook.py", line 103, in wrapper
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "/home/abdenour/miniconda3/envs/fl_with_flower/lib/python3.11/site-packages/ray/_private/worker.py", line 2667, in get
    values, debugger_breakpoint = worker.get_objects(object_refs, timeout=timeout)
                                  ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/abdenour/miniconda3/envs/fl_with_flower/lib/python3.11/site-packages/ray/_private/worker.py", line 866, in get_objects
    raise value
ray.exceptions.OutOfMemoryError: Task was killed due to the node running low on memory.
Memory on the node (IP: 115.145.171.128, ID: c687613ff34cdbf5097699e5c896055ef2dc44343c5ab44bd40309d5) where the task (actor ID: 2a8bb25e35bf7bb5a0a6c00401000000, name=ClientAppActor.__init__, pid=619620, memory used=1.97GB) was running was 29.48GB / 31.02GB (0.950487), which exceeds the memory usage threshold of 0.95. Ray killed this worker (ID: 38a4d571c3fa7ff405dd9f5d1555b4753efb95dc84bb70ce01b6ecb4) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 115.145.171.128`. To see the logs of the worker, use `ray logs worker-38a4d571c3fa7ff405dd9f5d1555b4753efb95dc84bb70ce01b6ecb4*out -ip 115.145.171.128. Top 10 memory users:
PID	MEM(GB)	COMMAND
4942	3.48	/snap/pycharm-professional/408/jbr/bin/java -classpath /snap/pycharm-professional/408/lib/platform-l...
616443	2.85	/home/abdenour/miniconda3/envs/fl_with_flower/bin/python /home/abdenour/PycharmProjects/fl_with_flow...
619615	2.06	ray::ClientAppActor
619616	2.04	ray::ClientAppActor
619611	2.04	ray::ClientAppActor.run
619619	2.03	ray::ClientAppActor.run
619612	1.98	ray::ClientAppActor.run
619613	1.97	ray::ClientAppActor.run
619620	1.97	ray::ClientAppActor
619617	1.96	ray::ClientAppActor
Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.

[2024-08-13 23:57:43,624][flwr][ERROR] - Task was killed due to the node running low on memory.
Memory on the node (IP: 115.145.171.128, ID: c687613ff34cdbf5097699e5c896055ef2dc44343c5ab44bd40309d5) where the task (actor ID: 2a8bb25e35bf7bb5a0a6c00401000000, name=ClientAppActor.__init__, pid=619620, memory used=1.97GB) was running was 29.48GB / 31.02GB (0.950487), which exceeds the memory usage threshold of 0.95. Ray killed this worker (ID: 38a4d571c3fa7ff405dd9f5d1555b4753efb95dc84bb70ce01b6ecb4) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 115.145.171.128`. To see the logs of the worker, use `ray logs worker-38a4d571c3fa7ff405dd9f5d1555b4753efb95dc84bb70ce01b6ecb4*out -ip 115.145.171.128. Top 10 memory users:
PID	MEM(GB)	COMMAND
4942	3.48	/snap/pycharm-professional/408/jbr/bin/java -classpath /snap/pycharm-professional/408/lib/platform-l...
616443	2.85	/home/abdenour/miniconda3/envs/fl_with_flower/bin/python /home/abdenour/PycharmProjects/fl_with_flow...
619615	2.06	ray::ClientAppActor
619616	2.04	ray::ClientAppActor
619611	2.04	ray::ClientAppActor.run
619619	2.03	ray::ClientAppActor.run
619612	1.98	ray::ClientAppActor.run
619613	1.97	ray::ClientAppActor.run
619620	1.97	ray::ClientAppActor
619617	1.96	ray::ClientAppActor
Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.
[2024-08-13 23:57:43,624][flwr][ERROR] - Traceback (most recent call last):
  File "/home/abdenour/miniconda3/envs/fl_with_flower/lib/python3.11/site-packages/flwr/simulation/ray_transport/ray_client_proxy.py", line 94, in _submit_job
    out_mssg, updated_context = self.actor_pool.get_client_result(
                                ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/abdenour/miniconda3/envs/fl_with_flower/lib/python3.11/site-packages/flwr/simulation/ray_transport/ray_actor.py", line 398, in get_client_result
    return self._fetch_future_result(cid)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/abdenour/miniconda3/envs/fl_with_flower/lib/python3.11/site-packages/flwr/simulation/ray_transport/ray_actor.py", line 279, in _fetch_future_result
    res_cid, out_mssg, updated_context = ray.get(
                                         ^^^^^^^^
  File "/home/abdenour/miniconda3/envs/fl_with_flower/lib/python3.11/site-packages/ray/_private/auto_init_hook.py", line 21, in auto_init_wrapper
    return fn(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^
  File "/home/abdenour/miniconda3/envs/fl_with_flower/lib/python3.11/site-packages/ray/_private/client_mode_hook.py", line 103, in wrapper
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "/home/abdenour/miniconda3/envs/fl_with_flower/lib/python3.11/site-packages/ray/_private/worker.py", line 2667, in get
    values, debugger_breakpoint = worker.get_objects(object_refs, timeout=timeout)
                                  ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/abdenour/miniconda3/envs/fl_with_flower/lib/python3.11/site-packages/ray/_private/worker.py", line 866, in get_objects
    raise value
ray.exceptions.OutOfMemoryError: Task was killed due to the node running low on memory.
Memory on the node (IP: 115.145.171.128, ID: c687613ff34cdbf5097699e5c896055ef2dc44343c5ab44bd40309d5) where the task (actor ID: 52365bb7997979e5a4c7ddbd01000000, name=ClientAppActor.__init__, pid=619618, memory used=1.84GB) was running was 29.49GB / 31.02GB (0.950707), which exceeds the memory usage threshold of 0.95. Ray killed this worker (ID: 7e48cd46c72821b17be65b05a4e61b5846ba998a57d76db44ca4f8fa) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 115.145.171.128`. To see the logs of the worker, use `ray logs worker-7e48cd46c72821b17be65b05a4e61b5846ba998a57d76db44ca4f8fa*out -ip 115.145.171.128. Top 10 memory users:
PID	MEM(GB)	COMMAND
4942	3.48	/snap/pycharm-professional/408/jbr/bin/java -classpath /snap/pycharm-professional/408/lib/platform-l...
616443	2.14	/home/abdenour/miniconda3/envs/fl_with_flower/bin/python /home/abdenour/PycharmProjects/fl_with_flow...
619615	1.90	ray::ClientAppActor
619616	1.88	ray::ClientAppActor
619611	1.87	ray::ClientAppActor
619617	1.86	ray::ClientAppActor
619620	1.85	ray::ClientAppActor
619618	1.84	ray::ClientAppActor
619613	1.82	ray::ClientAppActor
619619	1.80	ray::ClientAppActor
Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.

[2024-08-13 23:57:43,624][flwr][ERROR] - Task was killed due to the node running low on memory.
Memory on the node (IP: 115.145.171.128, ID: c687613ff34cdbf5097699e5c896055ef2dc44343c5ab44bd40309d5) where the task (actor ID: 52365bb7997979e5a4c7ddbd01000000, name=ClientAppActor.__init__, pid=619618, memory used=1.84GB) was running was 29.49GB / 31.02GB (0.950707), which exceeds the memory usage threshold of 0.95. Ray killed this worker (ID: 7e48cd46c72821b17be65b05a4e61b5846ba998a57d76db44ca4f8fa) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 115.145.171.128`. To see the logs of the worker, use `ray logs worker-7e48cd46c72821b17be65b05a4e61b5846ba998a57d76db44ca4f8fa*out -ip 115.145.171.128. Top 10 memory users:
PID	MEM(GB)	COMMAND
4942	3.48	/snap/pycharm-professional/408/jbr/bin/java -classpath /snap/pycharm-professional/408/lib/platform-l...
616443	2.14	/home/abdenour/miniconda3/envs/fl_with_flower/bin/python /home/abdenour/PycharmProjects/fl_with_flow...
619615	1.90	ray::ClientAppActor
619616	1.88	ray::ClientAppActor
619611	1.87	ray::ClientAppActor
619617	1.86	ray::ClientAppActor
619620	1.85	ray::ClientAppActor
619618	1.84	ray::ClientAppActor
619613	1.82	ray::ClientAppActor
619619	1.80	ray::ClientAppActor
Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.
[2024-08-13 23:57:45,100][flwr][INFO] - aggregate_fit: received 7 results and 3 failures
[2024-08-13 23:57:48,099][flwr][INFO] - fit progress: (277, 16.72553288936615, {'accuracy': 0.7532051282051282}, 1589.3596787900024)
[2024-08-13 23:57:48,100][flwr][INFO] - configure_evaluate: strategy sampled 2 clients (out of 20)
[2024-08-13 23:57:48,334][flwr][INFO] - aggregate_evaluate: received 2 results and 0 failures
[2024-08-13 23:57:48,334][flwr][INFO] - 
[2024-08-13 23:57:48,334][flwr][INFO] - [ROUND 278]
[2024-08-13 23:57:48,334][flwr][INFO] - configure_fit: strategy sampled 10 clients (out of 20)
[2024-08-13 23:57:48,484][flwr][ERROR] - Traceback (most recent call last):
  File "/home/abdenour/miniconda3/envs/fl_with_flower/lib/python3.11/site-packages/flwr/simulation/ray_transport/ray_client_proxy.py", line 94, in _submit_job
    out_mssg, updated_context = self.actor_pool.get_client_result(
                                ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/abdenour/miniconda3/envs/fl_with_flower/lib/python3.11/site-packages/flwr/simulation/ray_transport/ray_actor.py", line 398, in get_client_result
    return self._fetch_future_result(cid)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/abdenour/miniconda3/envs/fl_with_flower/lib/python3.11/site-packages/flwr/simulation/ray_transport/ray_actor.py", line 279, in _fetch_future_result
    res_cid, out_mssg, updated_context = ray.get(
                                         ^^^^^^^^
  File "/home/abdenour/miniconda3/envs/fl_with_flower/lib/python3.11/site-packages/ray/_private/auto_init_hook.py", line 21, in auto_init_wrapper
    return fn(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^
  File "/home/abdenour/miniconda3/envs/fl_with_flower/lib/python3.11/site-packages/ray/_private/client_mode_hook.py", line 103, in wrapper
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "/home/abdenour/miniconda3/envs/fl_with_flower/lib/python3.11/site-packages/ray/_private/worker.py", line 2667, in get
    values, debugger_breakpoint = worker.get_objects(object_refs, timeout=timeout)
                                  ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/abdenour/miniconda3/envs/fl_with_flower/lib/python3.11/site-packages/ray/_private/worker.py", line 866, in get_objects
    raise value
ray.exceptions.OutOfMemoryError: Task was killed due to the node running low on memory.
Memory on the node (IP: 115.145.171.128, ID: c687613ff34cdbf5097699e5c896055ef2dc44343c5ab44bd40309d5) where the task (actor ID: 52365bb7997979e5a4c7ddbd01000000, name=ClientAppActor.__init__, pid=619618, memory used=1.84GB) was running was 29.49GB / 31.02GB (0.950707), which exceeds the memory usage threshold of 0.95. Ray killed this worker (ID: 7e48cd46c72821b17be65b05a4e61b5846ba998a57d76db44ca4f8fa) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 115.145.171.128`. To see the logs of the worker, use `ray logs worker-7e48cd46c72821b17be65b05a4e61b5846ba998a57d76db44ca4f8fa*out -ip 115.145.171.128. Top 10 memory users:
PID	MEM(GB)	COMMAND
4942	3.48	/snap/pycharm-professional/408/jbr/bin/java -classpath /snap/pycharm-professional/408/lib/platform-l...
616443	2.14	/home/abdenour/miniconda3/envs/fl_with_flower/bin/python /home/abdenour/PycharmProjects/fl_with_flow...
619615	1.90	ray::ClientAppActor
619616	1.88	ray::ClientAppActor
619611	1.87	ray::ClientAppActor
619617	1.86	ray::ClientAppActor
619620	1.85	ray::ClientAppActor
619618	1.84	ray::ClientAppActor
619613	1.82	ray::ClientAppActor
619619	1.80	ray::ClientAppActor
Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.

[2024-08-13 23:57:48,484][flwr][ERROR] - Task was killed due to the node running low on memory.
Memory on the node (IP: 115.145.171.128, ID: c687613ff34cdbf5097699e5c896055ef2dc44343c5ab44bd40309d5) where the task (actor ID: 52365bb7997979e5a4c7ddbd01000000, name=ClientAppActor.__init__, pid=619618, memory used=1.84GB) was running was 29.49GB / 31.02GB (0.950707), which exceeds the memory usage threshold of 0.95. Ray killed this worker (ID: 7e48cd46c72821b17be65b05a4e61b5846ba998a57d76db44ca4f8fa) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 115.145.171.128`. To see the logs of the worker, use `ray logs worker-7e48cd46c72821b17be65b05a4e61b5846ba998a57d76db44ca4f8fa*out -ip 115.145.171.128. Top 10 memory users:
PID	MEM(GB)	COMMAND
4942	3.48	/snap/pycharm-professional/408/jbr/bin/java -classpath /snap/pycharm-professional/408/lib/platform-l...
616443	2.14	/home/abdenour/miniconda3/envs/fl_with_flower/bin/python /home/abdenour/PycharmProjects/fl_with_flow...
619615	1.90	ray::ClientAppActor
619616	1.88	ray::ClientAppActor
619611	1.87	ray::ClientAppActor
619617	1.86	ray::ClientAppActor
619620	1.85	ray::ClientAppActor
619618	1.84	ray::ClientAppActor
619613	1.82	ray::ClientAppActor
619619	1.80	ray::ClientAppActor
Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.
[2024-08-13 23:57:48,484][flwr][ERROR] - Traceback (most recent call last):
  File "/home/abdenour/miniconda3/envs/fl_with_flower/lib/python3.11/site-packages/flwr/simulation/ray_transport/ray_client_proxy.py", line 94, in _submit_job
    out_mssg, updated_context = self.actor_pool.get_client_result(
                                ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/abdenour/miniconda3/envs/fl_with_flower/lib/python3.11/site-packages/flwr/simulation/ray_transport/ray_actor.py", line 398, in get_client_result
    return self._fetch_future_result(cid)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/abdenour/miniconda3/envs/fl_with_flower/lib/python3.11/site-packages/flwr/simulation/ray_transport/ray_actor.py", line 279, in _fetch_future_result
    res_cid, out_mssg, updated_context = ray.get(
                                         ^^^^^^^^
  File "/home/abdenour/miniconda3/envs/fl_with_flower/lib/python3.11/site-packages/ray/_private/auto_init_hook.py", line 21, in auto_init_wrapper
    return fn(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^
  File "/home/abdenour/miniconda3/envs/fl_with_flower/lib/python3.11/site-packages/ray/_private/client_mode_hook.py", line 103, in wrapper
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "/home/abdenour/miniconda3/envs/fl_with_flower/lib/python3.11/site-packages/ray/_private/worker.py", line 2667, in get
    values, debugger_breakpoint = worker.get_objects(object_refs, timeout=timeout)
                                  ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/abdenour/miniconda3/envs/fl_with_flower/lib/python3.11/site-packages/ray/_private/worker.py", line 866, in get_objects
    raise value
ray.exceptions.OutOfMemoryError: Task was killed due to the node running low on memory.
Memory on the node (IP: 115.145.171.128, ID: c687613ff34cdbf5097699e5c896055ef2dc44343c5ab44bd40309d5) where the task (actor ID: 2a8bb25e35bf7bb5a0a6c00401000000, name=ClientAppActor.__init__, pid=619620, memory used=1.97GB) was running was 29.48GB / 31.02GB (0.950487), which exceeds the memory usage threshold of 0.95. Ray killed this worker (ID: 38a4d571c3fa7ff405dd9f5d1555b4753efb95dc84bb70ce01b6ecb4) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 115.145.171.128`. To see the logs of the worker, use `ray logs worker-38a4d571c3fa7ff405dd9f5d1555b4753efb95dc84bb70ce01b6ecb4*out -ip 115.145.171.128. Top 10 memory users:
PID	MEM(GB)	COMMAND
4942	3.48	/snap/pycharm-professional/408/jbr/bin/java -classpath /snap/pycharm-professional/408/lib/platform-l...
616443	2.85	/home/abdenour/miniconda3/envs/fl_with_flower/bin/python /home/abdenour/PycharmProjects/fl_with_flow...
619615	2.06	ray::ClientAppActor
619616	2.04	ray::ClientAppActor
619611	2.04	ray::ClientAppActor.run
619619	2.03	ray::ClientAppActor.run
619612	1.98	ray::ClientAppActor.run
619613	1.97	ray::ClientAppActor.run
619620	1.97	ray::ClientAppActor
619617	1.96	ray::ClientAppActor
Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.

[2024-08-13 23:57:48,484][flwr][ERROR] - Task was killed due to the node running low on memory.
Memory on the node (IP: 115.145.171.128, ID: c687613ff34cdbf5097699e5c896055ef2dc44343c5ab44bd40309d5) where the task (actor ID: 2a8bb25e35bf7bb5a0a6c00401000000, name=ClientAppActor.__init__, pid=619620, memory used=1.97GB) was running was 29.48GB / 31.02GB (0.950487), which exceeds the memory usage threshold of 0.95. Ray killed this worker (ID: 38a4d571c3fa7ff405dd9f5d1555b4753efb95dc84bb70ce01b6ecb4) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 115.145.171.128`. To see the logs of the worker, use `ray logs worker-38a4d571c3fa7ff405dd9f5d1555b4753efb95dc84bb70ce01b6ecb4*out -ip 115.145.171.128. Top 10 memory users:
PID	MEM(GB)	COMMAND
4942	3.48	/snap/pycharm-professional/408/jbr/bin/java -classpath /snap/pycharm-professional/408/lib/platform-l...
616443	2.85	/home/abdenour/miniconda3/envs/fl_with_flower/bin/python /home/abdenour/PycharmProjects/fl_with_flow...
619615	2.06	ray::ClientAppActor
619616	2.04	ray::ClientAppActor
619611	2.04	ray::ClientAppActor.run
619619	2.03	ray::ClientAppActor.run
619612	1.98	ray::ClientAppActor.run
619613	1.97	ray::ClientAppActor.run
619620	1.97	ray::ClientAppActor
619617	1.96	ray::ClientAppActor
Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.
[2024-08-13 23:57:48,485][flwr][ERROR] - Traceback (most recent call last):
  File "/home/abdenour/miniconda3/envs/fl_with_flower/lib/python3.11/site-packages/flwr/simulation/ray_transport/ray_client_proxy.py", line 94, in _submit_job
    out_mssg, updated_context = self.actor_pool.get_client_result(
                                ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/abdenour/miniconda3/envs/fl_with_flower/lib/python3.11/site-packages/flwr/simulation/ray_transport/ray_actor.py", line 398, in get_client_result
    return self._fetch_future_result(cid)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/abdenour/miniconda3/envs/fl_with_flower/lib/python3.11/site-packages/flwr/simulation/ray_transport/ray_actor.py", line 279, in _fetch_future_result
    res_cid, out_mssg, updated_context = ray.get(
                                         ^^^^^^^^
  File "/home/abdenour/miniconda3/envs/fl_with_flower/lib/python3.11/site-packages/ray/_private/auto_init_hook.py", line 21, in auto_init_wrapper
    return fn(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^
  File "/home/abdenour/miniconda3/envs/fl_with_flower/lib/python3.11/site-packages/ray/_private/client_mode_hook.py", line 103, in wrapper
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "/home/abdenour/miniconda3/envs/fl_with_flower/lib/python3.11/site-packages/ray/_private/worker.py", line 2667, in get
    values, debugger_breakpoint = worker.get_objects(object_refs, timeout=timeout)
                                  ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/abdenour/miniconda3/envs/fl_with_flower/lib/python3.11/site-packages/ray/_private/worker.py", line 866, in get_objects
    raise value
ray.exceptions.OutOfMemoryError: Task was killed due to the node running low on memory.
Memory on the node (IP: 115.145.171.128, ID: c687613ff34cdbf5097699e5c896055ef2dc44343c5ab44bd40309d5) where the task (actor ID: 85928d3f1a247800081c0e1101000000, name=ClientAppActor.__init__, pid=619614, memory used=1.73GB) was running was 29.50GB / 31.02GB (0.951049), which exceeds the memory usage threshold of 0.95. Ray killed this worker (ID: c7f644ac3cca20dffbfa1723daac4f87f04c7de03da25e7cad5fe33c) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 115.145.171.128`. To see the logs of the worker, use `ray logs worker-c7f644ac3cca20dffbfa1723daac4f87f04c7de03da25e7cad5fe33c*out -ip 115.145.171.128. Top 10 memory users:
PID	MEM(GB)	COMMAND
4942	3.49	/snap/pycharm-professional/408/jbr/bin/java -classpath /snap/pycharm-professional/408/lib/platform-l...
619616	1.80	ray::ClientAppActor.run
619615	1.78	ray::ClientAppActor
619617	1.77	ray::ClientAppActor
619611	1.73	ray::ClientAppActor
619618	1.73	ray::ClientAppActor
619614	1.73	ray::ClientAppActor
619612	1.72	ray::ClientAppActor
619619	1.70	ray::ClientAppActor
619620	1.68	ray::ClientAppActor.run
Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.

[2024-08-13 23:57:48,485][flwr][ERROR] - Task was killed due to the node running low on memory.
Memory on the node (IP: 115.145.171.128, ID: c687613ff34cdbf5097699e5c896055ef2dc44343c5ab44bd40309d5) where the task (actor ID: 85928d3f1a247800081c0e1101000000, name=ClientAppActor.__init__, pid=619614, memory used=1.73GB) was running was 29.50GB / 31.02GB (0.951049), which exceeds the memory usage threshold of 0.95. Ray killed this worker (ID: c7f644ac3cca20dffbfa1723daac4f87f04c7de03da25e7cad5fe33c) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 115.145.171.128`. To see the logs of the worker, use `ray logs worker-c7f644ac3cca20dffbfa1723daac4f87f04c7de03da25e7cad5fe33c*out -ip 115.145.171.128. Top 10 memory users:
PID	MEM(GB)	COMMAND
4942	3.49	/snap/pycharm-professional/408/jbr/bin/java -classpath /snap/pycharm-professional/408/lib/platform-l...
619616	1.80	ray::ClientAppActor.run
619615	1.78	ray::ClientAppActor
619617	1.77	ray::ClientAppActor
619611	1.73	ray::ClientAppActor
619618	1.73	ray::ClientAppActor
619614	1.73	ray::ClientAppActor
619612	1.72	ray::ClientAppActor
619619	1.70	ray::ClientAppActor
619620	1.68	ray::ClientAppActor.run
Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.
[2024-08-13 23:57:50,014][flwr][INFO] - aggregate_fit: received 7 results and 3 failures
[2024-08-13 23:57:53,472][flwr][INFO] - fit progress: (278, 17.28204506635666, {'accuracy': 0.7516025641025641}, 1594.7326354109973)
[2024-08-13 23:57:53,473][flwr][INFO] - configure_evaluate: strategy sampled 2 clients (out of 20)
[2024-08-13 23:57:53,764][flwr][INFO] - aggregate_evaluate: received 2 results and 0 failures
[2024-08-13 23:57:53,765][flwr][INFO] - 
[2024-08-13 23:57:53,765][flwr][INFO] - [ROUND 279]
[2024-08-13 23:57:53,765][flwr][INFO] - configure_fit: strategy sampled 10 clients (out of 20)
[2024-08-13 23:57:53,933][flwr][ERROR] - Traceback (most recent call last):
  File "/home/abdenour/miniconda3/envs/fl_with_flower/lib/python3.11/site-packages/flwr/simulation/ray_transport/ray_client_proxy.py", line 94, in _submit_job
    out_mssg, updated_context = self.actor_pool.get_client_result(
                                ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/abdenour/miniconda3/envs/fl_with_flower/lib/python3.11/site-packages/flwr/simulation/ray_transport/ray_actor.py", line 398, in get_client_result
    return self._fetch_future_result(cid)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/abdenour/miniconda3/envs/fl_with_flower/lib/python3.11/site-packages/flwr/simulation/ray_transport/ray_actor.py", line 279, in _fetch_future_result
    res_cid, out_mssg, updated_context = ray.get(
                                         ^^^^^^^^
  File "/home/abdenour/miniconda3/envs/fl_with_flower/lib/python3.11/site-packages/ray/_private/auto_init_hook.py", line 21, in auto_init_wrapper
    return fn(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^
  File "/home/abdenour/miniconda3/envs/fl_with_flower/lib/python3.11/site-packages/ray/_private/client_mode_hook.py", line 103, in wrapper
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "/home/abdenour/miniconda3/envs/fl_with_flower/lib/python3.11/site-packages/ray/_private/worker.py", line 2667, in get
    values, debugger_breakpoint = worker.get_objects(object_refs, timeout=timeout)
                                  ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/abdenour/miniconda3/envs/fl_with_flower/lib/python3.11/site-packages/ray/_private/worker.py", line 866, in get_objects
    raise value
ray.exceptions.OutOfMemoryError: Task was killed due to the node running low on memory.
Memory on the node (IP: 115.145.171.128, ID: c687613ff34cdbf5097699e5c896055ef2dc44343c5ab44bd40309d5) where the task (actor ID: 2a8bb25e35bf7bb5a0a6c00401000000, name=ClientAppActor.__init__, pid=619620, memory used=1.97GB) was running was 29.48GB / 31.02GB (0.950487), which exceeds the memory usage threshold of 0.95. Ray killed this worker (ID: 38a4d571c3fa7ff405dd9f5d1555b4753efb95dc84bb70ce01b6ecb4) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 115.145.171.128`. To see the logs of the worker, use `ray logs worker-38a4d571c3fa7ff405dd9f5d1555b4753efb95dc84bb70ce01b6ecb4*out -ip 115.145.171.128. Top 10 memory users:
PID	MEM(GB)	COMMAND
4942	3.48	/snap/pycharm-professional/408/jbr/bin/java -classpath /snap/pycharm-professional/408/lib/platform-l...
616443	2.85	/home/abdenour/miniconda3/envs/fl_with_flower/bin/python /home/abdenour/PycharmProjects/fl_with_flow...
619615	2.06	ray::ClientAppActor
619616	2.04	ray::ClientAppActor
619611	2.04	ray::ClientAppActor.run
619619	2.03	ray::ClientAppActor.run
619612	1.98	ray::ClientAppActor.run
619613	1.97	ray::ClientAppActor.run
619620	1.97	ray::ClientAppActor
619617	1.96	ray::ClientAppActor
Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.

[2024-08-13 23:57:53,934][flwr][ERROR] - Task was killed due to the node running low on memory.
Memory on the node (IP: 115.145.171.128, ID: c687613ff34cdbf5097699e5c896055ef2dc44343c5ab44bd40309d5) where the task (actor ID: 2a8bb25e35bf7bb5a0a6c00401000000, name=ClientAppActor.__init__, pid=619620, memory used=1.97GB) was running was 29.48GB / 31.02GB (0.950487), which exceeds the memory usage threshold of 0.95. Ray killed this worker (ID: 38a4d571c3fa7ff405dd9f5d1555b4753efb95dc84bb70ce01b6ecb4) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 115.145.171.128`. To see the logs of the worker, use `ray logs worker-38a4d571c3fa7ff405dd9f5d1555b4753efb95dc84bb70ce01b6ecb4*out -ip 115.145.171.128. Top 10 memory users:
PID	MEM(GB)	COMMAND
4942	3.48	/snap/pycharm-professional/408/jbr/bin/java -classpath /snap/pycharm-professional/408/lib/platform-l...
616443	2.85	/home/abdenour/miniconda3/envs/fl_with_flower/bin/python /home/abdenour/PycharmProjects/fl_with_flow...
619615	2.06	ray::ClientAppActor
619616	2.04	ray::ClientAppActor
619611	2.04	ray::ClientAppActor.run
619619	2.03	ray::ClientAppActor.run
619612	1.98	ray::ClientAppActor.run
619613	1.97	ray::ClientAppActor.run
619620	1.97	ray::ClientAppActor
619617	1.96	ray::ClientAppActor
Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.
[2024-08-13 23:57:53,934][flwr][ERROR] - Traceback (most recent call last):
  File "/home/abdenour/miniconda3/envs/fl_with_flower/lib/python3.11/site-packages/flwr/simulation/ray_transport/ray_client_proxy.py", line 94, in _submit_job
    out_mssg, updated_context = self.actor_pool.get_client_result(
                                ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/abdenour/miniconda3/envs/fl_with_flower/lib/python3.11/site-packages/flwr/simulation/ray_transport/ray_actor.py", line 398, in get_client_result
    return self._fetch_future_result(cid)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/abdenour/miniconda3/envs/fl_with_flower/lib/python3.11/site-packages/flwr/simulation/ray_transport/ray_actor.py", line 279, in _fetch_future_result
    res_cid, out_mssg, updated_context = ray.get(
                                         ^^^^^^^^
  File "/home/abdenour/miniconda3/envs/fl_with_flower/lib/python3.11/site-packages/ray/_private/auto_init_hook.py", line 21, in auto_init_wrapper
    return fn(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^
  File "/home/abdenour/miniconda3/envs/fl_with_flower/lib/python3.11/site-packages/ray/_private/client_mode_hook.py", line 103, in wrapper
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "/home/abdenour/miniconda3/envs/fl_with_flower/lib/python3.11/site-packages/ray/_private/worker.py", line 2667, in get
    values, debugger_breakpoint = worker.get_objects(object_refs, timeout=timeout)
                                  ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/abdenour/miniconda3/envs/fl_with_flower/lib/python3.11/site-packages/ray/_private/worker.py", line 866, in get_objects
    raise value
ray.exceptions.OutOfMemoryError: Task was killed due to the node running low on memory.
Memory on the node (IP: 115.145.171.128, ID: c687613ff34cdbf5097699e5c896055ef2dc44343c5ab44bd40309d5) where the task (actor ID: 52365bb7997979e5a4c7ddbd01000000, name=ClientAppActor.__init__, pid=619618, memory used=1.84GB) was running was 29.49GB / 31.02GB (0.950707), which exceeds the memory usage threshold of 0.95. Ray killed this worker (ID: 7e48cd46c72821b17be65b05a4e61b5846ba998a57d76db44ca4f8fa) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 115.145.171.128`. To see the logs of the worker, use `ray logs worker-7e48cd46c72821b17be65b05a4e61b5846ba998a57d76db44ca4f8fa*out -ip 115.145.171.128. Top 10 memory users:
PID	MEM(GB)	COMMAND
4942	3.48	/snap/pycharm-professional/408/jbr/bin/java -classpath /snap/pycharm-professional/408/lib/platform-l...
616443	2.14	/home/abdenour/miniconda3/envs/fl_with_flower/bin/python /home/abdenour/PycharmProjects/fl_with_flow...
619615	1.90	ray::ClientAppActor
619616	1.88	ray::ClientAppActor
619611	1.87	ray::ClientAppActor
619617	1.86	ray::ClientAppActor
619620	1.85	ray::ClientAppActor
619618	1.84	ray::ClientAppActor
619613	1.82	ray::ClientAppActor
619619	1.80	ray::ClientAppActor
Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.

[2024-08-13 23:57:53,934][flwr][ERROR] - Task was killed due to the node running low on memory.
Memory on the node (IP: 115.145.171.128, ID: c687613ff34cdbf5097699e5c896055ef2dc44343c5ab44bd40309d5) where the task (actor ID: 52365bb7997979e5a4c7ddbd01000000, name=ClientAppActor.__init__, pid=619618, memory used=1.84GB) was running was 29.49GB / 31.02GB (0.950707), which exceeds the memory usage threshold of 0.95. Ray killed this worker (ID: 7e48cd46c72821b17be65b05a4e61b5846ba998a57d76db44ca4f8fa) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 115.145.171.128`. To see the logs of the worker, use `ray logs worker-7e48cd46c72821b17be65b05a4e61b5846ba998a57d76db44ca4f8fa*out -ip 115.145.171.128. Top 10 memory users:
PID	MEM(GB)	COMMAND
4942	3.48	/snap/pycharm-professional/408/jbr/bin/java -classpath /snap/pycharm-professional/408/lib/platform-l...
616443	2.14	/home/abdenour/miniconda3/envs/fl_with_flower/bin/python /home/abdenour/PycharmProjects/fl_with_flow...
619615	1.90	ray::ClientAppActor
619616	1.88	ray::ClientAppActor
619611	1.87	ray::ClientAppActor
619617	1.86	ray::ClientAppActor
619620	1.85	ray::ClientAppActor
619618	1.84	ray::ClientAppActor
619613	1.82	ray::ClientAppActor
619619	1.80	ray::ClientAppActor
Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.
[2024-08-13 23:57:53,935][flwr][ERROR] - Traceback (most recent call last):
  File "/home/abdenour/miniconda3/envs/fl_with_flower/lib/python3.11/site-packages/flwr/simulation/ray_transport/ray_client_proxy.py", line 94, in _submit_job
    out_mssg, updated_context = self.actor_pool.get_client_result(
                                ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/abdenour/miniconda3/envs/fl_with_flower/lib/python3.11/site-packages/flwr/simulation/ray_transport/ray_actor.py", line 398, in get_client_result
    return self._fetch_future_result(cid)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/abdenour/miniconda3/envs/fl_with_flower/lib/python3.11/site-packages/flwr/simulation/ray_transport/ray_actor.py", line 279, in _fetch_future_result
    res_cid, out_mssg, updated_context = ray.get(
                                         ^^^^^^^^
  File "/home/abdenour/miniconda3/envs/fl_with_flower/lib/python3.11/site-packages/ray/_private/auto_init_hook.py", line 21, in auto_init_wrapper
    return fn(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^
  File "/home/abdenour/miniconda3/envs/fl_with_flower/lib/python3.11/site-packages/ray/_private/client_mode_hook.py", line 103, in wrapper
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "/home/abdenour/miniconda3/envs/fl_with_flower/lib/python3.11/site-packages/ray/_private/worker.py", line 2667, in get
    values, debugger_breakpoint = worker.get_objects(object_refs, timeout=timeout)
                                  ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/abdenour/miniconda3/envs/fl_with_flower/lib/python3.11/site-packages/ray/_private/worker.py", line 866, in get_objects
    raise value
ray.exceptions.OutOfMemoryError: Task was killed due to the node running low on memory.
Memory on the node (IP: 115.145.171.128, ID: c687613ff34cdbf5097699e5c896055ef2dc44343c5ab44bd40309d5) where the task (actor ID: 85928d3f1a247800081c0e1101000000, name=ClientAppActor.__init__, pid=619614, memory used=1.73GB) was running was 29.50GB / 31.02GB (0.951049), which exceeds the memory usage threshold of 0.95. Ray killed this worker (ID: c7f644ac3cca20dffbfa1723daac4f87f04c7de03da25e7cad5fe33c) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 115.145.171.128`. To see the logs of the worker, use `ray logs worker-c7f644ac3cca20dffbfa1723daac4f87f04c7de03da25e7cad5fe33c*out -ip 115.145.171.128. Top 10 memory users:
PID	MEM(GB)	COMMAND
4942	3.49	/snap/pycharm-professional/408/jbr/bin/java -classpath /snap/pycharm-professional/408/lib/platform-l...
619616	1.80	ray::ClientAppActor.run
619615	1.78	ray::ClientAppActor
619617	1.77	ray::ClientAppActor
619611	1.73	ray::ClientAppActor
619618	1.73	ray::ClientAppActor
619614	1.73	ray::ClientAppActor
619612	1.72	ray::ClientAppActor
619619	1.70	ray::ClientAppActor
619620	1.68	ray::ClientAppActor.run
Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.

[2024-08-13 23:57:53,935][flwr][ERROR] - Task was killed due to the node running low on memory.
Memory on the node (IP: 115.145.171.128, ID: c687613ff34cdbf5097699e5c896055ef2dc44343c5ab44bd40309d5) where the task (actor ID: 85928d3f1a247800081c0e1101000000, name=ClientAppActor.__init__, pid=619614, memory used=1.73GB) was running was 29.50GB / 31.02GB (0.951049), which exceeds the memory usage threshold of 0.95. Ray killed this worker (ID: c7f644ac3cca20dffbfa1723daac4f87f04c7de03da25e7cad5fe33c) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 115.145.171.128`. To see the logs of the worker, use `ray logs worker-c7f644ac3cca20dffbfa1723daac4f87f04c7de03da25e7cad5fe33c*out -ip 115.145.171.128. Top 10 memory users:
PID	MEM(GB)	COMMAND
4942	3.49	/snap/pycharm-professional/408/jbr/bin/java -classpath /snap/pycharm-professional/408/lib/platform-l...
619616	1.80	ray::ClientAppActor.run
619615	1.78	ray::ClientAppActor
619617	1.77	ray::ClientAppActor
619611	1.73	ray::ClientAppActor
619618	1.73	ray::ClientAppActor
619614	1.73	ray::ClientAppActor
619612	1.72	ray::ClientAppActor
619619	1.70	ray::ClientAppActor
619620	1.68	ray::ClientAppActor.run
Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.
[2024-08-13 23:57:55,335][flwr][INFO] - aggregate_fit: received 7 results and 3 failures
[2024-08-13 23:57:58,527][flwr][INFO] - fit progress: (279, 16.868700921535492, {'accuracy': 0.7548076923076923}, 1599.7877674679912)
[2024-08-13 23:57:58,528][flwr][INFO] - configure_evaluate: strategy sampled 2 clients (out of 20)
[2024-08-13 23:57:58,772][flwr][INFO] - aggregate_evaluate: received 2 results and 0 failures
[2024-08-13 23:57:58,772][flwr][INFO] - 
[2024-08-13 23:57:58,772][flwr][INFO] - [ROUND 280]
[2024-08-13 23:57:58,772][flwr][INFO] - configure_fit: strategy sampled 10 clients (out of 20)
[2024-08-13 23:57:58,932][flwr][ERROR] - Traceback (most recent call last):
  File "/home/abdenour/miniconda3/envs/fl_with_flower/lib/python3.11/site-packages/flwr/simulation/ray_transport/ray_client_proxy.py", line 94, in _submit_job
    out_mssg, updated_context = self.actor_pool.get_client_result(
                                ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/abdenour/miniconda3/envs/fl_with_flower/lib/python3.11/site-packages/flwr/simulation/ray_transport/ray_actor.py", line 398, in get_client_result
    return self._fetch_future_result(cid)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/abdenour/miniconda3/envs/fl_with_flower/lib/python3.11/site-packages/flwr/simulation/ray_transport/ray_actor.py", line 279, in _fetch_future_result
    res_cid, out_mssg, updated_context = ray.get(
                                         ^^^^^^^^
  File "/home/abdenour/miniconda3/envs/fl_with_flower/lib/python3.11/site-packages/ray/_private/auto_init_hook.py", line 21, in auto_init_wrapper
    return fn(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^
  File "/home/abdenour/miniconda3/envs/fl_with_flower/lib/python3.11/site-packages/ray/_private/client_mode_hook.py", line 103, in wrapper
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "/home/abdenour/miniconda3/envs/fl_with_flower/lib/python3.11/site-packages/ray/_private/worker.py", line 2667, in get
    values, debugger_breakpoint = worker.get_objects(object_refs, timeout=timeout)
                                  ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/abdenour/miniconda3/envs/fl_with_flower/lib/python3.11/site-packages/ray/_private/worker.py", line 866, in get_objects
    raise value
ray.exceptions.OutOfMemoryError: Task was killed due to the node running low on memory.
Memory on the node (IP: 115.145.171.128, ID: c687613ff34cdbf5097699e5c896055ef2dc44343c5ab44bd40309d5) where the task (actor ID: 85928d3f1a247800081c0e1101000000, name=ClientAppActor.__init__, pid=619614, memory used=1.73GB) was running was 29.50GB / 31.02GB (0.951049), which exceeds the memory usage threshold of 0.95. Ray killed this worker (ID: c7f644ac3cca20dffbfa1723daac4f87f04c7de03da25e7cad5fe33c) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 115.145.171.128`. To see the logs of the worker, use `ray logs worker-c7f644ac3cca20dffbfa1723daac4f87f04c7de03da25e7cad5fe33c*out -ip 115.145.171.128. Top 10 memory users:
PID	MEM(GB)	COMMAND
4942	3.49	/snap/pycharm-professional/408/jbr/bin/java -classpath /snap/pycharm-professional/408/lib/platform-l...
619616	1.80	ray::ClientAppActor.run
619615	1.78	ray::ClientAppActor
619617	1.77	ray::ClientAppActor
619611	1.73	ray::ClientAppActor
619618	1.73	ray::ClientAppActor
619614	1.73	ray::ClientAppActor
619612	1.72	ray::ClientAppActor
619619	1.70	ray::ClientAppActor
619620	1.68	ray::ClientAppActor.run
Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.

[2024-08-13 23:57:58,933][flwr][ERROR] - Task was killed due to the node running low on memory.
Memory on the node (IP: 115.145.171.128, ID: c687613ff34cdbf5097699e5c896055ef2dc44343c5ab44bd40309d5) where the task (actor ID: 85928d3f1a247800081c0e1101000000, name=ClientAppActor.__init__, pid=619614, memory used=1.73GB) was running was 29.50GB / 31.02GB (0.951049), which exceeds the memory usage threshold of 0.95. Ray killed this worker (ID: c7f644ac3cca20dffbfa1723daac4f87f04c7de03da25e7cad5fe33c) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 115.145.171.128`. To see the logs of the worker, use `ray logs worker-c7f644ac3cca20dffbfa1723daac4f87f04c7de03da25e7cad5fe33c*out -ip 115.145.171.128. Top 10 memory users:
PID	MEM(GB)	COMMAND
4942	3.49	/snap/pycharm-professional/408/jbr/bin/java -classpath /snap/pycharm-professional/408/lib/platform-l...
619616	1.80	ray::ClientAppActor.run
619615	1.78	ray::ClientAppActor
619617	1.77	ray::ClientAppActor
619611	1.73	ray::ClientAppActor
619618	1.73	ray::ClientAppActor
619614	1.73	ray::ClientAppActor
619612	1.72	ray::ClientAppActor
619619	1.70	ray::ClientAppActor
619620	1.68	ray::ClientAppActor.run
Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.
[2024-08-13 23:57:58,934][flwr][ERROR] - Traceback (most recent call last):
  File "/home/abdenour/miniconda3/envs/fl_with_flower/lib/python3.11/site-packages/flwr/simulation/ray_transport/ray_client_proxy.py", line 94, in _submit_job
    out_mssg, updated_context = self.actor_pool.get_client_result(
                                ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/abdenour/miniconda3/envs/fl_with_flower/lib/python3.11/site-packages/flwr/simulation/ray_transport/ray_actor.py", line 398, in get_client_result
    return self._fetch_future_result(cid)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/abdenour/miniconda3/envs/fl_with_flower/lib/python3.11/site-packages/flwr/simulation/ray_transport/ray_actor.py", line 279, in _fetch_future_result
    res_cid, out_mssg, updated_context = ray.get(
                                         ^^^^^^^^
  File "/home/abdenour/miniconda3/envs/fl_with_flower/lib/python3.11/site-packages/ray/_private/auto_init_hook.py", line 21, in auto_init_wrapper
    return fn(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^
  File "/home/abdenour/miniconda3/envs/fl_with_flower/lib/python3.11/site-packages/ray/_private/client_mode_hook.py", line 103, in wrapper
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "/home/abdenour/miniconda3/envs/fl_with_flower/lib/python3.11/site-packages/ray/_private/worker.py", line 2667, in get
    values, debugger_breakpoint = worker.get_objects(object_refs, timeout=timeout)
                                  ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/abdenour/miniconda3/envs/fl_with_flower/lib/python3.11/site-packages/ray/_private/worker.py", line 866, in get_objects
    raise value
ray.exceptions.OutOfMemoryError: Task was killed due to the node running low on memory.
Memory on the node (IP: 115.145.171.128, ID: c687613ff34cdbf5097699e5c896055ef2dc44343c5ab44bd40309d5) where the task (actor ID: 2a8bb25e35bf7bb5a0a6c00401000000, name=ClientAppActor.__init__, pid=619620, memory used=1.97GB) was running was 29.48GB / 31.02GB (0.950487), which exceeds the memory usage threshold of 0.95. Ray killed this worker (ID: 38a4d571c3fa7ff405dd9f5d1555b4753efb95dc84bb70ce01b6ecb4) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 115.145.171.128`. To see the logs of the worker, use `ray logs worker-38a4d571c3fa7ff405dd9f5d1555b4753efb95dc84bb70ce01b6ecb4*out -ip 115.145.171.128. Top 10 memory users:
PID	MEM(GB)	COMMAND
4942	3.48	/snap/pycharm-professional/408/jbr/bin/java -classpath /snap/pycharm-professional/408/lib/platform-l...
616443	2.85	/home/abdenour/miniconda3/envs/fl_with_flower/bin/python /home/abdenour/PycharmProjects/fl_with_flow...
619615	2.06	ray::ClientAppActor
619616	2.04	ray::ClientAppActor
619611	2.04	ray::ClientAppActor.run
619619	2.03	ray::ClientAppActor.run
619612	1.98	ray::ClientAppActor.run
619613	1.97	ray::ClientAppActor.run
619620	1.97	ray::ClientAppActor
619617	1.96	ray::ClientAppActor
Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.

[2024-08-13 23:57:58,934][flwr][ERROR] - Traceback (most recent call last):
  File "/home/abdenour/miniconda3/envs/fl_with_flower/lib/python3.11/site-packages/flwr/simulation/ray_transport/ray_client_proxy.py", line 94, in _submit_job
    out_mssg, updated_context = self.actor_pool.get_client_result(
                                ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/abdenour/miniconda3/envs/fl_with_flower/lib/python3.11/site-packages/flwr/simulation/ray_transport/ray_actor.py", line 398, in get_client_result
    return self._fetch_future_result(cid)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/abdenour/miniconda3/envs/fl_with_flower/lib/python3.11/site-packages/flwr/simulation/ray_transport/ray_actor.py", line 279, in _fetch_future_result
    res_cid, out_mssg, updated_context = ray.get(
                                         ^^^^^^^^
  File "/home/abdenour/miniconda3/envs/fl_with_flower/lib/python3.11/site-packages/ray/_private/auto_init_hook.py", line 21, in auto_init_wrapper
    return fn(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^
  File "/home/abdenour/miniconda3/envs/fl_with_flower/lib/python3.11/site-packages/ray/_private/client_mode_hook.py", line 103, in wrapper
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "/home/abdenour/miniconda3/envs/fl_with_flower/lib/python3.11/site-packages/ray/_private/worker.py", line 2667, in get
    values, debugger_breakpoint = worker.get_objects(object_refs, timeout=timeout)
                                  ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/abdenour/miniconda3/envs/fl_with_flower/lib/python3.11/site-packages/ray/_private/worker.py", line 866, in get_objects
    raise value
ray.exceptions.OutOfMemoryError: Task was killed due to the node running low on memory.
Memory on the node (IP: 115.145.171.128, ID: c687613ff34cdbf5097699e5c896055ef2dc44343c5ab44bd40309d5) where the task (actor ID: 52365bb7997979e5a4c7ddbd01000000, name=ClientAppActor.__init__, pid=619618, memory used=1.84GB) was running was 29.49GB / 31.02GB (0.950707), which exceeds the memory usage threshold of 0.95. Ray killed this worker (ID: 7e48cd46c72821b17be65b05a4e61b5846ba998a57d76db44ca4f8fa) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 115.145.171.128`. To see the logs of the worker, use `ray logs worker-7e48cd46c72821b17be65b05a4e61b5846ba998a57d76db44ca4f8fa*out -ip 115.145.171.128. Top 10 memory users:
PID	MEM(GB)	COMMAND
4942	3.48	/snap/pycharm-professional/408/jbr/bin/java -classpath /snap/pycharm-professional/408/lib/platform-l...
616443	2.14	/home/abdenour/miniconda3/envs/fl_with_flower/bin/python /home/abdenour/PycharmProjects/fl_with_flow...
619615	1.90	ray::ClientAppActor
619616	1.88	ray::ClientAppActor
619611	1.87	ray::ClientAppActor
619617	1.86	ray::ClientAppActor
619620	1.85	ray::ClientAppActor
619618	1.84	ray::ClientAppActor
619613	1.82	ray::ClientAppActor
619619	1.80	ray::ClientAppActor
Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.

[2024-08-13 23:57:58,934][flwr][ERROR] - Task was killed due to the node running low on memory.
Memory on the node (IP: 115.145.171.128, ID: c687613ff34cdbf5097699e5c896055ef2dc44343c5ab44bd40309d5) where the task (actor ID: 2a8bb25e35bf7bb5a0a6c00401000000, name=ClientAppActor.__init__, pid=619620, memory used=1.97GB) was running was 29.48GB / 31.02GB (0.950487), which exceeds the memory usage threshold of 0.95. Ray killed this worker (ID: 38a4d571c3fa7ff405dd9f5d1555b4753efb95dc84bb70ce01b6ecb4) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 115.145.171.128`. To see the logs of the worker, use `ray logs worker-38a4d571c3fa7ff405dd9f5d1555b4753efb95dc84bb70ce01b6ecb4*out -ip 115.145.171.128. Top 10 memory users:
PID	MEM(GB)	COMMAND
4942	3.48	/snap/pycharm-professional/408/jbr/bin/java -classpath /snap/pycharm-professional/408/lib/platform-l...
616443	2.85	/home/abdenour/miniconda3/envs/fl_with_flower/bin/python /home/abdenour/PycharmProjects/fl_with_flow...
619615	2.06	ray::ClientAppActor
619616	2.04	ray::ClientAppActor
619611	2.04	ray::ClientAppActor.run
619619	2.03	ray::ClientAppActor.run
619612	1.98	ray::ClientAppActor.run
619613	1.97	ray::ClientAppActor.run
619620	1.97	ray::ClientAppActor
619617	1.96	ray::ClientAppActor
Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.
[2024-08-13 23:57:58,934][flwr][ERROR] - Task was killed due to the node running low on memory.
Memory on the node (IP: 115.145.171.128, ID: c687613ff34cdbf5097699e5c896055ef2dc44343c5ab44bd40309d5) where the task (actor ID: 52365bb7997979e5a4c7ddbd01000000, name=ClientAppActor.__init__, pid=619618, memory used=1.84GB) was running was 29.49GB / 31.02GB (0.950707), which exceeds the memory usage threshold of 0.95. Ray killed this worker (ID: 7e48cd46c72821b17be65b05a4e61b5846ba998a57d76db44ca4f8fa) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 115.145.171.128`. To see the logs of the worker, use `ray logs worker-7e48cd46c72821b17be65b05a4e61b5846ba998a57d76db44ca4f8fa*out -ip 115.145.171.128. Top 10 memory users:
PID	MEM(GB)	COMMAND
4942	3.48	/snap/pycharm-professional/408/jbr/bin/java -classpath /snap/pycharm-professional/408/lib/platform-l...
616443	2.14	/home/abdenour/miniconda3/envs/fl_with_flower/bin/python /home/abdenour/PycharmProjects/fl_with_flow...
619615	1.90	ray::ClientAppActor
619616	1.88	ray::ClientAppActor
619611	1.87	ray::ClientAppActor
619617	1.86	ray::ClientAppActor
619620	1.85	ray::ClientAppActor
619618	1.84	ray::ClientAppActor
619613	1.82	ray::ClientAppActor
619619	1.80	ray::ClientAppActor
Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.
[2024-08-13 23:58:00,329][flwr][INFO] - aggregate_fit: received 7 results and 3 failures
[2024-08-13 23:58:03,639][flwr][INFO] - fit progress: (280, 17.93753743171692, {'accuracy': 0.7403846153846154}, 1604.8991273790016)
[2024-08-13 23:58:03,639][flwr][INFO] - configure_evaluate: strategy sampled 2 clients (out of 20)
[2024-08-13 23:58:03,847][flwr][INFO] - aggregate_evaluate: received 2 results and 0 failures
[2024-08-13 23:58:03,847][flwr][INFO] - 
[2024-08-13 23:58:03,848][flwr][INFO] - [ROUND 281]
[2024-08-13 23:58:03,848][flwr][INFO] - configure_fit: strategy sampled 10 clients (out of 20)
[2024-08-13 23:58:04,004][flwr][ERROR] - Traceback (most recent call last):
  File "/home/abdenour/miniconda3/envs/fl_with_flower/lib/python3.11/site-packages/flwr/simulation/ray_transport/ray_client_proxy.py", line 94, in _submit_job
    out_mssg, updated_context = self.actor_pool.get_client_result(
                                ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/abdenour/miniconda3/envs/fl_with_flower/lib/python3.11/site-packages/flwr/simulation/ray_transport/ray_actor.py", line 398, in get_client_result
    return self._fetch_future_result(cid)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/abdenour/miniconda3/envs/fl_with_flower/lib/python3.11/site-packages/flwr/simulation/ray_transport/ray_actor.py", line 279, in _fetch_future_result
    res_cid, out_mssg, updated_context = ray.get(
                                         ^^^^^^^^
  File "/home/abdenour/miniconda3/envs/fl_with_flower/lib/python3.11/site-packages/ray/_private/auto_init_hook.py", line 21, in auto_init_wrapper
    return fn(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^
  File "/home/abdenour/miniconda3/envs/fl_with_flower/lib/python3.11/site-packages/ray/_private/client_mode_hook.py", line 103, in wrapper
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "/home/abdenour/miniconda3/envs/fl_with_flower/lib/python3.11/site-packages/ray/_private/worker.py", line 2667, in get
    values, debugger_breakpoint = worker.get_objects(object_refs, timeout=timeout)
                                  ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/abdenour/miniconda3/envs/fl_with_flower/lib/python3.11/site-packages/ray/_private/worker.py", line 866, in get_objects
    raise value
ray.exceptions.OutOfMemoryError: Task was killed due to the node running low on memory.
Memory on the node (IP: 115.145.171.128, ID: c687613ff34cdbf5097699e5c896055ef2dc44343c5ab44bd40309d5) where the task (actor ID: 2a8bb25e35bf7bb5a0a6c00401000000, name=ClientAppActor.__init__, pid=619620, memory used=1.97GB) was running was 29.48GB / 31.02GB (0.950487), which exceeds the memory usage threshold of 0.95. Ray killed this worker (ID: 38a4d571c3fa7ff405dd9f5d1555b4753efb95dc84bb70ce01b6ecb4) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 115.145.171.128`. To see the logs of the worker, use `ray logs worker-38a4d571c3fa7ff405dd9f5d1555b4753efb95dc84bb70ce01b6ecb4*out -ip 115.145.171.128. Top 10 memory users:
PID	MEM(GB)	COMMAND
4942	3.48	/snap/pycharm-professional/408/jbr/bin/java -classpath /snap/pycharm-professional/408/lib/platform-l...
616443	2.85	/home/abdenour/miniconda3/envs/fl_with_flower/bin/python /home/abdenour/PycharmProjects/fl_with_flow...
619615	2.06	ray::ClientAppActor
619616	2.04	ray::ClientAppActor
619611	2.04	ray::ClientAppActor.run
619619	2.03	ray::ClientAppActor.run
619612	1.98	ray::ClientAppActor.run
619613	1.97	ray::ClientAppActor.run
619620	1.97	ray::ClientAppActor
619617	1.96	ray::ClientAppActor
Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.

[2024-08-13 23:58:04,011][flwr][ERROR] - Task was killed due to the node running low on memory.
Memory on the node (IP: 115.145.171.128, ID: c687613ff34cdbf5097699e5c896055ef2dc44343c5ab44bd40309d5) where the task (actor ID: 2a8bb25e35bf7bb5a0a6c00401000000, name=ClientAppActor.__init__, pid=619620, memory used=1.97GB) was running was 29.48GB / 31.02GB (0.950487), which exceeds the memory usage threshold of 0.95. Ray killed this worker (ID: 38a4d571c3fa7ff405dd9f5d1555b4753efb95dc84bb70ce01b6ecb4) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 115.145.171.128`. To see the logs of the worker, use `ray logs worker-38a4d571c3fa7ff405dd9f5d1555b4753efb95dc84bb70ce01b6ecb4*out -ip 115.145.171.128. Top 10 memory users:
PID	MEM(GB)	COMMAND
4942	3.48	/snap/pycharm-professional/408/jbr/bin/java -classpath /snap/pycharm-professional/408/lib/platform-l...
616443	2.85	/home/abdenour/miniconda3/envs/fl_with_flower/bin/python /home/abdenour/PycharmProjects/fl_with_flow...
619615	2.06	ray::ClientAppActor
619616	2.04	ray::ClientAppActor
619611	2.04	ray::ClientAppActor.run
619619	2.03	ray::ClientAppActor.run
619612	1.98	ray::ClientAppActor.run
619613	1.97	ray::ClientAppActor.run
619620	1.97	ray::ClientAppActor
619617	1.96	ray::ClientAppActor
Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.
[2024-08-13 23:58:04,024][flwr][ERROR] - Traceback (most recent call last):
  File "/home/abdenour/miniconda3/envs/fl_with_flower/lib/python3.11/site-packages/flwr/simulation/ray_transport/ray_client_proxy.py", line 94, in _submit_job
    out_mssg, updated_context = self.actor_pool.get_client_result(
                                ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/abdenour/miniconda3/envs/fl_with_flower/lib/python3.11/site-packages/flwr/simulation/ray_transport/ray_actor.py", line 398, in get_client_result
    return self._fetch_future_result(cid)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/abdenour/miniconda3/envs/fl_with_flower/lib/python3.11/site-packages/flwr/simulation/ray_transport/ray_actor.py", line 279, in _fetch_future_result
    res_cid, out_mssg, updated_context = ray.get(
                                         ^^^^^^^^
  File "/home/abdenour/miniconda3/envs/fl_with_flower/lib/python3.11/site-packages/ray/_private/auto_init_hook.py", line 21, in auto_init_wrapper
    return fn(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^
  File "/home/abdenour/miniconda3/envs/fl_with_flower/lib/python3.11/site-packages/ray/_private/client_mode_hook.py", line 103, in wrapper
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "/home/abdenour/miniconda3/envs/fl_with_flower/lib/python3.11/site-packages/ray/_private/worker.py", line 2667, in get
    values, debugger_breakpoint = worker.get_objects(object_refs, timeout=timeout)
                                  ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/abdenour/miniconda3/envs/fl_with_flower/lib/python3.11/site-packages/ray/_private/worker.py", line 866, in get_objects
    raise value
ray.exceptions.OutOfMemoryError: Task was killed due to the node running low on memory.
Memory on the node (IP: 115.145.171.128, ID: c687613ff34cdbf5097699e5c896055ef2dc44343c5ab44bd40309d5) where the task (actor ID: 52365bb7997979e5a4c7ddbd01000000, name=ClientAppActor.__init__, pid=619618, memory used=1.84GB) was running was 29.49GB / 31.02GB (0.950707), which exceeds the memory usage threshold of 0.95. Ray killed this worker (ID: 7e48cd46c72821b17be65b05a4e61b5846ba998a57d76db44ca4f8fa) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 115.145.171.128`. To see the logs of the worker, use `ray logs worker-7e48cd46c72821b17be65b05a4e61b5846ba998a57d76db44ca4f8fa*out -ip 115.145.171.128. Top 10 memory users:
PID	MEM(GB)	COMMAND
4942	3.48	/snap/pycharm-professional/408/jbr/bin/java -classpath /snap/pycharm-professional/408/lib/platform-l...
616443	2.14	/home/abdenour/miniconda3/envs/fl_with_flower/bin/python /home/abdenour/PycharmProjects/fl_with_flow...
619615	1.90	ray::ClientAppActor
619616	1.88	ray::ClientAppActor
619611	1.87	ray::ClientAppActor
619617	1.86	ray::ClientAppActor
619620	1.85	ray::ClientAppActor
619618	1.84	ray::ClientAppActor
619613	1.82	ray::ClientAppActor
619619	1.80	ray::ClientAppActor
Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.

[2024-08-13 23:58:04,024][flwr][ERROR] - Task was killed due to the node running low on memory.
Memory on the node (IP: 115.145.171.128, ID: c687613ff34cdbf5097699e5c896055ef2dc44343c5ab44bd40309d5) where the task (actor ID: 52365bb7997979e5a4c7ddbd01000000, name=ClientAppActor.__init__, pid=619618, memory used=1.84GB) was running was 29.49GB / 31.02GB (0.950707), which exceeds the memory usage threshold of 0.95. Ray killed this worker (ID: 7e48cd46c72821b17be65b05a4e61b5846ba998a57d76db44ca4f8fa) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 115.145.171.128`. To see the logs of the worker, use `ray logs worker-7e48cd46c72821b17be65b05a4e61b5846ba998a57d76db44ca4f8fa*out -ip 115.145.171.128. Top 10 memory users:
PID	MEM(GB)	COMMAND
4942	3.48	/snap/pycharm-professional/408/jbr/bin/java -classpath /snap/pycharm-professional/408/lib/platform-l...
616443	2.14	/home/abdenour/miniconda3/envs/fl_with_flower/bin/python /home/abdenour/PycharmProjects/fl_with_flow...
619615	1.90	ray::ClientAppActor
619616	1.88	ray::ClientAppActor
619611	1.87	ray::ClientAppActor
619617	1.86	ray::ClientAppActor
619620	1.85	ray::ClientAppActor
619618	1.84	ray::ClientAppActor
619613	1.82	ray::ClientAppActor
619619	1.80	ray::ClientAppActor
Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.
[2024-08-13 23:58:04,032][flwr][ERROR] - Traceback (most recent call last):
  File "/home/abdenour/miniconda3/envs/fl_with_flower/lib/python3.11/site-packages/flwr/simulation/ray_transport/ray_client_proxy.py", line 94, in _submit_job
    out_mssg, updated_context = self.actor_pool.get_client_result(
                                ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/abdenour/miniconda3/envs/fl_with_flower/lib/python3.11/site-packages/flwr/simulation/ray_transport/ray_actor.py", line 398, in get_client_result
    return self._fetch_future_result(cid)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/abdenour/miniconda3/envs/fl_with_flower/lib/python3.11/site-packages/flwr/simulation/ray_transport/ray_actor.py", line 279, in _fetch_future_result
    res_cid, out_mssg, updated_context = ray.get(
                                         ^^^^^^^^
  File "/home/abdenour/miniconda3/envs/fl_with_flower/lib/python3.11/site-packages/ray/_private/auto_init_hook.py", line 21, in auto_init_wrapper
    return fn(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^
  File "/home/abdenour/miniconda3/envs/fl_with_flower/lib/python3.11/site-packages/ray/_private/client_mode_hook.py", line 103, in wrapper
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "/home/abdenour/miniconda3/envs/fl_with_flower/lib/python3.11/site-packages/ray/_private/worker.py", line 2667, in get
    values, debugger_breakpoint = worker.get_objects(object_refs, timeout=timeout)
                                  ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/abdenour/miniconda3/envs/fl_with_flower/lib/python3.11/site-packages/ray/_private/worker.py", line 866, in get_objects
    raise value
ray.exceptions.OutOfMemoryError: Task was killed due to the node running low on memory.
Memory on the node (IP: 115.145.171.128, ID: c687613ff34cdbf5097699e5c896055ef2dc44343c5ab44bd40309d5) where the task (actor ID: 52365bb7997979e5a4c7ddbd01000000, name=ClientAppActor.__init__, pid=619618, memory used=1.84GB) was running was 29.49GB / 31.02GB (0.950707), which exceeds the memory usage threshold of 0.95. Ray killed this worker (ID: 7e48cd46c72821b17be65b05a4e61b5846ba998a57d76db44ca4f8fa) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 115.145.171.128`. To see the logs of the worker, use `ray logs worker-7e48cd46c72821b17be65b05a4e61b5846ba998a57d76db44ca4f8fa*out -ip 115.145.171.128. Top 10 memory users:
PID	MEM(GB)	COMMAND
4942	3.48	/snap/pycharm-professional/408/jbr/bin/java -classpath /snap/pycharm-professional/408/lib/platform-l...
616443	2.14	/home/abdenour/miniconda3/envs/fl_with_flower/bin/python /home/abdenour/PycharmProjects/fl_with_flow...
619615	1.90	ray::ClientAppActor
619616	1.88	ray::ClientAppActor
619611	1.87	ray::ClientAppActor
619617	1.86	ray::ClientAppActor
619620	1.85	ray::ClientAppActor
619618	1.84	ray::ClientAppActor
619613	1.82	ray::ClientAppActor
619619	1.80	ray::ClientAppActor
Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.

[2024-08-13 23:58:04,032][flwr][ERROR] - Task was killed due to the node running low on memory.
Memory on the node (IP: 115.145.171.128, ID: c687613ff34cdbf5097699e5c896055ef2dc44343c5ab44bd40309d5) where the task (actor ID: 52365bb7997979e5a4c7ddbd01000000, name=ClientAppActor.__init__, pid=619618, memory used=1.84GB) was running was 29.49GB / 31.02GB (0.950707), which exceeds the memory usage threshold of 0.95. Ray killed this worker (ID: 7e48cd46c72821b17be65b05a4e61b5846ba998a57d76db44ca4f8fa) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 115.145.171.128`. To see the logs of the worker, use `ray logs worker-7e48cd46c72821b17be65b05a4e61b5846ba998a57d76db44ca4f8fa*out -ip 115.145.171.128. Top 10 memory users:
PID	MEM(GB)	COMMAND
4942	3.48	/snap/pycharm-professional/408/jbr/bin/java -classpath /snap/pycharm-professional/408/lib/platform-l...
616443	2.14	/home/abdenour/miniconda3/envs/fl_with_flower/bin/python /home/abdenour/PycharmProjects/fl_with_flow...
619615	1.90	ray::ClientAppActor
619616	1.88	ray::ClientAppActor
619611	1.87	ray::ClientAppActor
619617	1.86	ray::ClientAppActor
619620	1.85	ray::ClientAppActor
619618	1.84	ray::ClientAppActor
619613	1.82	ray::ClientAppActor
619619	1.80	ray::ClientAppActor
Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.
[2024-08-13 23:58:05,546][flwr][INFO] - aggregate_fit: received 7 results and 3 failures
[2024-08-13 23:58:09,154][flwr][INFO] - fit progress: (281, 16.270223915576935, {'accuracy': 0.7532051282051282}, 1610.4147721400077)
[2024-08-13 23:58:09,155][flwr][INFO] - configure_evaluate: strategy sampled 2 clients (out of 20)
[2024-08-13 23:58:09,342][flwr][INFO] - aggregate_evaluate: received 2 results and 0 failures
[2024-08-13 23:58:09,342][flwr][INFO] - 
[2024-08-13 23:58:09,342][flwr][INFO] - [ROUND 282]
[2024-08-13 23:58:09,342][flwr][INFO] - configure_fit: strategy sampled 10 clients (out of 20)
[2024-08-13 23:58:09,512][flwr][ERROR] - Traceback (most recent call last):
  File "/home/abdenour/miniconda3/envs/fl_with_flower/lib/python3.11/site-packages/flwr/simulation/ray_transport/ray_client_proxy.py", line 94, in _submit_job
    out_mssg, updated_context = self.actor_pool.get_client_result(
                                ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/abdenour/miniconda3/envs/fl_with_flower/lib/python3.11/site-packages/flwr/simulation/ray_transport/ray_actor.py", line 398, in get_client_result
    return self._fetch_future_result(cid)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/abdenour/miniconda3/envs/fl_with_flower/lib/python3.11/site-packages/flwr/simulation/ray_transport/ray_actor.py", line 279, in _fetch_future_result
    res_cid, out_mssg, updated_context = ray.get(
                                         ^^^^^^^^
  File "/home/abdenour/miniconda3/envs/fl_with_flower/lib/python3.11/site-packages/ray/_private/auto_init_hook.py", line 21, in auto_init_wrapper
    return fn(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^
  File "/home/abdenour/miniconda3/envs/fl_with_flower/lib/python3.11/site-packages/ray/_private/client_mode_hook.py", line 103, in wrapper
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "/home/abdenour/miniconda3/envs/fl_with_flower/lib/python3.11/site-packages/ray/_private/worker.py", line 2667, in get
    values, debugger_breakpoint = worker.get_objects(object_refs, timeout=timeout)
                                  ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/abdenour/miniconda3/envs/fl_with_flower/lib/python3.11/site-packages/ray/_private/worker.py", line 866, in get_objects
    raise value
ray.exceptions.OutOfMemoryError: Task was killed due to the node running low on memory.
Memory on the node (IP: 115.145.171.128, ID: c687613ff34cdbf5097699e5c896055ef2dc44343c5ab44bd40309d5) where the task (actor ID: 52365bb7997979e5a4c7ddbd01000000, name=ClientAppActor.__init__, pid=619618, memory used=1.84GB) was running was 29.49GB / 31.02GB (0.950707), which exceeds the memory usage threshold of 0.95. Ray killed this worker (ID: 7e48cd46c72821b17be65b05a4e61b5846ba998a57d76db44ca4f8fa) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 115.145.171.128`. To see the logs of the worker, use `ray logs worker-7e48cd46c72821b17be65b05a4e61b5846ba998a57d76db44ca4f8fa*out -ip 115.145.171.128. Top 10 memory users:
PID	MEM(GB)	COMMAND
4942	3.48	/snap/pycharm-professional/408/jbr/bin/java -classpath /snap/pycharm-professional/408/lib/platform-l...
616443	2.14	/home/abdenour/miniconda3/envs/fl_with_flower/bin/python /home/abdenour/PycharmProjects/fl_with_flow...
619615	1.90	ray::ClientAppActor
619616	1.88	ray::ClientAppActor
619611	1.87	ray::ClientAppActor
619617	1.86	ray::ClientAppActor
619620	1.85	ray::ClientAppActor
619618	1.84	ray::ClientAppActor
619613	1.82	ray::ClientAppActor
619619	1.80	ray::ClientAppActor
Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.

[2024-08-13 23:58:09,512][flwr][ERROR] - Task was killed due to the node running low on memory.
Memory on the node (IP: 115.145.171.128, ID: c687613ff34cdbf5097699e5c896055ef2dc44343c5ab44bd40309d5) where the task (actor ID: 52365bb7997979e5a4c7ddbd01000000, name=ClientAppActor.__init__, pid=619618, memory used=1.84GB) was running was 29.49GB / 31.02GB (0.950707), which exceeds the memory usage threshold of 0.95. Ray killed this worker (ID: 7e48cd46c72821b17be65b05a4e61b5846ba998a57d76db44ca4f8fa) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 115.145.171.128`. To see the logs of the worker, use `ray logs worker-7e48cd46c72821b17be65b05a4e61b5846ba998a57d76db44ca4f8fa*out -ip 115.145.171.128. Top 10 memory users:
PID	MEM(GB)	COMMAND
4942	3.48	/snap/pycharm-professional/408/jbr/bin/java -classpath /snap/pycharm-professional/408/lib/platform-l...
616443	2.14	/home/abdenour/miniconda3/envs/fl_with_flower/bin/python /home/abdenour/PycharmProjects/fl_with_flow...
619615	1.90	ray::ClientAppActor
619616	1.88	ray::ClientAppActor
619611	1.87	ray::ClientAppActor
619617	1.86	ray::ClientAppActor
619620	1.85	ray::ClientAppActor
619618	1.84	ray::ClientAppActor
619613	1.82	ray::ClientAppActor
619619	1.80	ray::ClientAppActor
Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.
[2024-08-13 23:58:09,513][flwr][ERROR] - Traceback (most recent call last):
  File "/home/abdenour/miniconda3/envs/fl_with_flower/lib/python3.11/site-packages/flwr/simulation/ray_transport/ray_client_proxy.py", line 94, in _submit_job
    out_mssg, updated_context = self.actor_pool.get_client_result(
                                ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/abdenour/miniconda3/envs/fl_with_flower/lib/python3.11/site-packages/flwr/simulation/ray_transport/ray_actor.py", line 398, in get_client_result
    return self._fetch_future_result(cid)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/abdenour/miniconda3/envs/fl_with_flower/lib/python3.11/site-packages/flwr/simulation/ray_transport/ray_actor.py", line 279, in _fetch_future_result
    res_cid, out_mssg, updated_context = ray.get(
                                         ^^^^^^^^
  File "/home/abdenour/miniconda3/envs/fl_with_flower/lib/python3.11/site-packages/ray/_private/auto_init_hook.py", line 21, in auto_init_wrapper
    return fn(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^
  File "/home/abdenour/miniconda3/envs/fl_with_flower/lib/python3.11/site-packages/ray/_private/client_mode_hook.py", line 103, in wrapper
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "/home/abdenour/miniconda3/envs/fl_with_flower/lib/python3.11/site-packages/ray/_private/worker.py", line 2667, in get
    values, debugger_breakpoint = worker.get_objects(object_refs, timeout=timeout)
                                  ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/abdenour/miniconda3/envs/fl_with_flower/lib/python3.11/site-packages/ray/_private/worker.py", line 866, in get_objects
    raise value
ray.exceptions.OutOfMemoryError: Task was killed due to the node running low on memory.
Memory on the node (IP: 115.145.171.128, ID: c687613ff34cdbf5097699e5c896055ef2dc44343c5ab44bd40309d5) where the task (actor ID: 85928d3f1a247800081c0e1101000000, name=ClientAppActor.__init__, pid=619614, memory used=1.73GB) was running was 29.50GB / 31.02GB (0.951049), which exceeds the memory usage threshold of 0.95. Ray killed this worker (ID: c7f644ac3cca20dffbfa1723daac4f87f04c7de03da25e7cad5fe33c) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 115.145.171.128`. To see the logs of the worker, use `ray logs worker-c7f644ac3cca20dffbfa1723daac4f87f04c7de03da25e7cad5fe33c*out -ip 115.145.171.128. Top 10 memory users:
PID	MEM(GB)	COMMAND
4942	3.49	/snap/pycharm-professional/408/jbr/bin/java -classpath /snap/pycharm-professional/408/lib/platform-l...
619616	1.80	ray::ClientAppActor.run
619615	1.78	ray::ClientAppActor
619617	1.77	ray::ClientAppActor
619611	1.73	ray::ClientAppActor
619618	1.73	ray::ClientAppActor
619614	1.73	ray::ClientAppActor
619612	1.72	ray::ClientAppActor
619619	1.70	ray::ClientAppActor
619620	1.68	ray::ClientAppActor.run
Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.

[2024-08-13 23:58:09,514][flwr][ERROR] - Task was killed due to the node running low on memory.
Memory on the node (IP: 115.145.171.128, ID: c687613ff34cdbf5097699e5c896055ef2dc44343c5ab44bd40309d5) where the task (actor ID: 85928d3f1a247800081c0e1101000000, name=ClientAppActor.__init__, pid=619614, memory used=1.73GB) was running was 29.50GB / 31.02GB (0.951049), which exceeds the memory usage threshold of 0.95. Ray killed this worker (ID: c7f644ac3cca20dffbfa1723daac4f87f04c7de03da25e7cad5fe33c) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 115.145.171.128`. To see the logs of the worker, use `ray logs worker-c7f644ac3cca20dffbfa1723daac4f87f04c7de03da25e7cad5fe33c*out -ip 115.145.171.128. Top 10 memory users:
PID	MEM(GB)	COMMAND
4942	3.49	/snap/pycharm-professional/408/jbr/bin/java -classpath /snap/pycharm-professional/408/lib/platform-l...
619616	1.80	ray::ClientAppActor.run
619615	1.78	ray::ClientAppActor
619617	1.77	ray::ClientAppActor
619611	1.73	ray::ClientAppActor
619618	1.73	ray::ClientAppActor
619614	1.73	ray::ClientAppActor
619612	1.72	ray::ClientAppActor
619619	1.70	ray::ClientAppActor
619620	1.68	ray::ClientAppActor.run
Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.
[2024-08-13 23:58:09,514][flwr][ERROR] - Traceback (most recent call last):
  File "/home/abdenour/miniconda3/envs/fl_with_flower/lib/python3.11/site-packages/flwr/simulation/ray_transport/ray_client_proxy.py", line 94, in _submit_job
    out_mssg, updated_context = self.actor_pool.get_client_result(
                                ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/abdenour/miniconda3/envs/fl_with_flower/lib/python3.11/site-packages/flwr/simulation/ray_transport/ray_actor.py", line 398, in get_client_result
    return self._fetch_future_result(cid)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/abdenour/miniconda3/envs/fl_with_flower/lib/python3.11/site-packages/flwr/simulation/ray_transport/ray_actor.py", line 279, in _fetch_future_result
    res_cid, out_mssg, updated_context = ray.get(
                                         ^^^^^^^^
  File "/home/abdenour/miniconda3/envs/fl_with_flower/lib/python3.11/site-packages/ray/_private/auto_init_hook.py", line 21, in auto_init_wrapper
    return fn(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^
  File "/home/abdenour/miniconda3/envs/fl_with_flower/lib/python3.11/site-packages/ray/_private/client_mode_hook.py", line 103, in wrapper
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "/home/abdenour/miniconda3/envs/fl_with_flower/lib/python3.11/site-packages/ray/_private/worker.py", line 2667, in get
    values, debugger_breakpoint = worker.get_objects(object_refs, timeout=timeout)
                                  ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/abdenour/miniconda3/envs/fl_with_flower/lib/python3.11/site-packages/ray/_private/worker.py", line 866, in get_objects
    raise value
ray.exceptions.OutOfMemoryError: Task was killed due to the node running low on memory.
Memory on the node (IP: 115.145.171.128, ID: c687613ff34cdbf5097699e5c896055ef2dc44343c5ab44bd40309d5) where the task (actor ID: 2a8bb25e35bf7bb5a0a6c00401000000, name=ClientAppActor.__init__, pid=619620, memory used=1.97GB) was running was 29.48GB / 31.02GB (0.950487), which exceeds the memory usage threshold of 0.95. Ray killed this worker (ID: 38a4d571c3fa7ff405dd9f5d1555b4753efb95dc84bb70ce01b6ecb4) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 115.145.171.128`. To see the logs of the worker, use `ray logs worker-38a4d571c3fa7ff405dd9f5d1555b4753efb95dc84bb70ce01b6ecb4*out -ip 115.145.171.128. Top 10 memory users:
PID	MEM(GB)	COMMAND
4942	3.48	/snap/pycharm-professional/408/jbr/bin/java -classpath /snap/pycharm-professional/408/lib/platform-l...
616443	2.85	/home/abdenour/miniconda3/envs/fl_with_flower/bin/python /home/abdenour/PycharmProjects/fl_with_flow...
619615	2.06	ray::ClientAppActor
619616	2.04	ray::ClientAppActor
619611	2.04	ray::ClientAppActor.run
619619	2.03	ray::ClientAppActor.run
619612	1.98	ray::ClientAppActor.run
619613	1.97	ray::ClientAppActor.run
619620	1.97	ray::ClientAppActor
619617	1.96	ray::ClientAppActor
Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.

[2024-08-13 23:58:09,515][flwr][ERROR] - Task was killed due to the node running low on memory.
Memory on the node (IP: 115.145.171.128, ID: c687613ff34cdbf5097699e5c896055ef2dc44343c5ab44bd40309d5) where the task (actor ID: 2a8bb25e35bf7bb5a0a6c00401000000, name=ClientAppActor.__init__, pid=619620, memory used=1.97GB) was running was 29.48GB / 31.02GB (0.950487), which exceeds the memory usage threshold of 0.95. Ray killed this worker (ID: 38a4d571c3fa7ff405dd9f5d1555b4753efb95dc84bb70ce01b6ecb4) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 115.145.171.128`. To see the logs of the worker, use `ray logs worker-38a4d571c3fa7ff405dd9f5d1555b4753efb95dc84bb70ce01b6ecb4*out -ip 115.145.171.128. Top 10 memory users:
PID	MEM(GB)	COMMAND
4942	3.48	/snap/pycharm-professional/408/jbr/bin/java -classpath /snap/pycharm-professional/408/lib/platform-l...
616443	2.85	/home/abdenour/miniconda3/envs/fl_with_flower/bin/python /home/abdenour/PycharmProjects/fl_with_flow...
619615	2.06	ray::ClientAppActor
619616	2.04	ray::ClientAppActor
619611	2.04	ray::ClientAppActor.run
619619	2.03	ray::ClientAppActor.run
619612	1.98	ray::ClientAppActor.run
619613	1.97	ray::ClientAppActor.run
619620	1.97	ray::ClientAppActor
619617	1.96	ray::ClientAppActor
Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.
[2024-08-13 23:58:10,975][flwr][INFO] - aggregate_fit: received 7 results and 3 failures
[2024-08-13 23:58:13,883][flwr][INFO] - fit progress: (282, 18.317009389400482, {'accuracy': 0.7532051282051282}, 1615.143487438996)
[2024-08-13 23:58:13,883][flwr][INFO] - configure_evaluate: strategy sampled 2 clients (out of 20)
[2024-08-13 23:58:14,074][flwr][INFO] - aggregate_evaluate: received 2 results and 0 failures
[2024-08-13 23:58:14,074][flwr][INFO] - 
[2024-08-13 23:58:14,074][flwr][INFO] - [ROUND 283]
[2024-08-13 23:58:14,074][flwr][INFO] - configure_fit: strategy sampled 10 clients (out of 20)
[2024-08-13 23:58:14,223][flwr][ERROR] - Traceback (most recent call last):
  File "/home/abdenour/miniconda3/envs/fl_with_flower/lib/python3.11/site-packages/flwr/simulation/ray_transport/ray_client_proxy.py", line 94, in _submit_job
    out_mssg, updated_context = self.actor_pool.get_client_result(
                                ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/abdenour/miniconda3/envs/fl_with_flower/lib/python3.11/site-packages/flwr/simulation/ray_transport/ray_actor.py", line 398, in get_client_result
    return self._fetch_future_result(cid)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/abdenour/miniconda3/envs/fl_with_flower/lib/python3.11/site-packages/flwr/simulation/ray_transport/ray_actor.py", line 279, in _fetch_future_result
    res_cid, out_mssg, updated_context = ray.get(
                                         ^^^^^^^^
  File "/home/abdenour/miniconda3/envs/fl_with_flower/lib/python3.11/site-packages/ray/_private/auto_init_hook.py", line 21, in auto_init_wrapper
    return fn(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^
  File "/home/abdenour/miniconda3/envs/fl_with_flower/lib/python3.11/site-packages/ray/_private/client_mode_hook.py", line 103, in wrapper
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "/home/abdenour/miniconda3/envs/fl_with_flower/lib/python3.11/site-packages/ray/_private/worker.py", line 2667, in get
    values, debugger_breakpoint = worker.get_objects(object_refs, timeout=timeout)
                                  ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/abdenour/miniconda3/envs/fl_with_flower/lib/python3.11/site-packages/ray/_private/worker.py", line 866, in get_objects
    raise value
ray.exceptions.OutOfMemoryError: Task was killed due to the node running low on memory.
Memory on the node (IP: 115.145.171.128, ID: c687613ff34cdbf5097699e5c896055ef2dc44343c5ab44bd40309d5) where the task (actor ID: 2a8bb25e35bf7bb5a0a6c00401000000, name=ClientAppActor.__init__, pid=619620, memory used=1.97GB) was running was 29.48GB / 31.02GB (0.950487), which exceeds the memory usage threshold of 0.95. Ray killed this worker (ID: 38a4d571c3fa7ff405dd9f5d1555b4753efb95dc84bb70ce01b6ecb4) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 115.145.171.128`. To see the logs of the worker, use `ray logs worker-38a4d571c3fa7ff405dd9f5d1555b4753efb95dc84bb70ce01b6ecb4*out -ip 115.145.171.128. Top 10 memory users:
PID	MEM(GB)	COMMAND
4942	3.48	/snap/pycharm-professional/408/jbr/bin/java -classpath /snap/pycharm-professional/408/lib/platform-l...
616443	2.85	/home/abdenour/miniconda3/envs/fl_with_flower/bin/python /home/abdenour/PycharmProjects/fl_with_flow...
619615	2.06	ray::ClientAppActor
619616	2.04	ray::ClientAppActor
619611	2.04	ray::ClientAppActor.run
619619	2.03	ray::ClientAppActor.run
619612	1.98	ray::ClientAppActor.run
619613	1.97	ray::ClientAppActor.run
619620	1.97	ray::ClientAppActor
619617	1.96	ray::ClientAppActor
Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.

[2024-08-13 23:58:14,224][flwr][ERROR] - Task was killed due to the node running low on memory.
Memory on the node (IP: 115.145.171.128, ID: c687613ff34cdbf5097699e5c896055ef2dc44343c5ab44bd40309d5) where the task (actor ID: 2a8bb25e35bf7bb5a0a6c00401000000, name=ClientAppActor.__init__, pid=619620, memory used=1.97GB) was running was 29.48GB / 31.02GB (0.950487), which exceeds the memory usage threshold of 0.95. Ray killed this worker (ID: 38a4d571c3fa7ff405dd9f5d1555b4753efb95dc84bb70ce01b6ecb4) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 115.145.171.128`. To see the logs of the worker, use `ray logs worker-38a4d571c3fa7ff405dd9f5d1555b4753efb95dc84bb70ce01b6ecb4*out -ip 115.145.171.128. Top 10 memory users:
PID	MEM(GB)	COMMAND
4942	3.48	/snap/pycharm-professional/408/jbr/bin/java -classpath /snap/pycharm-professional/408/lib/platform-l...
616443	2.85	/home/abdenour/miniconda3/envs/fl_with_flower/bin/python /home/abdenour/PycharmProjects/fl_with_flow...
619615	2.06	ray::ClientAppActor
619616	2.04	ray::ClientAppActor
619611	2.04	ray::ClientAppActor.run
619619	2.03	ray::ClientAppActor.run
619612	1.98	ray::ClientAppActor.run
619613	1.97	ray::ClientAppActor.run
619620	1.97	ray::ClientAppActor
619617	1.96	ray::ClientAppActor
Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.
[2024-08-13 23:58:14,234][flwr][ERROR] - Traceback (most recent call last):
  File "/home/abdenour/miniconda3/envs/fl_with_flower/lib/python3.11/site-packages/flwr/simulation/ray_transport/ray_client_proxy.py", line 94, in _submit_job
    out_mssg, updated_context = self.actor_pool.get_client_result(
                                ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/abdenour/miniconda3/envs/fl_with_flower/lib/python3.11/site-packages/flwr/simulation/ray_transport/ray_actor.py", line 398, in get_client_result
    return self._fetch_future_result(cid)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/abdenour/miniconda3/envs/fl_with_flower/lib/python3.11/site-packages/flwr/simulation/ray_transport/ray_actor.py", line 279, in _fetch_future_result
    res_cid, out_mssg, updated_context = ray.get(
                                         ^^^^^^^^
  File "/home/abdenour/miniconda3/envs/fl_with_flower/lib/python3.11/site-packages/ray/_private/auto_init_hook.py", line 21, in auto_init_wrapper
    return fn(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^
  File "/home/abdenour/miniconda3/envs/fl_with_flower/lib/python3.11/site-packages/ray/_private/client_mode_hook.py", line 103, in wrapper
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "/home/abdenour/miniconda3/envs/fl_with_flower/lib/python3.11/site-packages/ray/_private/worker.py", line 2667, in get
    values, debugger_breakpoint = worker.get_objects(object_refs, timeout=timeout)
                                  ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/abdenour/miniconda3/envs/fl_with_flower/lib/python3.11/site-packages/ray/_private/worker.py", line 866, in get_objects
    raise value
ray.exceptions.OutOfMemoryError: Task was killed due to the node running low on memory.
Memory on the node (IP: 115.145.171.128, ID: c687613ff34cdbf5097699e5c896055ef2dc44343c5ab44bd40309d5) where the task (actor ID: 85928d3f1a247800081c0e1101000000, name=ClientAppActor.__init__, pid=619614, memory used=1.73GB) was running was 29.50GB / 31.02GB (0.951049), which exceeds the memory usage threshold of 0.95. Ray killed this worker (ID: c7f644ac3cca20dffbfa1723daac4f87f04c7de03da25e7cad5fe33c) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 115.145.171.128`. To see the logs of the worker, use `ray logs worker-c7f644ac3cca20dffbfa1723daac4f87f04c7de03da25e7cad5fe33c*out -ip 115.145.171.128. Top 10 memory users:
PID	MEM(GB)	COMMAND
4942	3.49	/snap/pycharm-professional/408/jbr/bin/java -classpath /snap/pycharm-professional/408/lib/platform-l...
619616	1.80	ray::ClientAppActor.run
619615	1.78	ray::ClientAppActor
619617	1.77	ray::ClientAppActor
619611	1.73	ray::ClientAppActor
619618	1.73	ray::ClientAppActor
619614	1.73	ray::ClientAppActor
619612	1.72	ray::ClientAppActor
619619	1.70	ray::ClientAppActor
619620	1.68	ray::ClientAppActor.run
Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.

[2024-08-13 23:58:14,235][flwr][ERROR] - Task was killed due to the node running low on memory.
Memory on the node (IP: 115.145.171.128, ID: c687613ff34cdbf5097699e5c896055ef2dc44343c5ab44bd40309d5) where the task (actor ID: 85928d3f1a247800081c0e1101000000, name=ClientAppActor.__init__, pid=619614, memory used=1.73GB) was running was 29.50GB / 31.02GB (0.951049), which exceeds the memory usage threshold of 0.95. Ray killed this worker (ID: c7f644ac3cca20dffbfa1723daac4f87f04c7de03da25e7cad5fe33c) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 115.145.171.128`. To see the logs of the worker, use `ray logs worker-c7f644ac3cca20dffbfa1723daac4f87f04c7de03da25e7cad5fe33c*out -ip 115.145.171.128. Top 10 memory users:
PID	MEM(GB)	COMMAND
4942	3.49	/snap/pycharm-professional/408/jbr/bin/java -classpath /snap/pycharm-professional/408/lib/platform-l...
619616	1.80	ray::ClientAppActor.run
619615	1.78	ray::ClientAppActor
619617	1.77	ray::ClientAppActor
619611	1.73	ray::ClientAppActor
619618	1.73	ray::ClientAppActor
619614	1.73	ray::ClientAppActor
619612	1.72	ray::ClientAppActor
619619	1.70	ray::ClientAppActor
619620	1.68	ray::ClientAppActor.run
Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.
[2024-08-13 23:58:14,235][flwr][ERROR] - Traceback (most recent call last):
  File "/home/abdenour/miniconda3/envs/fl_with_flower/lib/python3.11/site-packages/flwr/simulation/ray_transport/ray_client_proxy.py", line 94, in _submit_job
    out_mssg, updated_context = self.actor_pool.get_client_result(
                                ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/abdenour/miniconda3/envs/fl_with_flower/lib/python3.11/site-packages/flwr/simulation/ray_transport/ray_actor.py", line 398, in get_client_result
    return self._fetch_future_result(cid)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/abdenour/miniconda3/envs/fl_with_flower/lib/python3.11/site-packages/flwr/simulation/ray_transport/ray_actor.py", line 279, in _fetch_future_result
    res_cid, out_mssg, updated_context = ray.get(
                                         ^^^^^^^^
  File "/home/abdenour/miniconda3/envs/fl_with_flower/lib/python3.11/site-packages/ray/_private/auto_init_hook.py", line 21, in auto_init_wrapper
    return fn(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^
  File "/home/abdenour/miniconda3/envs/fl_with_flower/lib/python3.11/site-packages/ray/_private/client_mode_hook.py", line 103, in wrapper
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "/home/abdenour/miniconda3/envs/fl_with_flower/lib/python3.11/site-packages/ray/_private/worker.py", line 2667, in get
    values, debugger_breakpoint = worker.get_objects(object_refs, timeout=timeout)
                                  ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/abdenour/miniconda3/envs/fl_with_flower/lib/python3.11/site-packages/ray/_private/worker.py", line 866, in get_objects
    raise value
ray.exceptions.OutOfMemoryError: Task was killed due to the node running low on memory.
Memory on the node (IP: 115.145.171.128, ID: c687613ff34cdbf5097699e5c896055ef2dc44343c5ab44bd40309d5) where the task (actor ID: 2a8bb25e35bf7bb5a0a6c00401000000, name=ClientAppActor.__init__, pid=619620, memory used=1.97GB) was running was 29.48GB / 31.02GB (0.950487), which exceeds the memory usage threshold of 0.95. Ray killed this worker (ID: 38a4d571c3fa7ff405dd9f5d1555b4753efb95dc84bb70ce01b6ecb4) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 115.145.171.128`. To see the logs of the worker, use `ray logs worker-38a4d571c3fa7ff405dd9f5d1555b4753efb95dc84bb70ce01b6ecb4*out -ip 115.145.171.128. Top 10 memory users:
PID	MEM(GB)	COMMAND
4942	3.48	/snap/pycharm-professional/408/jbr/bin/java -classpath /snap/pycharm-professional/408/lib/platform-l...
616443	2.85	/home/abdenour/miniconda3/envs/fl_with_flower/bin/python /home/abdenour/PycharmProjects/fl_with_flow...
619615	2.06	ray::ClientAppActor
619616	2.04	ray::ClientAppActor
619611	2.04	ray::ClientAppActor.run
619619	2.03	ray::ClientAppActor.run
619612	1.98	ray::ClientAppActor.run
619613	1.97	ray::ClientAppActor.run
619620	1.97	ray::ClientAppActor
619617	1.96	ray::ClientAppActor
Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.

[2024-08-13 23:58:14,235][flwr][ERROR] - Task was killed due to the node running low on memory.
Memory on the node (IP: 115.145.171.128, ID: c687613ff34cdbf5097699e5c896055ef2dc44343c5ab44bd40309d5) where the task (actor ID: 2a8bb25e35bf7bb5a0a6c00401000000, name=ClientAppActor.__init__, pid=619620, memory used=1.97GB) was running was 29.48GB / 31.02GB (0.950487), which exceeds the memory usage threshold of 0.95. Ray killed this worker (ID: 38a4d571c3fa7ff405dd9f5d1555b4753efb95dc84bb70ce01b6ecb4) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 115.145.171.128`. To see the logs of the worker, use `ray logs worker-38a4d571c3fa7ff405dd9f5d1555b4753efb95dc84bb70ce01b6ecb4*out -ip 115.145.171.128. Top 10 memory users:
PID	MEM(GB)	COMMAND
4942	3.48	/snap/pycharm-professional/408/jbr/bin/java -classpath /snap/pycharm-professional/408/lib/platform-l...
616443	2.85	/home/abdenour/miniconda3/envs/fl_with_flower/bin/python /home/abdenour/PycharmProjects/fl_with_flow...
619615	2.06	ray::ClientAppActor
619616	2.04	ray::ClientAppActor
619611	2.04	ray::ClientAppActor.run
619619	2.03	ray::ClientAppActor.run
619612	1.98	ray::ClientAppActor.run
619613	1.97	ray::ClientAppActor.run
619620	1.97	ray::ClientAppActor
619617	1.96	ray::ClientAppActor
Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.
[2024-08-13 23:58:15,630][flwr][INFO] - aggregate_fit: received 7 results and 3 failures
[2024-08-13 23:58:18,571][flwr][INFO] - fit progress: (283, 20.265084147453308, {'accuracy': 0.7387820512820513}, 1619.8313843070064)
[2024-08-13 23:58:18,571][flwr][INFO] - configure_evaluate: strategy sampled 2 clients (out of 20)
[2024-08-13 23:58:18,777][flwr][INFO] - aggregate_evaluate: received 2 results and 0 failures
[2024-08-13 23:58:18,777][flwr][INFO] - 
[2024-08-13 23:58:18,777][flwr][INFO] - [ROUND 284]
[2024-08-13 23:58:18,777][flwr][INFO] - configure_fit: strategy sampled 10 clients (out of 20)
[2024-08-13 23:58:18,923][flwr][ERROR] - Traceback (most recent call last):
  File "/home/abdenour/miniconda3/envs/fl_with_flower/lib/python3.11/site-packages/flwr/simulation/ray_transport/ray_client_proxy.py", line 94, in _submit_job
    out_mssg, updated_context = self.actor_pool.get_client_result(
                                ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/abdenour/miniconda3/envs/fl_with_flower/lib/python3.11/site-packages/flwr/simulation/ray_transport/ray_actor.py", line 398, in get_client_result
    return self._fetch_future_result(cid)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/abdenour/miniconda3/envs/fl_with_flower/lib/python3.11/site-packages/flwr/simulation/ray_transport/ray_actor.py", line 279, in _fetch_future_result
    res_cid, out_mssg, updated_context = ray.get(
                                         ^^^^^^^^
  File "/home/abdenour/miniconda3/envs/fl_with_flower/lib/python3.11/site-packages/ray/_private/auto_init_hook.py", line 21, in auto_init_wrapper
    return fn(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^
  File "/home/abdenour/miniconda3/envs/fl_with_flower/lib/python3.11/site-packages/ray/_private/client_mode_hook.py", line 103, in wrapper
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "/home/abdenour/miniconda3/envs/fl_with_flower/lib/python3.11/site-packages/ray/_private/worker.py", line 2667, in get
    values, debugger_breakpoint = worker.get_objects(object_refs, timeout=timeout)
                                  ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/abdenour/miniconda3/envs/fl_with_flower/lib/python3.11/site-packages/ray/_private/worker.py", line 866, in get_objects
    raise value
ray.exceptions.OutOfMemoryError: Task was killed due to the node running low on memory.
Memory on the node (IP: 115.145.171.128, ID: c687613ff34cdbf5097699e5c896055ef2dc44343c5ab44bd40309d5) where the task (actor ID: 2a8bb25e35bf7bb5a0a6c00401000000, name=ClientAppActor.__init__, pid=619620, memory used=1.97GB) was running was 29.48GB / 31.02GB (0.950487), which exceeds the memory usage threshold of 0.95. Ray killed this worker (ID: 38a4d571c3fa7ff405dd9f5d1555b4753efb95dc84bb70ce01b6ecb4) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 115.145.171.128`. To see the logs of the worker, use `ray logs worker-38a4d571c3fa7ff405dd9f5d1555b4753efb95dc84bb70ce01b6ecb4*out -ip 115.145.171.128. Top 10 memory users:
PID	MEM(GB)	COMMAND
4942	3.48	/snap/pycharm-professional/408/jbr/bin/java -classpath /snap/pycharm-professional/408/lib/platform-l...
616443	2.85	/home/abdenour/miniconda3/envs/fl_with_flower/bin/python /home/abdenour/PycharmProjects/fl_with_flow...
619615	2.06	ray::ClientAppActor
619616	2.04	ray::ClientAppActor
619611	2.04	ray::ClientAppActor.run
619619	2.03	ray::ClientAppActor.run
619612	1.98	ray::ClientAppActor.run
619613	1.97	ray::ClientAppActor.run
619620	1.97	ray::ClientAppActor
619617	1.96	ray::ClientAppActor
Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.

[2024-08-13 23:58:18,923][flwr][ERROR] - Task was killed due to the node running low on memory.
Memory on the node (IP: 115.145.171.128, ID: c687613ff34cdbf5097699e5c896055ef2dc44343c5ab44bd40309d5) where the task (actor ID: 2a8bb25e35bf7bb5a0a6c00401000000, name=ClientAppActor.__init__, pid=619620, memory used=1.97GB) was running was 29.48GB / 31.02GB (0.950487), which exceeds the memory usage threshold of 0.95. Ray killed this worker (ID: 38a4d571c3fa7ff405dd9f5d1555b4753efb95dc84bb70ce01b6ecb4) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 115.145.171.128`. To see the logs of the worker, use `ray logs worker-38a4d571c3fa7ff405dd9f5d1555b4753efb95dc84bb70ce01b6ecb4*out -ip 115.145.171.128. Top 10 memory users:
PID	MEM(GB)	COMMAND
4942	3.48	/snap/pycharm-professional/408/jbr/bin/java -classpath /snap/pycharm-professional/408/lib/platform-l...
616443	2.85	/home/abdenour/miniconda3/envs/fl_with_flower/bin/python /home/abdenour/PycharmProjects/fl_with_flow...
619615	2.06	ray::ClientAppActor
619616	2.04	ray::ClientAppActor
619611	2.04	ray::ClientAppActor.run
619619	2.03	ray::ClientAppActor.run
619612	1.98	ray::ClientAppActor.run
619613	1.97	ray::ClientAppActor.run
619620	1.97	ray::ClientAppActor
619617	1.96	ray::ClientAppActor
Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.
[2024-08-13 23:58:18,924][flwr][ERROR] - Traceback (most recent call last):
  File "/home/abdenour/miniconda3/envs/fl_with_flower/lib/python3.11/site-packages/flwr/simulation/ray_transport/ray_client_proxy.py", line 94, in _submit_job
    out_mssg, updated_context = self.actor_pool.get_client_result(
                                ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/abdenour/miniconda3/envs/fl_with_flower/lib/python3.11/site-packages/flwr/simulation/ray_transport/ray_actor.py", line 398, in get_client_result
    return self._fetch_future_result(cid)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/abdenour/miniconda3/envs/fl_with_flower/lib/python3.11/site-packages/flwr/simulation/ray_transport/ray_actor.py", line 279, in _fetch_future_result
    res_cid, out_mssg, updated_context = ray.get(
                                         ^^^^^^^^
  File "/home/abdenour/miniconda3/envs/fl_with_flower/lib/python3.11/site-packages/ray/_private/auto_init_hook.py", line 21, in auto_init_wrapper
    return fn(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^
  File "/home/abdenour/miniconda3/envs/fl_with_flower/lib/python3.11/site-packages/ray/_private/client_mode_hook.py", line 103, in wrapper
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "/home/abdenour/miniconda3/envs/fl_with_flower/lib/python3.11/site-packages/ray/_private/worker.py", line 2667, in get
    values, debugger_breakpoint = worker.get_objects(object_refs, timeout=timeout)
                                  ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/abdenour/miniconda3/envs/fl_with_flower/lib/python3.11/site-packages/ray/_private/worker.py", line 866, in get_objects
    raise value
ray.exceptions.OutOfMemoryError: Task was killed due to the node running low on memory.
Memory on the node (IP: 115.145.171.128, ID: c687613ff34cdbf5097699e5c896055ef2dc44343c5ab44bd40309d5) where the task (actor ID: 85928d3f1a247800081c0e1101000000, name=ClientAppActor.__init__, pid=619614, memory used=1.73GB) was running was 29.50GB / 31.02GB (0.951049), which exceeds the memory usage threshold of 0.95. Ray killed this worker (ID: c7f644ac3cca20dffbfa1723daac4f87f04c7de03da25e7cad5fe33c) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 115.145.171.128`. To see the logs of the worker, use `ray logs worker-c7f644ac3cca20dffbfa1723daac4f87f04c7de03da25e7cad5fe33c*out -ip 115.145.171.128. Top 10 memory users:
PID	MEM(GB)	COMMAND
4942	3.49	/snap/pycharm-professional/408/jbr/bin/java -classpath /snap/pycharm-professional/408/lib/platform-l...
619616	1.80	ray::ClientAppActor.run
619615	1.78	ray::ClientAppActor
619617	1.77	ray::ClientAppActor
619611	1.73	ray::ClientAppActor
619618	1.73	ray::ClientAppActor
619614	1.73	ray::ClientAppActor
619612	1.72	ray::ClientAppActor
619619	1.70	ray::ClientAppActor
619620	1.68	ray::ClientAppActor.run
Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.

[2024-08-13 23:58:18,924][flwr][ERROR] - Task was killed due to the node running low on memory.
Memory on the node (IP: 115.145.171.128, ID: c687613ff34cdbf5097699e5c896055ef2dc44343c5ab44bd40309d5) where the task (actor ID: 85928d3f1a247800081c0e1101000000, name=ClientAppActor.__init__, pid=619614, memory used=1.73GB) was running was 29.50GB / 31.02GB (0.951049), which exceeds the memory usage threshold of 0.95. Ray killed this worker (ID: c7f644ac3cca20dffbfa1723daac4f87f04c7de03da25e7cad5fe33c) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 115.145.171.128`. To see the logs of the worker, use `ray logs worker-c7f644ac3cca20dffbfa1723daac4f87f04c7de03da25e7cad5fe33c*out -ip 115.145.171.128. Top 10 memory users:
PID	MEM(GB)	COMMAND
4942	3.49	/snap/pycharm-professional/408/jbr/bin/java -classpath /snap/pycharm-professional/408/lib/platform-l...
619616	1.80	ray::ClientAppActor.run
619615	1.78	ray::ClientAppActor
619617	1.77	ray::ClientAppActor
619611	1.73	ray::ClientAppActor
619618	1.73	ray::ClientAppActor
619614	1.73	ray::ClientAppActor
619612	1.72	ray::ClientAppActor
619619	1.70	ray::ClientAppActor
619620	1.68	ray::ClientAppActor.run
Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.
[2024-08-13 23:58:18,924][flwr][ERROR] - Traceback (most recent call last):
  File "/home/abdenour/miniconda3/envs/fl_with_flower/lib/python3.11/site-packages/flwr/simulation/ray_transport/ray_client_proxy.py", line 94, in _submit_job
    out_mssg, updated_context = self.actor_pool.get_client_result(
                                ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/abdenour/miniconda3/envs/fl_with_flower/lib/python3.11/site-packages/flwr/simulation/ray_transport/ray_actor.py", line 398, in get_client_result
    return self._fetch_future_result(cid)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/abdenour/miniconda3/envs/fl_with_flower/lib/python3.11/site-packages/flwr/simulation/ray_transport/ray_actor.py", line 279, in _fetch_future_result
    res_cid, out_mssg, updated_context = ray.get(
                                         ^^^^^^^^
  File "/home/abdenour/miniconda3/envs/fl_with_flower/lib/python3.11/site-packages/ray/_private/auto_init_hook.py", line 21, in auto_init_wrapper
    return fn(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^
  File "/home/abdenour/miniconda3/envs/fl_with_flower/lib/python3.11/site-packages/ray/_private/client_mode_hook.py", line 103, in wrapper
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "/home/abdenour/miniconda3/envs/fl_with_flower/lib/python3.11/site-packages/ray/_private/worker.py", line 2667, in get
    values, debugger_breakpoint = worker.get_objects(object_refs, timeout=timeout)
                                  ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/abdenour/miniconda3/envs/fl_with_flower/lib/python3.11/site-packages/ray/_private/worker.py", line 866, in get_objects
    raise value
ray.exceptions.OutOfMemoryError: Task was killed due to the node running low on memory.
Memory on the node (IP: 115.145.171.128, ID: c687613ff34cdbf5097699e5c896055ef2dc44343c5ab44bd40309d5) where the task (actor ID: 52365bb7997979e5a4c7ddbd01000000, name=ClientAppActor.__init__, pid=619618, memory used=1.84GB) was running was 29.49GB / 31.02GB (0.950707), which exceeds the memory usage threshold of 0.95. Ray killed this worker (ID: 7e48cd46c72821b17be65b05a4e61b5846ba998a57d76db44ca4f8fa) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 115.145.171.128`. To see the logs of the worker, use `ray logs worker-7e48cd46c72821b17be65b05a4e61b5846ba998a57d76db44ca4f8fa*out -ip 115.145.171.128. Top 10 memory users:
PID	MEM(GB)	COMMAND
4942	3.48	/snap/pycharm-professional/408/jbr/bin/java -classpath /snap/pycharm-professional/408/lib/platform-l...
616443	2.14	/home/abdenour/miniconda3/envs/fl_with_flower/bin/python /home/abdenour/PycharmProjects/fl_with_flow...
619615	1.90	ray::ClientAppActor
619616	1.88	ray::ClientAppActor
619611	1.87	ray::ClientAppActor
619617	1.86	ray::ClientAppActor
619620	1.85	ray::ClientAppActor
619618	1.84	ray::ClientAppActor
619613	1.82	ray::ClientAppActor
619619	1.80	ray::ClientAppActor
Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.

[2024-08-13 23:58:18,924][flwr][ERROR] - Task was killed due to the node running low on memory.
Memory on the node (IP: 115.145.171.128, ID: c687613ff34cdbf5097699e5c896055ef2dc44343c5ab44bd40309d5) where the task (actor ID: 52365bb7997979e5a4c7ddbd01000000, name=ClientAppActor.__init__, pid=619618, memory used=1.84GB) was running was 29.49GB / 31.02GB (0.950707), which exceeds the memory usage threshold of 0.95. Ray killed this worker (ID: 7e48cd46c72821b17be65b05a4e61b5846ba998a57d76db44ca4f8fa) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 115.145.171.128`. To see the logs of the worker, use `ray logs worker-7e48cd46c72821b17be65b05a4e61b5846ba998a57d76db44ca4f8fa*out -ip 115.145.171.128. Top 10 memory users:
PID	MEM(GB)	COMMAND
4942	3.48	/snap/pycharm-professional/408/jbr/bin/java -classpath /snap/pycharm-professional/408/lib/platform-l...
616443	2.14	/home/abdenour/miniconda3/envs/fl_with_flower/bin/python /home/abdenour/PycharmProjects/fl_with_flow...
619615	1.90	ray::ClientAppActor
619616	1.88	ray::ClientAppActor
619611	1.87	ray::ClientAppActor
619617	1.86	ray::ClientAppActor
619620	1.85	ray::ClientAppActor
619618	1.84	ray::ClientAppActor
619613	1.82	ray::ClientAppActor
619619	1.80	ray::ClientAppActor
Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.
[2024-08-13 23:58:20,489][flwr][INFO] - aggregate_fit: received 7 results and 3 failures
[2024-08-13 23:58:23,739][flwr][INFO] - fit progress: (284, 17.133547961711884, {'accuracy': 0.7516025641025641}, 1624.9989685740147)
[2024-08-13 23:58:23,739][flwr][INFO] - configure_evaluate: strategy sampled 2 clients (out of 20)
[2024-08-13 23:58:24,070][flwr][INFO] - aggregate_evaluate: received 2 results and 0 failures
[2024-08-13 23:58:24,070][flwr][INFO] - 
[2024-08-13 23:58:24,070][flwr][INFO] - [ROUND 285]
[2024-08-13 23:58:24,070][flwr][INFO] - configure_fit: strategy sampled 10 clients (out of 20)
[2024-08-13 23:58:24,204][flwr][ERROR] - Traceback (most recent call last):
  File "/home/abdenour/miniconda3/envs/fl_with_flower/lib/python3.11/site-packages/flwr/simulation/ray_transport/ray_client_proxy.py", line 94, in _submit_job
    out_mssg, updated_context = self.actor_pool.get_client_result(
                                ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/abdenour/miniconda3/envs/fl_with_flower/lib/python3.11/site-packages/flwr/simulation/ray_transport/ray_actor.py", line 398, in get_client_result
    return self._fetch_future_result(cid)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/abdenour/miniconda3/envs/fl_with_flower/lib/python3.11/site-packages/flwr/simulation/ray_transport/ray_actor.py", line 279, in _fetch_future_result
    res_cid, out_mssg, updated_context = ray.get(
                                         ^^^^^^^^
  File "/home/abdenour/miniconda3/envs/fl_with_flower/lib/python3.11/site-packages/ray/_private/auto_init_hook.py", line 21, in auto_init_wrapper
    return fn(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^
  File "/home/abdenour/miniconda3/envs/fl_with_flower/lib/python3.11/site-packages/ray/_private/client_mode_hook.py", line 103, in wrapper
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "/home/abdenour/miniconda3/envs/fl_with_flower/lib/python3.11/site-packages/ray/_private/worker.py", line 2667, in get
    values, debugger_breakpoint = worker.get_objects(object_refs, timeout=timeout)
                                  ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/abdenour/miniconda3/envs/fl_with_flower/lib/python3.11/site-packages/ray/_private/worker.py", line 866, in get_objects
    raise value
ray.exceptions.OutOfMemoryError: Task was killed due to the node running low on memory.
Memory on the node (IP: 115.145.171.128, ID: c687613ff34cdbf5097699e5c896055ef2dc44343c5ab44bd40309d5) where the task (actor ID: 52365bb7997979e5a4c7ddbd01000000, name=ClientAppActor.__init__, pid=619618, memory used=1.84GB) was running was 29.49GB / 31.02GB (0.950707), which exceeds the memory usage threshold of 0.95. Ray killed this worker (ID: 7e48cd46c72821b17be65b05a4e61b5846ba998a57d76db44ca4f8fa) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 115.145.171.128`. To see the logs of the worker, use `ray logs worker-7e48cd46c72821b17be65b05a4e61b5846ba998a57d76db44ca4f8fa*out -ip 115.145.171.128. Top 10 memory users:
PID	MEM(GB)	COMMAND
4942	3.48	/snap/pycharm-professional/408/jbr/bin/java -classpath /snap/pycharm-professional/408/lib/platform-l...
616443	2.14	/home/abdenour/miniconda3/envs/fl_with_flower/bin/python /home/abdenour/PycharmProjects/fl_with_flow...
619615	1.90	ray::ClientAppActor
619616	1.88	ray::ClientAppActor
619611	1.87	ray::ClientAppActor
619617	1.86	ray::ClientAppActor
619620	1.85	ray::ClientAppActor
619618	1.84	ray::ClientAppActor
619613	1.82	ray::ClientAppActor
619619	1.80	ray::ClientAppActor
Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.

[2024-08-13 23:58:24,205][flwr][ERROR] - Task was killed due to the node running low on memory.
Memory on the node (IP: 115.145.171.128, ID: c687613ff34cdbf5097699e5c896055ef2dc44343c5ab44bd40309d5) where the task (actor ID: 52365bb7997979e5a4c7ddbd01000000, name=ClientAppActor.__init__, pid=619618, memory used=1.84GB) was running was 29.49GB / 31.02GB (0.950707), which exceeds the memory usage threshold of 0.95. Ray killed this worker (ID: 7e48cd46c72821b17be65b05a4e61b5846ba998a57d76db44ca4f8fa) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 115.145.171.128`. To see the logs of the worker, use `ray logs worker-7e48cd46c72821b17be65b05a4e61b5846ba998a57d76db44ca4f8fa*out -ip 115.145.171.128. Top 10 memory users:
PID	MEM(GB)	COMMAND
4942	3.48	/snap/pycharm-professional/408/jbr/bin/java -classpath /snap/pycharm-professional/408/lib/platform-l...
616443	2.14	/home/abdenour/miniconda3/envs/fl_with_flower/bin/python /home/abdenour/PycharmProjects/fl_with_flow...
619615	1.90	ray::ClientAppActor
619616	1.88	ray::ClientAppActor
619611	1.87	ray::ClientAppActor
619617	1.86	ray::ClientAppActor
619620	1.85	ray::ClientAppActor
619618	1.84	ray::ClientAppActor
619613	1.82	ray::ClientAppActor
619619	1.80	ray::ClientAppActor
Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.
[2024-08-13 23:58:24,231][flwr][ERROR] - Traceback (most recent call last):
  File "/home/abdenour/miniconda3/envs/fl_with_flower/lib/python3.11/site-packages/flwr/simulation/ray_transport/ray_client_proxy.py", line 94, in _submit_job
    out_mssg, updated_context = self.actor_pool.get_client_result(
                                ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/abdenour/miniconda3/envs/fl_with_flower/lib/python3.11/site-packages/flwr/simulation/ray_transport/ray_actor.py", line 398, in get_client_result
    return self._fetch_future_result(cid)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/abdenour/miniconda3/envs/fl_with_flower/lib/python3.11/site-packages/flwr/simulation/ray_transport/ray_actor.py", line 279, in _fetch_future_result
    res_cid, out_mssg, updated_context = ray.get(
                                         ^^^^^^^^
  File "/home/abdenour/miniconda3/envs/fl_with_flower/lib/python3.11/site-packages/ray/_private/auto_init_hook.py", line 21, in auto_init_wrapper
    return fn(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^
  File "/home/abdenour/miniconda3/envs/fl_with_flower/lib/python3.11/site-packages/ray/_private/client_mode_hook.py", line 103, in wrapper
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "/home/abdenour/miniconda3/envs/fl_with_flower/lib/python3.11/site-packages/ray/_private/worker.py", line 2667, in get
    values, debugger_breakpoint = worker.get_objects(object_refs, timeout=timeout)
                                  ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/abdenour/miniconda3/envs/fl_with_flower/lib/python3.11/site-packages/ray/_private/worker.py", line 866, in get_objects
    raise value
ray.exceptions.OutOfMemoryError: Task was killed due to the node running low on memory.
Memory on the node (IP: 115.145.171.128, ID: c687613ff34cdbf5097699e5c896055ef2dc44343c5ab44bd40309d5) where the task (actor ID: 52365bb7997979e5a4c7ddbd01000000, name=ClientAppActor.__init__, pid=619618, memory used=1.84GB) was running was 29.49GB / 31.02GB (0.950707), which exceeds the memory usage threshold of 0.95. Ray killed this worker (ID: 7e48cd46c72821b17be65b05a4e61b5846ba998a57d76db44ca4f8fa) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 115.145.171.128`. To see the logs of the worker, use `ray logs worker-7e48cd46c72821b17be65b05a4e61b5846ba998a57d76db44ca4f8fa*out -ip 115.145.171.128. Top 10 memory users:
PID	MEM(GB)	COMMAND
4942	3.48	/snap/pycharm-professional/408/jbr/bin/java -classpath /snap/pycharm-professional/408/lib/platform-l...
616443	2.14	/home/abdenour/miniconda3/envs/fl_with_flower/bin/python /home/abdenour/PycharmProjects/fl_with_flow...
619615	1.90	ray::ClientAppActor
619616	1.88	ray::ClientAppActor
619611	1.87	ray::ClientAppActor
619617	1.86	ray::ClientAppActor
619620	1.85	ray::ClientAppActor
619618	1.84	ray::ClientAppActor
619613	1.82	ray::ClientAppActor
619619	1.80	ray::ClientAppActor
Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.

[2024-08-13 23:58:24,232][flwr][ERROR] - Task was killed due to the node running low on memory.
Memory on the node (IP: 115.145.171.128, ID: c687613ff34cdbf5097699e5c896055ef2dc44343c5ab44bd40309d5) where the task (actor ID: 52365bb7997979e5a4c7ddbd01000000, name=ClientAppActor.__init__, pid=619618, memory used=1.84GB) was running was 29.49GB / 31.02GB (0.950707), which exceeds the memory usage threshold of 0.95. Ray killed this worker (ID: 7e48cd46c72821b17be65b05a4e61b5846ba998a57d76db44ca4f8fa) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 115.145.171.128`. To see the logs of the worker, use `ray logs worker-7e48cd46c72821b17be65b05a4e61b5846ba998a57d76db44ca4f8fa*out -ip 115.145.171.128. Top 10 memory users:
PID	MEM(GB)	COMMAND
4942	3.48	/snap/pycharm-professional/408/jbr/bin/java -classpath /snap/pycharm-professional/408/lib/platform-l...
616443	2.14	/home/abdenour/miniconda3/envs/fl_with_flower/bin/python /home/abdenour/PycharmProjects/fl_with_flow...
619615	1.90	ray::ClientAppActor
619616	1.88	ray::ClientAppActor
619611	1.87	ray::ClientAppActor
619617	1.86	ray::ClientAppActor
619620	1.85	ray::ClientAppActor
619618	1.84	ray::ClientAppActor
619613	1.82	ray::ClientAppActor
619619	1.80	ray::ClientAppActor
Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.
[2024-08-13 23:58:24,233][flwr][ERROR] - Traceback (most recent call last):
  File "/home/abdenour/miniconda3/envs/fl_with_flower/lib/python3.11/site-packages/flwr/simulation/ray_transport/ray_client_proxy.py", line 94, in _submit_job
    out_mssg, updated_context = self.actor_pool.get_client_result(
                                ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/abdenour/miniconda3/envs/fl_with_flower/lib/python3.11/site-packages/flwr/simulation/ray_transport/ray_actor.py", line 398, in get_client_result
    return self._fetch_future_result(cid)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/abdenour/miniconda3/envs/fl_with_flower/lib/python3.11/site-packages/flwr/simulation/ray_transport/ray_actor.py", line 279, in _fetch_future_result
    res_cid, out_mssg, updated_context = ray.get(
                                         ^^^^^^^^
  File "/home/abdenour/miniconda3/envs/fl_with_flower/lib/python3.11/site-packages/ray/_private/auto_init_hook.py", line 21, in auto_init_wrapper
    return fn(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^
  File "/home/abdenour/miniconda3/envs/fl_with_flower/lib/python3.11/site-packages/ray/_private/client_mode_hook.py", line 103, in wrapper
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "/home/abdenour/miniconda3/envs/fl_with_flower/lib/python3.11/site-packages/ray/_private/worker.py", line 2667, in get
    values, debugger_breakpoint = worker.get_objects(object_refs, timeout=timeout)
                                  ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/abdenour/miniconda3/envs/fl_with_flower/lib/python3.11/site-packages/ray/_private/worker.py", line 866, in get_objects
    raise value
ray.exceptions.OutOfMemoryError: Task was killed due to the node running low on memory.
Memory on the node (IP: 115.145.171.128, ID: c687613ff34cdbf5097699e5c896055ef2dc44343c5ab44bd40309d5) where the task (actor ID: 85928d3f1a247800081c0e1101000000, name=ClientAppActor.__init__, pid=619614, memory used=1.73GB) was running was 29.50GB / 31.02GB (0.951049), which exceeds the memory usage threshold of 0.95. Ray killed this worker (ID: c7f644ac3cca20dffbfa1723daac4f87f04c7de03da25e7cad5fe33c) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 115.145.171.128`. To see the logs of the worker, use `ray logs worker-c7f644ac3cca20dffbfa1723daac4f87f04c7de03da25e7cad5fe33c*out -ip 115.145.171.128. Top 10 memory users:
PID	MEM(GB)	COMMAND
4942	3.49	/snap/pycharm-professional/408/jbr/bin/java -classpath /snap/pycharm-professional/408/lib/platform-l...
619616	1.80	ray::ClientAppActor.run
619615	1.78	ray::ClientAppActor
619617	1.77	ray::ClientAppActor
619611	1.73	ray::ClientAppActor
619618	1.73	ray::ClientAppActor
619614	1.73	ray::ClientAppActor
619612	1.72	ray::ClientAppActor
619619	1.70	ray::ClientAppActor
619620	1.68	ray::ClientAppActor.run
Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.

[2024-08-13 23:58:24,233][flwr][ERROR] - Task was killed due to the node running low on memory.
Memory on the node (IP: 115.145.171.128, ID: c687613ff34cdbf5097699e5c896055ef2dc44343c5ab44bd40309d5) where the task (actor ID: 85928d3f1a247800081c0e1101000000, name=ClientAppActor.__init__, pid=619614, memory used=1.73GB) was running was 29.50GB / 31.02GB (0.951049), which exceeds the memory usage threshold of 0.95. Ray killed this worker (ID: c7f644ac3cca20dffbfa1723daac4f87f04c7de03da25e7cad5fe33c) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 115.145.171.128`. To see the logs of the worker, use `ray logs worker-c7f644ac3cca20dffbfa1723daac4f87f04c7de03da25e7cad5fe33c*out -ip 115.145.171.128. Top 10 memory users:
PID	MEM(GB)	COMMAND
4942	3.49	/snap/pycharm-professional/408/jbr/bin/java -classpath /snap/pycharm-professional/408/lib/platform-l...
619616	1.80	ray::ClientAppActor.run
619615	1.78	ray::ClientAppActor
619617	1.77	ray::ClientAppActor
619611	1.73	ray::ClientAppActor
619618	1.73	ray::ClientAppActor
619614	1.73	ray::ClientAppActor
619612	1.72	ray::ClientAppActor
619619	1.70	ray::ClientAppActor
619620	1.68	ray::ClientAppActor.run
Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.
[2024-08-13 23:58:25,695][flwr][INFO] - aggregate_fit: received 7 results and 3 failures
[2024-08-13 23:58:28,642][flwr][INFO] - fit progress: (285, 17.29985386133194, {'accuracy': 0.7516025641025641}, 1629.902733526018)
[2024-08-13 23:58:28,643][flwr][INFO] - configure_evaluate: strategy sampled 2 clients (out of 20)
[2024-08-13 23:58:28,842][flwr][INFO] - aggregate_evaluate: received 2 results and 0 failures
[2024-08-13 23:58:28,843][flwr][INFO] - 
[2024-08-13 23:58:28,843][flwr][INFO] - [ROUND 286]
[2024-08-13 23:58:28,843][flwr][INFO] - configure_fit: strategy sampled 10 clients (out of 20)
[2024-08-13 23:58:29,011][flwr][ERROR] - Traceback (most recent call last):
  File "/home/abdenour/miniconda3/envs/fl_with_flower/lib/python3.11/site-packages/flwr/simulation/ray_transport/ray_client_proxy.py", line 94, in _submit_job
    out_mssg, updated_context = self.actor_pool.get_client_result(
                                ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/abdenour/miniconda3/envs/fl_with_flower/lib/python3.11/site-packages/flwr/simulation/ray_transport/ray_actor.py", line 398, in get_client_result
    return self._fetch_future_result(cid)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/abdenour/miniconda3/envs/fl_with_flower/lib/python3.11/site-packages/flwr/simulation/ray_transport/ray_actor.py", line 279, in _fetch_future_result
    res_cid, out_mssg, updated_context = ray.get(
                                         ^^^^^^^^
  File "/home/abdenour/miniconda3/envs/fl_with_flower/lib/python3.11/site-packages/ray/_private/auto_init_hook.py", line 21, in auto_init_wrapper
    return fn(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^
  File "/home/abdenour/miniconda3/envs/fl_with_flower/lib/python3.11/site-packages/ray/_private/client_mode_hook.py", line 103, in wrapper
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "/home/abdenour/miniconda3/envs/fl_with_flower/lib/python3.11/site-packages/ray/_private/worker.py", line 2667, in get
    values, debugger_breakpoint = worker.get_objects(object_refs, timeout=timeout)
                                  ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/abdenour/miniconda3/envs/fl_with_flower/lib/python3.11/site-packages/ray/_private/worker.py", line 866, in get_objects
    raise value
ray.exceptions.OutOfMemoryError: Task was killed due to the node running low on memory.
Memory on the node (IP: 115.145.171.128, ID: c687613ff34cdbf5097699e5c896055ef2dc44343c5ab44bd40309d5) where the task (actor ID: 52365bb7997979e5a4c7ddbd01000000, name=ClientAppActor.__init__, pid=619618, memory used=1.84GB) was running was 29.49GB / 31.02GB (0.950707), which exceeds the memory usage threshold of 0.95. Ray killed this worker (ID: 7e48cd46c72821b17be65b05a4e61b5846ba998a57d76db44ca4f8fa) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 115.145.171.128`. To see the logs of the worker, use `ray logs worker-7e48cd46c72821b17be65b05a4e61b5846ba998a57d76db44ca4f8fa*out -ip 115.145.171.128. Top 10 memory users:
PID	MEM(GB)	COMMAND
4942	3.48	/snap/pycharm-professional/408/jbr/bin/java -classpath /snap/pycharm-professional/408/lib/platform-l...
616443	2.14	/home/abdenour/miniconda3/envs/fl_with_flower/bin/python /home/abdenour/PycharmProjects/fl_with_flow...
619615	1.90	ray::ClientAppActor
619616	1.88	ray::ClientAppActor
619611	1.87	ray::ClientAppActor
619617	1.86	ray::ClientAppActor
619620	1.85	ray::ClientAppActor
619618	1.84	ray::ClientAppActor
619613	1.82	ray::ClientAppActor
619619	1.80	ray::ClientAppActor
Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.

[2024-08-13 23:58:29,015][flwr][ERROR] - Task was killed due to the node running low on memory.
Memory on the node (IP: 115.145.171.128, ID: c687613ff34cdbf5097699e5c896055ef2dc44343c5ab44bd40309d5) where the task (actor ID: 52365bb7997979e5a4c7ddbd01000000, name=ClientAppActor.__init__, pid=619618, memory used=1.84GB) was running was 29.49GB / 31.02GB (0.950707), which exceeds the memory usage threshold of 0.95. Ray killed this worker (ID: 7e48cd46c72821b17be65b05a4e61b5846ba998a57d76db44ca4f8fa) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 115.145.171.128`. To see the logs of the worker, use `ray logs worker-7e48cd46c72821b17be65b05a4e61b5846ba998a57d76db44ca4f8fa*out -ip 115.145.171.128. Top 10 memory users:
PID	MEM(GB)	COMMAND
4942	3.48	/snap/pycharm-professional/408/jbr/bin/java -classpath /snap/pycharm-professional/408/lib/platform-l...
616443	2.14	/home/abdenour/miniconda3/envs/fl_with_flower/bin/python /home/abdenour/PycharmProjects/fl_with_flow...
619615	1.90	ray::ClientAppActor
619616	1.88	ray::ClientAppActor
619611	1.87	ray::ClientAppActor
619617	1.86	ray::ClientAppActor
619620	1.85	ray::ClientAppActor
619618	1.84	ray::ClientAppActor
619613	1.82	ray::ClientAppActor
619619	1.80	ray::ClientAppActor
Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.
[2024-08-13 23:58:29,014][flwr][ERROR] - Traceback (most recent call last):
  File "/home/abdenour/miniconda3/envs/fl_with_flower/lib/python3.11/site-packages/flwr/simulation/ray_transport/ray_client_proxy.py", line 94, in _submit_job
    out_mssg, updated_context = self.actor_pool.get_client_result(
                                ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/abdenour/miniconda3/envs/fl_with_flower/lib/python3.11/site-packages/flwr/simulation/ray_transport/ray_actor.py", line 398, in get_client_result
    return self._fetch_future_result(cid)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/abdenour/miniconda3/envs/fl_with_flower/lib/python3.11/site-packages/flwr/simulation/ray_transport/ray_actor.py", line 279, in _fetch_future_result
    res_cid, out_mssg, updated_context = ray.get(
                                         ^^^^^^^^
  File "/home/abdenour/miniconda3/envs/fl_with_flower/lib/python3.11/site-packages/ray/_private/auto_init_hook.py", line 21, in auto_init_wrapper
    return fn(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^
  File "/home/abdenour/miniconda3/envs/fl_with_flower/lib/python3.11/site-packages/ray/_private/client_mode_hook.py", line 103, in wrapper
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "/home/abdenour/miniconda3/envs/fl_with_flower/lib/python3.11/site-packages/ray/_private/worker.py", line 2667, in get
    values, debugger_breakpoint = worker.get_objects(object_refs, timeout=timeout)
                                  ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/abdenour/miniconda3/envs/fl_with_flower/lib/python3.11/site-packages/ray/_private/worker.py", line 866, in get_objects
    raise value
ray.exceptions.OutOfMemoryError: Task was killed due to the node running low on memory.
Memory on the node (IP: 115.145.171.128, ID: c687613ff34cdbf5097699e5c896055ef2dc44343c5ab44bd40309d5) where the task (actor ID: 85928d3f1a247800081c0e1101000000, name=ClientAppActor.__init__, pid=619614, memory used=1.73GB) was running was 29.50GB / 31.02GB (0.951049), which exceeds the memory usage threshold of 0.95. Ray killed this worker (ID: c7f644ac3cca20dffbfa1723daac4f87f04c7de03da25e7cad5fe33c) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 115.145.171.128`. To see the logs of the worker, use `ray logs worker-c7f644ac3cca20dffbfa1723daac4f87f04c7de03da25e7cad5fe33c*out -ip 115.145.171.128. Top 10 memory users:
PID	MEM(GB)	COMMAND
4942	3.49	/snap/pycharm-professional/408/jbr/bin/java -classpath /snap/pycharm-professional/408/lib/platform-l...
619616	1.80	ray::ClientAppActor.run
619615	1.78	ray::ClientAppActor
619617	1.77	ray::ClientAppActor
619611	1.73	ray::ClientAppActor
619618	1.73	ray::ClientAppActor
619614	1.73	ray::ClientAppActor
619612	1.72	ray::ClientAppActor
619619	1.70	ray::ClientAppActor
619620	1.68	ray::ClientAppActor.run
Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.

[2024-08-13 23:58:29,015][flwr][ERROR] - Task was killed due to the node running low on memory.
Memory on the node (IP: 115.145.171.128, ID: c687613ff34cdbf5097699e5c896055ef2dc44343c5ab44bd40309d5) where the task (actor ID: 85928d3f1a247800081c0e1101000000, name=ClientAppActor.__init__, pid=619614, memory used=1.73GB) was running was 29.50GB / 31.02GB (0.951049), which exceeds the memory usage threshold of 0.95. Ray killed this worker (ID: c7f644ac3cca20dffbfa1723daac4f87f04c7de03da25e7cad5fe33c) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 115.145.171.128`. To see the logs of the worker, use `ray logs worker-c7f644ac3cca20dffbfa1723daac4f87f04c7de03da25e7cad5fe33c*out -ip 115.145.171.128. Top 10 memory users:
PID	MEM(GB)	COMMAND
4942	3.49	/snap/pycharm-professional/408/jbr/bin/java -classpath /snap/pycharm-professional/408/lib/platform-l...
619616	1.80	ray::ClientAppActor.run
619615	1.78	ray::ClientAppActor
619617	1.77	ray::ClientAppActor
619611	1.73	ray::ClientAppActor
619618	1.73	ray::ClientAppActor
619614	1.73	ray::ClientAppActor
619612	1.72	ray::ClientAppActor
619619	1.70	ray::ClientAppActor
619620	1.68	ray::ClientAppActor.run
Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.
[2024-08-13 23:58:29,018][flwr][ERROR] - Traceback (most recent call last):
  File "/home/abdenour/miniconda3/envs/fl_with_flower/lib/python3.11/site-packages/flwr/simulation/ray_transport/ray_client_proxy.py", line 94, in _submit_job
    out_mssg, updated_context = self.actor_pool.get_client_result(
                                ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/abdenour/miniconda3/envs/fl_with_flower/lib/python3.11/site-packages/flwr/simulation/ray_transport/ray_actor.py", line 398, in get_client_result
    return self._fetch_future_result(cid)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/abdenour/miniconda3/envs/fl_with_flower/lib/python3.11/site-packages/flwr/simulation/ray_transport/ray_actor.py", line 279, in _fetch_future_result
    res_cid, out_mssg, updated_context = ray.get(
                                         ^^^^^^^^
  File "/home/abdenour/miniconda3/envs/fl_with_flower/lib/python3.11/site-packages/ray/_private/auto_init_hook.py", line 21, in auto_init_wrapper
    return fn(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^
  File "/home/abdenour/miniconda3/envs/fl_with_flower/lib/python3.11/site-packages/ray/_private/client_mode_hook.py", line 103, in wrapper
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "/home/abdenour/miniconda3/envs/fl_with_flower/lib/python3.11/site-packages/ray/_private/worker.py", line 2667, in get
    values, debugger_breakpoint = worker.get_objects(object_refs, timeout=timeout)
                                  ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/abdenour/miniconda3/envs/fl_with_flower/lib/python3.11/site-packages/ray/_private/worker.py", line 866, in get_objects
    raise value
ray.exceptions.OutOfMemoryError: Task was killed due to the node running low on memory.
Memory on the node (IP: 115.145.171.128, ID: c687613ff34cdbf5097699e5c896055ef2dc44343c5ab44bd40309d5) where the task (actor ID: 85928d3f1a247800081c0e1101000000, name=ClientAppActor.__init__, pid=619614, memory used=1.73GB) was running was 29.50GB / 31.02GB (0.951049), which exceeds the memory usage threshold of 0.95. Ray killed this worker (ID: c7f644ac3cca20dffbfa1723daac4f87f04c7de03da25e7cad5fe33c) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 115.145.171.128`. To see the logs of the worker, use `ray logs worker-c7f644ac3cca20dffbfa1723daac4f87f04c7de03da25e7cad5fe33c*out -ip 115.145.171.128. Top 10 memory users:
PID	MEM(GB)	COMMAND
4942	3.49	/snap/pycharm-professional/408/jbr/bin/java -classpath /snap/pycharm-professional/408/lib/platform-l...
619616	1.80	ray::ClientAppActor.run
619615	1.78	ray::ClientAppActor
619617	1.77	ray::ClientAppActor
619611	1.73	ray::ClientAppActor
619618	1.73	ray::ClientAppActor
619614	1.73	ray::ClientAppActor
619612	1.72	ray::ClientAppActor
619619	1.70	ray::ClientAppActor
619620	1.68	ray::ClientAppActor.run
Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.

[2024-08-13 23:58:29,018][flwr][ERROR] - Task was killed due to the node running low on memory.
Memory on the node (IP: 115.145.171.128, ID: c687613ff34cdbf5097699e5c896055ef2dc44343c5ab44bd40309d5) where the task (actor ID: 85928d3f1a247800081c0e1101000000, name=ClientAppActor.__init__, pid=619614, memory used=1.73GB) was running was 29.50GB / 31.02GB (0.951049), which exceeds the memory usage threshold of 0.95. Ray killed this worker (ID: c7f644ac3cca20dffbfa1723daac4f87f04c7de03da25e7cad5fe33c) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 115.145.171.128`. To see the logs of the worker, use `ray logs worker-c7f644ac3cca20dffbfa1723daac4f87f04c7de03da25e7cad5fe33c*out -ip 115.145.171.128. Top 10 memory users:
PID	MEM(GB)	COMMAND
4942	3.49	/snap/pycharm-professional/408/jbr/bin/java -classpath /snap/pycharm-professional/408/lib/platform-l...
619616	1.80	ray::ClientAppActor.run
619615	1.78	ray::ClientAppActor
619617	1.77	ray::ClientAppActor
619611	1.73	ray::ClientAppActor
619618	1.73	ray::ClientAppActor
619614	1.73	ray::ClientAppActor
619612	1.72	ray::ClientAppActor
619619	1.70	ray::ClientAppActor
619620	1.68	ray::ClientAppActor.run
Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.
